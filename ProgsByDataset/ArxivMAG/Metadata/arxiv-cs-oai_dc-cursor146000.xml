<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2018-01-29T03:44:02Z</responseDate>
<request verb="ListRecords" resumptionToken="2369777|146001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08214</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Active Neural Localization</dc:title>
 <dc:creator>Chaplot, Devendra Singh</dc:creator>
 <dc:creator>Parisotto, Emilio</dc:creator>
 <dc:creator>Salakhutdinov, Ruslan</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:description>  Localization is the problem of estimating the location of an autonomous agent
from an observation and a map of the environment. Traditional methods of
localization, which filter the belief based on the observations, are
sub-optimal in the number of steps required, as they do not decide the actions
taken by the agent. We propose &quot;Active Neural Localizer&quot;, a fully
differentiable neural network that learns to localize accurately and
efficiently. The proposed model incorporates ideas of traditional
filtering-based localization methods, by using a structured belief of the state
with multiplicative interactions to propagate belief, and combines it with a
policy model to localize accurately while minimizing the number of steps
required for localization. Active Neural Localizer is trained end-to-end with
reinforcement learning. We use a variety of simulation environments for our
experiments which include random 2D mazes, random mazes in the Doom game engine
and a photo-realistic environment in the Unreal game engine. The results on the
2D environments show the effectiveness of the learned policy in an idealistic
setting while results on the 3D environments demonstrate the model's capability
of learning the policy and perceptual model jointly from raw-pixel based RGB
observations. We also show that a model trained on random textures in the Doom
environment generalizes well to a photo-realistic office space environment in
the Unreal engine.
</dc:description>
 <dc:description>Comment: Under Review at ICLR-18, 15 pages, 7 figures</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08214</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08225</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Knock-Knock: The unbearable lightness of Android Notifications</dc:title>
 <dc:creator>Patsakis, Constantinos</dc:creator>
 <dc:creator>Alepis, Efthimios</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  Android Notifications can be considered as essential parts in
Human-Smartphone interaction and inextricable modules of modern mobile
applications that can facilitate User Interaction and improve User Experience.
This paper presents how this well-crafted and thoroughly documented mechanism,
provided by the OS can be exploited by an adversary. More precisely, we present
attacks that result either in forging smartphone application notifications to
lure the user in disclosing sensitive information, or manipulate Android
Notifications to launch a Denial of Service attack to the users' device,
locally and remotely, rendering them unusable. This paper concludes by
proposing generic countermeasures for the discussed security threats.
</dc:description>
 <dc:description>Comment: Presented at International Conference on Information Systems Security
  and Privacy (ICISSP 2018)</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08225</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08228</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Visual-Inertial Odometry-enhanced Geometrically Stable ICP for Mapping
  Applications using Aerial Robots</dc:title>
 <dc:creator>Dang, Tung</dc:creator>
 <dc:creator>Khattak, Shehryar</dc:creator>
 <dc:creator>Papachristos, Christos</dc:creator>
 <dc:creator>Alexis, Kostas</dc:creator>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:description>  This paper presents a visual-inertial odometry-enhanced geometrically stable
Iterative Closest Point (ICP) algorithm for accurate mapping using aerial
robots. The proposed method employs a visual-inertial odometry framework in
order to provide robust priors to the ICP step and calculate the overlap among
point clouds derived from an onboard time-of-flight depth sensor. Within the
overlapping parts of the point clouds, the method samples points such that the
distribution of normals among them is as large as possible. As different
geometries and sensor trajectories will influence the performance of the
alignment process, evaluation of the expected geometric stability of the ICP
step is conducted. It is only when this test is successful that the matching,
outlier rejection, and minimization of the error metric ICP steps are conducted
and the new relative translation and rotational components are estimated,
otherwise the system relies on the visual-inertial odometry transformation
estimates. The proposed strategy was evaluated within handheld, automated and
fully autonomous exploration and mapping missions using a small aerial robot
and was shown to provide robust results of superior quality at an affordable
increase of the computational load.
</dc:description>
 <dc:description>Comment: 8 pages, 12 figures</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08228</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08230</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deep Interactive Evolution</dc:title>
 <dc:creator>Bontrager, Philip</dc:creator>
 <dc:creator>Lin, Wending</dc:creator>
 <dc:creator>Togelius, Julian</dc:creator>
 <dc:creator>Risi, Sebastian</dc:creator>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:description>  This paper describes an approach that combines generative adversarial
networks (GANs) with interactive evolutionary computation (IEC). While GANs can
be trained to produce lifelike images, they are normally sampled randomly from
the learned distribution, providing limited control over the resulting output.
On the other hand, interactive evolution has shown promise in creating various
artifacts such as images, music and 3D objects, but traditionally relies on a
hand-designed evolvable representation of the target domain. The main insight
in this paper is that a GAN trained on a specific target domain can act as a
compact and robust genotype-to-phenotype mapping (i.e. most produced phenotypes
do resemble valid domain artifacts). Once such a GAN is trained, the latent
vector given as input to the GAN's generator network can be put under
evolutionary control, allowing controllable and high-quality image generation.
In this paper, we demonstrate the advantage of this novel approach through a
user study in which participants were able to evolve images that strongly
resemble specific target images.
</dc:description>
 <dc:description>Comment: 16 pages, 5 figures, Published at EvoMUSART EvoStar 2018</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08230</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08234</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>When Vehicles See Pedestrians with Phones:A Multi-Cue Framework for
  Recognizing Phone-based Activities of Pedestrians</dc:title>
 <dc:creator>Rangesh, Akshay</dc:creator>
 <dc:creator>Trivedi, Mohan M.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  The intelligent vehicle community has devoted considerable efforts to model
driver behavior, and in particular to detect and overcome driver distraction in
an effort to reduce accidents caused by driver negligence. However, as the
domain increasingly shifts towards autonomous and semi-autonomous solutions,
the driver is no longer integral to the decision making process, indicating a
need to refocus efforts elsewhere. To this end, we propose to study pedestrian
distraction instead. In particular, we focus on detecting pedestrians who are
engaged in secondary activities involving their cellphones and similar handheld
multimedia devices from a purely vision-based standpoint. To achieve this
objective, we propose a pipeline incorporating articulated human pose
estimation, followed by a soft object label transfer from an ensemble of
exemplar SVMs trained on the nearest neighbors in pose feature space. We
additionally incorporate head gaze features and prior pose information to carry
out cellphone related pedestrian activity recognition. Finally, we offer a
method to reliably track the articulated pose of a pedestrian through a
sequence of images using a particle filter with a Gaussian Process Dynamical
Model (GPDM), which can then be used to estimate sequentially varying activity
scores at a very low computational cost. The entire framework is fast
(especially for sequential data) and accurate, and easily extensible to include
other secondary activities and sources of distraction.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08234</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08252</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Personalized Human Activity Recognition Using Convolutional Neural
  Networks</dc:title>
 <dc:creator>Rokni, Seyed Ali</dc:creator>
 <dc:creator>Nourollahi, Marjan</dc:creator>
 <dc:creator>Ghasemzadeh, Hassan</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  A major barrier to the personalized Human Activity Recognition using wearable
sensors is that the performance of the recognition model drops significantly
upon adoption of the system by new users or changes in physical/ behavioral
status of users. Therefore, the model needs to be retrained by collecting new
labeled data in the new context. In this study, we develop a transfer learning
framework using convolutional neural networks to build a personalized activity
recognition model with minimal user supervision.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08252</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08256</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Hilbert Space of Stationary Ergodic Processes</dc:title>
 <dc:creator>Chattopadhyay, Ishanu</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Quantitative Finance - Statistical Finance</dc:subject>
 <dc:subject>Statistics - Methodology</dc:subject>
 <dc:description>  Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about &quot;angles&quot; between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
</dc:description>
 <dc:description>Comment: 10 pages, 3 figures</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08256</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08266</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stochastic Successive Convex Approximation for Non-Convex Constrained
  Stochastic Optimization</dc:title>
 <dc:creator>Liu, An</dc:creator>
 <dc:creator>Lau, Vincent</dc:creator>
 <dc:creator>Kananian, Borna</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  This paper proposes a constrained stochastic successive convex approximation
(CSSCA) algorithm to find a stationary point for a general non-convex
stochastic optimization problem, whose objective and constraint functions are
non-convex and involve expectations over random states. The existing methods
for non-convex stochastic optimization, such as the stochastic (average)
gradient and stochastic majorization-minimization, only consider minimizing a
stochastic non-convex objective over a deterministic convex set. To the best of
our knowledge, this paper is the first attempt to handle stochastic non-convex
constraints in optimization problems, and it opens the way to solving more
challenging optimization problems that occur in many applications. The
algorithm is based on solving a sequence of convex objective/feasibility
optimization problems obtained by replacing the objective/constraint functions
in the original problems with some convex surrogate functions. The CSSCA
algorithm allows a wide class of surrogate functions and thus provides many
freedoms to design good surrogate functions for specific applications.
Moreover, it also facilitates parallel implementation for solving large scale
stochastic optimization problems, which arise naturally in today's signal
processing such as machine learning and big data analysis. We establish the
almost sure convergence of the CSSCA algorithm and customize the algorithmic
framework to solve several important application problems. Simulations show
that the CSSCA algorithm can achieve superior performance over existing
solutions.
</dc:description>
 <dc:description>Comment: submitted to IEEE Transactions on Signal Processing, 2017</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08266</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08267</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Visual Weather Temperature Prediction</dc:title>
 <dc:creator>Chu, Wei-Ta</dc:creator>
 <dc:creator>Ho, Kai-Chia</dc:creator>
 <dc:creator>Borji, Ali</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  In this paper, we attempt to employ convolutional recurrent neural networks
for weather temperature estimation using only image data. We study ambient
temperature estimation based on deep neural networks in two scenarios a)
estimating temperature of a single outdoor image, and b) predicting temperature
of the last image in an image sequence. In the first scenario, visual features
are extracted by a convolutional neural network trained on a large-scale image
dataset. We demonstrate that promising performance can be obtained, and analyze
how volume of training data influences performance. In the second scenario, we
consider the temporal evolution of visual appearance, and construct a recurrent
neural network to predict the temperature of the last image in a given image
sequence. We obtain better prediction accuracy compared to the state-of-the-art
models. Further, we investigate how performance varies when information is
extracted from different scene regions, and when images are captured in
different daytime hours. Our approach further reinforces the idea of using only
visual information for cost efficient weather prediction in the future.
</dc:description>
 <dc:description>Comment: 8 pages, accepted to WACV 2018</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08267</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08268</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Tutorial on Modeling and Inference in Undirected Graphical Models for
  Hyperspectral Image Analysis</dc:title>
 <dc:creator>Gewali, Utsav B.</dc:creator>
 <dc:creator>Monteiro, Sildomar T.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Electrical Engineering and Systems Science - Image and Video Processing</dc:subject>
 <dc:description>  Undirected graphical models have been successfully used to jointly model the
spatial and the spectral dependencies in earth observing hyperspectral images.
They produce less noisy, smooth, and spatially coherent land cover maps and
give top accuracies on many datasets. Moreover, they can easily be combined
with other state-of-the-art approaches, such as deep learning. This has made
them an essential tool for remote sensing researchers and practitioners.
However, graphical models have not been easily accessible to the larger remote
sensing community as they are not discussed in standard remote sensing
textbooks and not included in the popular remote sensing software and
toolboxes. In this tutorial, we provide a theoretical introduction to Markov
random fields and conditional random fields based spatial-spectral
classification for land cover mapping along with a detailed step-by-step
practical guide on applying these methods using freely available software.
Furthermore, the discussed methods are benchmarked on four public hyperspectral
datasets for a fair comparison among themselves and easy comparison with the
vast number of methods in literature which use the same datasets. The source
code necessary to reproduce all the results in the paper is published on-line
to make it easier for the readers to apply these techniques to different remote
sensing problems.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08268</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08270</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On the Design of Serially Concatenated LDGM Codes for BIAWGN Channels</dc:title>
 <dc:creator>Kharel, Amrit</dc:creator>
 <dc:creator>Cao, Lei</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  In this paper, we first prove a necessary condition for the successful
decoding of serially concatenated low-density generator-matrix (SCLDGM) codes
under sum-product (SP) decoding in binary input additive white Gaussian noise
(BIAWGN) channels. We show that the inner decoder of such codes must at least
produce a certain bit error rate (BER) called critical BER in order for the
outer decoder to achieve successful decoding. We verify this condition by
providing asymptotic curves for the SCLDGM codes using discretized density
evolution (DDE) method. We finally propose a DDE-based optimization approach
that utilizes the concept of the critical BER to construct optimized SCLDGM
codes that approach the Shannon limit. We also present asymptotic and
simulation results of our optimized codes that verify the effectiveness of the
proposed optimization approach.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08270</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08271</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Comprehensive Survey of Watermarking Relational Databases Research</dc:title>
 <dc:creator>Kamran, Muhammad</dc:creator>
 <dc:creator>Farooq, Muddassar</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  Watermarking and fingerprinting of relational databases are quite proficient
for ownership protection, tamper proofing, and proving data integrity. In past
few years several such techniques have been proposed. A survey of almost all
the work done, till date, in these fields has been presented in this paper. The
techniques have been classified on the basis of how and where they embed the
watermark. The analysis and comparison of these techniques on different merits
has also been provided. In the end, this paper points out the direction of
future research in these fields.
</dc:description>
 <dc:description>Comment: We intend to submit this paper for possible publication in a suitable
  journal in near future. Some part of this paper is included as a chapter in
  Ph.D. thesis of first author (Muhammad Kamran) under the supervision of
  second author (Muddassar Farooq). This thesis can be found at
  http://prr.hec.gov.pk/Thesis/2440S.pdf. We are uploading the manuscript at
  arxiv.org for public access</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08271</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08274</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stochastic Successive Convex Optimization for Two-timescale Hybrid
  Precoding in Massive MIMO</dc:title>
 <dc:creator>Liu, An</dc:creator>
 <dc:creator>Lau, Vincent</dc:creator>
 <dc:creator>Zhao, Min-Jian</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  Hybrid precoding, which consists of an RF precoder and a baseband precoder,
is a popular precoding architecture for massive MIMO due to its low hardware
cost and power consumption. In conventional hybrid precoding, both RF and
baseband precoders are adaptive to the real-time channel state information
(CSI). As a result, an individual RF precoder is required for each subcarrier
in wideband systems, leading to high implementation cost. To overcome this
issue, two-timescale hybrid precoding (THP), which adapts the RF precoder to
the channel statistics, has been proposed. Since the channel statistics are
approximately the same over different subcarriers, only a single RF precoder is
required in THP. Despite the advantages of THP, there lacks a unified and
efficient algorithm for its optimization due to the non-convex and stochastic
nature of the problem. Based on stochastic successive convex approximation
(SSCA), we propose an online algorithmic framework called SSCA-THP for general
THP optimization problems, in which the hybrid precoder is updated by solving a
quadratic surrogate optimization problem whenever a new channel sample is
obtained. Then we prove the convergence of SSCA-THP to stationary points.
Finally, we apply SSCA-THP to solve three important THP optimization problems
and verify its advantages over existing solutions.
</dc:description>
 <dc:description>Comment: submitted to IEEE Transactions on Signal Processing, 2017</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08274</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08276</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Timing Advance Estimation and Beamforming of Random Access Response in
  Crowded TDD Massive MIMO Systems</dc:title>
 <dc:creator>Mukherjee, Sudarshan</dc:creator>
 <dc:creator>Sinha, Alok Kumar</dc:creator>
 <dc:creator>Mohammed, Saif Khan</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  Timing advance (TA) estimation at the base station (BS) and reliable decoding
of random access response (RAR) at the users are the most important steps in
the initial random access (RA) procedure. However, due to the limited
availability of physical resources dedicated for RA, successful completion of
RA requests would become increasingly difficult in high user density scenarios,
due to contention among users requesting RA. In this paper, we propose to use
the large antenna array at the massive MIMO BS to jointly group RA requests
from different users using the same RA preamble. We then beamform the common
RAR of each detected user group onto the same frequency resource, in such a way
that most users in the group can reliably decode the RAR. The proposed RAR
beamforming therefore automatically resolves the problem of collision between
multiple RA requests on the same RA preamble, which reduces the RA latency
significantly as compared to LTE. Analysis and simulations also reveal that for
a fixed desired SINR of the received RAR, both the required per-user RA
preamble transmission power and the total RAR beamforming power can be
decreased roughly by 1.5 dB with every doubling in the number of BS antennas.
</dc:description>
 <dc:description>Comment: submitted to IEEE Transactions on Wireless Communications</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08276</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08284</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DKN: Deep Knowledge-Aware Network for News Recommendation</dc:title>
 <dc:creator>Wang, Hongwei</dc:creator>
 <dc:creator>Zhang, Fuzheng</dc:creator>
 <dc:creator>Xie, Xing</dc:creator>
 <dc:creator>Guo, Minyi</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Online news recommender systems aim to address the information explosion of
news and make personalized recommendation for users. In general, news language
is highly condensed, full of knowledge entities and common sense. However,
existing methods are unaware of such external knowledge and cannot fully
discover latent knowledge-level connections among news. The recommended results
for a user are consequently limited to simple patterns and cannot be extended
reasonably. Moreover, news recommendation also faces the challenges of high
time-sensitivity of news and dynamic diversity of users' interests. To solve
the above problems, in this paper, we propose a deep knowledge-aware network
(DKN) that incorporates knowledge graph representation into news
recommendation. DKN is a content-based deep recommendation framework for
click-through rate prediction. The key component of DKN is a multi-channel and
word-entity-aligned knowledge-aware convolutional neural network (KCNN) that
fuses semantic-level and knowledge-level representations of news. KCNN treats
words and entities as multiple channels, and explicitly keeps their alignment
relationship during convolution. In addition, to address users' diverse
interests, we also design an attention module in DKN to dynamically aggregate a
user's history with respect to current candidate news. Through extensive
experiments on a real online news platform, we demonstrate that DKN achieves
substantial gains over state-of-the-art deep recommendation models. We also
validate the efficacy of the usage of knowledge in DKN.
</dc:description>
 <dc:description>Comment: The 27th International Conference on World Wide Web (WWW'18)</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08284</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08287</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Directly Estimating the Variance of the {\lambda}-Return Using
  Temporal-Difference Methods</dc:title>
 <dc:creator>Sherstan, Craig</dc:creator>
 <dc:creator>Bennett, Brendan</dc:creator>
 <dc:creator>Young, Kenny</dc:creator>
 <dc:creator>Ashley, Dylan R.</dc:creator>
 <dc:creator>White, Adam</dc:creator>
 <dc:creator>White, Martha</dc:creator>
 <dc:creator>Sutton, Richard S.</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  This paper investigates estimating the variance of a temporal-difference
learning agent's update target. Most reinforcement learning methods use an
estimate of the value function, which captures how good it is for the agent to
be in a particular state and is mathematically expressed as the expected sum of
discounted future rewards (called the return). These values can be
straightforwardly estimated by averaging batches of returns using Monte Carlo
methods. However, if we wish to update the agent's value estimates during
learning--before terminal outcomes are observed--we must use a different
estimation target called the {\lambda}-return, which truncates the return with
the agent's own estimate of the value function. Temporal difference learning
methods estimate the expected {\lambda}-return for each state, allowing these
methods to update online and incrementally, and in most cases achieve better
generalization error and faster learning than Monte Carlo methods. Naturally
one could attempt to estimate higher-order moments of the {\lambda}-return.
This paper is about estimating the variance of the {\lambda}-return. Prior work
has shown that given estimates of the variance of the {\lambda}-return,
learning systems can be constructed to (1) mitigate risk in action selection,
and (2) automatically adapt the parameters of the learning process itself to
improve performance. Unfortunately, existing methods for estimating the
variance of the {\lambda}-return are complex and not well understood
empirically. We contribute a method for estimating the variance of the
{\lambda}-return directly using policy evaluation methods from reinforcement
learning. Our approach is significantly simpler than prior methods that
independently estimate the second moment of the {\lambda}-return. Empirically
our new approach behaves at least as well as existing approaches, but is
generally more robust.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08287</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08290</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Question-Focused Multi-Factor Attention Network for Question Answering</dc:title>
 <dc:creator>Kundu, Souvik</dc:creator>
 <dc:creator>Ng, Hwee Tou</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Neural network models recently proposed for question answering (QA) primarily
focus on capturing the passage-question relation. However, they have minimal
capability to link relevant facts distributed across multiple sentences which
is crucial in achieving deeper understanding, such as performing multi-sentence
reasoning, co-reference resolution, etc. They also do not explicitly focus on
the question and answer type which often plays a critical role in QA. In this
paper, we propose a novel end-to-end question-focused multi-factor attention
network for answer extraction. Multi-factor attentive encoding using
tensor-based transformation aggregates meaningful facts even when they are
located in multiple sentences. To implicitly infer the answer type, we also
propose a max-attentional question aggregation mechanism to encode a question
vector based on the important words in a question. During prediction, we
incorporate sequence-level encoding of the first wh-word and its immediately
following word as an additional source of question type information. Our
proposed model achieves significant improvements over the best prior
state-of-the-art results on three large-scale challenging QA datasets, namely
NewsQA, TriviaQA, and SearchQA.
</dc:description>
 <dc:description>Comment: 8 pages, AAAI 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08290</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08291</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Toward Cross-Layer Design for Non-Orthogonal Multiple Access: A
  Quality-of-Experience Perspective</dc:title>
 <dc:creator>Wang, Wei</dc:creator>
 <dc:creator>Liu, Yuanwei</dc:creator>
 <dc:creator>Luo, Zhiqing</dc:creator>
 <dc:creator>Jiang, Tao</dc:creator>
 <dc:creator>Zhang, Qian</dc:creator>
 <dc:creator>Nallanathan, Arumugam</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:description>  Recent years have seen proliferation in versatile mobile devices and an
upsurge in the growth of data-consuming application services. Orthogonal
multiple access (OMA) technologies in today's mobile systems fall inefficient
in the presence of such massive connectivity and traffic demands. In this
regards, non-orthogonal multiple access (NOMA) has been advocated by the
research community to embrace unprecedented requirements. Current NOMA designs
have been demonstrated to largely improve conventional system performance in
terms of throughput and latency, while their impact on the end users' perceived
experience has not yet been comprehensively understood. We envision that
quality-of-experience (QoE) awareness is a key pillar for NOMA designs to
fulfill versatile user demands in the 5th generation (5G) wireless
communication systems. This article systematically investigates QoE-aware NOMA
designs that translate the physical-layer benefits of NOMA to the improvement
of users' perceived experience in upper layers. We shed light on design
principles and key challenges in realizing QoE-aware NOMA designs. With these
principles and challenges in mind, we develop a general architecture with a
dynamic network scheduling scheme. We provide some implications for future
QoE-aware NOMA designs by conducting a case study in video streaming
applications.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08291</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08293</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Finding induced subgraphs in scale-free inhomogeneous random graphs</dc:title>
 <dc:creator>Cardinaels, Ellen</dc:creator>
 <dc:creator>van Leeuwaarden, Johan S. H.</dc:creator>
 <dc:creator>Stegehuis, Clara</dc:creator>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:description>  We study the induced subgraph isomorphism problem on inhomogeneous random
graphs with infinite variance power-law degrees. We provide a fast algorithm
that determines for any connected graph $H$ on $k$ vertices if it exists as
induced subgraph in a random graph with $n$ vertices. By exploiting the
scale-free graph structure, the algorithm runs in $O(n e^{k^4})$ time, and
finds for constant $k$ an instance of $H$ in linear time with high probability.
</dc:description>
 <dc:description>Comment: 14 pages</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08293</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08294</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Optimum Fairness for Non-Orthogonal Multiple Access</dc:title>
 <dc:creator>Qi, Ting</dc:creator>
 <dc:creator>Feng, Wei</dc:creator>
 <dc:creator>Chen, Yunfei</dc:creator>
 <dc:creator>Wang, Youzheng</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  This paper focuses on the fairness issue in non-orthogonal multiple access
(NOMA) and investigates the optimization problem that maximizes the worst
user's achievable rate. Unlike previous studies, we derive a closed-form
expression for the optimal value and solution, which are related to
Perron-Frobenius eigenvalue and eigenvector of a defined positive matrix. On
this basis, we propose an iterative algorithm to compute the optimal solution,
which has linear convergence and requires only about half iterations of the
classical bisection method.
</dc:description>
 <dc:description>Comment: 5 pages, 6 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08294</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08295</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discovering Markov Blanket from Multiple interventional Datasets</dc:title>
 <dc:creator>Yu, Kui</dc:creator>
 <dc:creator>Liu, Lin</dc:creator>
 <dc:creator>Li, Jiuyong</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  In this paper, we study the problem of discovering the Markov blanket (MB) of
a target variable from multiple interventional datasets. Datasets attained from
interventional experiments contain richer causal information than passively
observed data (observational data) for MB discovery. However, almost all
existing MB discovery methods are designed for finding MBs from a single
observational dataset. To identify MBs from multiple interventional datasets,
we face two challenges: (1) unknown intervention variables; (2) nonidentical
data distributions. To tackle the challenges, we theoretically analyze (a)
under what conditions we can find the correct MB of a target variable, and (b)
under what conditions we can identify the causes of the target variable via
discovering its MB. Based on the theoretical analysis, we propose a new
algorithm for discovering MBs from multiple interventional datasets, and
present the conditions/assumptions which assure the correctness of the
algorithm. To our knowledge, this work is the first to present the theoretical
analyses about the conditions for MB discovery in multiple interventional
datasets and the algorithm to find the MBs in relation to the conditions. Using
benchmark Bayesian networks and real-world datasets, the experiments have
validated the effectiveness and efficiency of the proposed algorithm in the
paper.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08295</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08297</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>NDDR-CNN: Layer-wise Feature Fusing in Multi-Task CNN by Neural
  Discriminative Dimensionality Reduction</dc:title>
 <dc:creator>Gao, Yuan</dc:creator>
 <dc:creator>She, Qi</dc:creator>
 <dc:creator>Ma, Jiayi</dc:creator>
 <dc:creator>Zhao, Mingbo</dc:creator>
 <dc:creator>Liu, Wei</dc:creator>
 <dc:creator>Yuille, Alan L.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  State-of-the-art Convolutional Neural Network (CNN) benefits a lot from
multi-task learning (MTL), which learns multiple related tasks simultaneously
to obtain shared or mutually related representations for different tasks. The
most widely-used MTL CNN structure is based on an empirical or heuristic split
on a specific layer (e.g., the last convolutional layer) to minimize different
task-specific losses. However, this heuristic sharing/splitting strategy may be
harmful to the final performance of one or multiple tasks. In this paper, we
propose a novel CNN structure for MTL, which enables automatic feature fusing
at every layer. Specifically, we first concatenate features from different
tasks according to their channel dimension, and then formulate the feature
fusing problem as discriminative dimensionality reduction. We show that this
discriminative dimensionality reduction can be done by 1x1 Convolution, Batch
Normalization, and Weight Decay in one CNN, which we refer to as Neural
Discriminative Dimensionality Reduction (NDDR). We perform ablation analysis in
details for different configurations in training the network. The experiments
carried out on different network structures and different task sets demonstrate
the promising performance and desirable generalizability of our proposed
method.
</dc:description>
 <dc:description>Comment: 11 pages, 5 figures, 7 tables</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08297</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08301</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Class label autoencoder for zero-shot learning</dc:title>
 <dc:creator>Lin, Guangfeng</dc:creator>
 <dc:creator>Fan, Caixia</dc:creator>
 <dc:creator>Chen, Wanjun</dc:creator>
 <dc:creator>Chen, Yajun</dc:creator>
 <dc:creator>Zhao, Fan</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Existing zero-shot learning (ZSL) methods usually learn a projection function
between a feature space and a semantic embedding space(text or attribute space)
in the training seen classes or testing unseen classes. However, the projection
function cannot be used between the feature space and multi-semantic embedding
spaces, which have the diversity characteristic for describing the different
semantic information of the same class. To deal with this issue, we present a
novel method to ZSL based on learning class label autoencoder (CLA). CLA can
not only build a uniform framework for adapting to multi-semantic embedding
spaces, but also construct the encoder-decoder mechanism for constraining the
bidirectional projection between the feature space and the class label space.
Moreover, CLA can jointly consider the relationship of feature classes and the
relevance of the semantic classes for improving zero-shot classification. The
CLA solution can provide both unseen class labels and the relation of the
different classes representation(feature or semantic information) that can
encode the intrinsic structure of classes. Extensive experiments demonstrate
the CLA outperforms state-of-art methods on four benchmark datasets, which are
AwA, CUB, Dogs and ImNet-2.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08301</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08322</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Abnormal Heartbeat Detection Using Recurrent Neural Networks</dc:title>
 <dc:creator>Latif, Siddique</dc:creator>
 <dc:creator>Usman, Muhammad</dc:creator>
 <dc:creator>Rana, Junaid Qadir Rajib</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  The observation and management of cardiac features (using automated cardiac
auscultation) is of significant interest to the healthcare community. In this
work, we propose for the first time the use of recurrent neural networks (RNNs)
for automated cardiac auscultation and detection of abnormal heartbeat
detection. The application of RNNs for this task is compelling since RNNs
represent the deep learning technique most adept at dealing with sequential or
temporal data. We explore the use of various RNNs models and show through our
experimental results that RNN delivers the best-recorded score with only 2.37\%
error on the test set for automated cardiac auscultation task.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08322</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08323</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Forward-Secure Group Signatures from Lattices</dc:title>
 <dc:creator>Ling, San</dc:creator>
 <dc:creator>Nguyen, Khoa</dc:creator>
 <dc:creator>Wang, Huaxiong</dc:creator>
 <dc:creator>Xu, Yanhong</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  Group signature is a fundamental cryptographic primitive, aiming to protect
anonymity and ensure accountability of users. It allows group members to
anonymously sign messages on behalf of the whole group, while incorporating a
tracing mechanism to identify the signer of any suspected signature. Most of
the existing group signature schemes, however, do not guarantee security once
users' secret keys are exposed. To reduce potential damages caused by key
exposure attacks, Song (CCS 2001) put forward the concept of forward-secure
group signatures (FSGS). For the time being, all known secure FSGS schemes are
based on number-theoretic assumptions, and are vulnerable against quantum
computers.
  In this work, we construct the first lattice-based FSGS scheme. In Nakanishi
et al.'s model, our scheme achieves forward-secure traceability under the Short
Integer Solution (SIS) assumption, and offers full anonymity under the Learning
With Errors (LWE) assumption. At the heart of our construction is a scalable
lattice-based key-evolving mechanism, allowing users to periodically update
their secret keys and to efficiently prove in zero-knowledge that the
key-evolution process is done correctly. To realize this essential building
block, we first employ the Bonsai-tree structure by Cash et al. (EUROCRYPT
2010) to handle the key evolution process, and then develop Langlois et al.'s
construction (PKC 2014) to design its supporting zero-knowledge protocol. In
comparison to all known lattice-based group signatures (that are \emph{not}
forward-secure), our scheme only incurs a very reasonable overhead: the
bit-sizes of keys and signatures are at most O(log N), where N is the number of
group users; and at most O(log^3 T), where T is the number of time periods.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08323</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08329</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Deep Autoencoders for Facial Expression Recognition</dc:title>
 <dc:creator>Usman, Muhammad</dc:creator>
 <dc:creator>Latif, Siddique</dc:creator>
 <dc:creator>Qadir, Junaid</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Feature descriptors involved in image processing are generally manually
chosen and high dimensional in nature. Selecting the most important features is
a very crucial task for systems like facial expression recognition. This paper
investigates the performance of deep autoencoders for feature selection and
dimension reduction for facial expression recognition on multiple levels of
hidden layers. The features extracted from the stacked autoencoder outperformed
when compared to other state-of-the-art feature selection and dimension
reduction techniques.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08329</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08336</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Big Data Visualization Tools</dc:title>
 <dc:creator>Bikakis, Nikos</dc:creator>
 <dc:subject>Computer Science - Databases</dc:subject>
 <dc:subject>Computer Science - Graphics</dc:subject>
 <dc:subject>Computer Science - Human-Computer Interaction</dc:subject>
 <dc:subject>97R50, 68P05, 68P15</dc:subject>
 <dc:subject>E.1</dc:subject>
 <dc:subject>H.2.8</dc:subject>
 <dc:subject>H.5.2</dc:subject>
 <dc:subject>H.4</dc:subject>
 <dc:description>  Data visualization is the presentation of data in a pictorial or graphical
format, and a data visualization tool is the software that generates this
presentation. Data visualization provides users with intuitive means to
interactively explore and analyze data, enabling them to effectively identify
interesting patterns, infer correlations and causalities, and supports
sense-making activities.
</dc:description>
 <dc:description>Comment: This article appears in Encyclopedia of Big Data Technologies,
  Springer, 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08336</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08337</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Continuous Space Reordering Models for Phrase-based MT</dc:title>
 <dc:creator>Durrani, Nadir</dc:creator>
 <dc:creator>Dalvi, Fahim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Bilingual sequence models improve phrase-based translation and reordering by
overcoming phrasal independence assumption and handling long range reordering.
However, due to data sparsity, these models often fall back to very small
context sizes. This problem has been previously addressed by learning sequences
over generalized representations such as POS tags or word clusters. In this
paper, we explore an alternative based on neural network models. More
concretely we train neuralized versions of lexicalized reordering and the
operation sequence models using feed-forward neural network. Our results show
improvements of up to 0.6 and 0.5 BLEU points on top of the baseline
German-&gt;English and English-&gt;German systems. We also observed improvements
compared to the systems that used POS tags and word clusters to train these
models. Because we modify the bilingual corpus to integrate reordering
operations, this allows us to also train a sequence-to-sequence neural MT model
having explicit reordering triggers. Our motivation was to directly enable
reordering information in the encoder-decoder framework, which otherwise relies
solely on the attention model to handle long range reordering. We tried both
coarser and fine-grained reordering operations. However, these experiments did
not yield any improvements over the baseline Neural MT systems.
</dc:description>
 <dc:description>Comment: IWSLT 2017</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08337</dc:identifier>
 <dc:identifier>The 14th International Workshop on Spoken Language Translation
  (IWSLT 2017)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08341</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Price of Indivisibility in Cake Cutting</dc:title>
 <dc:creator>Arunachaleswaran, Eshwar Ram</dc:creator>
 <dc:creator>Gopalakrishnan, Ragavendran</dc:creator>
 <dc:subject>Computer Science - Computer Science and Game Theory</dc:subject>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:description>  We consider the problem of envy-free cake cutting, which is the distribution
of a continuous heterogeneous resource among self interested players such that
nobody prefers what somebody else receives to what they get. Existing work has
focused on two distinct classes of solutions to this problem - allocations
which give each player a continuous piece of cake and allocations which give
each player arbitrarily many disjoint pieces of cake. Our aim is to investigate
allocations between these two extremes by parameterizing the maximum number of
disjoint pieces each player may receive. We characterize the Price of
Indivisibility (POI) as the gain achieved in social welfare (utilitarian and
egalitarian), by moving from allocations which give each player a continuous
piece of cake to allocations that may give each player up to $k$ disjoint
pieces of cake. Our results contain bounds for the Price of Indivisibility for
utilitarian as well as egalitarian social welfare, and for envy-free cake
cutting as well as cake cutting without any fairness constraints.
</dc:description>
 <dc:description>Comment: 19 pages, 5 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08341</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08350</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Theory of higher order interpretations and application to Basic Feasible
  Functions</dc:title>
 <dc:creator>Hainry, Emmanuel</dc:creator>
 <dc:creator>P&#xe9;choux, Romain</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:subject>Computer Science - Programming Languages</dc:subject>
 <dc:description>  Interpretation methods and their restrictions to polynomials have been deeply
used to control the termination and complexity of first-order term rewrite
systems. This paper extends interpretation methods to a pure higher order
functional language. We develop a theory of higher order functions that is
well-suited for the complexity analysis of this programming language. The
interpretation domain is a complete lattice and, consequently, we express
program interpretation in terms of a least fixpoint. As an application, by
bounding interpretations by higher order polynomials, we characterize Basic
Feasible Functions at any order.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08350</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08351</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Statistical Approach for RF Exposure Compliance Boundary Assessment in
  Massive MIMO Systems</dc:title>
 <dc:creator>Baracca, Paolo</dc:creator>
 <dc:creator>Weber, Andreas</dc:creator>
 <dc:creator>Wild, Thorsten</dc:creator>
 <dc:creator>Grangeat, Christophe</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  Massive multiple-input multiple-output (MIMO) is a fundamental enabler to
provide high data throughput in next generation cellular networks. By equipping
the base stations (BSs) with tens or hundreds of antenna elements, narrow and
high gain beams can be used to spatially multiplex several user equipment (UE)
devices. While increasing the achievable performance, focusing the transmit
power into specific UE directions also poses new issues when performing the
radio frequency (RF) exposure assessment. In fact, the spatial distribution of
the actual BS transmit power strongly depends on the deployment scenario and on
the position of the UEs. Traditional methods for assessing the RF exposure
compliance boundaries around BS sites are generally based on maximum transmit
power and static beams. In massive MIMO systems, these approaches tend to be
very conservative, in particular when time averaging is properly considered. In
this work, we propose to leverage the three dimensional spatial channel model
standardized by the Third Generation Partnership Project in order to assess
reasonably foreseeable compliance boundaries of massive MIMO BSs. The analysis
is performed by considering BSs fully loaded and different configurations of
active UEs per cell. Numerical results show that the statistical approach
developed in this paper allows reducing to nearly half the compliance distance
when compared to the traditional method.
</dc:description>
 <dc:description>Comment: Accepted at the International Workshop on Smart Antennas (WSA),
  Bochum (Germany), Mar. 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08351</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08353</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Secure and Privacy-preserving Protocol for Smart Metering Operational
  Data Collection</dc:title>
 <dc:creator>Mustafa, Mustafa A.</dc:creator>
 <dc:creator>Cleemput, Sara</dc:creator>
 <dc:creator>Aly, Abelrahaman</dc:creator>
 <dc:creator>Abidin, Aysajan</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  In this paper we propose a novel protocol that allows suppliers and grid
operators to collect users' aggregate metering data in a secure and
privacy-preserving manner. We use secure multiparty computation to ensure
privacy protection. In addition, we propose three different data aggregation
algorithms that offer different balances between privacy-protection and
performance. Our protocol is designed for a realistic scenario in which the
data need to be sent to different parties, such as grid operators and
suppliers. Furthermore, it facilitates an accurate calculation of transmission,
distribution and grid balancing fees in a privacy-preserving manner. We also
present a security analysis and a performance evaluation of our protocol based
on well known multiparty computation algorithms implemented in C++.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08353</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08354</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Secure and Privacy-Friendly Local Electricity Trading and Billing in
  Smart Grid</dc:title>
 <dc:creator>Abidin, Aysajan</dc:creator>
 <dc:creator>Aly, Abdelrahaman</dc:creator>
 <dc:creator>Cleemput, Sara</dc:creator>
 <dc:creator>Mustafa, Mustafa A.</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  This paper proposes two decentralised, secure and privacy-friendly protocols
for local electricity trading and billing, respectively. The trading protocol
employs a bidding algorithm based upon secure multiparty computations and
allows users to trade their excess electricity among themselves. The bid
selection and calculation of the trading price are performed in a decentralised
and oblivious manner. The billing protocol is based on a simple
privacy-friendly aggregation technique that allows suppliers to compute their
customers' monthly bills without learning their fine-grained electricity
consumption data. We also implemented and tested the performance of the trading
protocol with realistic data. Our results show that it can be performed for
2500 bids in less than five minutes in the on-line phase, showing its
feasibility for a typical electricity trading period of 30 minutes.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08354</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08360</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dual Asymmetric Deep Hashing Learning</dc:title>
 <dc:creator>Li, Jinxing</dc:creator>
 <dc:creator>Zhang, Bob</dc:creator>
 <dc:creator>Lu, Guangming</dc:creator>
 <dc:creator>Zhang, David</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Due to the impressive learning power, deep learning has achieved a remarkable
performance in supervised hash function learning. In this paper, we propose a
novel asymmetric supervised deep hashing method to preserve the semantic
structure among different categories and generate the binary codes
simultaneously. Specifically, two asymmetric deep networks are constructed to
reveal the similarity between each pair of images according to their semantic
labels. The deep hash functions are then learned through two networks by
minimizing the gap between the learned features and discrete codes.
Furthermore, since the binary codes in the Hamming space also should keep the
semantic affinity existing in the original space, another asymmetric pairwise
loss is introduced to capture the similarity between the binary codes and
real-value features. This asymmetric loss not only improves the retrieval
performance, but also contributes to a quick convergence at the training phase.
By taking advantage of the two-stream deep structures and two types of
asymmetric pairwise functions, an alternating algorithm is designed to optimize
the deep features and high-quality binary codes efficiently. Experimental
results on three real-world datasets substantiate the effectiveness and
superiority of our approach as compared with state-of-the-art.
</dc:description>
 <dc:description>Comment: 12 pages, 6 figures, 7 tables, 37 conferences</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08360</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08361</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Collaborative Large-Scale Dense 3D Reconstruction with Online
  Inter-Agent Pose Optimisation</dc:title>
 <dc:creator>Golodetz, Stuart</dc:creator>
 <dc:creator>Cavallari, Tommaso</dc:creator>
 <dc:creator>Lord, Nicholas A</dc:creator>
 <dc:creator>Prisacariu, Victor A</dc:creator>
 <dc:creator>Murray, David W</dc:creator>
 <dc:creator>Torr, Philip H S</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Reconstructing dense, volumetric models of real-world 3D scenes is important
for many tasks, but capturing large scenes can take significant time, and the
risk of transient changes to the scene goes up as the capture time increases.
These are good reasons to want instead to capture several smaller sub-scenes
that can be joined to make the whole scene. Achieving this has traditionally
been difficult: joining sub-scenes that may never have been viewed from the
same angle requires a high-quality relocaliser that can cope with novel poses,
and tracking drift in each sub-scene can prevent them from being joined to make
a consistent overall scene. Recent advances in mobile hardware, however, have
significantly improved our ability to capture medium-sized sub-scenes with
little to no tracking drift. Moreover, high-quality regression forest-based
relocalisers have recently been made more practical by the introduction of a
method to allow them to be trained and used online. In this paper, we leverage
these advances to present what to our knowledge is the first system to allow
multiple users to collaborate interactively to reconstruct dense, voxel-based
models of whole buildings. Using our system, an entire house or lab can be
captured and reconstructed in under half an hour using only consumer-grade
hardware.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08361</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08365</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Planning by Probabilistic Programming</dc:title>
 <dc:creator>Belle, Vaishak</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  Automated planning is a major topic of research in artificial intelligence,
and enjoys a long and distinguished history. The classical paradigm assumes a
distinguished initial state, comprised of a set of facts, and is defined over a
set of actions which change that state in one way or another. Planning in many
real-world settings, however, is much more involved: an agent's knowledge is
almost never simply a set of facts that are true, and actions that the agent
intends to execute never operate the way they are supposed to. Thus,
probabilistic planning attempts to incorporate stochastic models directly into
the planning process. In this article, we briefly report on probabilistic
planning through the lens of probabilistic programming: a programming paradigm
that aims to ease the specification of structured probability distributions. In
particular, we provide an overview of the features of two systems, HYPE and
ALLEGRO, which emphasise different strengths of probabilistic programming that
are particularly useful for complex modelling issues raised in probabilistic
planning. Among other things, with these systems, one can instantiate planning
problems with growing and shrinking state spaces, discrete and continuous
probability distributions, and non-unique prior distributions in a first-order
setting.
</dc:description>
 <dc:description>Comment: Article at AAAI-18 Workshop on Planning and Inference</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08365</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08376</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Persistent Betti numbers of random \v{C}ech complexes</dc:title>
 <dc:creator>Bauer, Ulrich</dc:creator>
 <dc:creator>Pausinger, Florian</dc:creator>
 <dc:subject>Mathematics - Algebraic Topology</dc:subject>
 <dc:subject>Computer Science - Computational Geometry</dc:subject>
 <dc:subject>Mathematics - Metric Geometry</dc:subject>
 <dc:description>  We study the persistent homology of random \v{C}ech complexes. Generalizing a
method of Penrose for studying random geometric graphs, we first describe an
appropriate theoretical framework in which we can state and address our main
questions. Then we define the kth persistent Betti number of a random \v{C}ech
complex and determine its asymptotic order in the subcritical regime. This
extends a result of Kahle on the asymptotic order of the ordinary kth Betti
number of such complexes to the persistent setting.
</dc:description>
 <dc:description>Comment: 11 pages, 1 figure</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08376</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08379</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DeepWriting: Making Digital Ink Editable via Deep Generative Modeling</dc:title>
 <dc:creator>Aksan, Emre</dc:creator>
 <dc:creator>Pece, Fabrizio</dc:creator>
 <dc:creator>Hilliges, Otmar</dc:creator>
 <dc:subject>Computer Science - Human-Computer Interaction</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>H.5.2</dc:subject>
 <dc:subject>I.2.6</dc:subject>
 <dc:description>  Digital ink promises to combine the flexibility and aesthetics of handwriting
and the ability to process, search and edit digital text. Character recognition
converts handwritten text into a digital representation, albeit at the cost of
losing personalized appearance due to the technical difficulties of separating
the interwoven components of content and style. In this paper, we propose a
novel generative neural network architecture that is capable of disentangling
style from content and thus making digital ink editable. Our model can
synthesize arbitrary text, while giving users control over the visual
appearance (style). For example, allowing for style transfer without changing
the content, editing of digital ink at the word level and other application
scenarios such as spell-checking and correction of handwritten text. We
furthermore contribute a new dataset of handwritten text with fine-grained
annotations at the character level and report results from an initial user
evaluation.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08379</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08380</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Hardness of Approximation for Morse Matching</dc:title>
 <dc:creator>Bauer, Ulrich</dc:creator>
 <dc:creator>Rathod, Abhishek</dc:creator>
 <dc:subject>Mathematics - Algebraic Topology</dc:subject>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:subject>Computer Science - Computational Geometry</dc:subject>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:description>  We consider the approximability of maximization and minimization variants of
the Morse matching problem, posed as open problems by Joswig and Pfetsch. We
establish hardness results for Max-Morse matching and Min-Morse matching. In
particular, we show that, for a simplicial complex with n simplices and
dimension $d \leq 3$, it is NP-hard to approximate Min-Morse matching within a
factor of $O(n^{1-\epsilon})$, for any $\epsilon &gt; 0$. Moreover, using an
L-reduction from Degree 3 Max-Acyclic Subgraph to Max-Morse matching, we show
that it is both NP-hard and UGC-hard to approximate Max-Morse matching for
simplicial complexes of dimension $d \leq 2$ within certain explicit constant
factors.
</dc:description>
 <dc:description>Comment: 14 pages, 1 figure</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08380</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08383</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Data-Driven Impulse Response Regularization via Deep Learning</dc:title>
 <dc:creator>Andersson, Carl</dc:creator>
 <dc:creator>Wahlstr&#xf6;m, Niklas</dc:creator>
 <dc:creator>Sch&#xf6;n, Thomas B.</dc:creator>
 <dc:subject>Computer Science - Systems and Control</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  We consider the problem of impulse response estimation for stable linear
single-input single-output systems. It is a well-studied problem where flexible
non-parametric models recently offered a leap in performance compared to the
classical finite-dimensional model structures. Inspired by this development and
the success of deep learning we propose a new flexible data-driven model. Our
experiments indicate that the new model is capable of exploiting even more of
the hidden patterns that are present in the input-output data as compared to
the non-parametric models.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08383</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08388</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Benchmark and Evaluation of Non-Rigid Structure from Motion</dc:title>
 <dc:creator>Jensen, Sebastian Hoppe Nesgaard</dc:creator>
 <dc:creator>Del Bue, Alessio</dc:creator>
 <dc:creator>Doest, Mads Emil Brix</dc:creator>
 <dc:creator>Aan&#xe6;s, Henrik</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Non-Rigid structure from motion (NRSfM), is a long standing and central
problem in computer vision, allowing us to obtain 3D information from multiple
images when the scene is dynamic. A main issue regarding the further
development of this important computer vision topic, is the lack of high
quality data sets. We here address this issue by presenting of data set
compiled for this purpose, which is made publicly available, and considerably
larger than previous state of the art. To validate the applicability of this
data set, and provide and investigation into the state of the art of NRSfM,
including potential directions forward, we here present a benchmark and a
scrupulous evaluation using this data set. This benchmark evaluates 16
different methods with available code, which we argue reasonably spans the
state of the art in NRSfM. We also hope, that the presented and public data set
and evaluation, will provide benchmark tools for further development in this
field.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08388</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08390</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Global and Local Consistent Age Generative Adversarial Networks</dc:title>
 <dc:creator>Li, Peipei</dc:creator>
 <dc:creator>Hu, Yibo</dc:creator>
 <dc:creator>Li, Qi</dc:creator>
 <dc:creator>He, Ran</dc:creator>
 <dc:creator>Sun, Zhenan</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Age progression/regression is a challenging task due to the complicated and
non-linear transformation in human aging process. Many researches have shown
that both global and local facial features are essential for face
representation, but previous GAN based methods mainly focused on the global
feature in age synthesis. To utilize both global and local facial information,
we propose a Global and Local Consistent Age Generative Adversarial Network
(GLCA-GAN). In our generator, a global network learns the whole facial
structure and simulates the aging trend of the whole face, while three crucial
facial patches are progressed or regressed by three local networks aiming at
imitating subtle changes of crucial facial subregions. To preserve most of the
details in age-attribute-irrelevant areas, our generator learns the residual
face. Moreover, we employ an identity preserving loss to better preserve the
identity information, as well as age preserving loss to enhance the accuracy of
age synthesis. A pixel loss is also adopted to preserve detailed facial
information of the input face. Our proposed method is evaluated on three face
aging datasets, i.e., CACD dataset, Morph dataset and FG-NET dataset.
Experimental results show appealing performance of the proposed method by
comparing with the state-of-the-art.
</dc:description>
 <dc:description>Comment: 6 pages, 8 figures, submitted to ICPR 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08390</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08391</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Understanding Human Behaviors in Crowds by Imitating the Decision-Making
  Process</dc:title>
 <dc:creator>Zou, Haosheng</dc:creator>
 <dc:creator>Su, Hang</dc:creator>
 <dc:creator>Song, Shihong</dc:creator>
 <dc:creator>Zhu, Jun</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Crowd behavior understanding is crucial yet challenging across a wide range
of applications, since crowd behavior is inherently determined by a sequential
decision-making process based on various factors, such as the pedestrians' own
destinations, interaction with nearby pedestrians and anticipation of upcoming
events. In this paper, we propose a novel framework of Social-Aware Generative
Adversarial Imitation Learning (SA-GAIL) to mimic the underlying
decision-making process of pedestrians in crowds. Specifically, we infer the
latent factors of human decision-making process in an unsupervised manner by
extending the Generative Adversarial Imitation Learning framework to anticipate
future paths of pedestrians. Different factors of human decision making are
disentangled with mutual information maximization, with the process modeled by
collision avoidance regularization and Social-Aware LSTMs. Experimental results
demonstrate the potential of our framework in disentangling the latent
decision-making factors of pedestrians and stronger abilities in predicting
future trajectories.
</dc:description>
 <dc:description>Comment: accepted to AAAI-18</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08391</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08405</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A smaller cover for closed unit curves</dc:title>
 <dc:creator>Wichiramala, Wacharin</dc:creator>
 <dc:subject>Computer Science - Computational Geometry</dc:subject>
 <dc:subject>52A38</dc:subject>
 <dc:description>  Forty years ago Schaer and Wetzel showed that a $\frac{1}{\pi}\times\frac
{1}{2\pi}\sqrt{\pi^{2}-4}$ rectangle, whose area is about $0.122\,74,$ is the
smallest rectangle that is a cover for the family of all closed unit arcs. More
recently F\&quot;{u}redi and Wetzel showed that one corner of this rectangle can be
clipped to form a pentagonal cover having area $0.11224$ for this family of
curves. Here we show that then the opposite corner can be clipped to form a
hexagonal cover of area less than $0.11023$ for this same family. This
irregular hexagon is the smallest cover currently known for this family of
arcs.
</dc:description>
 <dc:description>Comment: In the appendix, the computer code for numerical optimization is
  provided together with explanation. The link to the actual file, a
  Mathematica notebook, is at
  www.math.sc.chula.ac.th/~wacharin/optimization/closed%20arcs</dc:description>
 <dc:date>2018-01-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08405</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08406</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>C2MSNet: A Novel approach for single image haze removal</dc:title>
 <dc:creator>Dudhane, Akshay</dc:creator>
 <dc:creator>Murala, Subrahmanyam</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Degradation of image quality due to the presence of haze is a very common
phenomenon. Existing DehazeNet [3], MSCNN [11] tackled the drawbacks of hand
crafted haze relevant features. However, these methods have the problem of
color distortion in gloomy (poor illumination) environment. In this paper, a
cardinal (red, green and blue) color fusion network for single image haze
removal is proposed. In first stage, network fusses color information present
in hazy images and generates multi-channel depth maps. The second stage
estimates the scene transmission map from generated dark channels using multi
channel multi scale convolutional neural network (McMs-CNN) to recover the
original scene. To train the proposed network, we have used two standard
datasets namely: ImageNet [5] and D-HAZY [1]. Performance evaluation of the
proposed approach has been carried out using structural similarity index
(SSIM), mean square error (MSE) and peak signal to noise ratio (PSNR).
Performance analysis shows that the proposed approach outperforms the existing
state-of-the-art methods for single image dehazing.
</dc:description>
 <dc:description>Comment: Accepted in Winter Conference on Applications of Computer Vision
  (WACV-2018)</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08406</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08407</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Algebraic Geometric codes on Hirzebruch surfaces</dc:title>
 <dc:creator>Nardi, Jade</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Mathematics - Commutative Algebra</dc:subject>
 <dc:subject>Mathematics - Algebraic Geometry</dc:subject>
 <dc:description>  We define a linear code $C_\eta(\delta_T,\delta_X)$ by evaluating polynomials
of bidegree $(\delta_T,\delta_X)$ in the Cox ring on $\mathbb{F}_q$-rational
points of the Hirzebruch surface of parameter $\eta$ on the finite field
$\mathbb{F}_q$. We give explicit parameters of the code, notably using
Gr\&quot;obner bases. The minimum distance provides an upper bound of the number of
$\mathbb{F}_q$-rational points of a non-filling curve on a Hirzebruch surface.
We also display some punctured codes having optimal parameters.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08407</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08409</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computation of the State Bias and Initial States for Stochastic State
  Space Systems in the General 2-D Roesser Model Form</dc:title>
 <dc:creator>Ramos, Jos&#xe9; A.</dc:creator>
 <dc:creator>Merc&#xe8;re, Guillaume</dc:creator>
 <dc:subject>Mathematics - Numerical Analysis</dc:subject>
 <dc:subject>Computer Science - Systems and Control</dc:subject>
 <dc:description>  Recently \cite{Ramos2017a} presented a subspace system identification
algorithm for 2-D purely stochastic state space models in the general Roesser
form. However, since the exact problem requires an oblique projection of
$Y_f^h$ projected onto $W_p^h$ along $\widehat{X}_f^{vh}$, where $W_p^h=
\begin{bmatrix}\widehat{X}_p^{vh} \\ Y_p^h \end{bmatrix}$, this presents a
problem since $\{\widehat{X}_p^{vh},\widehat{X}_f^{vh}\}$ are unknown. In the
above mentioned paper, the authors found that by doing an orthogonal projection
$Y_f^h/Y_p^h$, one can identify the future horizontal state matrix
$\widehat{X}_f^{h}$ with a small bias due to the initial conditions that depend
on $\{\widehat{X}_p^{vh},\widehat{X}_f^{vh}\}$. Nevertheless, the results on
modeling 2-D images were very good despite lack of knowledge of
$\{\widehat{X}_p^{vh},\widehat{X}_f^{vh}\}$. In this note we delve into the
bias term and prove that it is insignificant, provided $i$ is chosen large
enough and the vertical and horizontal states are uncorrelated. That is, the
cross covariance of the state estimates $x_{r,s}^{h}$ and $x_{r,s}^{v}$ is
zero, or $P_{hv}=0_{n_x\times n_x}$ and $P_{vh}=0_{n_x\times n_x}$. Our
simulations use $i=30$. We also present a second iteration to improve the state
estimates by including the vertical states computed from a vertical data
processing step, i.e., by doing an orthogonal projection $Y_f^v/Y_p^v$. In this
revised algorithm we include a step to compute the initial states. This new
portion, in addition to the algorithm presented in \cite{Ramos2017a}, forms a
complete 2-D stochastic subspace system identification algorithm.
</dc:description>
 <dc:description>Comment: Companion paper of &quot;A Stochastic Subspace System Identification
  Algorithm for State Space Systems in the General 2-D Roesser Model Form&quot;
  published in International Journal of Control as a regular paper
  (https://doi.org/10.1080/00207179.2017.1418983)</dc:description>
 <dc:date>2018-01-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08409</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08410</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Relaxed Conditions for Secrecy in a Role-Based Specification</dc:title>
 <dc:creator>Fattahi, Jaouhar</dc:creator>
 <dc:creator>Mejri, Mohamed</dc:creator>
 <dc:creator>Houmani, Hanane</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  In this paper, we look at the property of secrecy through the growth of the
protocol. Intuitively, an increasing protocol preserves the secret. For that,
we need functions to estimate the security of messages. Here, we give relaxed
conditions on the functions and on the protocol and we prove that an increasing
protocol is correct when analyzed with functions that meet these conditions.
</dc:description>
 <dc:description>Comment: The initial version of this paper was published in the International
  Journal of Information Security, 2014. V1, pp 33-36. This is an enhanced
  version and an extended one of it that includes the proofs of correcntess of
  increasing protocols. It is a prerequisite for arXiv:1408.2774 and all the
  series of other papers related to witness functions</dc:description>
 <dc:date>2018-01-15</dc:date>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08410</dc:identifier>
 <dc:identifier>International Journal of Information Security, 2014. V1, pp 33-36
  (initial version)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08437</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast Algorithm for Calculating the Minimal Annihilating Polynomials of
  Matrices via Pseudo Annihilating Polynomials</dc:title>
 <dc:creator>Tajima, Shinichi</dc:creator>
 <dc:creator>Ohara, Katsuyoshi</dc:creator>
 <dc:creator>Terui, Akira</dc:creator>
 <dc:subject>Mathematics - Commutative Algebra</dc:subject>
 <dc:subject>Computer Science - Symbolic Computation</dc:subject>
 <dc:subject>15A18, 65F15, 68W30</dc:subject>
 <dc:description>  We propose a efficient method to calculate &quot;the minimal annihilating
polynomials&quot; for all the unit vectors, of square matrix over the integers or
the rational numbers. The minimal annihilating polynomials are useful for
improvement of efficiency in wide variety of algorithms in exact linear
algebra. We propose an efficient algorithm for calculating the minimal
annihilating polynomials for all the unit vectors via pseudo annihilating
polynomials with the key idea of binary splitting technique. Efficiency of the
proposed algorithm is shown by arithmetic time complexity analysis.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08437</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08439</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Analyzing Similarity in Mathematical Content To Enhance the Detection of
  Academic Plagiarism</dc:title>
 <dc:creator>Isele, Maurice-Roman</dc:creator>
 <dc:subject>Computer Science - Information Retrieval</dc:subject>
 <dc:description>  Despite the effort put into the detection of academic plagiarism, it
continues to be a ubiquitous problem spanning all disciplines. Various tools
have been developed to assist human inspectors by automatically identifying
suspicious documents. However, to our knowledge currently none of these tools
use mathematical content for their analysis. This is problematic, because
mathematical content potentially represents a significant amount of the
scientific contribution in academic documents. Hence, ignoring mathematical
content limits the detection of plagiarism considerably, especially in
disciplines with frequent use of mathematics.
  This paper aims to help close this gap by providing an overview of existing
approaches in mathematical information retrieval and an analysis of their
applicability for different possible cases of mathematical plagiarism. I find
that whereas syntax-based approaches perform particularly well in detecting
undisguised plagiarism, structure-based and hybrid approaches promise to also
detect forms of disguised mathematical plagiarism, such as plagiarism with
renamed identifiers. However, more research in this area is needed to enable
the detection of more complex mathematical plagiarism: the scope of current
approaches is restricted to the formula-level, an extension to the
section-level is needed. Additionally, the general detection of equivalence
transformations is currently not feasible. Despite these remaining problems, I
conclude that the presented approaches could already be used for a basic
automated detection system targeting mathematical plagiarism and therefore
enhance current plagiarism detection systems.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08439</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08441</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Finitary-based Domain Theory in Coq: An Early Report</dc:title>
 <dc:creator>AbdelGawad, Moez A.</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:subject>Computer Science - Programming Languages</dc:subject>
 <dc:description>  In domain theory every finite computable object can be represented by a
single mathematical object instead of a set of objects, using the notion of
finitary-basis. In this article we report on our effort to formalize domain
theory in Coq in terms of finitary-basis.
</dc:description>
 <dc:description>Comment: 13 pages</dc:description>
 <dc:date>2018-01-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08441</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08444</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Hardware implementation of auto-mutual information function for
  condition monitoring</dc:title>
 <dc:creator>Siljak, Harun</dc:creator>
 <dc:creator>Subasi, Abdulhamit</dc:creator>
 <dc:creator>Upadhyaya, Belle R.</dc:creator>
 <dc:subject>Computer Science - Other Computer Science</dc:subject>
 <dc:subject>Electrical Engineering and Systems Science - Signal Processing</dc:subject>
 <dc:description>  This study is aimed at showing applicability of mutual information, namely
auto-mutual information function for condition monitoring in electrical motors,
through age detection in accelerated motor aging. Vibration data collected in
artificial induction motor experiment is used for verification of both the
original auto-mutual information function algorithm and its hardware
implementation in Verilog, produced from an initial version made with Matlab
HDL (Hardware Description Language) Coder. A conceptual model for industry and
education based on a field programmable logic array development board is
developed and demonstrated on the auto-mutual information function example,
while suggesting other applications as well. It has also been shown that
attractor reconstruction for the vibration data cannot be straightforward.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08444</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08446</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Scalable Approach for Hardware Semiformal Verification</dc:title>
 <dc:creator>Grimm, Tomas</dc:creator>
 <dc:creator>Lettnin, Djones</dc:creator>
 <dc:creator>H&#xfc;bner, Michael</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:description>  The current verification flow of complex systems uses different engines
synergistically: virtual prototyping, formal verification, simulation,
emulation and FPGA prototyping. However, none is able to verify a complete
architecture. Furthermore, hybrid approaches aiming at complete verification
use techniques that lower the overall complexity by increasing the abstraction
level. This work focuses on the verification of complex systems at the RT level
to handle the hardware peculiarities. Our results show an improvement of 100\%
compared to the commercial tool's results for the prototype we used to validate
our approach.
</dc:description>
 <dc:date>2018-01-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08446</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08450</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reasoning about effects: from lists to cyber-physical agents</dc:title>
 <dc:creator>Mason, Ian A.</dc:creator>
 <dc:creator>Talcott, Carolyn L.</dc:creator>
 <dc:subject>Computer Science - Programming Languages</dc:subject>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:description>  Theories for reasoning about programs with effects initially focused on basic
manipulation of lists and other mutable data. The next challenge was to
consider higher-order programming, adding functions as first class objects to
mutable data. Reasoning about actors added the challenge of dealing with
distributed open systems of entities interacting asynchronously. The advent of
cyber-physical agents introduces the need to consider uncertainty, faults,
physical as well as logical effects. In addition cyber-physical agents have
sensors and actuators giving rise to a much richer class of effects with
broader scope: think of self-driving cars, autonomous drones, or smart medical
devices.
  This paper gives a retrospective on reasoning about effects highlighting key
principles and techniques and closing with challenges for future work.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08450</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08451</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Higher-dimensional automata modeling shared-variable systems</dc:title>
 <dc:creator>Kahl, Thomas</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:subject>Computer Science - Formal Languages and Automata Theory</dc:subject>
 <dc:subject>68Q85</dc:subject>
 <dc:description>  The purpose of this paper is to provide a construction to model
shared-variable systems using higher-dimensional automata which is
compositional in the sense that the parallel composition of completely
independent systems is modeled by the standard tensor product of HDAs and
nondeterministic choice is represented by the coproduct.
</dc:description>
 <dc:date>2018-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08451</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08459</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Finding ReMO (Related Memory Object): A Simple Neural Architecture for
  Text based Reasoning</dc:title>
 <dc:creator>Moon, Jihyung</dc:creator>
 <dc:creator>Yang, Hyochang</dc:creator>
 <dc:creator>Cho, Sungzoon</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  To solve the text-based question and answering task that requires relational
reasoning, it is necessary to memorize a large amount of information and find
out the question relevant information from the memory. Most approaches were
based on external memory and four components proposed by Memory Network. The
distinctive component among them was the way of finding the necessary
information and it contributes to the performance. Recently, a simple but
powerful neural network module for reasoning called Relation Network (RN) has
been introduced. We analyzed RN from the view of Memory Network, and realized
that its MLP component is able to reveal the complicate relation between
question and object pair. Motivated from it, we introduce which uses MLP to
find out relevant information on Memory Network architecture. It shows new
state-of-the-art results in jointly trained bAbI-10k story-based question
answering tasks and bAbI dialog-based question answering tasks.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08459</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08467</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Identifying Corresponding Patches in SAR and Optical Images with a
  Pseudo-Siamese CNN</dc:title>
 <dc:creator>Hughes, Lloyd H.</dc:creator>
 <dc:creator>Schmitt, Michael</dc:creator>
 <dc:creator>Mou, Lichao</dc:creator>
 <dc:creator>Wang, Yuanyuan</dc:creator>
 <dc:creator>Zhu, Xiao Xiang</dc:creator>
 <dc:subject>Electrical Engineering and Systems Science - Image and Video Processing</dc:subject>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  In this letter, we propose a pseudo-siamese convolutional neural network
(CNN) architecture that enables to solve the task of identifying corresponding
patches in very-high-resolution (VHR) optical and synthetic aperture radar
(SAR) remote sensing imagery. Using eight convolutional layers each in two
parallel network streams, a fully connected layer for the fusion of the
features learned in each stream, and a loss function based on binary
cross-entropy, we achieve a one-hot indication if two patches correspond or
not. The network is trained and tested on an automatically generated dataset
that is based on a deterministic alignment of SAR and optical imagery via
previously reconstructed and subsequently co-registered 3D point clouds. The
satellite images, from which the patches comprising our dataset are extracted,
show a complex urban scene containing many elevated objects (i.e. buildings),
thus providing one of the most difficult experimental environments. The
achieved results show that the network is able to predict corresponding patches
with high accuracy, thus indicating great potential for further development
towards a generalized multi-sensor key-point matching procedure. Index
Terms-synthetic aperture radar (SAR), optical imagery, data fusion, deep
learning, convolutional neural networks (CNN), image matching, deep matching
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08467</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08468</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Convolutional Invasion and Expansion Networks for Tumor Growth
  Prediction</dc:title>
 <dc:creator>Zhang, Ling</dc:creator>
 <dc:creator>Lu, Le</dc:creator>
 <dc:creator>Summers, Ronald M.</dc:creator>
 <dc:creator>Kebebew, Electron</dc:creator>
 <dc:creator>Yao, Jianhua</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Tumor growth is associated with cell invasion and mass-effect, which are
traditionally formulated by mathematical models, namely reaction-diffusion
equations and biomechanics. Such models can be personalized based on clinical
measurements to build the predictive models for tumor growth. In this paper, we
investigate the possibility of using deep convolutional neural networks
(ConvNets) to directly represent and learn the cell invasion and mass-effect,
and to predict the subsequent involvement regions of a tumor. The invasion
network learns the cell invasion from information related to metabolic rate,
cell density and tumor boundary derived from multimodal imaging data. The
expansion network models the mass-effect from the growing motion of tumor mass.
We also study different architectures that fuse the invasion and expansion
networks, in order to exploit the inherent correlations among them. Our network
can easily be trained on population data and personalized to a target patient,
unlike most previous mathematical modeling methods that fail to incorporate
population data. Quantitative experiments on a pancreatic tumor data set show
that the proposed method substantially outperforms a state-of-the-art
mathematical model-based approach in both accuracy and efficiency, and that the
information captured by each of the two subnetworks are complementary.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08468</dc:identifier>
 <dc:identifier>IEEE Transactions on Medical Imaging, 15 November 2017, Volume:PP
  Issue: 99</dc:identifier>
 <dc:identifier>doi:10.1109/TMI.2017.2774044</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08477</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Theory of Traffic Regulators for Deterministic Networks with
  Application to Interleaved Regulators</dc:title>
 <dc:creator>Boudec, Jean-Yves Le</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:description>  We define the minimal interleaved regulator, which generalizes the Urgency
Based Shaper that was recently proposed by Specht and Samii as a simpler
alternative to per-flow reshaping in deterministic networks with aggregate
scheduling. With this regulator, packets of multiple flows are processed in one
FIFO queue; the packet at the head of the queue is examined against the
regulation constraints of its flow; it is released at the earliest time at
which this is possible without violating the constraints. Packets that are not
at the head of the queue are not examined until they reach the head of the
queue. This regulator thus possibly delays the packet at the head of the queue
but also all following packets, which typically belong to other flows. However,
we show that, when it is placed after an arbitrary FIFO system, the worst case
delay of the combination is not increased. This shaping-for-free property is
well-known with per-flow shapers; surprisingly, it continues to hold here. To
derive this property, we introduce a new definition of traffic regulator, the
minimal Pi-regulator, which extends both the greedy shaper of network calculus
and Chang's max-plus regulator and also includes new types of regulators such
as packet rate limiters. Incidentally, we provide a new insight on the
equivalence between min-plus and max-plus formulations of regulators and
shapers.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08477</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08480</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Integrated Soft Computing Approach to a Multi-biometric Security
  Model</dc:title>
 <dc:creator>Sudhish, Prem Sewak</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Computers and Society</dc:subject>
 <dc:description>  The abstract of the thesis consists of three sections, videlicet,
  Motivation
  Chapter Organization
  Salient Contributions.
  The complete abstract is included with the thesis. The final section on
Salient Contributions is reproduced below.
  Salient Contributions
  The research presents the following salient contributions:
  i. A novel technique has been developed for comparing biographical
information, by combining the average impact of Levenshtein,
Damerau-Levenshtein, and editor distances. The impact is calculated as the
ratio of the edit distance to the maximum possible edit distance between two
strings of the same lengths as the given pair of strings. This impact lies in
the range [0, 1] and can easily be converted to a similarity (matching) score
by subtracting the impact from unity.
  ii. A universal soft computing framework is proposed for adaptively fusing
biometric and biographical information by making real-time decisions to
determine after consideration of each individual identifier whether computation
of matching scores and subsequent fusion of additional identifiers, including
biographical information is required. This proposed framework not only improves
the accuracy of the system by fusing less reliable information (e.g.
biographical information) only for instances where such a fusion is required,
but also improves the efficiency of the system by computing matching scores for
various available identifiers only when this computation is considered
necessary.
  iii. A scientific method for comparing efficiency of fusion strategies
through a predicted effort to error trade-off curve.
</dc:description>
 <dc:description>Comment: Ph.D. thesis of Prem Sewak Sudhish, Dayalbagh Educational Institute.
  The thesis has been formatted for duplex printing</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08480</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08485</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Sine Cosine Crow Search Algorithm: A powerful hybrid meta heuristic for
  global optimization</dc:title>
 <dc:creator>Pasandideh, Seyed Hamid Reza</dc:creator>
 <dc:creator>Khalilpourazari, Soheyl</dc:creator>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:description>  This paper presents a novel hybrid algorithm named Since Cosine Crow Search
Algorithm. To propose the SCCSA, two novel algorithms are considered including
Crow Search Algorithm (CSA) and Since Cosine Algorithm (SCA). The advantages of
the two algorithms are considered and utilize to design an efficient hybrid
algorithm which can perform significantly better in various benchmark
functions. The combination of concept and operators of the two algorithms
enable the SCCSA to make an appropriate trade-off between exploration and
exploitation abilities of the algorithm. To evaluate the performance of the
proposed SCCSA, seven well-known benchmark functions are utilized. The results
indicated that the proposed hybrid algorithm is able to provide very
competitive solution comparing to other state-of-the-art meta heuristics.
</dc:description>
 <dc:description>Comment: Third International Conference on Artificial Intelligence and Soft
  Computing</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08485</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08486</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Self-Learning to Detect and Segment Cysts in Lung CT Images without
  Manual Annotation</dc:title>
 <dc:creator>Zhang, Ling</dc:creator>
 <dc:creator>Gopalakrishnan, Vissagan</dc:creator>
 <dc:creator>Lu, Le</dc:creator>
 <dc:creator>Summers, Ronald M.</dc:creator>
 <dc:creator>Moss, Joel</dc:creator>
 <dc:creator>Yao, Jianhua</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Image segmentation is a fundamental problem in medical image analysis. In
recent years, deep neural networks achieve impressive performances on many
medical image segmentation tasks by supervised learning on large manually
annotated data. However, expert annotations on big medical datasets are
tedious, expensive or sometimes unavailable. Weakly supervised learning could
reduce the effort for annotation but still required certain amounts of
expertise. Recently, deep learning shows a potential to produce more accurate
predictions than the original erroneous labels. Inspired by this, we introduce
a very weakly supervised learning method, for cystic lesion detection and
segmentation in lung CT images, without any manual annotation. Our method works
in a self-learning manner, where segmentation generated in previous steps
(first by unsupervised segmentation then by neural networks) is used as ground
truth for the next level of network learning. Experiments on a cystic lung
lesion dataset show that the deep learning could perform better than the
initial unsupervised annotation, and progressively improve itself after
self-learning.
</dc:description>
 <dc:description>Comment: 4 pages, 3 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08486</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08494</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluating Predictive Models of Student Success: Closing the
  Methodological Gap</dc:title>
 <dc:creator>Gardner, Josh</dc:creator>
 <dc:creator>Brooks, Christopher</dc:creator>
 <dc:subject>Statistics - Applications</dc:subject>
 <dc:subject>Computer Science - Computers and Society</dc:subject>
 <dc:description>  Model evaluation -- the process of making inferences about the performance of
predictive models -- is a critical component of predictive modeling research in
learning analytics. In this work, we present an overview of the
state-of-the-practice of model evaluation in learning analytics, which
overwhelmingly uses only naive methods for model evaluation or, less commonly,
statistical tests which are not appropriate for predictive model evaluation. We
then provide an overview of more appropriate methods for model evaluation,
presenting both frequentist and a preferred Bayesian method. Finally, we apply
three methods -- the naive average commonly used in learning analytics,
frequentist null hypothesis significance test (NHST), and hierarchical Bayesian
model evaluation -- to a large set of MOOC data. We compare 96 different
predictive modeling techniques, including different feature sets, statistical
modeling algorithms, and tuning hyperparameters for each, using this case study
to demonstrate the different experimental conclusions these evaluation
techniques provide.
</dc:description>
 <dc:date>2018-01-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08494</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08513</identifier>
 <datestamp>2018-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Unmixing urban hyperspectral imagery with a Gaussian mixture model on
  endmember variability</dc:title>
 <dc:creator>Zhou, Yuan</dc:creator>
 <dc:creator>Wetherley, Erin B.</dc:creator>
 <dc:creator>Gader, Paul D.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  In this paper, we model a pixel as a linear combination of endmembers sampled
from probability distributions of Gaussian mixture models (GMM). The parameters
of the GMM distributions are estimated using spectral libraries. Abundances are
estimated based on the distribution parameters. The advantage of this algorithm
is that the model size grows very slowly as a function of the library size. To
validate this method, we used data collected by the AVIRIS sensor over the
Santa Barbara region: two 16 m spatial resolution and two 4 m spatial
resolution images. 64 validated regions of interest (ROI) (180 m by 180 m) were
used to assess estimate accuracy. Ground truth was obtained using 1 m images
leading to the following 6 classes: turfgrass, non-photosynthetic vegetation
(NPV), paved, roof, soil, and tree. Spectral libraries were built by manually
identifying and extracting pure spectra from both resolution images, resulting
in 3,287 spectra at 16 m and 15,426 spectra at 4 m. We then unmixed ROIs of
each resolution using the following unmixing algorithms: the set-based
algorithms MESMA and AAM, and the distribution-based algorithms GMM, NCM, and
BCM. The original libraries were used for the distribution-based algorithms
whereas set-based methods required a sophisticated reduction method, resulting
in reduced libraries of 61 spectra at 16 m and 95 spectra at 4 m. The results
show that GMM performs best among the distribution-based methods, producing
comparable accuracy to MESMA, and may be more robust across datasets.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08513</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08535</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>CommanderSong: A Systematic Approach for Practical Adversarial Voice
  Recognition</dc:title>
 <dc:creator>Yuan, Xuejing</dc:creator>
 <dc:creator>Chen, Yuxuan</dc:creator>
 <dc:creator>Zhao, Yue</dc:creator>
 <dc:creator>Long, Yunhui</dc:creator>
 <dc:creator>Liu, Xiaokang</dc:creator>
 <dc:creator>Chen, Kai</dc:creator>
 <dc:creator>Zhang, Shengzhi</dc:creator>
 <dc:creator>Huang, Heqing</dc:creator>
 <dc:creator>Wang, Xiaofeng</dc:creator>
 <dc:creator>Gunter, Carl A.</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Sound</dc:subject>
 <dc:subject>Electrical Engineering and Systems Science - Audio and Speech Processing</dc:subject>
 <dc:description>  ASR (automatic speech recognition) systems like Siri, Alexa, Google Voice or
Cortana has become quite popular recently. One of the key techniques enabling
the practical use of such systems in people's daily life is deep learning.
Though deep learning in computer vision is known to be vulnerable to
adversarial perturbations, little is known whether such perturbations are still
valid on the practical speech recognition. In this paper, we not only
demonstrate such attacks can happen in reality, but also show that the attacks
can be systematically conducted. To minimize users' attention, we choose to
embed the voice commands into a song, called CommandSong. In this way, the song
carrying the command can spread through radio, TV or even any media player
installed in the portable devices like smartphones, potentially impacting
millions of users in long distance. In particular, we overcome two major
challenges: minimizing the revision of a song in the process of embedding
commands, and letting the CommandSong spread through the air without losing the
voice &quot;command&quot;. Our evaluation demonstrates that we can craft random songs to
&quot;carry&quot; any commands and the modify is extremely difficult to be noticed.
Specially, the physical attack that we play the CommandSongs over the air and
record them can success with 94 percentage.
</dc:description>
 <dc:date>2018-01-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08535</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08558</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deep Learning for End-to-End Automatic Target Recognition from Synthetic
  Aperture Radar Imagery</dc:title>
 <dc:creator>Furukawa, Hidetoshi</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  The standard architecture of synthetic aperture radar (SAR) automatic target
recognition (ATR) consists of three stages: detection, discrimination, and
classification. In recent years, convolutional neural networks (CNNs) for SAR
ATR have been proposed, but most of them classify target classes from a target
chip extracted from SAR imagery, as a classification for the third stage of SAR
ATR. In this report, we propose a novel CNN for end-to-end ATR from SAR
imagery. The CNN named verification support network (VersNet) performs all
three stages of SAR ATR end-to-end. VersNet inputs a SAR image of arbitrary
sizes with multiple classes and multiple targets, and outputs a SAR ATR image
representing the position, class, and pose of each detected target. This report
describes the evaluation results of VersNet which trained to output scores of
all 12 classes: 10 target classes, a target front class, and a background
class, for each pixel using the moving and stationary target acquisition and
recognition (MSTAR) public dataset.
</dc:description>
 <dc:description>Comment: Technical Report, 6 pages, 7 figures, 7 tables, Copyright(C)2018
  IEICE</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08558</dc:identifier>
 <dc:identifier>IEICE Technical Report, vol.117, no.403, SANE2017-92, pp.35-40,
  Jan. 2018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08560</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Tractable Analysis of the Blind-spot Probability in Localization
  Networks under Correlated Blocking</dc:title>
 <dc:creator>Aditya, Sundar</dc:creator>
 <dc:creator>Dhillon, Harpreet S.</dc:creator>
 <dc:creator>Molisch, Andreas F.</dc:creator>
 <dc:creator>Behairy, Hatim</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:description>  In localization applications, the line-of-sight between anchors and targets
may be blocked by obstacles in the environment. A target without line-of-sight
to enough number of anchors cannot be unambiguously localized and is,
therefore, said to be in a blind-spot. In this paper, we analyze the blind-spot
probability of a typical target by using stochastic geometry to model the
randomness in the obstacle and anchor locations. In doing so, we handle
correlated anchor blocking induced by obstacles, unlike previous works that
assume independent anchor blocking. We first characterize the regime over which
the independent blocking assumption underestimates the blind-spot probability
of the typical target, which in turn, is characterized as a function of the
distribution of the unshadowed area, as seen from the target location. Since
this distribution is difficult to characterize exactly, we formulate the
nearest two-obstacle approximation, which is equivalent to considering
correlated blocking for only the nearest two obstacles from the target and
assuming independent blocking for the remaining obstacles. Based on this, we
derive a closed-form (approximate) expression for the blind-spot probability,
which helps determine the anchor deployment intensity needed for the blind-spot
probability of a typical target to be at most a threshold, $\mu$.
</dc:description>
 <dc:description>Comment: Submitted to IEEE Transactions on Wireless Communications</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08560</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08564</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tight Bound on the Number of Relevant Variables in a Bounded degree
  Boolean function</dc:title>
 <dc:creator>Chiarelli, John</dc:creator>
 <dc:creator>Hatami, Pooya</dc:creator>
 <dc:creator>Saks, Michael</dc:creator>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:description>  In this paper, we prove that a degree $d$ Boolean function depends on at most
$C\cdot 2^d$ variables for some $C&lt;22$, i.e. it is a $C\cdot 2^d$-junta. This
improves the $d\cdot 2^{d-1}$ upper bound of Nisan and Szegedy [NS94]. Our
proof uses a new weighting scheme where we assign weights to variables based on
the highest degree monomial they appear on.
  We note that a bound of $C\cdot 2^d$ is essentially tight. A lower bound of
$2^d-1$ can easily be achieved by a read-once decision tree of depth $d$ (see
[O'Donnell 14], Exercise 3.24). We slightly improve this bound by constructing
a Boolean function of degree $d$ with $3\cdot 2^{d-1}-2$ relevant variables.
This construction was independently observed by Avishay Tal, but has not
appeared in a publication before [Tal 17].
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08564</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08565</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Rollercoasters and Caterpillars</dc:title>
 <dc:creator>Biedl, Therese</dc:creator>
 <dc:creator>Biniaz, Ahmad</dc:creator>
 <dc:creator>Cummings, Robert</dc:creator>
 <dc:creator>Lubiw, Anna</dc:creator>
 <dc:creator>Manea, Florin</dc:creator>
 <dc:creator>Nowotka, Dirk</dc:creator>
 <dc:creator>Shallit, Jeffrey</dc:creator>
 <dc:subject>Computer Science - Computational Geometry</dc:subject>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  A rollercoaster is a sequence of real numbers for which every maximal
contiguous subsequence, that is increasing or decreasing, has length at least
three. By translating this sequence to a set of points in the plane, a
rollercoaster can be defined as a polygonal path for which every maximal
sub-path, with positive- or negative-slope edges, has at least three points.
Given a sequence of distinct real numbers, the rollercoaster problem asks for a
maximum-length subsequence that is a rollercoaster. It was conjectured that
every sequence of $n$ distinct real numbers contains a rollercoaster of length
at least $\lceil n/2\rceil$ for $n&gt;7$, while the best known lower bound is
$\Omega(n/\log n)$. In this paper we prove this conjecture. Our proof is
constructive and implies a linear-time algorithm for computing a rollercoaster
of this length. Extending the $O(n\log n)$-time algorithm for computing a
longest increasing subsequence, we show how to compute a maximum-length
rollercoaster within the same time bound. A maximum-length rollercoaster in a
permutation of $\{1,\dots,n\}$ can be computed in $O(n \log \log n)$ time.
  The search for rollercoasters was motivated by orthogeodesic point-set
embedding of caterpillars. A caterpillar is a tree such that deleting the
leaves gives a path, called the spine. A top-view caterpillar is one of degree
4 such that the two leaves adjacent to each vertex lie on opposite sides of the
spine. As an application of our result on rollercoasters, we are able to find a
planar drawing of every $n$-node top-view caterpillar on every set of
$\frac{25}{3}n$ points in the plane, such that each edge is an orthogonal path
with one bend. This improves the previous best known upper bound on the number
of required points, which is $O(n \log n)$. We also show that such a drawing
can be obtained in linear time, provided that the points are given in sorted
order.
</dc:description>
 <dc:description>Comment: 17 pages</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08565</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08570</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deep Learning in Pharmacogenomics: From Gene Regulation to Patient
  Stratification</dc:title>
 <dc:creator>Kalinin, Alexandr A.</dc:creator>
 <dc:creator>Higgins, Gerald A.</dc:creator>
 <dc:creator>Reamaroon, Narathip</dc:creator>
 <dc:creator>Soroushmehr, S. M. Reza</dc:creator>
 <dc:creator>Allyn-Feuer, Ari</dc:creator>
 <dc:creator>Dinov, Ivo D.</dc:creator>
 <dc:creator>Najarian, Kayvan</dc:creator>
 <dc:creator>Athey, Brian D.</dc:creator>
 <dc:subject>Quantitative Biology - Quantitative Methods</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
</dc:description>
 <dc:description>Comment: Alexandr A. Kalinin and Gerald A. Higgins contributed equally to this
  work. Corresponding author: Brian D. Athey, &lt;bleu@umich.edu&gt;</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08570</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08573</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Etymo: A New Discovery Engine for AI Research</dc:title>
 <dc:creator>Zhang, Weijian</dc:creator>
 <dc:creator>Deakin, Jonathan</dc:creator>
 <dc:creator>Higham, Nicholas J.</dc:creator>
 <dc:creator>Wang, Shuaiqiang</dc:creator>
 <dc:subject>Computer Science - Information Retrieval</dc:subject>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:description>  We present Etymo (https://etymo.io), a discovery engine to facilitate
artificial intelligence (AI) research and development. It aims to help readers
navigate a large number of AI-related papers published every week by using a
novel form of search that finds relevant papers and displays related papers in
a graphical interface. Etymo constructs and maintains an adaptive
similarity-based network of research papers as an all-purpose knowledge graph
for ranking, recommendation, and visualisation. The network is constantly
evolving and can learn from user feedback to adjust itself.
</dc:description>
 <dc:description>Comment: 7 pages, 2 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08573</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08577</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Effective Building Block Design for Deep Convolutional Neural Networks
  using Search</dc:title>
 <dc:creator>Dutta, Jayanta K</dc:creator>
 <dc:creator>Liu, Jiayi</dc:creator>
 <dc:creator>Kurup, Unmesh</dc:creator>
 <dc:creator>Shah, Mohak</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  Deep learning has shown promising results on many machine learning tasks but
DL models are often complex networks with large number of neurons and layers,
and recently, complex layer structures known as building blocks. Finding the
best deep model requires a combination of finding both the right architecture
and the correct set of parameters appropriate for that architecture. In
addition, this complexity (in terms of layer types, number of neurons, and
number of layers) also present problems with generalization since larger
networks are easier to overfit to the data. In this paper, we propose a search
framework for finding effective architectural building blocks for convolutional
neural networks (CNN). Our approach is much faster at finding models that are
close to state-of-the-art in performance. In addition, the models discovered by
our approach are also smaller than models discovered by similar techniques. We
achieve these twin advantages by designing our search space in such a way that
it searches over a reduced set of state-of-the-art building blocks for CNNs
including residual block, inception block, inception-residual block, ResNeXt
block and many others. We apply this technique to generate models for multiple
image datasets and show that these models achieve performance comparable to
state-of-the-art (and even surpassing the state-of-the-art in one case). We
also show that learned models are transferable between datasets.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08577</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08583</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Random Walk Fundamental Tensor and its Applications to Network Analysis</dc:title>
 <dc:creator>Golnari, Golshan</dc:creator>
 <dc:creator>Zhang, Zhi-Li</dc:creator>
 <dc:creator>Boley, Daniel</dc:creator>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:description>  We first present a comprehensive review of various random walk metrics used
in the literature and express them in a consistent framework. We then introduce
fundamental tensor -- a generalization of the well-known fundamental matrix --
and show that classical random walk metrics can be derived from it in a unified
manner. We provide a collection of useful relations for random walk metrics
that are useful and insightful for network studies. To demonstrate the
usefulness and efficacy of the proposed fundamental tensor in network analysis,
we present four important applications: 1) unification of network centrality
measures, 2) characterization of (generalized) network articulation points, 3)
identification of network most influential nodes, and 4) fast computation of
network reachability after failures.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08583</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08586</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reconstructing a cascade from temporal observations</dc:title>
 <dc:creator>Xiao, Han</dc:creator>
 <dc:creator>Rozenshtein, Polina</dc:creator>
 <dc:creator>Tatti, Nikolaj</dc:creator>
 <dc:creator>Gionis, Aristides</dc:creator>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:description>  Given a subset of active nodes in a network can we re- construct the cascade
that has generated these observa- tions? This is a problem that has been
studied in the literature, but here we focus in the case that tempo- ral
information is available about the active nodes. In particular, we assume that
in addition to the subset of active nodes we also know their activation time.
We formulate this cascade-reconstruction problem as a variant of a Steiner-tree
problem: we ask to find a tree that spans all reported active nodes while
satisfying temporal-consistency constraints. We present three approximation
algorithms. The best algorithm in terms of quality achieves a
O(\sqrt{k})-approximation guarantee, where k is the number of active nodes,
while the most efficient algorithm has linearithmic running time, making it
scalable to very large graphs. We evaluate our algorithms on real-world
networks with both simulated and real cascades. Our results in- dicate that
utilizing the available temporal information allows for more accurate cascade
reconstruction. Fur- thermore, our objective leads to finding the &quot;backbone&quot; of
the cascade and it gives solutions of high precision.
</dc:description>
 <dc:description>Comment: 12 pages, 7 figures, to appear in SDM 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08586</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08588</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Adaptive Detection of Complex Event Patterns</dc:title>
 <dc:creator>Kolchinsky, Ilya</dc:creator>
 <dc:creator>Schuster, Assaf</dc:creator>
 <dc:subject>Computer Science - Databases</dc:subject>
 <dc:description>  Complex event processing (CEP) is widely employed to detect occurrences of
predefined combinations (patterns) of events in massive data streams. As new
events are accepted, they are matched using some type of evaluation structure,
commonly optimized according to the statistical properties of the data items in
the input stream. However, in many real-life scenarios the data characteristics
are never known in advance or are subject to frequent on-the-fly changes. To
modify the evaluation structure as a reaction to such changes, adaptation
mechanisms are employed. These mechanisms typically function by monitoring a
set of properties and applying a new evaluation plan when significant deviation
from the initial values is observed. This strategy often leads to missing
important input changes or it may incur substantial computational overhead by
over-adapting. In this paper, we present an efficient and precise method for
dynamically deciding whether and how the evaluation structure should be
reoptimized. This method is based on a small set of constraints to be satisfied
by the monitored values, defined such that a better evaluation plan is
guaranteed if any of the constraints is violated. To the best of our knowledge,
our proposed mechanism is the first to provably avoid false positives on
reoptimization decisions. We formally prove this claim and demonstrate how our
method can be applied on known algorithms for evaluation plan generation. Our
extensive experimental evaluation on real-world datasets confirms the
superiority of our strategy over existing methods in terms of performance and
accuracy.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08588</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08589</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A New Algorithm for Double Scalar Multiplication over Koblitz Curves</dc:title>
 <dc:creator>Adikari, J.</dc:creator>
 <dc:creator>Dimitrov, V. S.</dc:creator>
 <dc:creator>Cintra, R. J.</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  Koblitz curves are a special set of elliptic curves and have improved
performance in computing scalar multiplication in elliptic curve cryptography
due to the Frobenius endomorphism. Double-base number system approach for
Frobenius expansion has improved the performance in single scalar
multiplication. In this paper, we present a new algorithm to generate a sparse
and joint $\tau$-adic representation for a pair of scalars and its application
in double scalar multiplication. The new algorithm is inspired from double-base
number system. We achieve 12% improvement in speed against state-of-the-art
$\tau$-adic joint sparse form.
</dc:description>
 <dc:description>Comment: 5 pages, 2 figures, 1 table</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08589</dc:identifier>
 <dc:identifier>Circuits and Systems (ISCAS), 2011 IEEE International Symposium on</dc:identifier>
 <dc:identifier>doi:10.1109/ISCAS.2011.5937664</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08590</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Individual testing is optimal for nonadaptive group testing in the
  linear regime</dc:title>
 <dc:creator>Aldridge, Matthew</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:subject>Mathematics - Probability</dc:subject>
 <dc:description>  We consider nonadaptive probabilistic group testing in the linear regime,
where each of n items is defective independently with probability p in (0,1),
where p is a constant independent of n. We show that testing each item
individually is optimal, in the sense that with fewer than n tests the error
probability is bounded away from zero.
</dc:description>
 <dc:description>Comment: 7 pages</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08590</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08598</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Scenarios for Development, Test and Validation of Automated Vehicles</dc:title>
 <dc:creator>Menzel, Till</dc:creator>
 <dc:creator>Bagschik, Gerrit</dc:creator>
 <dc:creator>Maurer, Markus</dc:creator>
 <dc:subject>Computer Science - Software Engineering</dc:subject>
 <dc:description>  The ISO 26262 standard from 2016 represents the state of the art for a
safety-guided development of safety-critical electric/electronic vehicle
systems. These vehicle systems include advanced driver assistance systems and
vehicle guidance systems. The development process proposed in the ISO 26262
standard is based upon multiple V-models, and defines activities and work
products for each process step. In many of these process steps, scenario based
approaches can be applied to achieve the defined work products for the
development of automated driving functions. To accomplish the work products of
different process steps, scenarios have to focus on various aspects like a
human understandable notation or a description via time-space variables. This
leads to contradictory requirements regarding the level of detail and way of
notation for the representation of scenarios. In this paper, the authors
present requirements for the representation of scenarios in different process
steps defined by the ISO 26262 standard, propose a consistent terminology based
on prior publications for the identified levels of abstraction, and demonstrate
how scenarios can be systematically evolved along the phases of the development
process outlined in the ISO 26262 standard.
</dc:description>
 <dc:description>Comment: Will be submitted to 2018 IEEE Intelligent Vehicles Symposium, 7
  pages, 5 figures</dc:description>
 <dc:date>2018-01-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08598</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08599</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deep LOGISMOS: Deep Learning Graph-based 3D Segmentation of Pancreatic
  Tumors on CT scans</dc:title>
 <dc:creator>Guo, Zhihui</dc:creator>
 <dc:creator>Zhang, Ling</dc:creator>
 <dc:creator>Lu, Le</dc:creator>
 <dc:creator>Bagheri, Mohammadhadi</dc:creator>
 <dc:creator>Summers, Ronald M.</dc:creator>
 <dc:creator>Sonka, Milan</dc:creator>
 <dc:creator>Yao, Jianhua</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  This paper reports Deep LOGISMOS approach to 3D tumor segmentation by
incorporating boundary information derived from deep contextual learning to
LOGISMOS - layered optimal graph image segmentation of multiple objects and
surfaces. Accurate and reliable tumor segmentation is essential to tumor growth
analysis and treatment selection. A fully convolutional network (FCN), UNet, is
first trained using three adjacent 2D patches centered at the tumor, providing
contextual UNet segmentation and probability map for each 2D patch. The UNet
segmentation is then refined by Gaussian Mixture Model (GMM) and morphological
operations. The refined UNet segmentation is used to provide the initial shape
boundary to build a segmentation graph. The cost for each node of the graph is
determined by the UNet probability maps. Finally, a max-flow algorithm is
employed to find the globally optimal solution thus obtaining the final
segmentation. For evaluation, we applied the method to pancreatic tumor
segmentation on a dataset of 51 CT scans, among which 30 scans were used for
training and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient
(DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4%
respectively, both are significantly improved (p&lt;0.05) compared with contextual
UNet and/or LOGISMOS alone.
</dc:description>
 <dc:description>Comment: 4 pages,3 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08599</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08601</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>MmWave Channel Estimation via Atomic Norm Minimization for Multi-User
  Hybrid Precoding</dc:title>
 <dc:creator>Deng, Junquan</dc:creator>
 <dc:creator>Tirkkonen, Olav</dc:creator>
 <dc:creator>Studer, Christoph</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  To perform multi-user multiple-input and multiple-output transmission in
millimeter-wave(mmWave) cellular systems, the high-dimensional channels need to
be estimated for designing the multi-user precoder. Conventional grid-based
compressed sensing (CS) methods for mmWave channel estimation suffer from the
basis mismatch problem, which prevents accurate channel reconstruction and
degrades the precoding performance. This paper formulates mmWave channel
estimation as an Atomic Norm Minimization (ANM) problem. In contrast to
grid-based CS methods which use discrete dictionaries, ANM uses a continuous
dictionary for representing the mmWave channel. We consider a continuous
dictionary based on sub-sampling in the antenna domain via a small number of
radio frequency chains. We show that mmWave channel estimation using ANM can be
formulated as a semidefinite programming (SDP) problem, and the user channel
can be accurately estimated via off-the-shelf SDP solvers in polynomial time.
Simulation results indicate that ANM can achieve much better estimation
accuracy compared to grid-based CS, and significantly improves the spectral
efficiency provided by multi-user precoding.
</dc:description>
 <dc:description>Comment: 6 pages, 6 figures, accepted for presentation in IEEE WCNC 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08601</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08603</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Structuring Spreadsheets with the &quot;Lish&quot; Data Model</dc:title>
 <dc:creator>Hall, Alan</dc:creator>
 <dc:creator>Wermelinger, Michel</dc:creator>
 <dc:creator>Hirst, Tony</dc:creator>
 <dc:creator>Phithakkitnukoon, Santi</dc:creator>
 <dc:subject>Computer Science - Software Engineering</dc:subject>
 <dc:description>  A spreadsheet is remarkably flexible in representing various forms of
structured data, but the individual cells have no knowledge of the larger
structures of which they may form a part. This can hamper comprehension and
increase formula replication, increasing the risk of error on both scores. We
explore a novel data model (called the &quot;lish&quot;) that could form an alternative
to the traditional grid in a spreadsheet-like environment. Its aim is to
capture some of these higher structures while preserving the simplicity that
makes a spreadsheet so attractive. It is based on cells organised into nested
lists, in each of which the user may optionally employ a template to prototype
repeating structures. These template elements can be likened to the marginal
&quot;cells&quot; in the borders of a traditional worksheet, but are proper members of
the sheet and may themselves contain internal structure. A small demonstration
application shows the &quot;lish&quot; in operation.
</dc:description>
 <dc:description>Comment: 4 colour figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08603</dc:identifier>
 <dc:identifier>Proceedings of the EuSpRIG 2017 Conference &quot;Spreadsheet Risk
  Management&quot;, Imperial College, London, pp107-119 ISBN: 978-1-905404-54-4</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08607</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Interactive Diversity Optimization of Environments</dc:title>
 <dc:creator>Berseth, Glen</dc:creator>
 <dc:creator>Khayatkhoei, Mahyar</dc:creator>
 <dc:creator>Haworth, Brandon</dc:creator>
 <dc:creator>Usman, Muhammad</dc:creator>
 <dc:creator>Kapadia, Mubbasir</dc:creator>
 <dc:creator>Faloutsos, Petros</dc:creator>
 <dc:subject>Computer Science - Human-Computer Interaction</dc:subject>
 <dc:subject>Computer Science - Computational Engineering, Finance, and Science</dc:subject>
 <dc:description>  The design of a building requires an architect to balance a wide range of
constraints: aesthetic, geometric, usability, lighting, safety, etc. At the
same time, there are often a multiplicity of diverse designs that can meet
these constraints equally well. Architects must use their skills and artistic
vision to explore these rich but highly constrained design spaces. A number of
computer-aided design tools use automation to provide useful analytical data
and optimal designs with respect to certain fitness criteria. However, this
automation can come at the expense of a designer's creative control.
  We propose uDOME, a user-in-the-loop system for computer-aided design
exploration that balances automation and control by efficiently exploring,
analyzing, and filtering the space of environment layouts to better inform an
architect's decision-making. At each design iteration, uDOME provides a set of
diverse designs which satisfy user-defined constraints and optimality criteria
within a user defined parameterization of the design space. The user then
selects a design and performs a similar optimization with the same or different
parameters and objectives. This exploration process can be repeated as many
times as the designer wishes. Our user studies indicates that \DOME, with its
diversity-based approach, improves the efficiency and effectiveness of even
novice users with minimal training, without compromising the quality of their
designs.
</dc:description>
 <dc:description>Comment: 20 pages</dc:description>
 <dc:date>2018-01-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08607</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08612</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A method for assessing the success and failure of community-level
  interventions in the presence of network diffusion, social reinforcement, and
  related social effects</dc:title>
 <dc:creator>Lee, Hsuan-Wei</dc:creator>
 <dc:creator>Gauthier, G. Robin</dc:creator>
 <dc:creator>Ivanich, Jerreed D.</dc:creator>
 <dc:creator>Wexler, Lisa</dc:creator>
 <dc:creator>Khan, Bilal</dc:creator>
 <dc:creator>Dombrowski, Kirk</dc:creator>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:subject>Physics - Physics and Society</dc:subject>
 <dc:description>  Prevention and intervention work done within community settings often face
unique analytic challenges for rigorous evaluations. Since community prevention
work (often geographically isolated) cannot be controlled in the same way other
prevention programs and these communities have an increased level of
interpersonal interactions, rigorous evaluations are needed. Even when the
`gold standard' randomized control trials are implemented within community
intervention work, the threats to internal validity can be called into question
given informal social spread of information in closed network settings. A new
prevention evaluation method is presented here to disentangle the social
influences assumed to influence prevention effects within communities. We
formally introduce the method and it's utility for a suicide prevention program
implemented in several Alaska Native villages. The results show promise to
explore eight sociological measures of intervention effects in the face of
social diffusion, social reinforcement, and direct treatment. Policy and
research implication are discussed.
</dc:description>
 <dc:description>Comment: 18 pages, 5 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08612</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08613</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Rapidly Deployable Classification System using Visual Data for the
  Application of Precision Weed Management</dc:title>
 <dc:creator>Hall, David</dc:creator>
 <dc:creator>Dayoub, Feras</dc:creator>
 <dc:creator>Perez, Tristan</dc:creator>
 <dc:creator>McCool, Chris</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  In this work we demonstrate a rapidly deployable weed classification system
that uses visual data to enable autonomous precision weeding without making
prior assumptions about which weed species are present in a given field.
Previous work in this area relies on having prior knowledge of the weed species
present in the field. This assumption cannot always hold true for every field,
and thus limits the use of weed classification systems based on this
assumption. In this work, we obviate this assumption and introduce a rapidly
deployable approach able to operate on any field without any weed species
assumptions prior to deployment. We present a three stage pipeline for the
implementation of our weed classification system consisting of initial field
surveillance, offline processing and selective labelling, and automated
precision weeding. The key characteristic of our approach is the combination of
plant clustering and selective labelling which is what enables our system to
operate without prior weed species knowledge. Testing using field data we are
able to label 12.3 times fewer images than traditional full labelling whilst
reducing classification accuracy by only 14%.
</dc:description>
 <dc:description>Comment: 36 pages, 14 figures, submitted to Computers and Electronics in
  Agriculture, under review</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08613</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08614</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans:
  Self-Paced 3D Mask Generation from RECIST</dc:title>
 <dc:creator>Cai, Jinzheng</dc:creator>
 <dc:creator>Tang, Youbao</dc:creator>
 <dc:creator>Lu, Le</dc:creator>
 <dc:creator>Harrison, Adam P.</dc:creator>
 <dc:creator>Yan, Ke</dc:creator>
 <dc:creator>Xiao, Jing</dc:creator>
 <dc:creator>Yang, Lin</dc:creator>
 <dc:creator>Summers, Ronald M.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Volumetric lesion segmentation via medical imaging is a powerful means to
precisely assess multiple time-point lesion/tumor changes. Because manual 3D
segmentation is prohibitively time consuming and requires radiological
experience, current practices rely on an imprecise surrogate called response
evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST
marks are commonly found in current hospital picture and archiving systems
(PACS), meaning they can provide a potentially powerful, yet extraordinarily
challenging, source of weak supervision for full 3D segmentation. Toward this
end, we introduce a convolutional neural network based weakly supervised
self-paced segmentation (WSSS) method to 1) generate the initial lesion
segmentation on the axial RECIST-slice; 2) learn the data distribution on
RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally
obtain a volumetric segmentation. In addition, we explore how super-resolution
images (2~5 times beyond the physical CT imaging), generated from a proposed
stacked generative adversarial network, can aid the WSSS performance. We employ
the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735
PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of
varying sizes, categories, body regions and surrounding contexts. These are
drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node
dataset, where 3D ground truth masks are available for all images. For the
DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices
and 76% in 3D lesion volumes. We further validate using a subjective user
study, where an experienced radiologist accepted our WSSS-generated lesion
segmentation results with a high probability of 92.4%.
</dc:description>
 <dc:description>Comment: v1: Main paper + supplementary material</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08614</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08616</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DeepPap: Deep Convolutional Networks for Cervical Cell Classification</dc:title>
 <dc:creator>Zhang, Ling</dc:creator>
 <dc:creator>Lu, Le</dc:creator>
 <dc:creator>Nogues, Isabella</dc:creator>
 <dc:creator>Summers, Ronald M.</dc:creator>
 <dc:creator>Liu, Shaoxiong</dc:creator>
 <dc:creator>Yao, Jianhua</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Automation-assisted cervical screening via Pap smear or liquid-based cytology
(LBC) is a highly effective cell imaging based cancer detection tool, where
cells are partitioned into &quot;abnormal&quot; and &quot;normal&quot; categories. However, the
success of most traditional classification methods relies on the presence of
accurate cell segmentations. Despite sixty years of research in this field,
accurate segmentation remains a challenge in the presence of cell clusters and
pathologies. Moreover, previous classification methods are only built upon the
extraction of hand-crafted features, such as morphology and texture. This paper
addresses these limitations by proposing a method to directly classify cervical
cells - without prior segmentation - based on deep features, using
convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on
a natural image dataset. It is subsequently fine-tuned on a cervical cell
dataset consisting of adaptively re-sampled image patches coarsely centered on
the nuclei. In the testing phase, aggregation is used to average the prediction
scores of a similar set of image patches. The proposed method is evaluated on
both Pap smear and LBC datasets. Results show that our method outperforms
previous algorithms in classification accuracy (98.3%), area under the curve
(AUC) (0.99) values, and especially specificity (98.3%), when applied to the
Herlev benchmark Pap smear dataset and evaluated using five-fold
cross-validation. Similar superior performances are also achieved on the HEMLBC
(H&amp;E stained manual LBC) dataset. Our method is promising for the development
of automation-assisted reading systems in primary cervical screening.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08616</dc:identifier>
 <dc:identifier>IEEE Journal of Biomedical and Health Informatics, 19 May 2017,
  Volume: 21 Issue: 6</dc:identifier>
 <dc:identifier>doi:10.1109/JBHI.2017.2705583</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08618</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>JointDNN: An Efficient Training and Inference Engine for Intelligent
  Mobile Cloud Computing Services</dc:title>
 <dc:creator>Eshratifar, Amir Erfan</dc:creator>
 <dc:creator>Abrishami, Mohammad Saeed</dc:creator>
 <dc:creator>Pedram, Massoud</dc:creator>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:subject>Computer Science - Performance</dc:subject>
 <dc:description>  Deep neural networks are among the most influential architectures of deep
learning algorithms, being deployed in many mobile intelligent applications.
End-side services, such as intelligent personal assistants (IPAs), autonomous
cars, and smart home services often employ either simple local models or
complex remote models on the cloud. Mobile-only and cloud-only computations are
currently the status quo approaches. In this paper, we propose an efficient,
adaptive, and practical engine, JointDNN, for collaborative computation between
a mobile device and cloud for DNNs in both inference and training phase.
JointDNN not only provides an energy and performance efficient method of
querying DNNs for the mobile side, but also benefits the cloud server by
reducing the amount of its workload and communications compared to the
cloud-only approach. Given the DNN architecture, we investigate the efficiency
of processing some layers on the mobile device and some layers on the cloud
server. We provide optimization formulations at layer granularity for forward
and backward propagation in DNNs, which can adapt to mobile battery limitations
and cloud server load constraints and quality of service. JointDNN achieves up
to 18X and 32X reductions on the latency and mobile energy consumption of
querying DNNs, respectively.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08618</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08620</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Queue-Aware Joint Dynamic Interference Coordination and Heterogeneous
  QoS Provisioning in OFDMA Networks</dc:title>
 <dc:creator>Sharifian, Alirezan</dc:creator>
 <dc:creator>Adve, Raviraj</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  We propose algorithms for cloud radio access networks that not only provide
heterogeneous quality of-service (QoS) for rate- and, importantly,
delay-sensitive applications, but also jointly optimize the frequency reuse
pattern. Importantly, unlike related works, we account for random arrivals,
through queue awareness and, unlike majority of works focusing on a single
frame only, we consider QoS measures averaged over multiple frames involving a
set of closed loop controls. We model this problem as multi-cell optimization
to maximize a sum utility subject to the QoS constraints, expressed as minimum
mean-rate or maximum mean-delay. Since we consider dynamic interference
coordination jointly with dynamic user association, the problem is not convex,
even after integer relaxation. We translate the problem into an optimization of
frame rates, amenable to a decomposition into intertwined primal and dual
problems. The solution to this optimization problem provides joint decisions on
scheduling, dynamic interference coordination, and, importantly, unlike most
works in this area, on dynamic user association. Additionally, we propose a
novel method to manage infeasible loads. Extensive simulations confirm that the
design responds to instantaneous loads, heterogeneous user and AP locations,
channel conditions, and QoS constraints while, if required, keeping outage low
when dealing with infeasible loads. Comparisons to the baseline proportional
fair scheme illustrate the gains achieved.
</dc:description>
 <dc:description>Comment: Accepted for publication in IEEE Transaction on Wireless
  Communications; 17 pages, 9 figures</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08620</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08621</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Investigating the Effects of Dynamic Precision Scaling on Neural Network
  Training</dc:title>
 <dc:creator>Stuart, Dylan Malone</dc:creator>
 <dc:creator>Taras, Ian</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Training neural networks is a time- and compute-intensive operation. This is
mainly due to the large amount of floating point tensor operations that are
required during training. These constraints limit the scope of design space
explorations (in terms of hyperparameter search) for data scientists and
researchers. Recent work has explored the possibility of reducing the numerical
precision used to represent parameters, activations, and gradients during
neural network training as a way to reduce the computational cost of training
(and thus reducing training time). In this paper we develop a novel dynamic
precision scaling scheme and evaluate its performance, comparing it to previous
works. Using stochastic fixed-point rounding, a quantization-error based
scaling scheme, and dynamic bit-widths during training, we achieve 98.8% test
accuracy on the MNIST dataset using an average bit-width of just 16 bits for
weights and 14 bits for activations. This beats the previous state-of-the-art
dynamic bit-width precision scaling algorithm.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08621</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08624</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Handwritten Chinese Characters using CycleGAN</dc:title>
 <dc:creator>Chang, Bo</dc:creator>
 <dc:creator>Zhang, Qiong</dc:creator>
 <dc:creator>Pan, Shenyi</dc:creator>
 <dc:creator>Meng, Lili</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Handwriting of Chinese has long been an important skill in East Asia.
However, automatic generation of handwritten Chinese characters poses a great
challenge due to the large number of characters. Various machine learning
techniques have been used to recognize Chinese characters, but few works have
studied the handwritten Chinese character generation problem, especially with
unpaired training data. In this work, we formulate the Chinese handwritten
character generation as a problem that learns a mapping from an existing
printed font to a personalized handwritten style. We further propose DenseNet
CycleGAN to generate Chinese handwritten characters. Our method is applied not
only to commonly used Chinese characters but also to calligraphy work with
aesthetic values. Furthermore, we propose content accuracy and style
discrepancy as the evaluation metrics to assess the quality of the handwritten
characters generated. We then use our proposed metrics to evaluate the
generated characters from CASIA dataset as well as our newly introduced Lanting
calligraphy dataset.
</dc:description>
 <dc:description>Comment: Accepted at WACV 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08624</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08629</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Forecasting Suspicious Account Activity at Large-Scale Online Service
  Providers</dc:title>
 <dc:creator>Halawa, Hassan</dc:creator>
 <dc:creator>Ripeanu, Matei</dc:creator>
 <dc:creator>Beznosov, Konstantin</dc:creator>
 <dc:creator>Coskun, Baris</dc:creator>
 <dc:creator>Liu, Meizhu</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  In the face of large-scale automated social engineering attacks to large
online services, fast detection and remediation of compromised accounts are
crucial to limit the spread of new attacks and to mitigate the overall damage
to users, companies, and the public at large. We advocate a fully automated
approach based on machine learning: we develop an early warning system that
harnesses account activity traces to predict which accounts are likely to be
compromised in the future and generate suspicious activity. We hypothesize that
this early warning is key for a more timely detection of compromised accounts
and consequently faster remediation. We demonstrate the feasibility and
applicability of the system through an experiment at a large-scale online
service provider using four months of real-world production data encompassing
hundreds of millions of users. We show that - even using only login data to
derive features with low computational cost, and a basic model selection
approach - our classifier can be tuned to achieve good classification precision
when used for forecasting. Our system correctly identifies up to one month in
advance the accounts later flagged as suspicious with precision, recall, and
false positive rates that indicate the mechanism is likely to prove valuable in
operational settings to support additional layers of defense.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08629</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08639</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast binary embeddings, and quantized compressed sensing with structured
  matrices</dc:title>
 <dc:creator>Huynh, Thang</dc:creator>
 <dc:creator>Saab, Rayan</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Electrical Engineering and Systems Science - Signal Processing</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  This paper deals with two related problems, namely distance-preserving binary
embeddings and quantization for compressed sensing . First, we propose fast
methods to replace points from a subset $\mathcal{X} \subset \mathbb{R}^n$,
associated with the Euclidean metric, with points in the cube $\{\pm 1\}^m$ and
we associate the cube with a pseudo-metric that approximates Euclidean distance
among points in $\mathcal{X}$. Our methods rely on quantizing fast
Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and
partial circulant ensembles, both of which admit fast transforms. Our
quantization methods utilize noise-shaping, and include Sigma-Delta schemes and
distributed noise-shaping schemes. The resulting approximation errors decay
polynomially and exponentially fast in $m$, depending on the embedding method.
This dramatically outperforms the current decay rates associated with binary
embeddings and Hamming distances. Additionally, it is the first such binary
embedding result that applies to fast Johnson-Lindenstrauss maps while
preserving $\ell_2$ norms.
  Second, we again consider noise-shaping schemes, albeit this time to quantize
compressed sensing measurements arising from bounded orthonormal ensembles and
partial circulant matrices. We show that these methods yield a reconstruction
error that again decays with the number of measurements (and bits), when using
convex optimization for reconstruction. Specifically, for Sigma-Delta schemes,
the error decays polynomially in the number of measurements, and it decays
exponentially for distributed noise-shaping schemes based on beta encoding.
These results are near optimal and the first of their kind dealing with bounded
orthonormal systems.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08639</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08640</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Transparent Model Distillation</dc:title>
 <dc:creator>Tan, Sarah</dc:creator>
 <dc:creator>Caruana, Rich</dc:creator>
 <dc:creator>Hooker, Giles</dc:creator>
 <dc:creator>Gordo, Albert</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Model distillation was originally designed to distill knowledge from a large,
complex teacher model to a faster, simpler student model without significant
loss in prediction accuracy. We investigate model distillation for another goal
-- transparency -- investigating if fully-connected neural networks can be
distilled into models that are transparent or interpretable in some sense. Our
teacher models are multilayer perceptrons, and we try two types of student
models: (1) tree-based generalized additive models (GA2Ms), a type of boosted,
short tree (2) gradient boosted trees (GBTs). More transparent student models
are forthcoming. Our results are not yet conclusive. GA2Ms show some promise
for distilling binary classification teachers, but not yet regression. GBTs are
not &quot;directly&quot; interpretable but may be promising for regression teachers. GA2M
models may provide a computationally viable alternative to additive
decomposition methods for global function approximation.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08640</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08641</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Knowledge Graph Embedding with Multiple Relation Projections</dc:title>
 <dc:creator>Do, Kien</dc:creator>
 <dc:creator>Tran, Truyen</dc:creator>
 <dc:creator>Venkatesh, Svetha</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  Knowledge graphs contain rich relational structures of the world, and thus
complement data-driven machine learning in heterogeneous data. One of the most
effective methods in representing knowledge graphs is to embed symbolic
relations and entities into continuous spaces, where relations are
approximately linear translation between projected images of entities in the
relation space. However, state-of-the-art relation projection methods such as
TransR, TransD or TransSparse do not model the correlation between relations,
and thus are not scalable to complex knowledge graphs with thousands of
relations, both in computational demand and in statistical robustness. To this
end we introduce TransF, a novel translation-based method which mitigates the
burden of relation projection by explicitly modeling the basis subspaces of
projection matrices. As a result, TransF is far more light weight than the
existing projection methods, and is robust when facing a high number of
relations. Experimental results on the canonical link prediction task show that
our proposed model outperforms competing rivals by a large margin and achieves
state-of-the-art performance. Especially, TransF improves by 9%/5% in the
head/tail entity prediction task for N-to-1/1-to-N relations over the best
performing translation-based method.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08641</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08648</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Pilot-Streaming: A Stream Processing Framework for High-Performance
  Computing</dc:title>
 <dc:creator>Luckow, Andre</dc:creator>
 <dc:creator>Chantzialexiou, George</dc:creator>
 <dc:creator>Jha, Shantenu</dc:creator>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  An increasing number of scientific applications rely on stream processing for
generating timely insights from data feeds of scientific instruments,
simulations, and Internet-of-Thing (IoT) sensors. The development of streaming
applications is a complex task and requires the integration of heterogeneous,
distributed infrastructure, frameworks, middleware and application components.
Different application components are often written in different languages using
different abstractions and frameworks. Often, additional components, such as a
message broker (e.g. Kafka), are required to decouple data production and
consumptions and avoiding issues, such as back-pressure. Streaming applications
may be extremely dynamic due to factors, such as variable data rates caused by
the data source, adaptive sampling techniques or network congestions, variable
processing loads caused by usage of different machine learning algorithms. As a
result application-level resource management that can respond to changes in one
of these factors is critical. We propose Pilot-Streaming, a framework for
supporting streaming frameworks, applications and their resource management
needs on HPC infrastructure. Pilot-Streaming is based on the Pilot-Job concept
and enables developers to manage distributed computing and data resources for
complex streaming applications. It enables applications to dynamically respond
to resource requirements by adding/removing resources at runtime. This
capability is critical for balancing complex streaming pipelines. To address
the complexity in developing and characterization of streaming applications, we
present the Streaming Mini- App framework, which supports different plug-able
algorithms for data generation and processing, e.g., for reconstructing light
source images using different techniques. We utilize the Mini-App framework to
conduct an evaluation of Pilot-Streaming.
</dc:description>
 <dc:description>Comment: 12 pages</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08648</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08650</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ontology-based Fuzzy Markup Language Agent for Student and Robot
  Co-Learning</dc:title>
 <dc:creator>Lee, Chang-Shing</dc:creator>
 <dc:creator>Wang, Mei-Hui</dc:creator>
 <dc:creator>Huang, Tzong-Xiang</dc:creator>
 <dc:creator>Chen, Li-Chung</dc:creator>
 <dc:creator>Huang, Yung-Ching</dc:creator>
 <dc:creator>Yang, Sheng-Chi</dc:creator>
 <dc:creator>Tseng, Chien-Hsun</dc:creator>
 <dc:creator>Hung, Pi-Hsia</dc:creator>
 <dc:creator>Kubota, Naoyuki</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  An intelligent robot agent based on domain ontology, machine learning
mechanism, and Fuzzy Markup Language (FML) for students and robot co-learning
is presented in this paper. The machine-human co-learning model is established
to help various students learn the mathematical concepts based on their
learning ability and performance. Meanwhile, the robot acts as a teacher's
assistant to co-learn with children in the class. The FML-based knowledge base
and rule base are embedded in the robot so that the teachers can get feedback
from the robot on whether students make progress or not. Next, we inferred
students' learning performance based on learning content's difficulty and
students' ability, concentration level, as well as teamwork sprit in the class.
Experimental results show that learning with the robot is helpful for
disadvantaged and below-basic children. Moreover, the accuracy of the
intelligent FML-based agent for student learning is increased after machine
learning mechanism.
</dc:description>
 <dc:description>Comment: This paper is submitted to IEEE WCCI 2018 Conference for review</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08650</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08652</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reducing Binary Quadratic Forms for More Scalable Quantum Annealing</dc:title>
 <dc:creator>Hahn, Georg</dc:creator>
 <dc:creator>Djidjev, Hristo</dc:creator>
 <dc:subject>Quantum Physics</dc:subject>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  Recent advances in the development of commercial quantum annealers such as
the D-Wave 2X allow solving NP-hard optimization problems that can be expressed
as quadratic unconstrained binary programs. However, the relatively small
number of available qubits (around 1000 for the D-Wave 2X quantum annealer)
poses a severe limitation to the range of problems that can be solved. This
paper explores the suitability of preprocessing methods for reducing the sizes
of the input programs and thereby the number of qubits required for their
solution on quantum computers. Such methods allow us to determine the value of
certain variables that hold in either any optimal solution (called strong
persistencies) or in at least one optimal solution (weak persistencies). We
investigate preprocessing methods for two important NP-hard graph problems, the
computation of a maximum clique and a maximum cut in a graph. We show that the
identification of strong and weak persistencies for those two optimization
problems is very instance-specific, but can lead to substantial reductions in
the number of variables.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08652</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08660</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Context Models for OOV Word Translation in Low-Resource Languages</dc:title>
 <dc:creator>Liu, Angli</dc:creator>
 <dc:creator>Kirchhoff, Katrin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>68T50</dc:subject>
 <dc:description>  Out-of-vocabulary word translation is a major problem for the translation of
low-resource languages that suffer from a lack of parallel training data. This
paper evaluates the contributions of target-language context models towards the
translation of OOV words, specifically in those cases where OOV translations
are derived from external knowledge sources, such as dictionaries. We develop
both neural and non-neural context models and evaluate them within both
phrase-based and self-attention based neural machine translation systems. Our
results show that neural language models that integrate additional context
beyond the current sentence are the most effective in disambiguating possible
OOV word translations. We present an efficient second-pass lattice-rescoring
method for wide-context neural language models and demonstrate performance
improvements over state-of-the-art self-attention based neural MT systems in
five out of six low-resource language pairs.
</dc:description>
 <dc:description>Comment: to be published at AMTA 2018</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08660</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08664</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Linear Complexity and Autocorrelation of two Classes of New Interleaved
  Sequences of Period $2N$</dc:title>
 <dc:creator>Zhang, Shidong</dc:creator>
 <dc:creator>Yan, Tongjiang</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  The autocorrelation and the linear complexity of a key stream sequence in a
stream cipher are important cryptographic properties. Many sequences with these
good properties have interleaved structure, three classes of binary sequences
of period $4N$ with optimal autocorrelation values have been constructed by
Tang and Gong based on interleaving certain kinds of sequences of period $N$.
In this paper, we use the interleaving technique to construct a binary sequence
with the optimal autocorrelation of period $2N$, then we calculate its
autocorrelation values and its distribution, and give a lower bound of linear
complexity. Results show that these sequences have low autocorrelation and the
linear complexity satisfies the requirements of cryptography.
</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08664</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08676</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Neural Algebra of Classifiers</dc:title>
 <dc:creator>Cruz, Rodrigo Santa</dc:creator>
 <dc:creator>Fernando, Basura</dc:creator>
 <dc:creator>Cherian, Anoop</dc:creator>
 <dc:creator>Gould, Stephen</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  The world is fundamentally compositional, so it is natural to think of visual
recognition as the recognition of basic visually primitives that are composed
according to well-defined rules. This strategy allows us to recognize unseen
complex concepts from simple visual primitives. However, the current trend in
visual recognition follows a data greedy approach where huge amounts of data
are required to learn models for any desired visual concept. In this paper, we
build on the compositionality principle and develop an &quot;algebra&quot; to compose
classifiers for complex visual concepts. To this end, we learn neural network
modules to perform boolean algebra operations on simple visual classifiers.
Since these modules form a complete functional set, a classifier for any
complex visual concept defined as a boolean expression of primitives can be
obtained by recursively applying the learned modules, even if we do not have a
single training sample. As our experiments show, using such a framework, we can
compose classifiers for complex visual concepts outperforming standard
baselines on two well-known visual recognition benchmarks. Finally, we present
a qualitative analysis of our method and its properties.
</dc:description>
 <dc:description>Comment: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08676</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08682</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stop talking to me -- a communication-avoiding ADER-DG realisation</dc:title>
 <dc:creator>Charrier, Dominic E.</dc:creator>
 <dc:creator>Weinzierl, Tobias</dc:creator>
 <dc:subject>Computer Science - Mathematical Software</dc:subject>
 <dc:subject>Computer Science - Numerical Analysis</dc:subject>
 <dc:description>  We present a communication- and data-sensitive formulation of ADER-DG for
hyperbolic differential equation systems. Sensitive here has multiple flavours:
First, the formulation reduces the persistent memory footprint. This reduces
pressure on the memory subsystem. Second, the formulation realises the
underlying predictor-corrector scheme with single-touch semantics, i.e., each
degree of freedom is read on average only once per time step from the main
memory. This reduces communication through the memory controllers. Third, the
formulation breaks up the tight coupling of the explicit time stepping's
algorithmic steps to mesh traversals. This averages out data access peaks.
Different operations and algorithmic steps are ran on different grid entities.
Finally, the formulation hides distributed memory data transfer behind the
computation aligned with the mesh traversal. This reduces pressure on the
machine interconnects. All techniques applied by our formulation are elaborated
by means of a rigorous task formalism. They break up ADER-DG's tight causal
coupling of compute steps and can be generalised to other predictor-corrector
schemes.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08682</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08693</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improved Finite Blocklength Converses for Slepian-Wolf Coding via Linear
  Programming</dc:title>
 <dc:creator>Jose, Sharu Theresa</dc:creator>
 <dc:creator>Kulkarni, Ankur A.</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Mathematics - Optimization and Control</dc:subject>
 <dc:subject>94A15, 68P30, 90C05, 90C26</dc:subject>
 <dc:subject>H.1.1</dc:subject>
 <dc:subject>G.1.6</dc:subject>
 <dc:description>  A new finite blocklength converse for the Slepian- Wolf coding problem is
presented which significantly improves on the best known converse for this
problem, due to Miyake and Kanaya [2]. To obtain this converse, an extension of
the linear programming (LP) based framework for finite blocklength point-
to-point coding problems from [3] is employed. However, a direct application of
this framework demands a complicated analysis for the Slepian-Wolf problem. An
analytically simpler approach is presented wherein LP-based finite blocklength
converses for this problem are synthesized from point-to-point lossless source
coding problems with perfect side-information at the decoder. New finite
blocklength metaconverses for these point-to-point problems are derived by
employing the LP-based framework, and the new converse for Slepian-Wolf coding
is obtained by an appropriate combination of these converses.
</dc:description>
 <dc:description>Comment: under review with the IEEE Transactions on Information Theory</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08693</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08694</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>PDNet: Semantic Segmentation integrated with a Primal-Dual Network for
  Document binarization</dc:title>
 <dc:creator>Ayyalasomayajula, Kalyan Ram</dc:creator>
 <dc:creator>Malmberg, Filip</dc:creator>
 <dc:creator>Brun, Anders</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Binarization of digital documents is the task of classifying each pixel in an
image of the document as belonging to the background (parchment/paper) or
foreground (text/ink). Historical documents are often subject to degradations,
that make the task challenging. In the current work a deep neural network
architecture is proposed that combines a fully convolutional network with an
unrolled primal-dual network that can be trained end-to-end in order to achieve
state of the art binarization on four out of seven datasets. Document
binarization is formulated as a energy minimization problem. A fully
convolutional neural network is trained for semantic labeling of pixels to
provide class labeling cost associated with each pixel. This cost estimate is
refined along the edges to compensate for any over or under estimation of the
under represented fore-ground class using a primal-dual approach. We provide
necessary overview on proximal operator that facilitates theoretical
underpinning in order to train a primal-dual network using a gradient descent
algorithm. Numerical instabilities encountered due to the recurrent nature of
primal-dual approach are handled. We provide experimental results on document
binarization competition dataset along with network changes and hyperparameter
tuning required for stability and performance of the network. The network when
pre-trained on synthetic dataset performs better as per the competition
metrics.
</dc:description>
 <dc:description>Comment: Under consideration for Pattern Recognition Letters Special Issue on
  Graphonomics for e-citizens: e-health, e-society, e-education 10 pages, 7
  figures, 2 tables</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08694</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08702</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improving Bi-directional Generation between Different Modalities with
  Variational Autoencoders</dc:title>
 <dc:creator>Suzuki, Masahiro</dc:creator>
 <dc:creator>Nakayama, Kotaro</dc:creator>
 <dc:creator>Matsuo, Yutaka</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  We investigate deep generative models that can exchange multiple modalities
bi-directionally, e.g., generating images from corresponding texts and vice
versa. A major approach to achieve this objective is to train a model that
integrates all the information of different modalities into a joint
representation and then to generate one modality from the corresponding other
modality via this joint representation. We simply applied this approach to
variational autoencoders (VAEs), which we call a joint multimodal variational
autoencoder (JMVAE). However, we found that when this model attempts to
generate a large dimensional modality missing at the input, the joint
representation collapses and this modality cannot be generated successfully.
Furthermore, we confirmed that this difficulty cannot be resolved even using a
known solution. Therefore, in this study, we propose two models to prevent this
difficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that
these methods can prevent the difficulty above and that they generate
modalities bi-directionally with equal or higher likelihood than conventional
VAE methods, which generate in only one direction. Moreover, we confirm that
these methods can obtain the joint representation appropriately, so that they
can generate various variations of modality by moving over the joint
representation or changing the value of another modality.
</dc:description>
 <dc:description>Comment: Updated version of arXiv:1611.01891</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08702</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08704</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Event-triggered stabilization of disturbed linear systems over digital
  channels</dc:title>
 <dc:creator>Khojasteh, Mohammad Javad</dc:creator>
 <dc:creator>Hedayatpour, Mojtaba</dc:creator>
 <dc:creator>Cortes, Jorge</dc:creator>
 <dc:creator>Franceschetti, Massimo</dc:creator>
 <dc:subject>Mathematics - Optimization and Control</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>Computer Science - Systems and Control</dc:subject>
 <dc:description>  We present an event-triggered control strategy for stabilizing a scalar,
continuous-time, time-invariant, linear system over a digital communication
channel having bounded delay, and in the presence of bounded system
disturbance. We propose an encoding-decoding scheme, and determine lower bounds
on the packet size and on the information transmission rate which are
sufficient for stabilization. We show that for small values of the delay, the
timing information implicit in the triggering events is enough to stabilize the
system with any positive rate. In contrast, when the delay increases beyond a
critical threshold, the timing information alone is not enough to stabilize the
system and the transmission rate begins to increase. Finally, large values of
the delay require transmission rates higher than what prescribed by the classic
data-rate theorem. The results are numerically validated using a linearized
model of an inverted pendulum.
</dc:description>
 <dc:description>Comment: To appear in the 52th Annual Conference on Information Sciences and
  Systems (CISS), Princeton, New Jersey, USA, 2018</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08704</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08706</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Cloud Detection From RGB Color Remote Sensing Images With Deep Pyramid
  Networks</dc:title>
 <dc:creator>Ozkan, Savas</dc:creator>
 <dc:creator>Efendioglu, Mehmet</dc:creator>
 <dc:creator>Demirpolat, Caner</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Cloud detection from remotely observed data is a critical pre-processing step
for various remote sensing applications. In particular, this problem becomes
even harder for RGB color images, since there is no distinct spectral pattern
for clouds, which is directly separable from the Earth surface. In this paper,
we adapt a deep pyramid network (DPN) to tackle this problem. For this purpose,
the network is enhanced with a pre-trained parameter model at the encoder
layer. Moreover, the method is able to obtain accurate pixel-level segmentation
and classification results from a set of noisy labeled RGB color images. In
order to demonstrate the superiority of the method, we collect and label data
with the corresponding cloud/non-cloudy masks acquired from low-orbit Gokturk-2
and RASAT satellites. The experimental results validates that the proposed
method outperforms several baselines even for hard cases (e.g. snowy mountains)
that are perceptually difficult to distinguish by human eyes.
</dc:description>
 <dc:description>Comment: Submitted to IGARSS 2018</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08706</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08707</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Logic characterisation of p/q-recognisable sets</dc:title>
 <dc:creator>Marsault, Victor</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Computer Science - Formal Languages and Automata Theory</dc:subject>
 <dc:description>  Let $\frac{p}{q}$ be a rational number. Numeration in base $\frac{p}{q}$ is
defined by a function that evaluates each finite word over
$A_p=\{0,1,\ldots,{p-1}\}$ to a rational number in some set $N_{\frac{p}{q}}$.
In particular, $N_{\frac{p}{q}}$ contains all integers and the literature on
base $\frac{p}{q}$ usually focuses on the set of words that are evaluated to
integers; it is a rather chaotic language which is not context-free. On the
contrary, we study here the subsets of $(N_{\frac{p}{q}})^d$ that are
$\frac{p}{q}$-recognisable, i.e. realised by finite automata over $(A_p)^d$.
First, we give a characterisation of these sets as those definable in a
first-order logic, similar to the one given by the B\&quot;uchi-Bruy\`ere Theorem
for integer bases. Second, we show that the order relation and the modulo-$q$
operator are not $\frac{p}{q}$-recognisable.
</dc:description>
 <dc:description>Comment: 15 pages</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08707</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08709</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adaptive Lower Bound for Testing Monotonicity on the Line</dc:title>
 <dc:creator>Belovs, Aleksandrs</dc:creator>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  In the property testing model, the task is to distinguish objects possessing
some property from the objects that are far from it. One of such properties is
monotonicity, when the objects are functions from one poset to another. It is
an active area of research.
  Recently, Pallavoor, Raskhodnikova and Varma (ITCS'17) proposed an
$\varepsilon$-tester for monotonicity of a function $f\colon [n]\to[r]$, whose
complexity depends on the size of the range as $O(\frac{\log r}{\varepsilon})$.
In this paper, we prove a nearly matching lower bound of $\Omega(\frac{\log
r}{\log \log r})$ for adaptive two-sided testers. Additionally, we give an
alternative proof of the $\Omega(\varepsilon^{-1}d\log n -
\varepsilon^{-1}\log\varepsilon^{-1})$ lower bound for testing monotonicity on
the hypergrid $[n]^d$ due to Chackrabarty and Seshadhri (RANDOM'13).
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08709</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08710</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Enhancing Byzantine fault tolerance using MD5 checksum and delay
  variation in Cloud services</dc:title>
 <dc:creator>Sathya, C</dc:creator>
 <dc:creator>Agilan, S</dc:creator>
 <dc:creator>Aruna, A G</dc:creator>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  Cloud computing management are beyond typical human narratives. However if a
virtual system is not effectively designed to tolerate Byzantine faults, it
could lead to a faultily executed mission rather than a cloud crash. The cloud
could recover from the crash but it could not recover from the loss of
credibility. Moreover no amount of replication or fault handling measures can
be helpful in facing a Byzantine fault unless the virtual system is designed to
detect, tolerate and eliminate such faults. However research efforts that are
made to address Byzantine faults have not provided convincing solutions vastly
due to their limited capabilities in detecting the Byzantine faults. As a
result, in this paper the Cloud system is modeled as a discrete system to
determine the virtual system behavior at varying time intervals. A delay
variation variable as a measure of deviation for the expected processing delay
associated with the virtual nodes takes values from the set of P {low, normal,
high, extreme}. Similarly, a check sum error variable which is even computed
for intra nodes that have no attachment to TCP/IP stack takes values from the
set of P {no error, error}. These conditions are then represented by the
occurrence of faulty events that cause specific component mode transition from
fail safe to fail-stop or byzantine prone.
</dc:description>
 <dc:description>Comment: 22 pages</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08710</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08712</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Classification of sparsely labeled spatio-temporal data through
  semi-supervised adversarial learning</dc:title>
 <dc:creator>Mirchev, Atanas</dc:creator>
 <dc:creator>Ahmadi, Seyed-Ahmad</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  In recent years, Generative Adversarial Networks (GAN) have emerged as a
powerful method for learning the mapping from noisy latent spaces to realistic
data samples in high-dimensional space. So far, the development and application
of GANs have been predominantly focused on spatial data such as images. In this
project, we aim at modeling of spatio-temporal sensor data instead, i.e.
dynamic data over time. The main goal is to encode temporal data into a global
and low-dimensional latent vector that captures the dynamics of the
spatio-temporal signal. To this end, we incorporate auto-regressive RNNs,
Wasserstein GAN loss, spectral norm weight constraints and a semi-supervised
learning scheme into InfoGAN, a method for retrieval of meaningful latents in
adversarial learning. To demonstrate the modeling capability of our method, we
encode full-body skeletal human motion from a large dataset representing 60
classes of daily activities, recorded in a multi-Kinect setup. Initial results
indicate competitive classification performance of the learned latent
representations, compared to direct CNN/RNN inference. In future work, we plan
to apply this method on a related problem in the medical domain, i.e. on
recovery of meaningful latents in gait analysis of patients with vertigo and
balance disorders.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08712</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08718</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Invariant Checking of NRA Transition Systems via Incremental Reduction
  to LRA with EUF</dc:title>
 <dc:creator>Cimatti, Alessandro</dc:creator>
 <dc:creator>Griggio, Alberto</dc:creator>
 <dc:creator>Irfan, Ahmed</dc:creator>
 <dc:creator>Roveri, Marco</dc:creator>
 <dc:creator>Sebastiani, Roberto</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:description>  Model checking invariant properties of designs, represented as transition
systems, with non-linear real arithmetic (NRA), is an important though very
hard problem. On the one hand NRA is a hard-to-solve theory; on the other hand
most of the powerful model checking techniques lack support for NRA. In this
paper, we present a counterexample-guided abstraction refinement (CEGAR)
approach that leverages linearization techniques from differential calculus to
enable the use of mature and efficient model checking algorithms for transition
systems on linear real arithmetic (LRA) with uninterpreted functions (EUF). The
results of an empirical evaluation confirm the validity and potential of this
approach.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08718</dc:identifier>
 <dc:identifier>doi:10.1007/978-3-662-54577-5_4</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08720</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Identifying single influential publications in a research field: New
  analysis opportunities of the CRExplorer</dc:title>
 <dc:creator>Thor, Andreas</dc:creator>
 <dc:creator>Bornmann, Lutz</dc:creator>
 <dc:creator>Marx, Werner</dc:creator>
 <dc:creator>Mutz, R&#xfc;diger</dc:creator>
 <dc:subject>Computer Science - Digital Libraries</dc:subject>
 <dc:description>  Reference Publication Year Spectroscopy (RPYS) has been developed for
identifying the cited references (CRs) with the greatest influence in a given
paper set (mostly sets of papers on certain topics or fields). The program
CRExplorer (see www.crexplorer.net) was specifically developed by Thor, Marx,
Leydesdorff, and Bornmann (2016a, 2016b) for applying RPYS to publication sets
downloaded from Scopus or Web of Science. In this study, we present some
advanced methods which have been newly developed for CRExplorer. These methods
are able to identify and characterize the CRs which have been influential
across a longer period (many citing years). The new methods are demonstrated in
this study using all the papers published in Scientometrics between 1978 and
2016. The indicators N_TOP50, N_TOP25, and N_TOP10 can be used to identify
those CRswhich belong to the 50%, 25%, or 10% most frequently cited
publications (CRs) over many citing publication years. In the Scientometrics
dataset, for example, Lotka's (1926) paper on the distribution of scientific
productivity belongs to the top 10% publications (CRs) in 36 citing years.
Furthermore, the new version of CRExplorer analyzes the impact sequence of CRs
across citing years. CRs can have below average (-), average (0), or above
average (+) impact in citing years. The sequence (e.g. 00++---0--00) is used by
the program to identify papers with typical impact distributions. For example,
CRs can have early, but not late impact (&quot;hot papers&quot;, e.g. +++---) or vice
versa (&quot;sleeping beauties&quot;, e.g. ---0000---++).
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08720</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08723</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Satisfiability Modulo Transcendental Functions via Incremental
  Linearization</dc:title>
 <dc:creator>Cimatti, Alessandro</dc:creator>
 <dc:creator>Griggio, Alberto</dc:creator>
 <dc:creator>Irfan, Ahmed</dc:creator>
 <dc:creator>Roveri, Marco</dc:creator>
 <dc:creator>Sebastiani, Roberto</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:description>  In this paper we present an abstraction-refinement approach to Satisfiability
Modulo the theory of transcendental functions, such as exponentiation and
trigonometric functions. The transcendental functions are represented as
uninterpreted in the abstract space, which is described in terms of the
combined theory of linear arithmetic on the rationals with uninterpreted
functions, and are incrementally axiomatized by means of upper- and
lower-bounding piecewise-linear functions. Suitable numerical techniques are
used to ensure that the abstractions of the transcendental functions are sound
even in presence of irrationals. Our experimental evaluation on benchmarks from
verification and mathematics demonstrates the potential of our approach,
showing that it compares favorably with delta-satisfiability /interval
propagation and methods based on theorem proving.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08723</dc:identifier>
 <dc:identifier>doi:10.1007/978-3-319-63046-5_7</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08730</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>What is Large in Large-Scale? A Taxonomy of Scale for Agile Software
  Development</dc:title>
 <dc:creator>Dings&#xf8;yr, Torgeir</dc:creator>
 <dc:creator>F&#xe6;gri, Tor Erlend</dc:creator>
 <dc:creator>Itkonen, Juha</dc:creator>
 <dc:subject>Computer Science - Software Engineering</dc:subject>
 <dc:description>  Positive experience of agile development methods in smaller projects has
created interest in the applicability of such methods in larger scale projects.
However, there is a lack of conceptual clarity regarding what large-scale agile
software development is. This inhibits effective collaboration and progress in
the research area. In this paper, we suggest a taxonomy of scale for agile
software development projects that has the potential to clarify what topics
researchers are studying and ease discussion of research priorities.
</dc:description>
 <dc:description>Comment: Postprint of article published at PROFES2014</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08730</dc:identifier>
 <dc:identifier>Dings{\o}yr, T., F{\ae}gri, T., &amp; Itkonen, J. (2014). What Is
  Large in Large-Scale? A Taxonomy of Scale for Agile Software Development.
  Product-Focused Software Process Improvement Vol. 8892, pp. 273-276: Springer</dc:identifier>
 <dc:identifier>doi:10.1007/978-3-319-13835-0_20</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08737</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lattice-Based Group Signatures: Achieving Full Dynamicity (and
  Deniability) with Ease</dc:title>
 <dc:creator>Ling, San</dc:creator>
 <dc:creator>Nguyen, Khoa</dc:creator>
 <dc:creator>Wang, Huaxiong</dc:creator>
 <dc:creator>Xu, Yanhong</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:subject>CS-CR</dc:subject>
 <dc:description>  In this work, we provide the first lattice-based group signature that offers
full dynamicity (i.e., users have the flexibility in joining and leaving the
group), and thus, resolve a prominent open problem posed by previous works.
Moreover, we achieve this non-trivial feat in a relatively simple manner.
Starting with Libert et al.'s fully static construction (Eurocrypt 2016) -
which is arguably the most efficient lattice-based group signature to date, we
introduce simple-but-insightful tweaks that allow to upgrade it directly into
the fully dynamic setting. More startlingly, our scheme even produces slightly
shorter signatures than the former, thanks to an adaptation of a technique
proposed by Ling et al. (PKC 2013), allowing to prove inequalities in
zero-knowledge. Our design approach consists of upgrading Libert et al.'s
static construction (EUROCRYPT 2016) - which is arguably the most efficient
lattice-based group signature to date - into the fully dynamic setting.
Somewhat surprisingly, our scheme produces slightly shorter signatures than the
former, thanks to a new technique for proving inequality in zero-knowledge
without relying on any inequality check. The scheme satisfies the strong
security requirements of Bootle et al.'s model (ACNS 2016), under the Short
Integer Solution (SIS) and the Learning With Errors (LWE) assumptions.
  Furthermore, we demonstrate how to equip the obtained group signature scheme
with the deniability functionality in a simple way. This attractive
functionality, put forward by Ishida et al. (CANS 2016), enables the tracing
authority to provide an evidence that a given user is not the owner of a
signature in question. In the process, we design a zero-knowledge protocol for
proving that a given LWE ciphertext does not decrypt to a particular message.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08737</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08747</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Weakly Supervised Object Detection with Pointwise Mutual Information</dc:title>
 <dc:creator>Grzeszick, Rene</dc:creator>
 <dc:creator>Sudholt, Sebastian</dc:creator>
 <dc:creator>Fink, Gernot A.</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  In this work a novel approach for weakly supervised object detection that
incorporates pointwise mutual information is presented. A fully convolutional
neural network architecture is applied in which the network learns one filter
per object class. The resulting feature map indicates the location of objects
in an image, yielding an intuitive representation of a class activation map.
While traditionally such networks are learned by a softmax or binary logistic
regression (sigmoid cross-entropy loss), a learning approach based on a cosine
loss is introduced. A pointwise mutual information layer is incorporated in the
network in order to project predictions and ground truth presence labels in a
non-categorical embedding space. Thus, the cosine loss can be employed in this
non-categorical representation. Besides integrating image level annotations, it
is shown how to integrate point-wise annotations using a Spatial Pyramid
Pooling layer. The approach is evaluated on the VOC2012 dataset for
classification, point localization and weakly supervised bounding box
localization. It is shown that the combination of pointwise mutual information
and a cosine loss eases the learning process and thus improves the accuracy.
The integration of coarse point-wise localizations further improves the results
at minimal annotation costs.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08747</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08754</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>For Whom the Bell Trolls: Troll Behaviour in the Twitter Brexit Debate</dc:title>
 <dc:creator>Llewellyn, Clare</dc:creator>
 <dc:creator>Cram, Laura</dc:creator>
 <dc:creator>Favero, Adrian</dc:creator>
 <dc:creator>Hill, Robin L.</dc:creator>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:subject>Computer Science - Computers and Society</dc:subject>
 <dc:description>  In a review into automated and malicious activity Twitter released a list of
accounts that they believed were connected to state sponsored manipulation of
the 2016 American Election. This list details 2,752 accounts Twitter believed
to be controlled by Russian operatives. In the absence of a similar list of
operatives active within the debate on the 2016 UK referendum on membership of
the European Union (Brexit) we investigated the behaviour of the same American
Election focused accounts in the production of content related to the UK-EU
referendum. We found that within our dataset we had Brexit-related content from
419 of these accounts, leading to 3,485 identified tweets gathered between the
29th August 2015 and 3rd October 2017. The behaviour of the accounts altered
radically on the day of the referendum, shifting from generalised disruptive
tweeting to retweeting each other in order to amplify content produced by other
troll accounts. We also demonstrate that, while these accounts are, in general,
designed to resemble American citizens, accounts created in 2016 often
contained German locations and terms in the user profiles.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08754</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08757</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Safe Exploration in Continuous Action Spaces</dc:title>
 <dc:creator>Dalal, Gal</dc:creator>
 <dc:creator>Dvijotham, Krishnamurthy</dc:creator>
 <dc:creator>Vecerik, Matej</dc:creator>
 <dc:creator>Hester, Todd</dc:creator>
 <dc:creator>Paduraru, Cosmin</dc:creator>
 <dc:creator>Tassa, Yuval</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:description>  We address the problem of deploying a reinforcement learning (RL) agent on a
physical system such as a datacenter cooling unit or robot, where critical
constraints must never be violated. We show how to exploit the typically smooth
dynamics of these systems and enable RL algorithms to never violate constraints
during learning. Our technique is to directly add to the policy a safety layer
that analytically solves an action correction formulation per each state. The
novelty of obtaining an elegant closed-form solution is attained due to a
linearized model, learned on past trajectories consisting of arbitrary actions.
This is to mimic the real-world circumstances where data logs were generated
with a behavior policy that is implausible to describe mathematically; such
cases render the known safety-aware off-policy methods inapplicable. We
demonstrate the efficacy of our approach on new representative physics-based
environments, and prevail where reward shaping fails by maintaining zero
constraint violations.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08757</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08760</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Comparison of Visualisation Methods for Disambiguating Verbal Requests
  in Human-Robot Interaction</dc:title>
 <dc:creator>Sibirtseva, Elena</dc:creator>
 <dc:creator>Kontogiorgos, Dimosthenis</dc:creator>
 <dc:creator>Nykvist, Olov</dc:creator>
 <dc:creator>Karaoguz, Hakan</dc:creator>
 <dc:creator>Leite, Iolanda</dc:creator>
 <dc:creator>Gustafson, Joakim</dc:creator>
 <dc:creator>Kragic, Danica</dc:creator>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:subject>Computer Science - Human-Computer Interaction</dc:subject>
 <dc:description>  Picking up objects requested by a human user is a common task in human-robot
interaction. When multiple objects match the user's verbal description, the
robot needs to clarify which object the user is referring to before executing
the action. Previous research has focused on perceiving user's multimodal
behaviour to complement verbal commands or minimising the number of follow up
questions to reduce task time. In this paper, we propose a system for reference
disambiguation based on visualisation and compare three methods to disambiguate
natural language instructions. In a controlled experiment with a YuMi robot, we
investigated real-time augmentations of the workspace in three conditions --
mixed reality, augmented reality, and a monitor as the baseline -- using
objective measures such as time and accuracy, and subjective measures like
engagement, immersion, and display interference. Significant differences were
found in accuracy and engagement between the conditions, but no differences
were found in task time. Despite the higher error rates in the mixed reality
condition, participants found that modality more engaging than the other two,
but overall showed preference for the augmented reality condition over the
monitor and mixed reality conditions.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08760</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08761</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>White-box methodologies, programming abstractions and libraries</dc:title>
 <dc:creator>Ha, Phuong Hoai</dc:creator>
 <dc:creator>Tran, Vi Ngoc-Nha</dc:creator>
 <dc:creator>Umar, Ibrahim</dc:creator>
 <dc:creator>Atalar, Aras</dc:creator>
 <dc:creator>Gidenstam, Anders</dc:creator>
 <dc:creator>Renaud-Goud, Paul</dc:creator>
 <dc:creator>Tsigas, Philippas</dc:creator>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  This deliverable reports the results of white-box methodologies and early
results of the first prototype of libraries and programming abstractions as
available by project month 18 by Work Package 2 (WP2). It reports i) the latest
results of Task 2.2 on white-box methodologies, programming abstractions and
libraries for developing energy-efficient data structures and algorithms and
ii) the improved results of Task 2.1 on investigating and modeling the
trade-off between energy and performance of concurrent data structures and
algorithms. The work has been conducted on two main EXCESS platforms: Intel
platforms with recent Intel multicore CPUs and Movidius Myriad1 platform.
Regarding white-box methodologies, we have devised new relaxed cache-oblivious
models and proposed a new power model for Myriad1 platform and an energy model
for lock-free queues on CPU platforms. For Myriad1 platform, the im- proved
model now considers both computation and data movement cost as well as
architecture and application properties. The model has been evaluated with a
set of micro-benchmarks and application benchmarks. For Intel platforms, we
have generalized the model for concurrent queues on CPU platforms to offer more
flexibility according to the workers calling the data structure (parallel
section sizes of enqueuers and dequeuers are decoupled). Regarding programming
abstractions and libraries, we have continued investigat- ing the trade-offs
between energy consumption and performance of data structures such as
concurrent queues and concurrent search trees based on the early results of
Task 2.1.The preliminary results show that our concurrent trees are faster and
more energy efficient than the state-of-the-art on commodity HPC and embedded
platforms.
</dc:description>
 <dc:description>Comment: Execution Models for Energy-Efficient Computing Systems, an EU
  Project, ID: 611183, 108 pages</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08761</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08764</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Coordinating Knowledge Work in Multi-Team Programs: Findings from a
  Large-Scale Agile Development Program</dc:title>
 <dc:creator>Dings&#xf8;yr, Torgeir</dc:creator>
 <dc:creator>Moe, Nils Brede</dc:creator>
 <dc:creator>Seim, Eva Amdahl</dc:creator>
 <dc:subject>Computer Science - Software Engineering</dc:subject>
 <dc:description>  Software development projects have undergone remarkable changes with the
arrival of agile development methods. While intended for small, self-managing
teams, these methods are increasingly used also for large development programs.
A major challenge in programs is to coordinate the work of many teams, due to
high uncertainty in tasks, a high degree of interdependence between tasks and
because of the large number of people involved. This revelatory case study
focuses on how knowledge work is coordinated in large-scale agile development
programs by providing a rich description of the coordination practices used and
how these practices change over time in a four year development program with 12
development teams. The main findings highlight the role of coordination modes
based on feedback, the use of a number of mechanisms far beyond what is
described in practitioner advice, and finally how coordination practices change
over time. The findings are important to improve the outcome of large
knowledge-based development programs by tailoring coordination practices to
needs and ensuring adjustment over time.
</dc:description>
 <dc:description>Comment: To appear in Project Management Journal</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08764</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08766</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Relational Equivalence Proofs Between Imperative and MapReduce
  Algorithms</dc:title>
 <dc:creator>Beckert, Bernhard</dc:creator>
 <dc:creator>Bingmann, Timo</dc:creator>
 <dc:creator>Kiefer, Moritz</dc:creator>
 <dc:creator>Sanders, Peter</dc:creator>
 <dc:creator>Ulbrich, Mattias</dc:creator>
 <dc:creator>Weigl, Alexander</dc:creator>
 <dc:subject>Computer Science - Logic in Computer Science</dc:subject>
 <dc:subject>Computer Science - Programming Languages</dc:subject>
 <dc:description>  MapReduce frameworks are widely used for the implementation of distributed
algorithms. However, translating imperative algorithms into these frameworks
requires significant structural changes to the algorithm. As the costs of
running faulty algorithms at scale can be severe, it is highly desirable to
verify the correctness of the translation, i.e., to prove that the MapReduce
version is equivalent to the imperative original. We present a novel approach
for proving equivalence between imperative and MapReduce algorithms based on
partitioning the equivalence proof into a sequence of equivalence proofs
between intermediate programs with smaller differences. Our approach is based
on the insight that two kinds of sub-proofs are required: (1) uniform
transformations changing the controlflow structure that are mostly independent
of the particular context in which they are applied; and (2) context-dependent
transformations that are not uniform but that preserve the overall structure
and can be proved correct using coupling invariants. We demonstrate the
feasibility of our approach by evaluating it on two prototypical algorithms
commonly used as examples in MapReduce frameworks: k-means and PageRank. To
carry out the proofs, we use the interactive theorem prover Coq with partial
proof automation. The results show that our approach and its prototypical
implementation based on Coq enables equivalence proofs of non-trivial
algorithms and could be automated to a large degree.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08766</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08771</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Modeling of languages for tensor manipulation</dc:title>
 <dc:creator>Rink, Norman A.</dc:creator>
 <dc:subject>Computer Science - Programming Languages</dc:subject>
 <dc:description>  Numerical applications and, more recently, machine learning applications rely
on high-dimensional data that is typically organized into multi-dimensional
tensors. Many existing frameworks, libraries, and domain-specific languages
support the development of efficient code for manipulating tensors and tensor
expressions. However, such frameworks and languages that are used in practice
often lack formal specifications. The present report formally defines a model
language for expressing tensor operations. The model language is simple and yet
general enough so that it captures the fundamental tensor operations common to
most existing languages and frameworks. It is shown that the given formal
semantics are sensible, in the sense that well-typed programs in the model
language execute correctly, without error. Moreover, an alternative
implementation of the model language is formally defined. The alternative
implementation introduces padding into the storage of tensors, which may
benefit performance on modern hardware platforms. Based on their formal
definitions, the original implementation of the model language and the
implementation with padding are proven equivalent. Finally, some possible
extensions of the presented model language for tensor manipulation are
discussed.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08771</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08779</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Received Signal Strength for Randomly Distributed Molecular Nanonodes</dc:title>
 <dc:creator>Ansari, Rafay Iqbal</dc:creator>
 <dc:creator>Chrysostomou, Chrysostomos</dc:creator>
 <dc:creator>Saeed, Taqwa</dc:creator>
 <dc:creator>Lestas, Marios</dc:creator>
 <dc:creator>Pitsillides, Andreas</dc:creator>
 <dc:subject>Computer Science - Emerging Technologies</dc:subject>
 <dc:description>  We consider nanonodes randomly distributed in a circular area and
characterize the received signal strength when a pair of these nodes employ
molecular communication. Two communication methods are investigated, namely
free diffusion and diffusion with drift. Since the nodes are randomly
distributed, the distance between them can be represented as a random variable,
which results in a stochastic process representation of the received signal
strength. We derive the probability density function of this process for both
molecular communication methods. Specifically for the case of free diffusion we
also derive the cumulative distribution function, which can be used to derive
transmission success probabilities. The presented work constitutes a first step
towards the characterization of the signal to noise ratio in the considered
setting for a number of molecular communication methods.
</dc:description>
 <dc:description>Comment: 6 pages, 6 figures, Nanocom 2017 conference</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08779</dc:identifier>
 <dc:identifier>doi:10.1145/3109453.3109466</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08788</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multivariate normal mixture modeling, clustering and classification with
  the rebmix package</dc:title>
 <dc:creator>Nagode, Marko</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:subject>62H30, 62F10</dc:subject>
 <dc:description>  The rebmix package provides R functions for random univariate and
multivariate finite mixture model generation, estimation, clustering and
classification. The paper is focused on multivariate normal mixture models with
unrestricted variance-covariance matrices. The objective is to show how to
generate datasets for a known number of components, numbers of observations and
component parameters, how to estimate the number of components, component
weights and component parameters and how to predict cluster and class
membership based upon a model trained by the REBMIX algorithm. The accompanying
plotting, bootstrapping and other features of the package are dealt with, too.
For demonstration purpose a multivariate normal dataset with unrestricted
variance-covariance matrices is studied.
</dc:description>
 <dc:description>Comment: 15 pages, 6 figures, R code</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08788</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08797</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Exploiting Multiple Access in Clustered Millimeter Wave Networks: NOMA
  or OMA?</dc:title>
 <dc:creator>Yi, Wenqiang</dc:creator>
 <dc:creator>Liu, Yuanwei</dc:creator>
 <dc:creator>Nallanathan, Arumugam</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  In this paper, we introduce a clustered millimeter wave network with
non-orthogonal multiple access (NOMA), where the base station (BS) is located
at the center of each cluster and all users follow a Poisson Cluster Process.
To provide a realistic directional beamforming, an actual antenna pattern is
deployed at all BSs. We provide a nearest-random scheme, in which near user is
the closest node to the corresponding BS and far user is selected at random, to
appraise the coverage performance and universal throughput of our system. Novel
closed-form expressions are derived under a loose network assumption. Moreover,
we present several Monte Carlo simulations and numerical results, which show
that: 1) NOMA outperforms orthogonal multiple access regarding the system rate;
2) the coverage probability is proportional to the number of possible NOMA
users and a negative relationship with the variance of intra-cluster receivers;
and 3) an optimal number of the antenna elements is existed for maximizing the
system throughput.
</dc:description>
 <dc:description>Comment: This paper has been accepted by IEEE International Conference on
  Communications (ICC), May, USA, 2018. Please cite the format version of this
  paper</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08797</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08801</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Modeling and Analysis of MmWave Communications in Cache-enabled HetNets</dc:title>
 <dc:creator>Yi, Wenqiang</dc:creator>
 <dc:creator>Liu, Yuanwei</dc:creator>
 <dc:creator>Nallanathan, Arumugam</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  In this paper, we consider a novel cache-enabled heterogeneous network
(HetNet), where macro base stations (BSs) with traditional sub-6 GHz are
overlaid by dense millimeter wave (mmWave) pico BSs. These two-tier BSs, which
are modeled as two independent homogeneous Poisson Point Processes, cache
multimedia contents following the popularity rank. High-capacity backhauls are
utilized between macro BSs and the core server. A maximum received power
strategy is introduced for deducing novel algorithms of the success probability
and area spectral efficiency (ASE). Moreover, Monte Carlo simulations are
presented to verify the analytical conclusions and numerical results
demonstrate that: 1) the proposed HetNet is an interference-limited system and
it outperforms the traditional HetNets; 2) there exists an optimal pre-decided
rate threshold that contributes to the maximum ASE; and 3) 73 GHz is the best
mmWave carrier frequency regarding ASE due to the large antenna scale.
</dc:description>
 <dc:description>Comment: This paper has been accepted by IEEE International Conference on
  Communications (ICC), May, USA, 2018. Please cite the format version of this
  paper</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08801</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08808</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Optimal Redistribution Mechanisms through Neural Networks</dc:title>
 <dc:creator>Manisha, P</dc:creator>
 <dc:creator>Jawahar, C V</dc:creator>
 <dc:creator>Gujar, Sujit</dc:creator>
 <dc:subject>Computer Science - Computer Science and Game Theory</dc:subject>
 <dc:description>  We consider a setting where $p$ public resources are to be allocated among
$n$ competing and strategic agents so as to maximize social welfare (the
objects should be allocated to those who value them the most). This is called
allocative efficiency (AE). We need the agents to report their valuations for
obtaining these resources, truthfully referred to as dominant strategy
incentive compatibility (DSIC). We use auction-based mechanisms to achieve AE
and DSIC yet budget balance cannot be ensured, due to Green-Laffont
Impossibility Theorem. That is, the net transfer of money cannot be zero. This
problem has been addressed by designing a redistribution mechanism so as to
ensure a minimum surplus of money as well as AE and DSIC. The objective could
be to minimize surplus in expectation or in the worst case and these $p$
objects could be homogeneous or heterogeneous. Designing redistribution
mechanisms which perform well in expectation becomes analytically challenging
for heterogeneous settings. In this paper, we take a completely different,
data-driven approach. We train a neural network to determine an optimal
redistribution mechanism based on given settings with both the objectives,
optimal in expectation and optimal in the worst case. We also propose a loss
function to train a neural network to optimize worst case. We design neural
networks with the underlying rebate functions being linear as well as nonlinear
in terms of bids of the agents. Our networks' performances are same as the
theoretical guarantees for the cases where it has been solved. We observe that
a neural network based redistribution mechanism for homogeneous settings which
uses nonlinear rebate functions outperforms linear rebate functions when the
objective is optimal in expectation. Our approach also yields an optimal in
expectation redistribution mechanism for heterogeneous settings.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08808</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08811</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constructing LDPC Codes from Partition and Latin-Style Splicing</dc:title>
 <dc:creator>Zhang, Guohua</dc:creator>
 <dc:creator>Hu, Yulin</dc:creator>
 <dc:creator>He, Qinwei</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  A novel method guaranteeing nondecreasing girth is presented for constructing
longer low-density parity-check (LDPC) codes from shorter ones. The
parity-check matrix of a shorter base code is decomposed into N (N&gt;=2)
non-overlapping components with the same size. Then, these components are
combined together to form the parity-check matrix of a longer code, according
to a given N*N Latin square. To illustrate this method, longer quasi-cyclic
(QC) LDPC codes are obtained with girth at least eight and satisfactory
performance, via shorter QC-LDPC codes with girth eight but poor performance.
The proposed method naturally includes several well-known methods as special
cases, but is much more general compared with these existing approaches.
</dc:description>
 <dc:description>Comment: 7 pages, 2 figures</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08811</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08819</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Social Influence with Recurrent Mobility with multiple options</dc:title>
 <dc:creator>Michaud, J&#xe9;r&#xf4;me</dc:creator>
 <dc:creator>Szilva, Attila</dc:creator>
 <dc:subject>Physics - Physics and Society</dc:subject>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:description>  In this paper, we discuss the possible generalizations of the Social
Influence with Recurrent Mobility (SIRM) model developed in Phys. Rev. Lett.
112, 158701 (2014). Although the SIRM model worked approximately satisfying
when US election was modelled, it has its limits: it has been developed only
for two-party systems and can lead to unphysical behaviour when one of the
parties has extreme vote share close to 0 or 1. We propose here generalizations
to the SIRM model by its extension for multi-party systems that are
mathematically well-posed in case of extreme vote shares, too, by handling the
noise term in a different way. In addition, we show that our method opens new
applications for the study of elections by using a new calibration procedure,
and makes possible to analyse the influence of the &quot;free will&quot; (creating a new
party) and other local effects for different commuting network topologies.
</dc:description>
 <dc:description>Comment: 10 pages, 6 figures</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08819</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08823</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>MengeROS: a Crowd Simulation Tool for Autonomous Robot Navigation</dc:title>
 <dc:creator>Aroor, Anoop</dc:creator>
 <dc:creator>Epstein, Susan L.</dc:creator>
 <dc:creator>Korpan, Raj</dc:creator>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:description>  While effective navigation in large, crowded environments is essential for an
autonomous robot, preliminary testing of algorithms to support it requires
simulation across a broad range of crowd scenarios. Most available simulation
tools provide either realistic crowds without robots or realistic robots
without realistic crowds. This paper introduces MengeROS, a 2-D simulator that
realistically integrates multiple robots and crowds. MengeROS provides a broad
range of settings in which to test the capabilities and performance of
navigation algorithms designed for large crowded environments.
</dc:description>
 <dc:description>Comment: In AAAI 2017 Fall symposium on AI for HRI</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08823</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08825</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Election campaigning on social media: Politicians, audiences and the
  mediation of political communication on Facebook and Twitter</dc:title>
 <dc:creator>Stier, Sebastian</dc:creator>
 <dc:creator>Bleier, Arnim</dc:creator>
 <dc:creator>Lietz, Haiko</dc:creator>
 <dc:creator>Strohmaier, Markus</dc:creator>
 <dc:subject>Computer Science - Computers and Society</dc:subject>
 <dc:description>  Although considerable research has concentrated on online campaigning, it is
still unclear how politicians use different social media platforms in political
communication. Focusing on the German federal election campaign 2013, this
article investigates whether election candidates address the topics most
important to the mass audience and to which extent their communication is
shaped by the characteristics of Facebook and Twitter. Based on open-ended
responses from a representative survey conducted during the election campaign,
we train a human-interpretable Bayesian language model to identify political
topics. Applying the model to social media messages of candidates and their
direct audiences, we find that both prioritize different topics than the mass
audience. The analysis also shows that politicians use Facebook and Twitter for
different purposes. We relate the various findings to the mediation of
political communication on social media induced by the particular
characteristics of audiences and sociotechnical environments.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08825</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08829</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Symbol Emergence in Cognitive Developmental Systems: a Survey</dc:title>
 <dc:creator>Taniguchi, Tadahiro</dc:creator>
 <dc:creator>Ugur, Emre</dc:creator>
 <dc:creator>Hoffmann, Matej</dc:creator>
 <dc:creator>Jamone, Lorenzo</dc:creator>
 <dc:creator>Nagai, Takayuki</dc:creator>
 <dc:creator>Rosman, Benjamin</dc:creator>
 <dc:creator>Matsuka, Toshihiko</dc:creator>
 <dc:creator>Iwahashi, Naoto</dc:creator>
 <dc:creator>Oztop, Erhan</dc:creator>
 <dc:creator>Piater, Justus</dc:creator>
 <dc:creator>W&#xf6;rg&#xf6;tter, Florentin</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Computer Science - Robotics</dc:subject>
 <dc:description>  Symbol emergence through a robot's own interactive exploration of the world
without human intervention has been investigated now for several decades.
However, methods that enable a machine to form symbol systems in a robust
bottom-up manner are still missing. Clearly, this shows that we still do not
have an appropriate computational understanding that explains symbol emergence
in biological and artificial systems. Over the years it became more and more
clear that symbol emergence has to be posed as a multi-faceted problem.
Therefore, we will first review the history of the symbol emergence problem in
different fields showing their mutual relations. Then we will describe recent
work and approaches to solve this problem with the aim of providing an
integrative and comprehensive overview of symbol emergence for future research.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08829</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08831</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Multilayer Convolutional Encoder-Decoder Neural Network for
  Grammatical Error Correction</dc:title>
 <dc:creator>Chollampatt, Shamil</dc:creator>
 <dc:creator>Ng, Hwee Tou</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We improve automatic correction of grammatical, orthographic, and collocation
errors in text using a multilayer convolutional encoder-decoder neural network.
The network is initialized with embeddings that make use of character N-gram
information to better suit this task. When evaluated on common benchmark test
data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior
neural approaches on this task as well as strong statistical machine
translation-based systems with neural and task-specific features trained on the
same data. Our analysis shows the superiority of convolutional neural networks
over recurrent neural networks such as long short-term memory (LSTM) networks
in capturing the local context via attention, and thereby improving the
coverage in correcting grammatical errors. By ensembling multiple models, and
incorporating an N-gram language model and edit features via rescoring, our
novel method becomes the first neural approach to outperform the current
state-of-the-art statistical machine translation-based approach, both in terms
of grammaticality and fluency.
</dc:description>
 <dc:description>Comment: 8 pages, 3 figures, In Proceedings of AAAI 2018</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08831</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08839</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Instance Segmentation Annotation by Geometry-guided GAN</dc:title>
 <dc:creator>Xu, Wenqiang</dc:creator>
 <dc:creator>Li, Yonglu</dc:creator>
 <dc:creator>Lu, Cewu</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:description>  Instance segmentation is a problem of significance in computer vision.
However, preparing annotated data for this task is extremely time-consuming and
costly. By combining the advantages of 3D scanning, physical reasoning, and GAN
techniques, we introduce a novel pipeline named Geometry-guided GAN (GeoGAN) to
obtain large quantities of training samples with minor annotation. Our pipeline
is well-suited to most indoor and some outdoor scenarios. To evaluate our
performance, we build a new Instance-60K dataset, with various of common
objects categories. Extensive experiments show that our pipeline can achieve
decent instance segmentation performance given very low human annotation cost.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08839</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08841</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>FlashRL: A Reinforcement Learning Platform for Flash Games</dc:title>
 <dc:creator>Andersen, Per-Arne</dc:creator>
 <dc:creator>Goodwin, Morten</dc:creator>
 <dc:creator>Granmo, Ole-Christoffer</dc:creator>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Computer Science - Computer Science and Game Theory</dc:subject>
 <dc:description>  Reinforcement Learning (RL) is a research area that has blossomed
tremendously in recent years and has shown remarkable potential in among others
successfully playing computer games. However, there only exists a few game
platforms that provide diversity in tasks and state-space needed to advance RL
algorithms. The existing platforms offer RL access to Atari- and a few
web-based games, but no platform fully expose access to Flash games. This is
unfortunate because applying RL to Flash games have potential to push the
research of RL algorithms.
  This paper introduces the Flash Reinforcement Learning platform (FlashRL)
which attempts to fill this gap by providing an environment for thousands of
Flash games on a novel platform for Flash automation. It opens up easy
experimentation with RL algorithms for Flash games, which has previously been
challenging. The platform shows excellent performance with as little as 5% CPU
utilization on consumer hardware. It shows promising results for novel
reinforcement learning algorithms.
</dc:description>
 <dc:description>Comment: 12 Pages, Proceedings of the 30th Norwegian Informatics Conference,
  Oslo, Norway 2017</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08841</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08843</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Characterizing Docker Overhead in Mobile Edge Computing Scenarios</dc:title>
 <dc:creator>Avino, Giuseppe</dc:creator>
 <dc:creator>Malinverno, Marco</dc:creator>
 <dc:creator>Malandrino, Francesco</dc:creator>
 <dc:creator>Casetti, Claudio</dc:creator>
 <dc:creator>Chiasserini, Carla-Fabiana</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:description>  Mobile Edge Computing (MEC) is an emerging network paradigm that provides
cloud and IT services at the point of access of the network. Such proximity to
the end user translates into ultra-low latency and high bandwidth, while, at
the same time, it alleviates traffic congestion in the network core. Due to the
need to run servers on edge nodes (e.g., an LTE-A macro eNodeB), a key element
of MEC architectures is to ensure server portability and low overhead. A
possible tool that can be used for this purpose is Docker, a framework that
allows easy, fast deployment of Linux containers. This paper addresses the
suitability of Docker in MEC scenar- ios by quantifying the CPU consumed by
Docker when running two different containerized services: multiplayer gam- ing
and video streaming. Our tests, run with varying numbers of clients and
servers, yield different results for the two case studies: for the gaming
service, the overhead logged by Docker increases only with the number of
servers; con- versely, for the video streaming case, the overhead is not
affected by the number of either clients or servers.
</dc:description>
 <dc:description>Comment: 6 Pages, 9 images, 2 tables</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08843</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08850</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On bounded pitch inequalities for the min-knapsack polytope</dc:title>
 <dc:creator>Faenza, Yuri</dc:creator>
 <dc:creator>Malinovi&#x107;, Igor</dc:creator>
 <dc:creator>Mastrolilli, Monaldo</dc:creator>
 <dc:creator>Svensson, Ola</dc:creator>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  In the min-knapsack problem one aims at choosing a set of objects with
minimum total cost and total profit above a given threshold. In this paper, we
study a class of valid inequalities for min-knapsack known as bounded pitch
inequalities, which generalize the well-known unweighted cover inequalities.
While separating over pitch-1 inequalities is NP-hard, we show that approximate
separation over the set of pitch-1 and pitch-2 inequalities can be done in
polynomial time. We also investigate integrality gaps of linear relaxations for
min-knapsack when these inequalities are added. Among other results, we show
that, for any fixed $t$, the $t$-th CG closure of the natural linear relaxation
has the unbounded integrality gap.
</dc:description>
 <dc:description>Comment: 14 pages</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08850</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08856</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Correlations and dynamics of consumption patterns in social-economic
  networks</dc:title>
 <dc:creator>Leo, Yannick</dc:creator>
 <dc:creator>Karsai, M&#xe1;rton</dc:creator>
 <dc:creator>Sarraute, Carlos</dc:creator>
 <dc:creator>Fleury, Eric</dc:creator>
 <dc:subject>Computer Science - Social and Information Networks</dc:subject>
 <dc:subject>Physics - Physics and Society</dc:subject>
 <dc:description>  We analyse a coupled dataset collecting the mobile phone communications and
bank transactions history of a large number of individuals living in a Latin
American country. After mapping the social structure and introducing indicators
of socioeconomic status, demographic features, and purchasing habits of
individuals we show that typical consumption patterns are strongly correlated
with identified socioeconomic classes leading to patterns of stratification in
the social structure. In addition we measure correlations between merchant
categories and introduce a correlation network, which emerges with a meaningful
community structure. We detect multivariate relations between merchant
categories and show correlations in purchasing habits of individuals. Finally,
by analysing individual consumption histories, we detect dynamical patterns in
purchase behaviour and their correlations with the socioeconomic status,
demographic characters and the egocentric social network of individuals. Our
work provides novel and detailed insight into the relations between social and
consuming behaviour with potential applications in resource allocation,
marketing, and recommendation system design.
</dc:description>
 <dc:description>Comment: 20 pages, 8 figures, 1 table. Accepted in Social Analysis Network and
  Mining (Springer). arXiv admin note: substantial text overlap with
  arXiv:1609.03756</dc:description>
 <dc:date>2018-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08856</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08863</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>3D Scanning: A Comprehensive Survey</dc:title>
 <dc:creator>Daneshmand, Morteza</dc:creator>
 <dc:creator>Helmi, Ahmed</dc:creator>
 <dc:creator>Avots, Egils</dc:creator>
 <dc:creator>Noroozi, Fatemeh</dc:creator>
 <dc:creator>Alisinanoglu, Fatih</dc:creator>
 <dc:creator>Arslan, Hasan Sait</dc:creator>
 <dc:creator>Gorbova, Jelena</dc:creator>
 <dc:creator>Haamer, Rain Eric</dc:creator>
 <dc:creator>Ozcinar, Cagri</dc:creator>
 <dc:creator>Anbarjafari, Gholamreza</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Graphics</dc:subject>
 <dc:description>  This paper provides an overview of 3D scanning methodologies and technologies
proposed in the existing scientific and industrial literature. Throughout the
paper, various types of the related techniques are reviewed, which consist,
mainly, of close-range, aerial, structure-from-motion and terrestrial
photogrammetry, and mobile, terrestrial and airborne laser scanning, as well as
time-of-flight, structured-light and phase-comparison methods, along with
comparative and combinational studies, the latter being intended to help make a
clearer distinction on the relevance and reliability of the possible choices.
Moreover, outlier detection and surface fitting procedures are discussed
concisely, which are necessary post-processing stages.
</dc:description>
 <dc:description>Comment: 18 pages, 3 figures</dc:description>
 <dc:date>2018-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08863</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08865</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On the Broadcast Rate of Index Coding Problems with Symmetric and
  Consecutive Interference</dc:title>
 <dc:creator>Vaddi, Mahesh Babu</dc:creator>
 <dc:creator>Rajan, B. Sundar</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  A single unicast index coding problem (SUICP) with symmetric and consecutive
interference (SCI) has $K$ messages and $K$ receivers, the $k$th receiver $R_k$
wanting the $k$th message $x_k$ and having interference $\mathcal{I}_k=
\{x_{k-U-m},\dots,x_{k-m-2},x_{k-m-1}\}\cup\{x_{k+m+1},
x_{k+m+2},\dots,x_{k+m+D}\}$ and side-information $\mathcal{K}_k=(\mathcal{I}_k
\cup x_k)^c$. In this paper, we derive a lowerbound on the broadcast rate of
single unicast index coding problem with symmetric and consecutive interference
(SUICP(SCI)). In the SUICP(SCI), if $m=0$, we refer this as single unicast
index coding problem with symmetric and neighboring interference (SUICP(SNI)).
In our previous work\cite{VaR5}, we gave the construction of near-optimal
vector linear index codes for SUICP(SNI) with arbitrary $K,D,U$. In this paper,
we convert the SUICP(SCI) into SUICP(SNI) and give the construction of
near-optimal vector linear index codes for SUICP(SCI) with arbitrary $K,U,D$
and $m$. The constructed codes are independent of field size. The near-optimal
vector linear index codes of SUICP(SNI) is a special case of near-optimal
vector linear index codes constructed in this paper for SUICP(SCI) with $m=0$.
In our previous work\cite{VaR6}, we derived an upperbound on broadcast rate of
SUICP(SNI). In this paper, we give an upperbound on the broadcast rate of
SUICP(SCI) by using our earlier result on the upperbound on the broadcast rate
of SUICP(SNI). We derive the capacity of SUICP(SCI) for some special cases.
</dc:description>
 <dc:description>Comment: 11 pages, 3 figures and 4 tables. arXiv admin note: text overlap with
  arXiv:1705.10614, arXiv:1707.00455</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08865</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08867</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>LEDAkem: a post-quantum key encapsulation mechanism based on QC-LDPC
  codes</dc:title>
 <dc:creator>Baldi, Marco</dc:creator>
 <dc:creator>Barenghi, Alessandro</dc:creator>
 <dc:creator>Chiaraluce, Franco</dc:creator>
 <dc:creator>Pelosi, Gerardo</dc:creator>
 <dc:creator>Santini, Paolo</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  This work presents a new code-based key encapsulation mechanism (KEM) called
LEDAkem. It is built on the Niederreiter cryptosystem and relies on
quasi-cyclic low-density parity-check codes as secret codes, providing high
decoding speeds and compact keypairs. LEDAkem uses ephemeral keys to foil known
statistical attacks, and takes advantage of a new decoding algorithm that
provides faster decoding than the classical bit-flipping decoder commonly
adopted in this kind of systems. The main attacks against LEDAkem are
investigated, taking into account quantum speedups. Some instances of LEDAkem
are designed to achieve different security levels against classical and quantum
computers. Some performance figures obtained through an efficient C99
implementation of LEDAkem are provided.
</dc:description>
 <dc:description>Comment: 21 pages, 3 tables</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08867</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08873</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Mirrored and Hybrid Disk Arrays: Organization, Scheduling, Reliability,
  and Performance</dc:title>
 <dc:creator>Thomasian, Alexander</dc:creator>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:subject>Computer Science - Operating Systems</dc:subject>
 <dc:subject>Computer Science - Performance</dc:subject>
 <dc:description>  Basic mirroring (BM) classified as RAID level 1 replicates data on two disks,
thus doubling disk access bandwidth for read requests. RAID1/0 is an array of
BM pairs with balanced loads due to striping. When a disk fails the read load
on its pair is doubled, which results in halving the maximum attainable
bandwidth. We review RAID1 organizations which attain a balanced load upon disk
failure, but as shown by reliability analysis tend to be less reliable than
RAID1/0. Hybrid disk arrays which store XORed instead of replicated data tend
to have a higher reliability than mirrored disks, but incur a higher overhead
in updating data. Read request response time can be improved by processing them
at a higher priority than writes, since they have a direct effect on
application response time. Shortest seek distance and affinity based routing
both shorten seek time. Anticipatory arm placement places arms optimally to
minimize the seek distance. The analysis of RAID1 in normal, degraded, and
rebuild mode is provided to quantify RAID1/0 performance. We compare the
reliability of mirrored disk organizations against each other and hybrid disks
and erasure coded disk arrays.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08873</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08876</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On the algorithmic complexity of decomposing graphs into
  regular/irregular structures</dc:title>
 <dc:creator>Ahadi, Arash</dc:creator>
 <dc:creator>Dehghan, Ali</dc:creator>
 <dc:creator>Sadeghi, Mohammad-Reza</dc:creator>
 <dc:creator>Stevens, Brett</dc:creator>
 <dc:subject>Computer Science - Discrete Mathematics</dc:subject>
 <dc:subject>Mathematics - Combinatorics</dc:subject>
 <dc:description>  A locally irregular graph is a graph whose adjacent vertices have distinct
degrees, a regular graph is a graph where each vertex has the same degree and a
locally regular graph is a graph where for every two adjacent vertices u, v,
their degrees are equal. In this work, we study the set of all problems which
are related to decomposition of graphs into regular, locally regular and/or
locally irregular subgraphs and we present some polynomial time algorithms,
NP-completeness results, lower bounds and upper bounds for them. Among our
results, one of our lower bounds makes use of mutually orthogonal Latin squares
which is relatively novel.
</dc:description>
 <dc:description>Comment: 30 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1701.05934</dc:description>
 <dc:date>2018-01-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08876</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08881</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Correlated Components Analysis --- Extracting Reliable Dimensions in
  Multivariate Data</dc:title>
 <dc:creator>Parra, Lucas C.</dc:creator>
 <dc:creator>Haufe, Stefan</dc:creator>
 <dc:creator>Dmochowski, Jacek P.</dc:creator>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  How does one find data dimensions that are reliably expressed across
repetitions? For example, in neuroscience one may want to identify combinations
of brain signals that are reliably activated across multiple trials or
subjects. For a clinical assessment with multiple ratings, one may want to
identify an aggregate score that is reliably reproduced across raters. The
approach proposed here --- &quot;correlated components analysis&quot; --- is to identify
components that maximally correlate between repetitions (e.g. trials, subjects,
raters). This can be expressed as the maximization of the ratio of
between-repetition to within-repetition covariance, resulting in a generalized
eigenvalue problem. We show that covariances can be computed efficiently
without explicitly considering all pairs of repetitions, that the result is
equivalent to multi-class linear discriminant analysis for unbiased signals,
and that the approach also maximize reliability, defined as the mean divided by
the deviation across repetitions. We also extend the method to non-linear
components using kernels, discuss regularization to improve numerical
stability, present parametric and non-parametric tests to establish statistical
significance, and provide code.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08881</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08901</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Detecting Changes in Fully Polarimetric SAR Imagery with Statistical
  Information Theory</dc:title>
 <dc:creator>Nascimento, Abra&#xe3;o D. C.</dc:creator>
 <dc:creator>Frery, Alejandro C.</dc:creator>
 <dc:creator>Cintra, Renato J.</dc:creator>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  Images obtained from coherent illumination processes are contaminated with
speckle. A prominent example of such imagery systems is the polarimetric
synthetic aperture radar (PolSAR). For such remote sensing tool the speckle
interference pattern appears in the form of a positive definite Hermitian
matrix, which requires specialized models and makes change detection a hard
task. The scaled complex Wishart distribution is a widely used model for PolSAR
images. Such distribution is defined by two parameters: the number of looks and
the complex covariance matrix. The last parameter contains all the necessary
information to characterize the backscattered data and, thus, identifying
changes in a sequence of images can be formulated as a problem of verifying
whether the complex covariance matrices differ at two or more takes. This paper
proposes a comparison between a classical change detection method based on the
likelihood ratio and three statistical methods that depend on
information-theoretic measures: the Kullback-Leibler distance and two
entropies. The performance of these four tests was quantified in terms of their
sample test powers and sizes using simulated data. The tests are then applied
to actual PolSAR data. The results provide evidence that tests based on
entropies may outperform those based on the Kullback-Leibler distance and
likelihood ratio statistics.
</dc:description>
 <dc:description>Comment: Submitted to IEEE Transactions on Geoscience and Remote Sensing</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08901</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08912</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Secure Distributed State Estimation of an LTI System over Time-Varying
  Networks and Analog Erasure Channels</dc:title>
 <dc:creator>Mitra, Aritra</dc:creator>
 <dc:creator>Sundaram, Shreyas</dc:creator>
 <dc:subject>Computer Science - Systems and Control</dc:subject>
 <dc:description>  We study the problem of collaboratively estimating the state of an LTI system
monitored by a network of sensors, subject to the following important practical
considerations: (i) certain sensors might be arbitrarily compromised by an
adversary and (ii) the underlying communication graph governing the flow of
information across sensors might be time-varying. We first analyze a scenario
involving intermittent communication losses that preserve certain information
flow patterns over bounded intervals of time. By equipping the sensors with
adequate memory, we show that one can obtain a fully distributed, provably
correct state estimation algorithm that accounts for arbitrary adversarial
behavior, provided certain conditions are met by the network topology. We then
argue that our approach can handle bounded communication delays as well. Next,
we explore a case where each communication link stochastically drops packets
based on an analog erasure channel model. For this setup, we propose state
estimate update and information exchange rules, along with conditions on the
network topology and packet drop probabilities, that guarantee mean-square
stability despite arbitrary adversarial attacks.
</dc:description>
 <dc:description>Comment: To appear in the Proceedings of the American Control Conference, 2018</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08912</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08917</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning to Evade Static PE Machine Learning Malware Models via
  Reinforcement Learning</dc:title>
 <dc:creator>Anderson, Hyrum S.</dc:creator>
 <dc:creator>Kharkar, Anant</dc:creator>
 <dc:creator>Filar, Bobby</dc:creator>
 <dc:creator>Evans, David</dc:creator>
 <dc:creator>Roth, Phil</dc:creator>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  Machine learning is a popular approach to signatureless malware detection
because it can generalize to never-before-seen malware families and polymorphic
strains. This has resulted in its practical use for either primary detection
engines or for supplementary heuristic detection by anti-malware vendors.
Recent work in adversarial machine learning has shown that deep learning models
are susceptible to gradient-based attacks, whereas non-differentiable models
that report a score can be attacked by genetic algorithms that aim to
systematically reduce the score. We propose a more general framework based on
reinforcement learning (RL) for attacking static portable executable (PE)
anti-malware engines. The general framework does not require a differentiable
model nor does it require the engine to produce a score. Instead, an RL agent
is equipped with a set of functionality-preserving operations that it may
perform on the PE file. Through a series of games played against the
anti-malware engine, it learns which sequences of operations are likely to
result in evading the detector for any given malware sample. This enables
completely black-box attacks against static PE anti-malware, and produces
functional evasive malware samples as a direct result. We show in experiments
that our method can attack a gradient-boosted machine learning model with
evasion rates that are substantial and appear to be strongly dependent on the
dataset. We demonstrate that attacks against this model appear to also evade
components of publicly hosted antivirus engines. Adversarial training results
are also presented: by retraining the model on evasive ransomware samples, a
subsequent attack is 33% less effective. Importantly, we release an OpenAI gym
to allow researchers to study evasion rates against their own machine learning
models, malware samples, and their own RL agents.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08917</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08925</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Supersaliency: Predicting Smooth Pursuit-Based Attention with Slicing
  CNNs Improves Fixation Prediction for Naturalistic Videos</dc:title>
 <dc:creator>Startsev, Mikhail</dc:creator>
 <dc:creator>Dorr, Michael</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Human-Computer Interaction</dc:subject>
 <dc:description>  Predicting attention is a popular topic at the intersection of human and
computer vision, but video saliency prediction has only recently begun to
benefit from deep learning-based approaches. Even though most of the available
video-based saliency data sets and models claim to target human observers'
fixations, they fail to differentiate them from smooth pursuit (SP), a major
eye movement type that is unique to perception of dynamic scenes. In this work,
we aim to make this distinction explicit, to which end we (i) use both
algorithmic and manual annotations of SP traces and other eye movements for two
well-established video saliency data sets, (ii) train Slicing Convolutional
Neural Networks (S-CNN) for saliency prediction on either fixation- or
SP-salient locations, and (iii) evaluate ours and over 20 popular published
saliency models on the two annotated data sets for predicting both SP and
fixations, as well as on another data set of human fixations. Our proposed
model, trained on an independent set of videos, outperforms the
state-of-the-art saliency models in the task of SP prediction on all considered
data sets. Moreover, this model also demonstrates superior performance in the
prediction of &quot;classical&quot; fixation-based saliency. Our results emphasize the
importance of selectively approaching training set construction for attention
modelling.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08925</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08926</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deflecting Adversarial Attacks with Pixel Deflection</dc:title>
 <dc:creator>Prakash, Aaditya</dc:creator>
 <dc:creator>Moran, Nick</dc:creator>
 <dc:creator>Garber, Solomon</dc:creator>
 <dc:creator>DiLillo, Antonella</dc:creator>
 <dc:creator>Storer, James</dc:creator>
 <dc:subject>Computer Science - Computer Vision and Pattern Recognition</dc:subject>
 <dc:subject>Computer Science - Cryptography and Security</dc:subject>
 <dc:description>  CNNs are poised to become integral parts of many critical systems. Despite
their robustness to natural variations, image pixel values can be manipulated,
via small, carefully crafted, imperceptible perturbations, to cause a model to
misclassify images. We present an algorithm to process an image so that
classification accuracy is significantly preserved in the presence of such
adversarial manipulations. Image classifiers tend to be robust to natural
noise, and adversarial attacks tend to be agnostic to object location. These
observations motivate our strategy, which leverages model robustness to defend
against adversarial perturbations by forcing the image to match natural image
statistics. Our algorithm locally corrupts the image by redistributing pixel
values via a process we term pixel deflection. A subsequent wavelet-based
denoising operation softens this corruption, as well as some of the adversarial
changes. We demonstrate experimentally that the combination of these techniques
enables the effective recovery of the true class, against a variety of robust
attacks. Our results compare favorably with current state-of-the-art defenses,
without requiring retraining or modifying the CNN.
</dc:description>
 <dc:description>Comment: 17 pages, 14 figures, 9 tables</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08926</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08928</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatically Extracting Web API Specifications from HTML Documentation</dc:title>
 <dc:creator>Yang, Jinqiu</dc:creator>
 <dc:creator>Wittern, Erik</dc:creator>
 <dc:creator>Ying, Annie T. T.</dc:creator>
 <dc:creator>Dolby, Julian</dc:creator>
 <dc:creator>Tan, Lin</dc:creator>
 <dc:subject>Computer Science - Software Engineering</dc:subject>
 <dc:description>  Web API specifications are machine-readable descriptions of APIs. These
specifications, in combination with related tooling, simplify and support the
consumption of APIs. However, despite the increased distribution of web APIs,
specifications are rare and their creation and maintenance heavily relies on
manual efforts by third parties. In this paper, we propose an automatic
approach and an associated tool called D2Spec for extracting specifications
from web API documentation pages. Given a seed online documentation page on an
API, D2Spec first crawls all documentation pages on the API, and then uses a
set of machine learning techniques to extract the base URL, path templates, and
HTTP methods, which collectively describe the endpoints of an API. We evaluated
whether D2Spec can accurately extract endpoints from documentation on 120 web
APIs. The results showed that D2Spec achieved a precision of 87.5% in
identifying base URLs, a precision of 81.3% and a recall of 80.6% in generating
path templates, and a precision of 84.4% and a recall of 76.2% in extracting
HTTP methods. In addition, we found that D2Spec was useful when applied to APIs
with pre-existing API specifications: D2Spec revealed many inconsistencies
between web API documentation and their corresponding publicly available
specifications. Thus, D2Spec can be used by web API providers to keep
documentation and specifications in synchronization.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08928</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08930</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Recasting Gradient-Based Meta-Learning as Hierarchical Bayes</dc:title>
 <dc:creator>Grant, Erin</dc:creator>
 <dc:creator>Finn, Chelsea</dc:creator>
 <dc:creator>Levine, Sergey</dc:creator>
 <dc:creator>Darrell, Trevor</dc:creator>
 <dc:creator>Griffiths, Thomas</dc:creator>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  Meta-learning allows an intelligent agent to leverage prior learning episodes
as a basis for quickly improving performance on a novel task. Bayesian
hierarchical modeling provides a theoretical framework for formalizing
meta-learning as inference for a set of parameters that are shared across
tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML)
of Finn et al. (2017) as a method for probabilistic inference in a hierarchical
Bayesian model. In contrast to prior methods for meta-learning via hierarchical
Bayes, MAML is naturally applicable to complex function approximators through
its use of a scalable gradient descent procedure for posterior inference.
Furthermore, the identification of MAML as hierarchical Bayes provides a way to
understand the algorithm's operation as a meta-learning procedure, as well as
an opportunity to make use of computational strategies for efficient inference.
We use this opportunity to propose an improvement to the MAML algorithm that
makes use of techniques from approximate inference and curvature estimation.
</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08930</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1801.08938</identifier>
 <datestamp>2018-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Simulation for L3 Volumetric Attack Detection</dc:title>
 <dc:creator>Rutishauser, Oliver</dc:creator>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:subject>C.2</dc:subject>
 <dc:description>  The detection of a volumetric attack involves collecting statistics on the
network traffic, and identifying suspicious activities. We assume that
available statistical information includes the number of packets and the number
of bytes passed per flow. We apply methods of machine learning to detect
malicious traffic. A prototype project is implemented as a module for the
Floodlight controller. The prototype was tested on the Mininet simulation
platform. The simulated topology includes a number of edge switches, a
connected graph of core switches, and a number of server and user hosts. The
server hosts run simple web servers. The user hosts simulate web clients. The
controller employs Dijkstra's algorithm to find the best flow in the graph. The
controller periodically polls the edge switches and provides current and
historical statistics on each active flow. The streaming analytics evaluates
the traffic volume and detects volumetric attacks.
</dc:description>
 <dc:description>Comment: 8 pages with figures, code, and conclusions</dc:description>
 <dc:date>2018-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/1801.08938</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9807003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Development and Evolution of Neural Networks in an Artificial Chemistry</dc:title>
 <dc:creator>Astor, Jens C.</dc:creator>
 <dc:creator>Adami, Christoph</dc:creator>
 <dc:subject>Nonlinear Sciences - Adaptation and Self-Organizing Systems</dc:subject>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:subject>Quantitative Biology - Populations and Evolution</dc:subject>
 <dc:description>  We present a model of decentralized growth for Artificial Neural Networks
(ANNs) inspired by the development and the physiology of real nervous systems.
In this model, each individual artificial neuron is an autonomous unit whose
behavior is determined only by the genetic information it harbors and local
concentrations of substrates modeled by a simple artificial chemistry. Gene
expression is manifested as axon and dendrite growth, cell division and
differentiation, substrate production and cell stimulation. We demonstrate the
model's power with a hand-written genome that leads to the growth of a simple
network which performs classical conditioning. To evolve more complex
structures, we implemented a platform-independent, asynchronous, distributed
Genetic Algorithm (GA) that allows users to participate in evolutionary
experiments via the World Wide Web.
</dc:description>
 <dc:description>Comment: 8 pages LaTeX, style file included, 8 embedded postscript figures. To
  be published in Proc. of 3rd German Workshop on Artificial Life (GWAL)</dc:description>
 <dc:date>1998-07-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/adap-org/9807003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9903003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evolution of genetic organization in digital organisms</dc:title>
 <dc:creator>Ofria, Charles</dc:creator>
 <dc:creator>Adami, Christoph</dc:creator>
 <dc:subject>Nonlinear Sciences - Adaptation and Self-Organizing Systems</dc:subject>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:subject>Quantitative Biology - Populations and Evolution</dc:subject>
 <dc:description>  We examine the evolution of expression patterns and the organization of
genetic information in populations of self-replicating digital organisms.
Seeding the experiments with a linearly expressed ancestor, we witness the
development of complex, parallel secondary expression patterns. Using
principles from information theory, we demonstrate an evolutionary pressure
towards overlapping expressions causing variation (and hence further evolution)
to sharply drop. Finally, we compare the overlapping sections of dominant
genomes to those portions which are singly expressed and observe a significant
difference in the entropy of their encoding.
</dc:description>
 <dc:description>Comment: 18 pages with 5 embedded figures. Proc. of DIMACS workshop on
  &quot;Evolution as Computation&quot;, Jan. 11-12, Princeton, NJ. L. Landweber and E.
  Winfree, eds. (Springer, 1999)</dc:description>
 <dc:date>1999-03-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/adap-org/9903003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:adap-org/9909006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Performance of data networks with random links</dc:title>
 <dc:creator>Fuks, Henryk</dc:creator>
 <dc:creator>Lawniczak, Anna T.</dc:creator>
 <dc:subject>Nonlinear Sciences - Adaptation and Self-Organizing Systems</dc:subject>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:description>  We investigate simplified models of computer data networks and examine how
the introduction of additional random links influences the performance of these
net works. In general, the impact of additional random links on the performance
of the network strongly depends on the routing algorithm used in the network.
Significant performance gains can be achieved if the routing is based on
&quot;geometrical distance&quot; or shortest path reduced table routing. With shortest
path full table routing degradation of performance is observed.
</dc:description>
 <dc:description>Comment: 20 pages, 10 figures</dc:description>
 <dc:date>1999-09-21</dc:date>
 <dc:date>2001-01-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/adap-org/9909006</dc:identifier>
 <dc:identifier>Mathematics and Computers in Simulation 51 103-119 (1999)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:alg-geom/9608018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Rank Two Bundles on Algebraic Curves and Decoding of Goppa Codes</dc:title>
 <dc:creator>Johnsen, Trygve</dc:creator>
 <dc:subject>Mathematics - Algebraic Geometry</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:subject>14D20, 94B</dc:subject>
 <dc:description>  We study a connection between two topics: Decoding of Goppa codes arising
from an algebraic curve, and rank two extensions of certain line bundles on the
curve.
</dc:description>
 <dc:description>Comment: An error in (what is now called) Theorem 3.4 has been corrected</dc:description>
 <dc:date>1996-08-21</dc:date>
 <dc:date>2003-01-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/alg-geom/9608018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0005101</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Data Streams from the Low Frequency Instrument On-Board the Planck
  Satellite: Statistical Analysis and Compression Efficiency</dc:title>
 <dc:creator>Maris, M.</dc:creator>
 <dc:creator>Maino, D.</dc:creator>
 <dc:creator>Burigana, C.</dc:creator>
 <dc:creator>Pasian, F.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Other Computer Science</dc:subject>
 <dc:subject>Physics - Data Analysis, Statistics and Probability</dc:subject>
 <dc:subject>Physics - Space Physics</dc:subject>
 <dc:description>  The expected data rate produced by the Low Frequency Instrument (LFI) planned
to fly on the ESA Planck mission in 2007, is over a factor 8 larger than the
bandwidth allowed by the spacecraft transmission system to download the LFI
data. We discuss the application of lossless compression to Planck/LFI data
streams in order to reduce the overall data flow. We perform both theoretical
analysis and experimental tests using realistically simulated data streams in
order to fix the statistical properties of the signal and the maximal
compression rate allowed by several lossless compression algorithms. We studied
the influence of signal composition and of acquisition parameters on the
compression rate Cr and develop a semiempirical formalism to account for it.
The best performing compressor tested up to now is the arithmetic compression
of order 1, designed for optimizing the compression of white noise like
signals, which allows an overall compression rate &lt;Cr&gt; = 2.65 +/- 0.02. We find
that such result is not improved by other lossless compressors, being the
signal almost white noise dominated. Lossless compression algorithms alone will
not solve the bandwidth problem but needs to be combined with other techniques.
</dc:description>
 <dc:description>Comment: May 3, 2000 release, 61 pages, 6 figures coded as eps, 9 tables (4
  included as eps), LaTeX 2.09 + assms4.sty, style file included, submitted for
  the pubblication on PASP May 3, 2000</dc:description>
 <dc:date>2000-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0005101</dc:identifier>
 <dc:identifier>doi:10.1051/aas:2000289</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0008307</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Science User Scenarios for a Virtual Observatory Design Reference
  Mission: Science Requirements for Data Mining</dc:title>
 <dc:creator>Borne, Kirk D.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Databases</dc:subject>
 <dc:subject>Computer Science - Digital Libraries</dc:subject>
 <dc:subject>Computer Science - Information Retrieval</dc:subject>
 <dc:description>  The knowledge discovery potential of the new large astronomical databases is
vast. When these are used in conjunction with the rich legacy data archives,
the opportunities for scientific discovery multiply rapidly. A Virtual
Observatory (VO) framework will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed. The
development of a VO should address the data mining needs of various
astronomical research constituencies. By way of example, two user scenarios are
presented which invoke applications and linkages of data across the catalog and
image domains in order to address specific astrophysics research problems.
These illustrate a subset of the desired capabilities and power of the VO, and
as such they represent potential components of a VO Design Reference Mission.
</dc:description>
 <dc:description>Comment: 4 pages. Paper to appear in the proceedings of the June 2000 &quot;Virtual
  Observatories of the Future&quot; conference at Caltech, edited by R. J. Brunner,
  S. G. Djorgovski, &amp; A. Szalay. (For figures and demos related to sample user
  scenarios, see
  http://adc.gsfc.nasa.gov/adc/adc_science/adc-science-scenario-papers.html .)</dc:description>
 <dc:date>2000-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0008307</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0010583</identifier>
 <datestamp>2009-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Data Mining in Astronomical Databases</dc:title>
 <dc:creator>Borne, Kirk D.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Databases</dc:subject>
 <dc:subject>Computer Science - Digital Libraries</dc:subject>
 <dc:subject>Computer Science - Information Retrieval</dc:subject>
 <dc:description>  A Virtual Observatory (VO) will enable transparent and efficient access,
search, retrieval, and visualization of data across multiple data repositories,
which are generally heterogeneous and distributed. Aspects of data mining that
apply to a variety of science user scenarios with a VO are reviewed.
</dc:description>
 <dc:description>Comment: 3 pages. Uses eso.sty style file. Paper to appear in the proceedings
  of the August 2000 &quot;Mining the Sky&quot; conference at MPA/ESO/MPE, Garching,
  Germany. (For figures and demos related to sample user scenarios, see
  http://adc.gsfc.nasa.gov/adc/adc_science/adc-science-scenario-papers.html .)
  (Revised version v2 only corrected these comments.)</dc:description>
 <dc:date>2000-10-27</dc:date>
 <dc:date>2000-10-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0010583</dc:identifier>
 <dc:identifier>doi:10.1007/10849171_88</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0107084</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Multi-Threaded Fast Convolver for Dynamically Parallel Image Filtering</dc:title>
 <dc:creator>Kepner, Jeremy</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  2D convolution is a staple of digital image processing. The advent of large
format imagers makes it possible to literally ``pave'' with silicon the focal
plane of an optical sensor, which results in very large images that can require
a significant amount computation to process. Filtering of large images via 2D
convolutions is often complicated by a variety of effects (e.g.,
non-uniformities found in wide field of view instruments). This paper describes
a fast (FFT based) method for convolving images, which is also well suited to
very large images. A parallel version of the method is implemented using a
multi-threaded approach, which allows more efficient load balancing and a
simpler software architecture. The method has been implemented within in a high
level interpreted language (IDL), while also exploiting open standards vector
libraries (VSIPL) and open standards parallel directives (OpenMP). The parallel
approach and software architecture are generally applicable to a variety of
algorithms and has the advantage of enabling users to obtain the convenience of
an easy operating environment while also delivering high performance using a
fully portable code.
</dc:description>
 <dc:description>Comment: 25 pages including color figures. Submitted to the Journal of
  Parallel and Distributed Computing</dc:description>
 <dc:date>2001-07-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0107084</dc:identifier>
 <dc:identifier>Journal of Parallel and Distributed Computing archive Volume 63
  Issue 3, March 2003 Pages 360 - 372</dc:identifier>
 <dc:identifier>doi:10.1016/S0743-7315(02)00054-0</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0112092</identifier>
 <datestamp>2009-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Systolic and Hyper-Systolic Algorithms for the Gravitational N-Body
  Problem, with an Application to Brownian Motion</dc:title>
 <dc:creator>Dorband, E. N.</dc:creator>
 <dc:creator>Hemsendorf, Marc</dc:creator>
 <dc:creator>Merritt, David</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:subject>Physics - Computational Physics</dc:subject>
 <dc:description>  A systolic algorithm rhythmically computes and passes data through a network
of processors. We investigate the performance of systolic algorithms for
implementing the gravitational N-body problem on distributed-memory computers.
Systolic algorithms minimize memory requirements by distributing the particles
between processors. We show that the performance of systolic routines can be
greatly enhanced by the use of non-blocking communication, which allows
particle coordinates to be communicated at the same time that force
calculations are being carried out. Hyper-systolic algorithms reduce the
communication complexity at the expense of increased memory demands. As an
example of an application requiring large N, we use the systolic algorithm to
carry out direct-summation simulations using 10^6 particles of the Brownian
motion of the supermassive black hole at the center of the Milky Way galaxy. We
predict a 3D random velocity of 0.4 km/s for the black hole.
</dc:description>
 <dc:description>Comment: 33 pages, 10 postscript figures</dc:description>
 <dc:date>2001-12-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0112092</dc:identifier>
 <dc:identifier>J.Comput.Phys. 185 (2003) 484-511</dc:identifier>
 <dc:identifier>doi:10.1016/S0021-9991(02)00067-0</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0305447</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast n-point correlation functions and three-point lensing application</dc:title>
 <dc:creator>Zhang, Lucy Liuxuan</dc:creator>
 <dc:creator>Pen, Ue-Li</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:subject>Computer Science - Data Structures and Algorithms</dc:subject>
 <dc:description>  We present a new algorithm to rapidly compute the two-point (2PCF),
three-point (3PCF) and n-point (n-PCF) correlation functions in roughly O(N log
N) time for N particles, instead of O(N^n) as required by brute force
approaches. The algorithm enables an estimate of the full 3PCF for as many as
10^6 galaxies. This technique exploits node-to-node correlations of a recursive
bisectional binary tree. A balanced tree construction minimizes the depth of
the tree and the worst case error at each node. The algorithm presented in this
paper can be applied to problems with arbitrary geometry.
  We describe the detailed implementation to compute the two point function and
all eight components of the 3PCF for a two-component field, with attention to
shear fields generated by gravitational lensing. We also generalize the
algorithm to compute the n-point correlation function for a scalar field in k
dimensions where n and k are arbitrary positive integers.
</dc:description>
 <dc:description>Comment: 37 pages, 6 figures, LaTeX; added and modified figures, modified
  theoretical estimate of computing time; accepted by New Astronomy</dc:description>
 <dc:date>2003-05-23</dc:date>
 <dc:date>2005-04-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0305447</dc:identifier>
 <dc:identifier>New Astron. 10 (2005) 569-590</dc:identifier>
 <dc:identifier>doi:10.1016/j.newast.2005.04.002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0402591</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evolutionary design of photometric systems and its application to Gaia</dc:title>
 <dc:creator>Bailer-Jones, C. A. L.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:subject>Statistics - Machine Learning</dc:subject>
 <dc:description>  Designing a photometric system to best fulfil a set of scientific goals is a
complex task, demanding a compromise between conflicting requirements and
subject to various constraints. A specific example is the determination of
stellar astrophysical parameters (APs) - effective temperature, metallicity
etc. - across a wide range of stellar types. I present a novel approach to this
problem which makes minimal assumptions about the required filter system. By
considering a filter system as a set of free parameters it may be designed by
optimizing some figure-of-merit (FoM) with respect to these parameters. In the
example considered, the FoM is a measure of how well the filter system can
`separate' stars with different APs. This separation is vectorial in nature, in
the sense that the local directions of AP variance are preferably mutually
orthogonal to avoid AP degeneracy. The optimization is carried out with an
evolutionary algorithm, which uses principles of evolutionary biology to search
the parameter space. This model, HFD (Heuristic Filter Design), is applied to
the design of photometric systems for the Gaia space astrometry mission. The
optimized systems show a number of interesting features, not least the
persistence of broad, overlapping filters. These HFD systems perform as least
as well as other proposed systems for Gaia, although inadequacies remain in
all. The principles underlying HFD are quite generic and may be applied to
filter design for numerous other projects, such as the search for specific
types of objects or photometric redshift determination.
</dc:description>
 <dc:description>Comment: Accepted by Astronomy &amp; Astrophysics</dc:description>
 <dc:date>2004-02-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0402591</dc:identifier>
 <dc:identifier>Astron.Astrophys. 419 (2004) 385-403</dc:identifier>
 <dc:identifier>doi:10.1051/0004-6361:20035779</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0502164</identifier>
 <datestamp>2009-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Particle Swarm Optimization: An efficient method for tracing periodic
  orbits in 3D galactic potentials</dc:title>
 <dc:creator>Skokos, Ch.</dc:creator>
 <dc:creator>Parsopoulos, K. E.</dc:creator>
 <dc:creator>Patsis, P. A.</dc:creator>
 <dc:creator>Vrahatis, M. N.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Numerical Analysis</dc:subject>
 <dc:subject>Computer Science - Neural and Evolutionary Computing</dc:subject>
 <dc:subject>Mathematics - Numerical Analysis</dc:subject>
 <dc:subject>Nonlinear Sciences - Chaotic Dynamics</dc:subject>
 <dc:description>  We propose the Particle Swarm Optimization (PSO) as an alternative method for
locating periodic orbits in a three--dimensional (3D) model of barred galaxies.
We develop an appropriate scheme that transforms the problem of finding
periodic orbits into the problem of detecting global minimizers of a function,
which is defined on the Poincar\'{e} Surface of Section (PSS) of the
Hamiltonian system. By combining the PSO method with deflection techniques, we
succeeded in tracing systematically several periodic orbits of the system. The
method succeeded in tracing the initial conditions of periodic orbits in cases
where Newton iterative techniques had difficulties. In particular, we found
families of 2D and 3D periodic orbits associated with the inner 8:1 to 12:1
resonances, between the radial 4:1 and corotation resonances of our 3D Ferrers
bar model. The main advantages of the proposed algorithm is its simplicity, its
ability to work using function values solely, as well as its ability to locate
many periodic orbits per run at a given Jacobian constant.
</dc:description>
 <dc:description>Comment: 12 pages, 8 figures, accepted for publication in MNRAS</dc:description>
 <dc:date>2005-02-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0502164</dc:identifier>
 <dc:identifier>Mon.Not.Roy.Astron.Soc. 359 (2005) 251-260</dc:identifier>
 <dc:identifier>doi:10.1111/j.1365-2966.2005.08892.x</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0504006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Virtual Observatory: From Concept to Implementation</dc:title>
 <dc:creator>Djorgovski, S. G.</dc:creator>
 <dc:creator>Williams, R.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Computational Engineering, Finance, and Science</dc:subject>
 <dc:description>  We review the origins of the Virtual Observatory (VO) concept, and the
current status of the efforts in this field. VO is the response of the
astronomical community to the challenges posed by the modern massive and
complex data sets. It is a framework in which information technology is
harnessed to organize, maintain, and explore the rich information content of
the exponentially growing data sets, and to enable a qualitatively new science
to be done with them. VO will become a complete, open, distributed, web-based
framework for astronomy of the early 21st century. A number of significant
efforts worldwide are now striving to convert this vision into reality. The
technological and methodological challenges posed by the information-rich
astronomy are also common to many other fields. We see a fundamental change in
the way all science is done, driven by the information technology revolution.
</dc:description>
 <dc:description>Comment: Invited review, to appear in proc. &quot;From Clark Lake to the Long
  Wavelength Array: Bill Erickson's Radio Science&quot;, eds. N. Kassim, M. Perez,
  W. Junor &amp; P. Henning, ASPCS vol. 3xx, in press (2005). Latex file, 14 pages,
  4 eps figures, all included</dc:description>
 <dc:date>2005-03-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0504006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0506110</identifier>
 <datestamp>2009-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Galactic Gradients, Postbiological Evolution and the Apparent Failure of
  SETI</dc:title>
 <dc:creator>Cirkovic, Milan M.</dc:creator>
 <dc:creator>Bradbury, Robert J.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Artificial Intelligence</dc:subject>
 <dc:subject>Physics - Physics and Society</dc:subject>
 <dc:description>  Motivated by recent developments impacting our view of Fermi's paradox
(absence of extraterrestrials and their manifestations from our past light
cone), we suggest a reassessment of the problem itself, as well as of
strategies employed by SETI projects so far. The need for such reevaluation is
fueled not only by the failure of searches thus far, but also by great advances
recently made in astrophysics, astrobiology, computer science and future
studies, which have remained largely ignored in SETI practice. As an example of
the new approach, we consider the effects of the observed metallicity and
temperature gradients in the Milky Way on the spatial distribution of
hypothetical advanced extraterrestrial intelligent communities. While,
obviously, properties of such communities and their sociological and
technological preferences are entirely unknown, we assume that (1) they operate
in agreement with the known laws of physics, and (2) that at some point they
typically become motivated by a meta-principle embodying the central role of
information-processing; a prototype of the latter is the recently suggested
Intelligence Principle of Steven J. Dick. There are specific conclusions of
practical interest to be drawn from coupling of these reasonable assumptions
with the astrophysical and astrochemical structure of the Galaxy. In
particular, we suggest that the outer regions of the Galactic disk are most
likely locations for advanced SETI targets, and that intelligent communities
will tend to migrate outward through the Galaxy as their capacities of
information-processing increase, for both thermodynamical and astrochemical
reasons. This can also be regarded as a possible generalization of the Galactic
Habitable Zone, concept currently much investigated in astrobiology.
</dc:description>
 <dc:description>Comment: 30 pages, 2 figures</dc:description>
 <dc:date>2005-06-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0506110</dc:identifier>
 <dc:identifier>New Astron. 11 (2006) 628-639</dc:identifier>
 <dc:identifier>doi:10.1016/j.newast.2006.04.003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0506308</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast directional continuous spherical wavelet transform algorithms</dc:title>
 <dc:creator>McEwen, J. D.</dc:creator>
 <dc:creator>Hobson, M. P.</dc:creator>
 <dc:creator>Mortlock, D. J.</dc:creator>
 <dc:creator>Lasenby, A. N.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  We describe the construction of a spherical wavelet analysis through the
inverse stereographic projection of the Euclidean planar wavelet framework,
introduced originally by Antoine and Vandergheynst and developed further by
Wiaux et al. Fast algorithms for performing the directional continuous wavelet
analysis on the unit sphere are presented. The fast directional algorithm,
based on the fast spherical convolution algorithm developed by Wandelt and
Gorski, provides a saving of O(sqrt(Npix)) over a direct quadrature
implementation for Npix pixels on the sphere, and allows one to perform a
directional spherical wavelet analysis of a 10^6 pixel map on a personal
computer.
</dc:description>
 <dc:description>Comment: 10 pages, 3 figures, replaced to match version accepted by IEEE
  Trans. Sig. Proc</dc:description>
 <dc:date>2005-06-14</dc:date>
 <dc:date>2006-05-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0506308</dc:identifier>
 <dc:identifier>IEEE Trans.Signal Process. 55 (2007) 520-529</dc:identifier>
 <dc:identifier>doi:10.1109/TSP.2006.887148</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0510041</identifier>
 <datestamp>2009-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Acceleration of adaptive optics simulations using programmable logic</dc:title>
 <dc:creator>Basden, A. G.</dc:creator>
 <dc:creator>Assemat, F.</dc:creator>
 <dc:creator>Butterley, T.</dc:creator>
 <dc:creator>Geng, D.</dc:creator>
 <dc:creator>Saunter, C. D.</dc:creator>
 <dc:creator>Wilson, R. W.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  Numerical Simulation is an essential part of the design and optimisation of
astronomical adaptive optics systems. Simulations of adaptive optics are
computationally expensive and the problem scales rapidly with telescope
aperture size, as the required spatial order of the correcting system
increases. Practical realistic simulations of AO systems for extremely large
telescopes are beyond the capabilities of all but the largest of modern
parallel supercomputers. Here we describe a more cost effective approach
through the use of hardware acceleration using field programmable gate arrays.
By transferring key parts of the simulation into programmable logic, large
increases in computational bandwidth can be expected. We show that the
calculation of wavefront sensor image centroids can be accelerated by a factor
of four by transferring the algorithm into hardware. Implementing more
demanding parts of the adaptive optics simulation in hardware will lead to much
greater performance improvements, of up to 1000 times.
</dc:description>
 <dc:description>Comment: 6 pages accepted by MNRAS</dc:description>
 <dc:date>2005-10-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0510041</dc:identifier>
 <dc:identifier>Mon.Not.Roy.Astron.Soc.364:1413-1418,2005</dc:identifier>
 <dc:identifier>doi:10.1111/j.1365-2966.2005.09670.x</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0510688</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using the Parallel Virtual Machine for Everyday Analysis</dc:title>
 <dc:creator>Noble, M. S.</dc:creator>
 <dc:creator>Houck, J. C.</dc:creator>
 <dc:creator>Davis, J. E.</dc:creator>
 <dc:creator>Young, A.</dc:creator>
 <dc:creator>Nowak, M.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  A review of the literature reveals that while parallel computing is sometimes
employed by astronomers for custom, large-scale calculations, no package
fosters the routine application of parallel methods to standard problems in
astronomical data analysis. This paper describes our attempt to close that gap
by wrapping the Parallel Virtual Machine (PVM) as a scriptable S-Lang module.
Using PVM within ISIS, the Interactive Spectral Interpretation System, we've
distributed a number of representive calculations over a network of 25+ CPUs to
achieve dramatic reductions in execution times. We discuss how the approach
applies to a wide class of modeling problems, outline our efforts to make it
more transparent for common use, and note its growing importance in the context
of the large, multi-wavelength datasets used in modern analysis.
</dc:description>
 <dc:description>Comment: 4 pages; manuscript for oral presentation given at ADASS XV, Madrid</dc:description>
 <dc:date>2005-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0510688</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0605042</identifier>
 <datestamp>2009-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>How accurate are the time delay estimates in gravitational lensing?</dc:title>
 <dc:creator>Cuevas-Tello, Juan C.</dc:creator>
 <dc:creator>Tino, Peter</dc:creator>
 <dc:creator>Raychaudhury, Somak</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  We present a novel approach to estimate the time delay between light curves
of multiple images in a gravitationally lensed system, based on Kernel methods
in the context of machine learning. We perform various experiments with
artificially generated irregularly-sampled data sets to study the effect of the
various levels of noise and the presence of gaps of various size in the
monitoring data. We compare the performance of our method with various other
popular methods of estimating the time delay and conclude, from experiments
with artificial data, that our method is least vulnerable to missing data and
irregular sampling, within reasonable bounds of Gaussian noise. Thereafter, we
use our method to determine the time delays between the two images of quasar
Q0957+561 from radio monitoring data at 4 cm and 6 cm, and conclude that if
only the observations at epochs common to both wavelengths are used, the time
delay gives consistent estimates, which can be combined to yield 408\pm 12
days. The full 6 cm dataset, which covers a longer monitoring period, yields a
value which is 10% larger, but this can be attributed to differences in
sampling and missing data.
</dc:description>
 <dc:description>Comment: 14 pages, 12 figures; accepted for publication in Astronomy &amp;
  Astrophysics</dc:description>
 <dc:date>2006-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0605042</dc:identifier>
 <dc:identifier>Astron.Astrophys. 454 (2006) 695-706</dc:identifier>
 <dc:identifier>doi:10.1051/0004-6361:20054652</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0605514</identifier>
 <datestamp>2008-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An algorithm for solving the pulsar equation</dc:title>
 <dc:creator>Bratek, Lukasz</dc:creator>
 <dc:creator>Kolonko, Marcin</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Numerical Analysis</dc:subject>
 <dc:description>  We present an algorithm of finding numerical solutions of pulsar equation.
The problem of finding the solutions was reduced to finding expansion
coefficients of the source term of the equation in a base of orthogo- nal
functions defined on the unit interval by minimizing a multi-variable mismatch
function defined on the light cylinder. We applied the algorithm to Scharlemann
&amp; Wagoner boundary conditions by which a smooth solu- tion is reconstructed
that by construction passes success- fully the Gruzinov's test of the source
function exponent.
</dc:description>
 <dc:description>Comment: 4 pages, 4 figures, accepted for publication in ApSS (a shortened
  version of the previous one)</dc:description>
 <dc:date>2006-05-20</dc:date>
 <dc:date>2006-11-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0605514</dc:identifier>
 <dc:identifier>Astrophys.SpaceSci.309:231-234,2007</dc:identifier>
 <dc:identifier>doi:10.1007/s10509-007-9406-y</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0609159</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A directional continuous wavelet transform on the sphere</dc:title>
 <dc:creator>McEwen, J. D.</dc:creator>
 <dc:creator>Hobson, M. P.</dc:creator>
 <dc:creator>Lasenby, A. N.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  A new construction of a directional continuous wavelet analysis on the sphere
is derived herein. We adopt the harmonic scaling idea for the spherical
dilation operator recently proposed by Sanz et al. but extend the analysis to a
more general directional framework. Directional wavelets are a powerful
extension that allow one to also probe oriented structure in the analysed
function. Our spherical wavelet methodology has the advantage that all
functions and operators are defined directly on the sphere. The construction of
wavelets in our framework is demonstrated with an example.
</dc:description>
 <dc:description>Comment: 7 pages, 2 figures</dc:description>
 <dc:date>2006-09-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0609159</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0609794</identifier>
 <datestamp>2011-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Future of Technical Libraries</dc:title>
 <dc:creator>Kurtz, Michael J.</dc:creator>
 <dc:creator>Eichhorn, Guenther</dc:creator>
 <dc:creator>Accomazzi, Alberto</dc:creator>
 <dc:creator>Grant, Carolyn</dc:creator>
 <dc:creator>Henneken, Edwin</dc:creator>
 <dc:creator>Thompson, Donna</dc:creator>
 <dc:creator>Bohlen, Elizabeth</dc:creator>
 <dc:creator>Murray, Stephen S.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Digital Libraries</dc:subject>
 <dc:description>  Technical libraries are currently experiencing very rapid change. In the near
future their mission will change, their physical nature will change, and the
skills of their employees will change. While some will not be able to make
these changes, and will fail, others will lead us into a new era.
</dc:description>
 <dc:description>Comment: To appear in Library and Information Systems in Astronomy V</dc:description>
 <dc:date>2006-09-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0609794</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0612688</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Optimal filters on the sphere</dc:title>
 <dc:creator>McEwen, J. D.</dc:creator>
 <dc:creator>Hobson, M. P.</dc:creator>
 <dc:creator>Lasenby, A. N.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Information Theory</dc:subject>
 <dc:description>  We derive optimal filters on the sphere in the context of detecting compact
objects embedded in a stochastic background process. The matched filter and the
scale adaptive filter are derived on the sphere in the most general setting,
allowing for directional template profiles and filters. The performance and
relative merits of the two optimal filters are discussed. The application of
optimal filter theory on the sphere to the detection of compact objects is
demonstrated on simulated mock data. A naive detection strategy is adopted,
with an initial aim of illustrating the application of the new optimal filters
derived on the sphere. Nevertheless, this simple object detection strategy is
demonstrated to perform well, even a low signal-to-noise ratio. Code written to
compute optimal filters on the sphere (S2FIL), to perform fast directional
filtering on the sphere (FastCSWT) and to construct the simulated mock data
(COMB) are all made publicly available from http://www.mrao.cam.ac.uk/~jdm57/
</dc:description>
 <dc:description>Comment: 10 pages, 5 figures, replaced to match version accepted by IEEE Sig.
  Proc</dc:description>
 <dc:date>2006-12-22</dc:date>
 <dc:date>2008-07-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0612688</dc:identifier>
 <dc:identifier>IEEETrans.SignalProcess.56:3813-3823,2008</dc:identifier>
 <dc:identifier>doi:10.1109/TSP.2008.923198</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/0703485</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards Distributed Petascale Computing</dc:title>
 <dc:creator>Hoekstra, A. G.</dc:creator>
 <dc:creator>Zwart, S. F. Portegies</dc:creator>
 <dc:creator>Bubak, M.</dc:creator>
 <dc:creator>Sloot, P. M. A.</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  In this chapter we will argue that studying such multi-scale multi-science
systems gives rise to inherently hybrid models containing many different
algorithms best serviced by different types of computing environments (ranging
from massively parallel computers, via large-scale special purpose machines to
clusters of PC's) whose total integrated computing capacity can easily reach
the PFlop/s scale. Such hybrid models, in combination with the by now
inherently distributed nature of the data on which the models `feed' suggest a
distributed computing model, where parts of the multi-scale multi-science model
are executed on the most suitable computing environment, and/or where the
computations are carried out close to the required data (i.e. bring the
computations to the data instead of the other way around). We presents an
estimate for the compute requirements to simulate the Galaxy as a typical
example of a multi-scale multi-physics application, requiring distributed
Petaflop/s computational power.
</dc:description>
 <dc:description>Comment: To appear in D. Bader (Ed.) Petascale, Computing: Algorithms and
  Applications, Chapman &amp; Hall / CRC Press, Taylor and Francis Group</dc:description>
 <dc:date>2007-03-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/0703485</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:astro-ph/9912134</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Interfacing Interpreted and Compiled Languages to Support Applications
  on a Massively Parallel Network of Workstations (MP-NOW)</dc:title>
 <dc:creator>Kepner, Jeremy</dc:creator>
 <dc:creator>Gokhale, Maya</dc:creator>
 <dc:creator>Minnich, Ron</dc:creator>
 <dc:creator>Marks, Aaron</dc:creator>
 <dc:creator>DeGood, John</dc:creator>
 <dc:subject>Astrophysics</dc:subject>
 <dc:subject>Computer Science - Distributed, Parallel, and Cluster Computing</dc:subject>
 <dc:description>  Astronomers are increasingly using Massively Parallel Network of Workstations
(MP-NOW) to address their most challenging computing problems. Fully exploiting
these systems is made more difficult as more and more modeling and data
analysis software is written in interpreted languages (such as IDL, MATLAB, and
Mathematica) which do not lend themselves to parallel computing. We present a
specific example of a very simple, but generic solution to this problem. Our
example uses an interpreted language (IDL) to set up a calculation and then
interfaces with a computational kernel written in a compiled language (C). The
IDL code then calls the C code as an external library. We have added to the
computational kernel an additional layer, which manages multiple copies of the
kernel running on a MP-NOW and returns the results back to the interpreted
layer. Our implementation uses The Next generation Taskbag (TNT) library
developed at Sarnoff to provide an efficient means for implementing task
parallelism. A test problem (taken from Astronomy) has been implemented on the
Sarnoff Cyclone computer which consists of 160 heterogeneous nodes connected by
a ``fat'' tree 100 Mb/s switched Ethernet running the RedHat Linux and FreeBSD
operating systems. Our first results in this ongoing project have demonstrated
the feasibility of this approach and produced speedups of greater than 50 on 60
processors.
</dc:description>
 <dc:description>Comment: To appear in Cluster Computing, 22 pages including 8 color figures</dc:description>
 <dc:date>1999-12-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/astro-ph/9912134</dc:identifier>
 <dc:identifier>Cluster Computing 07-2000, Volume 3, Issue 1, pp 35-44</dc:identifier>
 <dc:identifier>doi:10.1023/A:1019011716367</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:chao-dyn/9905036</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Restart Strategies and Internet Congestion</dc:title>
 <dc:creator>Maurer, Sebastian M.</dc:creator>
 <dc:creator>Huberman, Bernardo A.</dc:creator>
 <dc:subject>Nonlinear Sciences - Chaotic Dynamics</dc:subject>
 <dc:subject>Computer Science - Networking and Internet Architecture</dc:subject>
 <dc:subject>Nonlinear Sciences - Adaptation and Self-Organizing Systems</dc:subject>
 <dc:description>  We recently presented a methodology for quantitatively reducing the risk and
cost of executing electronic transactions in a bursty network environment such
as the Internet. In the language of portfolio theory, time to complete a
transaction and its variance replace the expected return and risk associated
with a security, whereas restart times replace combinations of securities.
While such a strategy works well with single users, the question remains as to
its usefulness when used by many. By using mean field arguments and agent-based
simulations, we determine that a restart strategy remains advantageous even if
everybody uses it.
</dc:description>
 <dc:description>Comment: 15 pages, 8 figures</dc:description>
 <dc:date>1999-05-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/chao-dyn/9905036</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:chao-dyn/9909031</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Noncommutative Martin-Lof randomness : on the concept of a random
  sequence of qubits</dc:title>
 <dc:creator>Segre, Gavriel</dc:creator>
 <dc:subject>Nonlinear Sciences - Chaotic Dynamics</dc:subject>
 <dc:subject>Computer Science - Computational Complexity</dc:subject>
 <dc:subject>Mathematical Physics</dc:subject>
 <dc:subject>Nonlinear Sciences - Adaptation and Self-Organizing Systems</dc:subject>
 <dc:subject>Quantum Physics</dc:subject>
 <dc:description>  Martin-Lof's definition of random sequences of cbits as those not belonging
to any set of constructive zero Lebesgue measure is reformulated in the
language of Algebraic Probability Theory.
  The adoption of the Pour-El Richards theory of computability structures on
Banach spaces allows us to give a natural noncommutative extension of
Martin-Lof's definition, characterizing the random elements of a chain Von
Neumann algebra.
  In the particular case of the minimally informative noncommutative alphabet
our definition reduces to the definition of a random sequence of qubits.
</dc:description>
 <dc:description>Comment: revised version</dc:description>
 <dc:date>1999-09-21</dc:date>
 <dc:date>1999-10-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/chao-dyn/9909031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Alternative Conception of Tree-Adjoining Derivation</dc:title>
 <dc:creator>Schabes, Yves</dc:creator>
 <dc:creator>Shieber, Stuart M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The precise formulation of derivation for tree-adjoining grammars has
important ramifications for a wide variety of uses of the formalism, from
syntactic analysis to semantic interpretation and statistical language
modeling. We argue that the definition of tree-adjoining derivation must be
reformulated in order to manifest the proper linguistic dependencies in
derivations. The particular proposal is both precisely characterizable through
a definition of TAG derivations as equivalence classes of ordered derivation
trees, and computationally operational, by virtue of a compilation to linear
indexed grammars together with an efficient algorithm for recognition and
parsing according to the compiled grammar.
</dc:description>
 <dc:description>Comment: 33 pages</dc:description>
 <dc:date>1994-04-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404001</dc:identifier>
 <dc:identifier>Computational Linguistics 20(1):91-124</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lessons from a Restricted Turing Test</dc:title>
 <dc:creator>Shieber, Stuart M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We report on the recent Loebner prize competition inspired by Turing's test
of intelligent behavior. The presentation covers the structure of the
competition and the outcome of its first instantiation in an actual event, and
an analysis of the purpose, design, and appropriateness of such a competition.
We argue that the competition has no clear purpose, that its design prevents
any useful outcome, and that such a competition is inappropriate given the
current level of technology. We then speculate as to suitable alternatives to
the Loebner prize.
</dc:description>
 <dc:description>Comment: 20 pages</dc:description>
 <dc:date>1994-04-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Restricting the Weak-Generative Capacity of Synchronous Tree-Adjoining
  Grammars</dc:title>
 <dc:creator>Shieber, Stuart M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The formalism of synchronous tree-adjoining grammars, a variant of standard
tree-adjoining grammars (TAG), was intended to allow the use of TAGs for
language transduction in addition to language specification. In previous work,
the definition of the transduction relation defined by a synchronous TAG was
given by appeal to an iterative rewriting process. The rewriting definition of
derivation is problematic in that it greatly extends the expressivity of the
formalism and makes the design of parsing algorithms difficult if not
impossible. We introduce a simple, natural definition of synchronous
tree-adjoining derivation, based on isomorphisms between standard
tree-adjoining derivations, that avoids the expressivity and implementability
problems of the original rewriting definition. The decrease in expressivity,
which would otherwise make the method unusable, is offset by the incorporation
of an alternative definition of standard tree-adjoining derivation, previously
proposed for completely separate reasons, thereby making it practical to
entertain using the natural definition of synchronous derivation. Nonetheless,
some remaining problematic cases call for yet more flexibility in the
definition; the isomorphism requirement may have to be relaxed. It remains for
future research to tune the exact requirements on the allowable mappings.
</dc:description>
 <dc:description>Comment: 21 pages, uses lingmacros.sty, psfig.sty, fullname.sty; minor
  typographical changes only</dc:description>
 <dc:date>1994-04-03</dc:date>
 <dc:date>1994-08-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404003</dc:identifier>
 <dc:identifier>Computational Intelligence 10(4):371-385, November 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Empirically Motivated Reinterpretation of Dependency Grammar</dc:title>
 <dc:creator>Covington, Michael A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Dependency grammar is usually interpreted as equivalent to a strict form of
X--bar theory that forbids the stacking of nodes of the same bar level (e.g.,
N' immediately dominating N' with the same head). But adequate accounts of
_one_--anaphora and of the semantics of multiple modifiers require such
stacking and accordingly argue against dependency grammar. Dependency grammar
can be salvaged by reinterpreting its claims about phrase structure, so that
modifiers map onto binary--branching X--bar trees rather than ``flat'' ones.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX (PostScript version available by ftp from ai.uga.edu)</dc:description>
 <dc:date>1994-04-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Memoization in Constraint Logic Programming</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows how to apply memoization (caching of subgoals and associated
answer substitutions) in a constraint logic programming setting. The research
is is motivated by the desire to apply constraint logic programming (CLP) to
problems in natural language processing that involve (constraint) interleaving
or coroutining, such as GB and HPSG parsing.
</dc:description>
 <dc:description>Comment: 11 pages</dc:description>
 <dc:date>1994-04-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for
  medical purposes</dc:title>
 <dc:creator>CHANDLER-BURNS, R. M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  S92 research was begun in 1987 to analyze word frequencies in present-day
Spanish for making speech pathology evaluation tools. 500 2,000-word samples of
children, adolescents and adults' language were input between 1988-1991,
calculations done in 1992; statistical and Lewandowski analyses were carried
out in 1993.
</dc:description>
 <dc:description>Comment: 20 pages</dc:description>
 <dc:date>1994-04-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constraint-Based Categorial Grammar</dc:title>
 <dc:creator>Bouma, Gosse</dc:creator>
 <dc:creator>van Noord, Gertjan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a generalization of Categorial Grammar in which lexical categories
are defined by means of recursive constraints. In particular, the introduction
of relational constraints allows one to capture the effects of (recursive)
lexical rules in a computationally attractive manner. We illustrate the
linguistic merits of the new approach by showing how it accounts for the syntax
of Dutch cross-serial dependencies and the position and scope of adjuncts in
such constructions. Delayed evaluation is used to process grammars containing
recursive constraints.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX</dc:description>
 <dc:date>1994-04-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404007</dc:identifier>
 <dc:identifier>Proceedings ACL 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Principles and Implementation of Deductive Parsing</dc:title>
 <dc:creator>Shieber, Stuart M.</dc:creator>
 <dc:creator>Schabes, Yves</dc:creator>
 <dc:creator>Pereira, Fernando C. N.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a system for generating parsers based directly on the metaphor of
parsing as deduction. Parsing algorithms can be represented directly as
deduction systems, and a single deduction engine can interpret such deduction
systems so as to implement the corresponding parser. The method generalizes
easily to parsers for augmented phrase structure formalisms, such as
definite-clause grammars and other logic grammar formalisms, and has been used
for rapid prototyping of parsing algorithms for a variety of formalisms
including variants of tree-adjoining grammars, categorial grammars, and
lexicalized context-free grammars.
</dc:description>
 <dc:description>Comment: 69 pages, includes full Prolog code</dc:description>
 <dc:date>1994-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Deductive Account of Quantification in LFG</dc:title>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Lamping, John</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Saraswat, Vijay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The relationship between Lexical-Functional Grammar (LFG) functional
structures (f-structures) for sentences and their semantic interpretations can
be expressed directly in a fragment of linear logic in a way that explains
correctly the constrained interactions between quantifier scope ambiguity and
bound anaphora. The use of a deductive framework to account for the
compositional properties of quantifying expressions in natural language
obviates the need for additional mechanisms, such as Cooper storage, to
represent the different scopes that a quantifier might take. Instead, the
semantic contribution of a quantifier is recorded as an ordinary logical
formula, one whose use in a proof will establish the scope of the quantifier.
The properties of linear logic ensure that each quantifier is scoped exactly
once. Our analysis of quantifier scope can be seen as a recasting of Pereira's
analysis (Pereira, 1991), which was expressed in higher-order intuitionistic
logic. But our use of LFG and linear logic provides a much more direct and
computationally more flexible interpretation mechanism for at least the same
range of phenomena. We have developed a preliminary Prolog implementation of
the linear deductions described in this work.
</dc:description>
 <dc:description>Comment: 27 pages, extensively revised</dc:description>
 <dc:date>1994-04-27</dc:date>
 <dc:date>1994-05-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intensional Verbs Without Type-Raising or Lexical Ambiguity</dc:title>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Lamping, John</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Saraswat, Vijay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an analysis of the semantic interpretation of intensional verbs
such as seek that allows them to take direct objects of either individual or
quantifier type, producing both de dicto and de re readings in the quantifier
case, all without needing to stipulate type-raising or quantifying-in rules.
This simple account follows directly from our use of logical deduction in
linear logic to express the relationship between syntactic structures and
meanings. While our analysis resembles current categorial approaches in
important ways, it differs from them in allowing the greater type flexibility
of categorial semantics while maintaining a precise connection to syntax. As a
result, we are able to provide derivations for certain readings of sentences
with intensional verbs and complex direct objects that are not derivable in
current purely categorial accounts of the syntax-semantics interface. The
analysis forms a part of our ongoing work on semantic interpretation within the
framework of Lexical-Functional Grammar.
</dc:description>
 <dc:description>Comment: 16 pages, revised and extended, to appear in the proceedings of the
  Conference on Information-Oriented Approaches to Logic, Language and
  Computation</dc:description>
 <dc:date>1994-04-27</dc:date>
 <dc:date>1994-08-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9404011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adjuncts and the Processing of Lexical Rules</dc:title>
 <dc:creator>van Noord, Gertjan</dc:creator>
 <dc:creator>Bouma, Gosse</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The standard HPSG analysis of Germanic verb clusters can not explain the
observed narrow-scope readings of adjuncts in such verb clusters. We present an
extension of the HPSG analysis that accounts for the systematic ambiguity of
the scope of adjuncts in verb cluster constructions, by treating adjuncts as
members of the subcat list. The extension uses powerful recursive lexical
rules, implemented as complex constraints. We show how `delayed evaluation'
techniques from constraint-logic programming can be used to process such
lexical rules.
</dc:description>
 <dc:description>Comment: 8 pages (a4wide), to be published in Coling-94</dc:description>
 <dc:date>1994-04-28</dc:date>
 <dc:date>1994-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9404011</dc:identifier>
 <dc:identifier>Proceedings of Coling 1994 Kyoto</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Similarity-Based Estimation of Word Cooccurrence Probabilities</dc:title>
 <dc:creator>Dagan, Ido</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Lee, Lillian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In many applications of natural language processing it is necessary to
determine the likelihood of a given word combination. For example, a speech
recognizer may need to determine which of the two word combinations ``eat a
peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine
the likelihood of a word combination according to its frequency in a training
corpus. However, the nature of language is such that many word combinations are
infrequent and do not occur in a given corpus. In this work we propose a method
for estimating the probability of such previously unseen word combinations
using available information on ``most similar'' words. We describe a
probabilistic word association model based on distributional word similarity,
and apply it to improving probability estimates for unseen word bigrams in a
variant of Katz's back-off model. The similarity-based method yields a 20%
perplexity improvement in the prediction of unseen bigrams and statistically
significant reductions in speech-recognition error.
</dc:description>
 <dc:description>Comment: 13 pages, to appear in proceedings of ACL-94</dc:description>
 <dc:date>1994-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Temporal Relations: Reference or Discourse Coherence?</dc:title>
 <dc:creator>Kehler, Andrew</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The temporal relations that hold between events described by successive
utterances are often left implicit or underspecified. We address the role of
two phenomena with respect to the recovery of these relations: (1) the
referential properties of tense, and (2) the role of temporal constraints
imposed by coherence relations. We account for several facets of the
identification of temporal relations through an integration of these.
</dc:description>
 <dc:description>Comment: To appear in the Proceedings of ACL-94, Student Session. 5 pages,
  LaTeX source, requires lingmacros. Comments are welcome</dc:description>
 <dc:date>1994-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405002</dc:identifier>
 <dc:identifier>ACL-94 (Student Session), Las Cruces, New Mexico</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Some Bibliographical References on Intonation and Intonational Meaning</dc:title>
 <dc:creator>Hirschberg, Julia</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A by-no-means-complete collection of references for those interested in
intonational meaning, with other miscellaneous references on intonation
included. Additional references are welcome, and should be sent to
julia@research.att.com.
</dc:description>
 <dc:description>Comment: 14 pp of text and citations, bibtex added as separate file</dc:description>
 <dc:date>1994-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Syntactic-Head-Driven Generation</dc:title>
 <dc:creator>Koenig, Esther</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The previously proposed semantic-head-driven generation methods run into
problems if none of the daughter constituents in the syntacto-semantic rule
schemata of a grammar fits the definition of a semantic head given in Shieber
et al. 1990. This is the case for the semantic analysis rules of certain
constraint-based semantic representations, e.g. Underspecified Discourse
Representation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven
generation in general has its merits, we simply return to a syntactic
definition of `head' and demonstrate the feasibility of syntactic-head-driven
generation. In addition to its generality, a syntactic-head-driven algorithm
provides a basis for a logically well-defined treatment of the movement of
(syntactic) heads, for which only ad-hoc solutions existed, so far.
</dc:description>
 <dc:description>Comment: 7 pages, to appear in Proceedings of COLING 94</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Pearl: A Probabilistic Chart Parser</dc:title>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:creator>Marcus, Mitchell P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a natural language parsing algorithm for unrestricted
text which uses a probability-based scoring function to select the &quot;best&quot; parse
of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser
with Earley-type top-down prediction which pursues the highest-scoring theory
in the chart, where the score of a theory represents the extent to which the
context of the sentence predicts that interpretation. This parser differs from
previous attempts at stochastic parsers in that it uses a richer form of
conditional probabilities based on context to predict likelihood. Pearl also
provides a framework for incorporating the results of previous work in
part-of-speech assignment, unknown word models, and other probabilistic models
of linguistic features into one parsing tool, interleaving these techniques
instead of using the traditional pipeline architecture. In preliminary tests,
Pearl has been successful at resolving part-of-speech and word (in speech
processing) ambiguity, determining categories for unknown words, and selecting
correct parses first using a very loosely fitting covering grammar.
</dc:description>
 <dc:description>Comment: 7 pages</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405005</dc:identifier>
 <dc:identifier>Proceedings, 2nd International Workshop for Parsing Technologies</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficiency, Robustness, and Accuracy in Picky Chart Parsing</dc:title>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:creator>Weir, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes Picky, a probabilistic agenda-based chart parsing
algorithm which uses a technique called {\em probabilistic prediction} to
predict which grammar rules are likely to lead to an acceptable parse of the
input. Using a suboptimal search method, Picky significantly reduces the number
of edges produced by CKY-like chart parsing algorithms, while maintaining the
robustness of pure bottom-up parsers and the accuracy of existing probabilistic
parsers. Experiments using Picky demonstrate how probabilistic modelling can
impact upon the efficiency, robustness and accuracy of a parser.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405006</dc:identifier>
 <dc:identifier>Proceedings, ACL 1992</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards History-based Grammars: Using Richer Models for Probabilistic
  Parsing</dc:title>
 <dc:creator>Black, Ezra</dc:creator>
 <dc:creator>Jelinek, Fred</dc:creator>
 <dc:creator>Lafferty, John</dc:creator>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:creator>Mercer, Robert</dc:creator>
 <dc:creator>Roukos, Salim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a generative probabilistic model of natural language, which we
call HBG, that takes advantage of detailed linguistic information to resolve
ambiguity. HBG incorporates lexical, syntactic, semantic, and structural
information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Treebank, in combination with
decision tree building to tease out the relevant aspects of a parse tree that
will determine the correct parse of a sentence. This stands in contrast to the
usual approach of further grammar tailoring via the usual linguistic
introspection in the hope of generating the correct parse. In head-to-head
tests against one of the best existing robust probabilistic parsing models,
which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing
the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405007</dc:identifier>
 <dc:identifier>Proceedings, DARPA Speech and Natural Language Workshop, 1992</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese</dc:title>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:creator>Shih, Chilin</dc:creator>
 <dc:creator>Gale, William</dc:creator>
 <dc:creator>Chang, Nancy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a stochastic finite-state model for segmenting Chinese text into
dictionary entries and productively derived words, and providing pronunciations
for these words; the method incorporates a class-based model in its treatment
of personal names. We also evaluate the system's performance, taking into
account the fact that people often do not agree on a single segmentation.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of ACL-94</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:date>1994-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405008</dc:identifier>
 <dc:identifier>in Proceedings of ACL 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405009</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural Language Parsing as Statistical Pattern Recognition</dc:title>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Traditional natural language parsers are based on rewrite rule systems
developed in an arduous, time-consuming manner by grammarians. A majority of
the grammarian's efforts are devoted to the disambiguation process, first
hypothesizing rules which dictate constituent categories and relationships
among words in ambiguous sentences, and then seeking exceptions and corrections
to these rules.
  In this work, I propose an automatic method for acquiring a statistical
parser from a set of parsed sentences which takes advantage of some initial
linguistic input, but avoids the pitfalls of the iterative and seemingly
endless grammar development process. Based on distributionally-derived and
linguistically-based features of language, this parser acquires a set of
statistical decision trees which assign a probability distribution on the space
of parse trees given the input sentence. These decision trees take advantage of
significant amount of contextual information, potentially including all of the
lexical information in the sentence, to produce highly accurate statistical
models of the disambiguation process. By basing the disambiguation criteria
selection on entropy reduction rather than human intuition, this parser
development method is able to consider more sentences than a human grammarian
can when making individual disambiguation rules.
  In experiments between a parser, acquired using this statistical framework,
and a grammarian's rule-based parser, developed over a ten-year period, both
using the same training material and test sentences, the decision tree parser
significantly outperformed the grammar-based parser on the accuracy measure
which the grammarian was trying to maximize, achieving an accuracy of 78%
compared to the grammar-based parser's 69%.
</dc:description>
 <dc:description>Comment: 160+ pages, doctoral dissertation (latex with figures)</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:date>1994-05-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Common Topics and Coherent Situations: Interpreting Ellipsis in the
  Context of Discourse Inference</dc:title>
 <dc:creator>Kehler, Andrew</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  It is claimed that a variety of facts concerning ellipsis, event reference,
and interclausal coherence can be explained by two features of the linguistic
form in question: (1) whether the form leaves behind an empty constituent in
the syntax, and (2) whether the form is anaphoric in the semantics. It is
proposed that these features interact with one of two types of discourse
inference, namely {\it Common Topic} inference and {\it Coherent Situation}
inference. The differing ways in which these types of inference utilize
syntactic and semantic representations predicts phenomena for which it is
otherwise difficult to account.
</dc:description>
 <dc:description>Comment: To be presented at ACL-94. 13 pages, LaTeX source, accompanying
  PostScript figures, requires psfig and lingmacros. Comments are welcome</dc:description>
 <dc:date>1994-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405010</dc:identifier>
 <dc:identifier>ACL-94, Las Cruces, New Mexico</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Plan-Based Model for Response Generation in Collaborative
  Task-Oriented Dialogues</dc:title>
 <dc:creator>Chu-Carroll, Jennifer</dc:creator>
 <dc:creator>Carberry, Sandra</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a plan-based architecture for response generation in
collaborative consultation dialogues, with emphasis on cases in which the
system (consultant) and user (executing agent) disagree. Our work contributes
to an overall system for collaborative problem-solving by providing a
plan-based framework that captures the {\em Propose-Evaluate-Modify} cycle of
collaboration, and by allowing the system to initiate subdialogues to negotiate
proposed additions to the shared plan and to provide support for its claims. In
addition, our system handles in a unified manner the negotiation of proposed
domain actions, proposed problem-solving actions, and beliefs proposed by
discourse actions. Furthermore, it captures cooperative responses within the
collaborative framework and accounts for why questions are sometimes never
answered.
</dc:description>
 <dc:description>Comment: 8 pages, to appear in the Proceedings of AAAI-94. LaTeX source file,
  requires aaai.sty and epsf.tex. Figures included in separate files</dc:description>
 <dc:date>1994-05-05</dc:date>
 <dc:date>1994-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integration Of Visual Inter-word Constraints And Linguistic Knowledge In
  Degraded Text Recognition</dc:title>
 <dc:creator>Hong, Tao</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Degraded text recognition is a difficult task. Given a noisy text image, a
word recognizer can be applied to generate several candidates for each word
image. High-level knowledge sources can then be used to select a decision from
the candidate set for each word image. In this paper, we propose that visual
inter-word constraints can be used to facilitate candidate selection. Visual
inter-word constraints provide a way to link word images inside the text page,
and to interpret them systematically.
</dc:description>
 <dc:description>Comment: 3 pages, PostScript File, to appear in the Proceedings of ACL-94,
  Student Session</dc:description>
 <dc:date>1994-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405012</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94 (Student Session)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Collaboration on reference to objects that are not mutually known</dc:title>
 <dc:creator>Edmonds, Philip G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In conversation, a person sometimes has to refer to an object that is not
previously known to the other participant. We present a plan-based model of how
agents collaborate on reference of this sort. In making a reference, an agent
uses the most salient attributes of the referent. In understanding a reference,
an agent determines his confidence in its adequacy as a means of identifying
the referent. To collaborate, the agents use judgment, suggestion, and
elaboration moves to refashion an inadequate referring expression.
</dc:description>
 <dc:description>Comment: 6 pages, to appear in proceedings of COLING-94, LaTeX (now uses
  fullname.sty, fullname.bst)</dc:description>
 <dc:date>1994-05-06</dc:date>
 <dc:date>1994-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Classifying Cue Phrases in Text and Speech Using Machine Learning</dc:title>
 <dc:creator>Litman, Diane J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification rules from sets
of pre-classified cue phrases and their features. Machine learning is shown to
be an effective technique for not only automating the generation of
classification rules, but also for improving upon previous results.
</dc:description>
 <dc:description>Comment: 8 pages, PostScript File, to appear in the Proceedings of AAAI-94</dc:description>
 <dc:date>1994-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intention-based Segmentation: Human Reliability and Correlation with
  Linguistic Cues</dc:title>
 <dc:creator>Passonneau, Rebecca J.</dc:creator>
 <dc:creator>Litman, Diane J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Certain spans of utterances in a discourse, referred to here as segments, are
widely assumed to form coherent units. Further, the segmental structure of
discourse has been claimed to constrain and be constrained by many phenomena.
However, there is weak consensus on the nature of segments and the criteria for
recognizing or generating them. We present quantitative results of a two part
study using a corpus of spontaneous, narrative monologues. The first part
evaluates the statistical reliability of human segmentation of our corpus,
where speaker intention is the segmentation criterion. We then use the
subjects' segmentations to evaluate the correlation of discourse segmentation
with three linguistic cues (referential noun phrases, cue words, and pauses),
using information retrieval metrics.
</dc:description>
 <dc:description>Comment: 8 pages, PostScript File, in Proceedings of ACL-93</dc:description>
 <dc:date>1994-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Precise n-gram Probabilities from Stochastic Context-free Grammars</dc:title>
 <dc:creator>Stolcke, Andreas</dc:creator>
 <dc:creator>Segal, Jonathan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an algorithm for computing n-gram probabilities from stochastic
context-free grammars, a procedure that can alleviate some of the standard
problems associated with n-grams (estimation from sparse data, lack of
linguistic structure, among others). The method operates via the computation of
substring expectations, which in turn is accomplished by solving systems of
linear equations derived from the grammar. We discuss efficient implementation
of the algorithm and report our practical experience with it.
</dc:description>
 <dc:description>Comment: 12 pages, to appear in ACL-94</dc:description>
 <dc:date>1994-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405016</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Best-first Model Merging for Hidden Markov Model Induction</dc:title>
 <dc:creator>Stolcke, Andreas</dc:creator>
 <dc:creator>Omohundro, Stephen M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This report describes a new technique for inducing the structure of Hidden
Markov Models from data which is based on the general `model merging' strategy
(Omohundro 1992). The process begins with a maximum likelihood HMM that
directly encodes the training data. Successively more general models are
produced by merging HMM states. A Bayesian posterior probability criterion is
used to determine which states to merge and when to stop generalizing. The
procedure may be considered a heuristic search for the HMM structure with the
highest posterior probability. We discuss a variety of possible priors for
HMMs, as well as a number of approximations which improve the computational
efficiency of the algorithm. We studied three applications to evaluate the
procedure. The first compares the merging algorithm with the standard
Baum-Welch approach in inducing simple finite-state languages from small,
positive-only training samples. We found that the merging procedure is more
robust and accurate, particularly with a small amount of training data. The
second application uses labelled speech data from the TIMIT database to build
compact, multiple-pronunciation word models that can be used in speech
recognition. Finally, we describe how the algorithm was incorporated in an
operational speech understanding system, where it is combined with neural
network acoustic likelihood estimators to improve performance over
single-pronunciation word models.
</dc:description>
 <dc:description>Comment: 63 pages</dc:description>
 <dc:date>1994-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Memory-Based Lexical Acquisition and Processing</dc:title>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Current approaches to computational lexicology in language technology are
knowledge-based (competence-oriented) and try to abstract away from specific
formalisms, domains, and applications. This results in severe complexity,
acquisition and reusability bottlenecks. As an alternative, we propose a
particular performance-oriented approach to Natural Language Processing based
on automatic memory-based learning of linguistic (lexical) tasks. The
consequences of the approach for computational lexicology are discussed, and
the application of the approach on a number of lexical acquisition and
disambiguation tasks in phonology, morphology and syntax is described.
</dc:description>
 <dc:description>Comment: 18 pages</dc:description>
 <dc:date>1994-05-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405018</dc:identifier>
 <dc:identifier>Steffens (ed.) Machine Translation &amp; Lexion. Springer, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Determination of referential property and number of nouns in Japanese
  sentences for machine translation into English</dc:title>
 <dc:creator>Murata, Masaki</dc:creator>
 <dc:creator>Nagao, Makoto</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  When translating Japanese nouns into English, we face the problem of articles
and numbers which the Japanese language does not have, but which are necessary
for the English composition. To solve this difficult problem we classified the
referential property and the number of nouns into three types respectively.
This paper shows that the referential property and the number of nouns in a
sentence can be estimated fairly reliably by the words in the sentence. Many
rules for the estimation were written in forms similar to rewriting rules in
expert systems. We obtained the correct recognition scores of 85.5\% and 89.0\%
in the estimation of the referential property and the number respectively for
the sentences which were used for the construction of our rules. We tested
these rules for some other texts, and obtained the scores of 68.9\% and 85.6\%
respectively.
</dc:description>
 <dc:description>Comment: 8 pages, TMI-93</dc:description>
 <dc:date>1994-05-19</dc:date>
 <dc:date>1994-05-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Capturing CFLs with Tree Adjoining Grammars</dc:title>
 <dc:creator>Rogers, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We define a decidable class of TAGs that is strongly equivalent to CFGs and
is cubic-time parsable. This class serves to lexicalize CFGs in the same manner
as the LCFGs of Schabes and Waters but with considerably less restriction on
the form of the grammars. The class provides a normal form for TAGs that
generate local sets in much the same way that regular grammars provide a normal
form for CFGs that generate regular sets.
</dc:description>
 <dc:description>Comment: 8 pages, 3 figures. To appear in proceedings of ACL'94</dc:description>
 <dc:date>1994-05-23</dc:date>
 <dc:date>1994-05-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405020</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Precondition Expressions in Instructional Text</dc:title>
 <dc:creator>Linden, Keith Vander</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This study employs a knowledge intensive corpus analysis to identify the
elements of the communicative context which can be used to determine the
appropriate lexical and grammatical form of instructional texts. \ig, an
instructional text generation system based on this analysis, is presented,
particularly with reference to its expression of precondition relations.
</dc:description>
 <dc:description>Comment: 8 pages, in postscript format (compressed and uuencoded), one of the
  figures has been removed due to excessive size</dc:description>
 <dc:date>1994-05-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405021</dc:identifier>
 <dc:identifier>proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Grammar Specialization through Entropy Thresholds</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Explanation-based generalization is used to extract a specialized grammar
from the original one using a training corpus of parse trees. This allows very
much faster parsing and gives a lower error rate, at the price of a small loss
in coverage. Previously, it has been necessary to specify the tree-cutting
criteria (or operationality criteria) manually; here they are derived
automatically from the training set and the desired coverage of the specialized
grammar. This is done by assigning an entropy value to each node in the parse
trees and cutting in the nodes with sufficiently high entropy values.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1994-05-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405022</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Integrated Heuristic Scheme for Partial Parse Evaluation</dc:title>
 <dc:creator>Lavie, Alon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  GLR* is a recently developed robust version of the Generalized LR Parser,
that can parse almost ANY input sentence by ignoring unrecognizable parts of
the sentence. On a given input sentence, the parser returns a collection of
parses that correspond to maximal, or close to maximal, parsable subsets of the
original input. This paper describes recent work on developing an integrated
heuristic scheme for selecting the parse that is deemed ``best'' from such a
collection. We describe the heuristic measures used and their combination
scheme. Preliminary results from experiments conducted on parsing speech
recognized spontaneous speech are also reported.
</dc:description>
 <dc:description>Comment: 3 pages, 1 table, LaTeX source, uses latex-acl.sty and named.sty To
  appear in proceedings of ACL-94 (student sessions)</dc:description>
 <dc:date>1994-05-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405023</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94 (student session)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Abductive Equivalential Translation and its application to Natural
  Language Database Interfacing</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The thesis describes a logical formalization of natural-language database
interfacing. We assume the existence of a ``natural language engine'' capable
of mediating between surface linguistic string and their representations as
``literal'' logical forms: the focus of interest will be the question of
relating ``literal'' logical forms to representations in terms of primitives
meaningful to the underlying database engine. We begin by describing the nature
of the problem, and show how a variety of interface functionalities can be
considered as instances of a type of formal inference task which we call
``Abductive Equivalential Translation'' (AET); functionalities which can be
reduced to this form include answering questions, responding to commands,
reasoning about the completeness of answers, answering meta-questions of type
``Do you know...'', and generating assertions and questions. In each case, a
``linguistic domain theory'' (LDT) $\Gamma$ and an input formula $F$ are given,
and the goal is to construct a formula with certain properties which is
equivalent to $F$, given $\Gamma$ and a set of permitted assumptions. If the
LDT is of a certain specified type, whose formulas are either conditional
equivalences or Horn-clauses, we show that the AET problem can be reduced to a
goal-directed inference method. We present an abstract description of this
method, and sketch its realization in Prolog. The relationship between AET and
several problems previously discussed in the literature is discussed. In
particular, we show how AET can provide a simple and elegant solution to the
so-called ``Doctor on Board'' problem, and in effect allows a
``relativization'' of the Closed World Assumption. The ideas in the thesis have
all been implemented concretely within the SRI CLARE project, using a real
projects and payments database. The LDT for the example database is described
in detail, and examples of the types of functionality that can be achieved
within the example domain are presented.
</dc:description>
 <dc:description>Comment: 162 pages, Latex source, PhD thesis (U Stockholm, 1993). Uses
  style-file ustockholm_thesis.sty</dc:description>
 <dc:date>1994-05-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Optimal Tabular Parsing Algorithm</dc:title>
 <dc:creator>Nederhof, Mark-Jan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we relate a number of parsing algorithms which have been
developed in very different areas of parsing theory, and which include
deterministic algorithms, tabular algorithms, and a parallel algorithm. We show
that these algorithms are based on the same underlying ideas. By relating
existing ideas, we hope to provide an opportunity to improve some algorithms
based on features of others. A second purpose of this paper is to answer a
question which has come up in the area of tabular parsing, namely how to obtain
a parsing algorithm with the property that the table will contain as little
entries as possible, but without the possibility that two entries represent the
same subderivation.
</dc:description>
 <dc:description>Comment: 8 pages, Unix compressed, uuencoded</dc:description>
 <dc:date>1994-05-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405025</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Extended Theory of Head-Driven Parsing</dc:title>
 <dc:creator>Nederhof, Mark-Jan</dc:creator>
 <dc:creator>Satta, Giorgio</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We show that more head-driven parsing algorithms can be formulated than those
occurring in the existing literature. These algorithms are inspired by a family
of left-to-right parsing algorithms from a recent publication. We further
introduce a more advanced notion of ``head-driven parsing'' which allows more
detailed specification of the processing order of non-head elements in the
right-hand side. We develop a parsing algorithm for this strategy, based on LR
parsing techniques.
</dc:description>
 <dc:description>Comment: 8 pages, Unix compressed, uuencoded</dc:description>
 <dc:date>1994-05-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405026</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Acquiring Receptive Morphology: A Connectionist Model</dc:title>
 <dc:creator>Gasser, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a modular connectionist model of the acquisition of
receptive inflectional morphology. The model takes inputs in the form of phones
one at a time and outputs the associated roots and inflections. Simulations
using artificial language stimuli demonstrate the capacity of the model to
learn suffixation, prefixation, infixation, circumfixation, mutation, template,
and deletion rules. Separate network modules responsible for syllables enable
to the network to learn simple reduplication rules as well. The model also
embodies constraints against association-line crossing.
</dc:description>
 <dc:description>Comment: 8 pages, Postscript file; extract with Unix uudecode and uncompress</dc:description>
 <dc:date>1994-05-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405027</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semantics of Complex Sentences in Japanese</dc:title>
 <dc:creator>Nakagawa, Hiroshi</dc:creator>
 <dc:creator>Nishizawa, Shin'ichiro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The important part of semantics of complex sentence is captured as relations
among semantic roles in subordinate and main clause respectively. However if
there can be relations between every pair of semantic roles, the amount of
computation to identify the relations that hold in the given sentence is
extremely large. In this paper, for semantics of Japanese complex sentence, we
introduce new pragmatic roles called `observer' and `motivated' respectively to
bridge semantic roles of subordinate and those of main clauses. By these new
roles constraints on the relations among semantic/pragmatic roles are known to
be almost local within subordinate or main clause. In other words, as for the
semantics of the whole complex sentence, the only role we should deal with is a
motivated.
</dc:description>
 <dc:description>Comment: 10pages, To appear at COLING-94</dc:description>
 <dc:date>1994-05-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Structural Tags, Annealing and Automatic Word Classification</dc:title>
 <dc:creator>McMahon, John</dc:creator>
 <dc:creator>Smith, F. J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an automatic word classification system which uses a
locally optimal annealing algorithm and average class mutual information. A new
word-class representation, the structural tag is introduced and its advantages
for use in statistical language modelling are presented. A summary of some
results with the one million word LOB corpus is given; the algorithm is also
shown to discover the vowel-consonant distinction and displays an ability to
cluster words syntactically in a Latin corpus. Finally, a comparison is made
between the current classification system and several leading alternative
systems, which shows that the current system performs respectably well.
</dc:description>
 <dc:description>Comment: 14 Page Paper. PostScript File</dc:description>
 <dc:date>1994-05-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Priority Union and Generalization in Discourse Grammars</dc:title>
 <dc:creator>Grover, Claire</dc:creator>
 <dc:creator>Brew, Chris</dc:creator>
 <dc:creator>Manandhar, Suresh</dc:creator>
 <dc:creator>Moens, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an implementation in Carpenter's typed feature formalism, ALE, of
a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine
their method for resolving parallelism-dependent anaphora and show that there
is a coherent feature-structural rendition of this type of grammar which uses
the operations of priority union and generalization. We describe an
augmentation of the ALE system to encompass these operations and we show that
an appropriate choice of definition for priority union gives the desired
multiple output for examples of VP-ellipsis which exhibit a strict/sloppy
ambiguity.
</dc:description>
 <dc:description>Comment: 8 pages, Unix compressed and uuencoded Postscript file To appear in
  ACL-94</dc:description>
 <dc:date>1994-05-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405030</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Attributive Logic of Set Descriptions and Set Operations</dc:title>
 <dc:creator>Manandhar, Suresh</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper provides a model theoretic semantics to feature terms augmented
with set descriptions. We provide constraints to specify HPSG style set
descriptions, fixed cardinality set descriptions, set-membership constraints,
restricted universal role quantifications, set union, intersection, subset and
disjointness. A sound, complete and terminating consistency checking procedure
is provided to determine the consistency of any given term in the logic. It is
shown that determining consistency of terms is a NP-complete problem.
</dc:description>
 <dc:description>Comment: 8 pages, epsf.sty, leqno.sty, LaTeX, ACL'94</dc:description>
 <dc:date>1994-05-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Modularity in a Connectionist Model of Morphology Acquisition</dc:title>
 <dc:creator>Gasser, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a modular connectionist model of the acquisition of
receptive inflectional morphology. The model takes inputs in the form of phones
one at a time and outputs the associated roots and inflections. In its simplest
version, the network consists of separate simple recurrent subnetworks for root
and inflection identification; both networks take the phone sequence as inputs.
It is shown that the performance of the two separate modular networks is
superior to a single network responsible for both root and inflection
identification. In a more elaborate version of the model, the network learns to
use separate hidden-layer modules to solve the separate tasks of root and
inflection identification.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded compressed Postscript file</dc:description>
 <dc:date>1994-05-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405032</dc:identifier>
 <dc:identifier>Proceedings of COLING 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Relating Complexity to Practical Performance in Parsing with
  Wide-Coverage Unification Grammars</dc:title>
 <dc:creator>Carroll, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper demonstrates that exponential complexities with respect to grammar
size and input length have little impact on the performance of three
unification-based parsing algorithms, using a wide-coverage grammar. The
results imply that the study and optimisation of unification-based parsing must
rely on empirical data until complexity theory can more accurately predict the
practical behaviour of such parsers.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX source (one figure not included) To appear in ACL-94</dc:description>
 <dc:date>1994-05-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405033</dc:identifier>
 <dc:identifier>32nd Annual Meeting of the ACL, 287-294</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and
  Its Automatic Evaluation</dc:title>
 <dc:creator>Chen, Kuang-hua</dc:creator>
 <dc:creator>Chen, Hsin-Hsi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  To acquire noun phrases from running texts is useful for many applications,
such as word grouping,terminology indexing, etc. The reported literatures adopt
pure probabilistic approach, or pure rule-based noun phrases grammar to tackle
this problem. In this paper, we apply a probabilistic chunker to deciding the
implicit boundaries of constituents and utilize the linguistic knowledge to
extract the noun phrases by a finite state mechanism. The test texts are
SUSANNE Corpus and the results are evaluated by comparing the parse field of
SUSANNE Corpus automatically. The results of this preliminary experiment are
encouraging.
</dc:description>
 <dc:description>Comment: 8 pages, Postscript file, Unix compressed, uuencoded</dc:description>
 <dc:date>1994-05-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405034</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9405035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dual-Coding Theory and Connectionist Lexical Selection</dc:title>
 <dc:creator>Wang, Ye-Yi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce the bilingual dual-coding theory as a model for bilingual mental
representation. Based on this model, lexical selection neural networks are
implemented for a connectionist transfer project in machine translation. This
lexical selection approach has two advantages. First, it is learnable. Little
human effort on knowledge engineering is required. Secondly, it is
psycholinguistically well-founded.
</dc:description>
 <dc:description>Comment: 3 pages, ACL94 student session</dc:description>
 <dc:date>1994-05-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9405035</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intentions and Information in Discourse</dc:title>
 <dc:creator>Asher, Nicholas</dc:creator>
 <dc:creator>Lascarides, Alex</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper is about the flow of inference between communicative intentions,
discourse structure and the domain during discourse processing. We augment a
theory of discourse interpretation with a theory of distinct mental attitudes
and reasoning about them, in order to provide an account of how the attitudes
interact with reasoning about discourse structure.
</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406001</dc:identifier>
 <dc:identifier>Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Speech Dialogue with Facial Displays: Multimodal Human-Computer
  Conversation</dc:title>
 <dc:creator>Nagao, Katashi</dc:creator>
 <dc:creator>Takeuchi, Akikazu</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Human face-to-face conversation is an ideal model for human-computer
dialogue. One of the major features of face-to-face communication is its
multiplicity of communication channels that act on multiple modalities. To
realize a natural multimodal dialogue, it is necessary to study how humans
perceive information and determine the information to which humans are
sensitive. A face is an independent communication channel that conveys
emotional and conversational signals, encoded as facial expressions. We have
developed an experimental system that integrates speech dialogue and facial
animation, to investigate the effect of introducing communicative facial
expressions as a new modality in human-computer conversation. Our experiments
have shown that facial expressions are helpful, especially upon first contact
with the system. We have also discovered that featuring facial expressions at
an early stage improves subsequent interaction.
</dc:description>
 <dc:description>Comment: 8 pages, Postscript file, UNIX compressed, uuencoded</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Learning Approach to Natural Language Understanding</dc:title>
 <dc:creator>Pieraccini, Roberto</dc:creator>
 <dc:creator>Levin, Esther</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we propose a learning paradigm for the problem of understanding
spoken language. The basis of the work is in a formalization of the
understanding problem as a communication problem. This results in the
definition of a stochastic model of the production of speech or text starting
from the meaning of a sentence. The resulting understanding algorithm consists
in a Viterbi maximization procedure, analogous to that commonly used for
recognizing speech. The algorithm was implemented for building
</dc:description>
 <dc:description>Comment: 18 pages, Latex file + compressed figures</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406003</dc:identifier>
 <dc:identifier>&quot;New Advances and Trends in Speech Recognition and Coding&quot;, NATO
  ASI Series, Springer-Verlag, proceedings of the 1993 NATO ASI Summer School,
  Bubion, Spain, June-July 1993</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards a Principled Representation of Discourse Plans</dc:title>
 <dc:creator>Young, R. Michael</dc:creator>
 <dc:creator>Moore, Johanna D.</dc:creator>
 <dc:creator>Pollack, Martha E.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We argue that discourse plans must capture the intended causal and
decompositional relations between communicative actions. We present a planning
algorithm, DPOCL, that builds plan structures that properly capture these
relations, and show how these structures are used to solve the problems that
plagued previous discourse planners, and allow a system to participate
effectively and flexibly in an ongoing dialogue.
</dc:description>
 <dc:description>Comment: requires cogsci94.sty, psfig.sty</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406004</dc:identifier>
 <dc:identifier>To appear in Proceedings of the Sixteenth Annual Conference of the
  Cognitive Science Society, Atlanta, Ga, August, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Word-Sense Disambiguation Using Decomposable Models</dc:title>
 <dc:creator>Bruce, Rebecca</dc:creator>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most probabilistic classifiers used for word-sense disambiguation have either
been based on only one contextual feature or have used a model that is simply
assumed to characterize the interdependencies among multiple contextual
features. In this paper, a different approach to formulating a probabilistic
model is presented along with a case study of the performance of models
produced in this manner for the disambiguation of the noun &quot;interest&quot;. We
describe a method for formulating probabilistic models that use multiple
contextual features for word-sense disambiguation, without requiring untested
assumptions regarding the form of the model. Using this approach, the joint
distribution of all variables is described by only the most systematic variable
interactions, thereby limiting the number of parameters to be estimated,
supporting computational efficiency, and providing an understanding of the
data.
</dc:description>
 <dc:description>Comment: 8 pages, Unix compressed, uuencoded Postscript file</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Detecting and Correcting Speech Repairs</dc:title>
 <dc:creator>Heeman, Peter</dc:creator>
 <dc:creator>Allen, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Interactive spoken dialog provides many new challenges for spoken language
systems. One of the most critical is the prevalence of speech repairs. This
paper presents an algorithm that detects and corrects speech repairs based on
finding the repair pattern. The repair pattern is built by finding word matches
and word replacements, and identifying fragments and editing terms. Rather than
using a set of prebuilt templates, we build the pattern on the fly. In a fair
test, our method, when combined with a statistical model to filter possible
repairs, was successful at detecting and correcting 80\% of the repairs,
without using prosodic information or a parser.
</dc:description>
 <dc:description>Comment: 8 pages, to appear in acl-94, Added latex version</dc:description>
 <dc:date>1994-06-01</dc:date>
 <dc:date>1994-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406007</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Aligning a Parallel English-Chinese Corpus Statistically with Lexical
  Criteria</dc:title>
 <dc:creator>Wu, Dekai</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe our experience with automatic alignment of sentences in parallel
English-Chinese texts. Our report concerns three related topics:
  (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus;
  (2) experiments addressing the applicability of Gale &amp; Church's length-based
statistical method to the task of alignment involving a non-Indo-European
language; and
  (3) an improved statistical method that also incorporates domain-specific
lexical cues.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded compressed PostScript</dc:description>
 <dc:date>1994-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406007</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing Turkish with the Lexical Functional Grammar Formalism</dc:title>
 <dc:creator>Gungordu, Zelal</dc:creator>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes our work on parsing Turkish using the lexical-functional
grammar formalism. This work represents the first significant effort for
parsing Turkish. Our implementation is based on Tomita's parser developed at
Carnegie-Mellon University Center for Machine Translation. The grammar covers a
substantial subset of Turkish including simple and complex sentences, and deals
with a reasonable amount of word order freeness. The complex agglutinative
morphology of Turkish lexical structures is handled using a separate two-level
morphological analyzer. After a discussion of key relevant issues regarding
Turkish grammar, we discuss aspects of our system and present results from our
implementation. Our initial results suggest that our system can parse about
82\% of the sentences directly and almost all the remaining with very minor
pre-editing.
</dc:description>
 <dc:description>Comment: 7 pages, Postscript (compressed (gzip) and uuencoded)</dc:description>
 <dc:date>1994-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406008</dc:identifier>
 <dc:identifier>Proceedings of COLING'94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on
  Derivations</dc:title>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper defines multiset-valued linear index grammar and unordered vector
grammar with dominance links. The former models certain uses of multiset-valued
feature structures in unification-based formalisms, while the latter is
motivated by word order variation and by ``quasi-trees'', a generalization of
trees. The two formalisms are weakly equivalent, and an important subset is at
most context-sensitive and polynomially parsable.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded compressed ps file</dc:description>
 <dc:date>1994-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406009</dc:identifier>
 <dc:identifier>Proc ACL 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Some Advances in Transformation-Based Part of Speech Tagging</dc:title>
 <dc:creator>Brill, Eric</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most recent research in trainable part of speech taggers has explored
stochastic tagging. While these taggers obtain high accuracy, linguistic
information is captured indirectly, typically in tens of thousands of lexical
and contextual probabilities. In [Brill92], a trainable rule-based tagger was
described that obtained performance comparable to that of stochastic taggers,
but captured relevant linguistic information in a small number of simple
non-stochastic rules. In this paper, we describe a number of extensions to this
rule-based tagger. First, we describe a method for expressing lexical relations
in tagging that are not captured by stochastic taggers. Next, we show a
rule-based approach to tagging unknown words. Finally, we show how the tagger
can be extended into a k-best tagger, where multiple tags can be assigned to
words in some cases of uncertainty.
</dc:description>
 <dc:description>Comment: 6 Pages. Code available</dc:description>
 <dc:date>1994-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406010</dc:identifier>
 <dc:identifier>Proceedings of AAAI94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Exploring the Statistical Derivation of Transformational Rule Sequences
  for Part-of-Speech Tagging</dc:title>
 <dc:creator>Ramshaw, Lance A.</dc:creator>
 <dc:creator>Marcus, Mitchell P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Eric Brill has recently proposed a simple and powerful corpus-based language
modeling approach that can be applied to various tasks including part-of-speech
tagging and building phrase structure trees. The method learns a series of
symbolic transformational rules, which can then be applied in sequence to a
test corpus to produce predictions. The learning process only requires counting
matches for a given set of rule templates, allowing the method to survey a very
large space of possible contextual factors. This paper analyses Brill's
approach as an interesting variation on existing decision tree methods, based
on experiments involving part-of-speech tagging for both English and ancient
Greek corpora. In particular, the analysis throws light on why the new
mechanism seems surprisingly resistant to overtraining. A fast, incremental
implementation and a mechanism for recording the dependencies that underlie the
resulting rule sequence are also described.
</dc:description>
 <dc:description>Comment: 10 pages, in proceedings of the ACL Balancing Act workshop</dc:description>
 <dc:date>1994-06-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406011</dc:identifier>
 <dc:identifier>ACL Balancing Act Workshop proceedings, July 94, pp. 86-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Self-Organizing Machine Translation: Example-Driven Induction of
  Transfer Functions</dc:title>
 <dc:creator>Juola, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  With the advent of faster computers, the notion of doing machine translation
from a huge stored database of translation examples is no longer unreasonable.
This paper describes an attempt to merge the Example-Based Machine Translation
(EBMT) approach with psycholinguistic principles. A new formalism for context-
free grammars, called *marker-normal form*, is demonstrated and used to
describe language data in a way compatible with psycholinguistic theories. By
embedding this formalism in a standard multivariate optimization framework, a
system can be built that infers correct transfer functions for a set of
bilingual sentence pairs and then uses those functions to translate novel
sentences. The validity of this line of reasoning has been tested in the
development of a system called METLA-1. This system has been used to infer
English-&gt;French and English-&gt;Urdu transfer functions from small corpora. The
results of those experiments are examined, both in engineering terms as well as
in more linguistic terms. In general, the results of these experiments were
psycho- logically and linguistically well-grounded while still achieving a
respectable level of success when compared against a similar prototype using
Hidden Markov Models.
</dc:description>
 <dc:description>Comment: PostScript, 30 pages, contact author for LaTeX version</dc:description>
 <dc:date>1994-06-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Graded Unification: A Framework for Interactive Processing</dc:title>
 <dc:creator>Kim, Albert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An extension to classical unification, called {\em graded unification} is
presented. It is capable of combining contradictory information. An interactive
processing paradigm and parser based on this new operator are also presented.
</dc:description>
 <dc:description>Comment: 3 pages, To appear in ACL-94 Student Session, Postscript file;
  extract with Unix uudecode and uncompress</dc:description>
 <dc:date>1994-06-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Hybrid Reasoning Model for Indirect Answers</dc:title>
 <dc:creator>Green, Nancy</dc:creator>
 <dc:creator>Carberry, Sandra</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents our implemented computational model for interpreting and
generating indirect answers to Yes-No questions. Its main features are 1) a
discourse-plan-based approach to implicature, 2) a reversible architecture for
generation and interpretation, 3) a hybrid reasoning model that employs both
plan inference and logical inference, and 4) use of stimulus conditions to
model a speaker's motivation for providing appropriate, unrequested
information. The model handles a wider range of types of indirect answers than
previous computational models and has several significant advantages.
</dc:description>
 <dc:description>Comment: To appear in Proc. of ACL-94. 8 pages, uuencoded compressed
  Postscript file; extract with Unix uudecode and uncompress. Contact Author
  for latex version</dc:description>
 <dc:date>1994-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Statistical Augmentation of a Chinese Machine-Readable Dictionary</dc:title>
 <dc:creator>Fung, Pascale</dc:creator>
 <dc:creator>Wu, Dekai</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a method of using statistically-collected Chinese character
groups from a corpus to augment a Chinese dictionary. The method is
particularly useful for extracting domain-specific and regional words not
readily available in machine-readable dictionaries. Output was evaluated both
using human evaluators and against a previously available dictionary. We also
evaluated performance improvement in automatic Chinese tokenization. Results
show that our method outputs legitimate words, acronymic constructions, idioms,
names and titles, as well as technical compounds, many of which were lacking
from the original dictionary.
</dc:description>
 <dc:description>Comment: 17 pages, uuencoded compressed PostScript</dc:description>
 <dc:date>1994-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406015</dc:identifier>
 <dc:identifier>In WVLC-94, Second Annual Workshop on Very Large</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Corpus-Driven Knowledge Acquisition for Discourse Analysis</dc:title>
 <dc:creator>Soderland, Stephen</dc:creator>
 <dc:creator>Lehnert, Wendy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The availability of large on-line text corpora provides a natural and
promising bridge between the worlds of natural language processing (NLP) and
machine learning (ML). In recent years, the NLP community has been aggressively
investigating statistical techniques to drive part-of-speech taggers, but
application-specific text corpora can be used to drive knowledge acquisition at
much higher levels as well. In this paper we will show how ML techniques can be
used to support knowledge acquisition for information extraction systems. It is
often very difficult to specify an explicit domain model for many information
extraction applications, and it is always labor intensive to implement
hand-coded heuristics for each new domain. We have discovered that it is
nevertheless possible to use ML algorithms in order to capture knowledge that
is only implicitly present in a representative text corpus. Our work addresses
issues traditionally associated with discourse analysis and intersentential
inference generation, and demonstrates the utility of ML algorithms at this
higher level of language analysis. The benefits of our work address the
portability and scalability of information extraction (IE) technologies. When
hand-coded heuristics are used to manage discourse analysis in an information
extraction system, months of programming effort are easily needed to port a
successful IE system to a new domain. We will show how ML algorithms can reduce
this
</dc:description>
 <dc:description>Comment: 6 pages, AAAI-94</dc:description>
 <dc:date>1994-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Automatic Method of Finding Topic Boundaries</dc:title>
 <dc:creator>Reynar, Jeffrey C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This article outlines a new method of locating discourse boundaries based on
lexical cohesion and a graphical technique called dotplotting. The application
of dotplotting to discourse segmentation can be performed either manually, by
examining a graph, or automatically, using an optimization algorithm. The
results of two experiments involving automatically locating boundaries between
a series of concatenated documents are presented. Areas of application and
future directions for this work are also outlined.
</dc:description>
 <dc:description>Comment: 3 pages, in student session of ACL 1994</dc:description>
 <dc:date>1994-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>TDL--- A Type Description Language for Constraint-Based Grammars</dc:title>
 <dc:creator>Krieger, Hans-Ulrich</dc:creator>
 <dc:creator>Sch&#xe4;fer, Ulrich</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents \tdl, a typed feature-based representation language and
inference system. Type definitions in \tdl\ consist of type and feature
constraints over the boolean connectives. \tdl\ supports open- and closed-world
reasoning over types and allows for partitions and incompatible types. Working
with partially as well as with fully expanded types is possible. Efficient
reasoning in \tdl\ is accomplished through specialized modules.
</dc:description>
 <dc:description>Comment: Will Appear in Proc. COLING-94</dc:description>
 <dc:date>1994-06-08</dc:date>
 <dc:date>1994-06-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Complete and Recursive Feature Theory</dc:title>
 <dc:creator>Backofen, Rolf</dc:creator>
 <dc:creator>Smolka, Gert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Various feature descriptions are being employed in logic programming
languages and constrained-based grammar formalisms. The common notational
primitive of these descriptions are functional attributes called features. The
descriptions considered in this paper are the possibly quantified first-order
formulae obtained from a signature of binary and unary predicates called
features and sorts, respectively. We establish a first-order theory FT by means
of three axiom schemes, show its completeness, and construct three elementarily
equivalent models. One of the models consists of so-called feature graphs, a
data structure common in computational linguistics. The other two models
consist of so-called feature trees, a record-like data structure generalizing
the trees corresponding to first-order terms. Our completeness proof exhibits a
terminating simplification system deciding validity and satisfiability of
possibly quantified feature descriptions.
</dc:description>
 <dc:description>Comment: Short version appeared in the 1992 Annual Meeting of the Association
  for Computational Linguistics</dc:description>
 <dc:date>1994-06-10</dc:date>
 <dc:date>1994-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DPOCL: A Principled Approach to Discourse Planning</dc:title>
 <dc:creator>Young, R. Michael</dc:creator>
 <dc:creator>Moore, Johanna D.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Research in discourse processing has identified two representational
requirements for discourse planning systems. First, discourse plans must
adequately represent the intentional structure of the utterances they produce
in order to enable a computational discourse agent to respond effectively to
communicative failures \cite{MooreParisCL}. Second, discourse plans must
represent the informational structure of utterances. In addition to these
representational requirements, we argue that discourse planners should be
formally characterizable in terms of soundness and completeness.
</dc:description>
 <dc:date>1994-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406020</dc:identifier>
 <dc:identifier>proceedings of the Seventh International Workshop on Natural
  Langauge Generation, Kennebunkport, ME, June, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A symbolic description of punning riddles and its computer
  implementation</dc:title>
 <dc:creator>Binsted, Kim</dc:creator>
 <dc:creator>Ritchie, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Riddles based on simple puns can be classified according to the patterns of
word, syllable or phrase similarity they depend upon. We have devised a formal
model of the semantic and syntactic regularities underlying some of the simpler
types of punning riddle. We have also implemented this preliminary theory in a
computer program which can generate riddles from a lexicon containing general
data about words and phrases; that is, the lexicon content is not customised to
produce jokes. Informal evaluation of the program's results by a set of human
judges suggest that the riddles produced by this program are of comparable
quality to those in general circulation among school children.
</dc:description>
 <dc:description>Comment: 40 pages, also available by email (kimb@aisb.ed.ac.uk) and www
  (http://www.dai.ed.ac.uk/students/kimb). Longer version of an AAAI-94 paper.
  Figs and bib separate</dc:description>
 <dc:date>1994-06-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An implemented model of punning riddles</dc:title>
 <dc:creator>Binsted, Kim</dc:creator>
 <dc:creator>Ritchie, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we discuss a model of simple question-answer punning,
implemented in a program, JAPE, which generates riddles from humour-independent
lexical entries. The model uses two main types of structure: schemata, which
determine the relationships between key words in a joke, and templates, which
produce the surface form of the joke. JAPE succeeds in generating pieces of
text that are recognizably jokes, but some of them are not very good jokes. We
mention some potential improvements and extensions, including post-production
heuristics for ordering the jokes according to quality.
</dc:description>
 <dc:description>Comment: 6 pages, using aaai.sty and aaai.bst (in cmp-lg style list). Figs and
  bib in separate file. Also available by email (kimb@aisb.ed.ac.uk) and www
  (http://www.dai.ed.ac.uk/students/kimb)</dc:description>
 <dc:date>1994-06-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406022</dc:identifier>
 <dc:identifier>In proceedings of AAAI-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406023</identifier>
 <datestamp>2016-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Spanish Tagset for the CRATER Project</dc:title>
 <dc:creator>Le&#xf3;n, Fernando S&#xe1;nchez</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This working paper describes the Spanish tagset to be used in the context of
CRATER, a CEC funded project aiming at the creation of a multilingual (English,
French, Spanish) aligned corpus using the International Telecommunications
Union corpus. In this respect, each version of the corpus will be (or is
currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to
perform the tagging of the Spanish version. This tagset has been devised as the
ideal one for Spanish, and has been posted to several lists in order to get
feedback to it.
</dc:description>
 <dc:description>Comment: 20 pages, LaTeX format</dc:description>
 <dc:date>1994-06-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Fault-tolerant Speech Parsing with SCREEN</dc:title>
 <dc:creator>Wermter, Stefan</dc:creator>
 <dc:creator>Weber, Volker</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a new approach and a system SCREEN for fault-tolerant
speech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for
Natural language. Speech parsing describes the syntactic and semantic analysis
of spontaneous spoken language. The general approach is based on incremental
immediate flat analysis, learning of syntactic and semantic speech parsing,
parallel integration of current hypotheses, and the consideration of various
forms of speech related errors. The goal for this approach is to explore the
parallel interactions between various knowledge sources for learning
incremental fault-tolerant speech parsing. This approach is examined in a
system SCREEN using various hybrid connectionist techniques. Hybrid
connectionist techniques are examined because of their promising properties of
inherent fault tolerance, learning, gradedness and parallel constraint
integration. The input for SCREEN is hypotheses about recognized words of a
spoken utterance potentially analyzed by a speech system, the output is
hypotheses about the flat syntactic and semantic analysis of the utterance. In
this paper we focus on the general approach, the overall architecture, and
examples for learning flat syntactic speech parsing. Different from most other
speech language architectures SCREEN emphasizes an interactive rather than an
autonomous position, learning rather than encoding, flat analysis rather than
in-depth analysis, and fault-tolerant processing of phonetic, syntactic and
semantic knowledge.
</dc:description>
 <dc:description>Comment: 6 pages, postscript, compressed, uuencoded to appear in Proceedings
  of AAAI 94</dc:description>
 <dc:date>1994-06-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Emergent Parsing and Generation with Generalized Chart</dc:title>
 <dc:creator>Koiti, HASIDA</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A new, flexible inference method for Horn logic program is proposed, which is
a drastic generalization of chart parsing, partial instantiations of clauses in
a program roughly corresponding to arcs in a chart. Chart-like parsing and
semantic-head-driven generation emerge from this method. With a parsimonious
instantiation scheme for ambiguity packing, the parsing complexity reduces to
that of standard chart-based algorithms.
</dc:description>
 <dc:description>Comment: 11 pages. To appear in COLING-94. LaTeX source. psfig-scale.tex,
  a4wide.sty, and fullname.sty needed</dc:description>
 <dc:date>1994-06-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Very Idea of Dynamic Semantics</dc:title>
 <dc:creator>Israel, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  &quot;Natural languages are programming languages for minds.&quot; Can we or should we
take this slogan seriously? If so, how? Can answers be found by looking at the
various &quot;dynamic&quot; treatments of natural language developed over the last decade
or so, mostly in response to problems associated with donkey anaphora? In
Dynamic Logic of Programs, the meaning of a program is a binary relation on the
set of states of some abstract machine. This relation is meant to model aspects
of the effects of the execution of the program, in particular its input-output
behavior. What, if anything, are the dynamic aspects of various proposed
dynamic semantics for natural languages supposed to model? Is there anything
dynamic to be modeled? If not, what is all the full about? We shall try to
answer some, at least, of these questions and provide materials for answers to
others.
</dc:description>
 <dc:description>Comment: 22 pages. Vanilla LaTex</dc:description>
 <dc:date>1994-06-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406026</dc:identifier>
 <dc:identifier>Proc. Ninth Amsterdam Colloquium, 1993</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Analyzing and Improving Statistical Language Models for Speech
  Recognition</dc:title>
 <dc:creator>Ueberla, Joerg P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In many current speech recognizers, a statistical language model is used to
indicate how likely it is that a certain word will be spoken next, given the
words recognized so far. How can statistical language models be improved so
that more complex speech recognition tasks can be tackled? Since the knowledge
of the weaknesses of any theory often makes improving the theory easier, the
central idea of this thesis is to analyze the weaknesses of existing
statistical language models in order to subsequently improve them. To that end,
we formally define a weakness of a statistical language model in terms of the
logarithm of the total probability, LTP, a term closely related to the standard
perplexity measure used to evaluate statistical language models. We apply our
definition of a weakness to a frequently used statistical language model,
called a bi-pos model. This results, for example, in a new modeling of unknown
words which improves the performance of the model by 14% to 21%. Moreover, one
of the identified weaknesses has prompted the development of our generalized
N-pos language model, which is also outlined in this thesis. It can incorporate
linguistic knowledge even if it extends over many words and this is not
feasible in a traditional N-pos model. This leads to a discussion of
whatknowledge should be added to statistical language models in general and we
give criteria for selecting potentially useful knowledge. These results show
the usefulness of both our definition of a weakness and of performing an
analysis of weaknesses of statistical language models in general.
</dc:description>
 <dc:description>Comment: 140 pages, postscript, approx 500KB, if problems with delivery, mail
  to ueberla@cs.sfu.ca</dc:description>
 <dc:date>1994-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Resolution of Syntactic Ambiguity: the Case of New Subjects</dc:title>
 <dc:creator>Niv, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I review evidence for the claim that syntactic ambiguities are resolved on
the basis of the meaning of the competing analyses, not their structure. I
identify a collection of ambiguities that do not yet have a meaning-based
account and propose one which is based on the interaction of discourse and
grammatical function. I provide evidence for my proposal by examining
statistical properties of the Penn Treebank of syntactically annotated text.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX source</dc:description>
 <dc:date>1994-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406028</dc:identifier>
 <dc:identifier>COGSCI-93</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Computational Model of Syntactic Processing: Ambiguity Resolution from
  Interpretation</dc:title>
 <dc:creator>Niv, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Syntactic ambiguity abounds in natural language, yet humans have no
difficulty coping with it. In fact, the process of ambiguity resolution is
almost always unconscious. But it is not infallible, however, as example 1
demonstrates.
  1. The horse raced past the barn fell.
  This sentence is perfectly grammatical, as is evident when it appears in the
following context:
  2. Two horses were being shown off to a prospective buyer. One was raced past
a meadow. and the other was raced past a barn. ...
  Grammatical yet unprocessable sentences such as 1 are called `garden-path
sentences.' Their existence provides an opportunity to investigate the human
sentence processing mechanism by studying how and when it fails. The aim of
this thesis is to construct a computational model of language understanding
which can predict processing difficulty. The data to be modeled are known
examples of garden path and non-garden path sentences, and other results from
psycholinguistics.
  It is widely believed that there are two distinct loci of computation in
sentence processing: syntactic parsing and semantic interpretation. One
longstanding controversy is which of these two modules bears responsibility for
the immediate resolution of ambiguity. My claim is that it is the latter, and
that the syntactic processing module is a very simple device which blindly and
faithfully constructs all possible analyses for the sentence up to the current
point of processing. The interpretive module serves as a filter, occasionally
discarding certain of these analyses which it deems less appropriate for the
ongoing discourse than their competitors.
  This document is divided into three parts. The first is introductory, and
reviews a selection of proposals from the sentence processing literature. The
second part explores a body of data which has been adduced in support of a
theory of structural preferences --- one that is inconsistent with the present
claim. I show how the current proposal can be specified to account for the
available data, and moreover to predict where structural preference theories
will go wrong. The third part is a theoretical investigation of how well the
proposed architecture can be realized using current conceptions of linguistic
competence. In it, I present a parsing algorithm and a meaning-based ambiguity
resolution method.
</dc:description>
 <dc:description>Comment: 128 pages, LaTeX source compressed and uuencoded, figures separate
  macros: rotate.sty, lingmacros.sty, psfig.tex. Dissertation, Computer and
  Information Science Dept., October 1993</dc:description>
 <dc:date>1994-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406029</dc:identifier>
 <dc:identifier>Dissertation, Computer and Information Science Dept. 1993</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The complexity of normal form rewrite sequences for Associativity</dc:title>
 <dc:creator>Niv, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The complexity of a particular term-rewrite system is considered: the rule of
associativity (x*y)*z --&gt; x*(y*z). Algorithms and exact calculations are given
for the longest and shortest sequences of applications of --&gt; that result in
normal form (NF). The shortest NF sequence for a term x is always n-drm(x),
where n is the number of occurrences of * in x and drm(x) is the depth of the
rightmost leaf of x. The longest NF sequence for any term is of length
n(n-1)/2.
</dc:description>
 <dc:description>Comment: 5 pages</dc:description>
 <dc:date>1994-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Psycholinguistically Motivated Parser for CCG</dc:title>
 <dc:creator>Niv, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Considering the speed in which humans resolve syntactic ambiguity, and the
overwhelming evidence that syntactic ambiguity is resolved through selection of
the analysis whose interpretation is the most `sensible', one comes to the
conclusion that interpretation, hence parsing take place incrementally, just
about every word. Considerations of parsimony in the theory of the syntactic
processor lead one to explore the simplest of parsers: one which represents
only analyses as defined by the grammar and no other information.
  Toward this aim of a simple, incremental parser I explore the proposal that
the competence grammar is a Combinatory Categorial Grammar (CCG). I address the
problem of the proliferating analyses that stem from CCG's associativity of
derivation. My solution involves maintaining only the maximally incremental
analysis and, when necessary, computing the maximally right-branching analysis.
I use results from the study of rewrite systems to show that this computation
is efficient.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, requires psfig.tex, 2 figures in separate file</dc:description>
 <dc:date>1994-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406031</dc:identifier>
 <dc:identifier>ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Anytime Algorithms for Speech Parsing?</dc:title>
 <dc:creator>Goerz, Guenther</dc:creator>
 <dc:creator>Kesseler, Marcus</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses to which extent the concept of ``anytime algorithms''
can be applied to parsing algorithms with feature unification. We first try to
give a more precise definition of what an anytime algorithm is. We arque that
parsing algorithms have to be classified as contract algorithms as opposed to
(truly) interruptible algorithms. With the restriction that the transaction
being active at the time an interrupt is issued has to be completed before the
interrupt can be executed, it is possible to provide a parser with limited
anytime behavior, which is in fact being realized in our research prototype.
</dc:description>
 <dc:description>Comment: 5 pages, 2 figures</dc:description>
 <dc:date>1994-06-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406032</dc:identifier>
 <dc:identifier>COLING-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Verb Semantics and Lexical Selection</dc:title>
 <dc:creator>Wu, Zhibiao</dc:creator>
 <dc:creator>Palmer, Martha</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper will focus on the semantic representation of verbs in computer
systems and its impact on lexical selection problems in machine translation
(MT). Two groups of English and Chinese verbs are examined to show that lexical
selection must be based on interpretation of the sentence as well as selection
restrictions placed on the verb arguments. A novel representation scheme is
suggested, and is compared to representations with selection restrictions used
in transfer-based MT. We see our approach as closely aligned with
knowledge-based MT approaches (KBMT), and as a separate component that could be
incorporated into existing systems. Examples and experimental results will show
that, using this scheme, inexact matches can achieve correct lexical selection.
</dc:description>
 <dc:description>Comment: 6 pages, Figures and bib files are in part 2</dc:description>
 <dc:date>1994-06-21</dc:date>
 <dc:date>1994-06-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406033</dc:identifier>
 <dc:identifier>Proceedings of ACL 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Decision Lists for Lexical Ambiguity Resolution: Application to Accent
  Restoration in Spanish and French</dc:title>
 <dc:creator>Yarowsky, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a statistical decision procedure for lexical ambiguity
resolution. The algorithm exploits both local syntactic patterns and more
distant collocational evidence, generating an efficient, effective, and highly
perspicuous recipe for resolving a given ambiguity. By identifying and
utilizing only the single best disambiguating evidence in a target context, the
algorithm avoids the problematic complex modeling of statistical dependencies.
Although directly applicable to a wide class of ambiguities, the algorithm is
described and evaluated in a realistic case study, the problem of restoring
missing accents in Spanish and French text.
</dc:description>
 <dc:description>Comment: 8 pages, latex-acl, to appear in ACL-94</dc:description>
 <dc:date>1994-06-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406034</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DISCO---An HPSG-based NLP System and its Application for Appointment
  Scheduling (Project Note)</dc:title>
 <dc:creator>Uszkoreit, Hans</dc:creator>
 <dc:creator>Backofen, Rolf</dc:creator>
 <dc:creator>Busemann, Stephan</dc:creator>
 <dc:creator>Diagne, Abdel Kader</dc:creator>
 <dc:creator>Hinkelman, Elizabeth A.</dc:creator>
 <dc:creator>Kasper, Walter</dc:creator>
 <dc:creator>Kiefer, Bernd</dc:creator>
 <dc:creator>Krieger, Hans-Ulrich</dc:creator>
 <dc:creator>Netter, Klaus</dc:creator>
 <dc:creator>Neumann, Guenter</dc:creator>
 <dc:creator>Oepen, Stephan</dc:creator>
 <dc:creator>Spackman, Stephen P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The natural language system DISCO is described. It combines o a powerful and
flexible grammar development system; o linguistic competence for German
including morphology, syntax and semantics; o new methods for linguistic
performance modelling on the basis of high-level competence grammars; o new
methods for modelling multi-agent dialogue competence; o an interesting sample
application for appointment scheduling and calendar management.
</dc:description>
 <dc:date>1994-06-23</dc:date>
 <dc:date>1994-06-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406035</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406036</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Text Analysis Tools in Spoken Language Processing</dc:title>
 <dc:creator>Riley, Michael</dc:creator>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This submission contains the postscript of the final version of the slides
used in our ACL-94 tutorial.
</dc:description>
 <dc:description>Comment: Slides for ACL-94 Tutorial</dc:description>
 <dc:date>1994-06-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406036</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multi-Paragraph Segmentation of Expository Text</dc:title>
 <dc:creator>Hearst, Marti A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes TextTiling, an algorithm for partitioning expository
texts into coherent multi-paragraph discourse units which reflect the subtopic
structure of the texts. The algorithm uses domain-independent lexical frequency
and distribution information to recognize the interactions of multiple
simultaneous themes. Two fully-implemented versions of the algorithm are
described and shown to produce segmentation that corresponds well to human
judgments of the major subtopic boundaries of thirteen lengthy texts.
</dc:description>
 <dc:description>Comment: To Appear in ACL '94 Proceedings; 8 pages POSTSCRIPT format</dc:description>
 <dc:date>1994-06-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406037</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Empirical Model of Acknowledgment for Spoken-Language Systems</dc:title>
 <dc:creator>Novick, David G.</dc:creator>
 <dc:creator>Sutton, Stephen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We refine and extend prior views of the description, purposes, and
contexts-of-use of acknowledgment acts through empirical examination of the use
of acknowledgments in task-based conversation. We distinguish three broad
classes of acknowledgments (other--&gt;ackn, self--&gt;other--&gt;ackn, and self+ackn)
and present a catalogue of 13 patterns within these classes that account for
the specific uses of acknowledgment in the corpus.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded compressed tar file</dc:description>
 <dc:date>1994-06-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406038</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406039</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Three studies of grammar-based surface-syntactic parsing of unrestricted
  English text. A summary and orientation</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The dissertation addresses the design of parsing grammars for automatic
surface-syntactic analysis of unconstrained English text. It consists of a
summary and three articles. {\it Morphological disambiguation} documents a
grammar for morphological (or part-of-speech) disambiguation of English, done
within the Constraint Grammar framework proposed by Fred Karlsson. The
disambiguator seeks to discard those of the alternative morphological analyses
proposed by the lexical analyser that are contextually illegitimate. The 1,100
constraints express some 23 general, essentially syntactic statements as
restrictions on the linear order of morphological tags. The error rate of the
morphological disambiguator is about ten times smaller than that of another
state-of-the-art probabilistic disambiguator, given that both are allowed to
leave some of the hardest ambiguities unresolved. This accuracy suggests the
viability of the grammar-based approach to natural language parsing, thus also
contributing to the more general debate concerning the viability of
probabilistic vs.\ linguistic techniques. {\it Experiments with heuristics}
addresses the question of how to resolve those ambiguities that survive the
morphological disambiguator. Two approaches are presented and empirically
evaluated: (i) heuristic disambiguation constraints and (ii) techniques for
learning from the fully disambiguated part of the corpus and then applying this
information to resolving remaining ambiguities.
</dc:description>
 <dc:description>Comment: PhD dissertation. 36pp, gzipped and uuencoded .ps file</dc:description>
 <dc:date>1994-06-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406039</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9406040</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning unification-based grammars using the Spoken English Corpus</dc:title>
 <dc:creator>Osborne, Miles</dc:creator>
 <dc:creator>Bridge, Derek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a grammar learning system that combines model-based and
data-driven learning within a single framework. Our results from learning
grammars using the Spoken English Corpus (SEC) suggest that combined
model-based and data-driven learning can produce a more plausible grammar than
is the case when using either learning style isolation.
</dc:description>
 <dc:description>Comment: 10 pages</dc:description>
 <dc:date>1994-06-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9406040</dc:identifier>
 <dc:identifier>ICGI-94 Colloquium</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Morphology with a Null-Interface</dc:title>
 <dc:creator>Trost, Harald</dc:creator>
 <dc:creator>Matiasek, Johannes</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an integrated architecture for word-level and sentence-level
processing in a unification-based paradigm. The core of the system is a CLP
implementation of a unification engine for feature structures supporting
relational values. In this framework an HPSG-style grammar is implemented.
Word-level processing uses X2MorF, a morphological component based on an
extended version of two-level morphology. This component is tightly integrated
with the grammar as a relation. The advantage of this approach is that
morphology and syntax are kept logically autonomous while at the same time
minimizing interface problems.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX, uses fullname.sty, to appear in COLING'95</dc:description>
 <dc:date>1994-07-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407001</dc:identifier>
 <dc:identifier>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994, pp. 141-147</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm</dc:title>
 <dc:creator>Mohri, Mehryar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Local grammars can be represented in a very convenient way by automata. This
paper describes and illustrates an efficient algorithm for the application of
local grammars put in this form to lemmatized texts.
</dc:description>
 <dc:description>Comment: 13 pages,Postscript uuencoded,to appear in Proceedings of COMPLEX '94</dc:description>
 <dc:date>1994-07-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compact Representations by Finite-State Transducers</dc:title>
 <dc:creator>Mohri, Mehryar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Finite-state transducers give efficient representations of many Natural
Language phenomena. They allow to account for complex lexicon restrictions
encountered, without involving the use of a large set of complex rules
difficult to analyze. We here show that these representations can be made very
compact, indicate how to perform the corresponding minimization, and point out
interesting linguistic side-effects of this operation.
</dc:description>
 <dc:description>Comment: 5 pages,latex+5figures,to appear in Proceedings of ACL 94</dc:description>
 <dc:date>1994-07-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407004</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Japanese word sense disambiguation based on examples of synonyms</dc:title>
 <dc:creator>Matsumoto, Mitsutaka</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  (This is not the abstract): The language is Japanese. If your printer does
not have fonts for Japases characters, the characters in figures will not be
printed out correctly. Dissertation for Bachelor's degree at Kyoto
University(Nagao lab.),March 1994.
</dc:description>
 <dc:description>Comment: 41 pages, PostScript file,Compressed and encoded with &quot;uufiles&quot;</dc:description>
 <dc:date>1994-07-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Corrective Training Algorithm for Adaptive Learning in Bag Generation</dc:title>
 <dc:creator>Chen, Hsin-Hsi</dc:creator>
 <dc:creator>Lee, Yue-Shi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The sampling problem in training corpus is one of the major sources of errors
in corpus-based applications. This paper proposes a corrective training
algorithm to best-fit the run-time context domain in the application of bag
generation. It shows which objects to be adjusted and how to adjust their
probabilities. The resulting techniques are greatly simplified and the
experimental results demonstrate the promising effects of the training
algorithm from generic domain to specific domain. In general, these techniques
can be easily extended to various language models and corpus-based
applications.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded compressed PostScript file; extract with Unix
  uudecode and uncompress</dc:description>
 <dc:date>1994-07-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407005</dc:identifier>
 <dc:identifier>proceedings of NeMLaP-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser</dc:title>
 <dc:creator>Dowding, John</dc:creator>
 <dc:creator>Moore, Robert</dc:creator>
 <dc:creator>Andry, Francois</dc:creator>
 <dc:creator>Moran, Douglas</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an efficient bottom-up parser that interleaves syntactic and
semantic structure building. Two techniques are presented for reducing search
by reducing local ambiguity: Limited left-context constraints are used to
reduce local syntactic ambiguity, and deferred sortal-constraint application is
used to reduce local semantic ambiguity. We experimentally evaluate these
techniques, and show dramatic reductions in both number of chart-edges and
total parsing time. The robust processing capabilities of the parser are
demonstrated in its use in improving the accuracy of a speech recognizer.
</dc:description>
 <dc:description>Comment: 8 pages, postscript</dc:description>
 <dc:date>1994-07-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407006</dc:identifier>
 <dc:identifier>32nd ACL, Las Cruces, New Mexico, June 1994, pp. 110-116</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>GEMINI: A Natural Language System for Spoken-Language Understanding</dc:title>
 <dc:creator>Dowding, John</dc:creator>
 <dc:creator>Gawron, Jean Mark</dc:creator>
 <dc:creator>Appelt, Doug</dc:creator>
 <dc:creator>Bear, John</dc:creator>
 <dc:creator>Cherny, Lynn</dc:creator>
 <dc:creator>Moore, Robert</dc:creator>
 <dc:creator>Moran, Douglas</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Gemini is a natural language understanding system developed for spoken
language applications. The paper describes the architecture of Gemini, paying
particular attention to resolving the tension between robustness and
overgeneration. Gemini features a broad-coverage unification-based grammar of
English, fully interleaved syntactic and semantic processing in an all-paths,
bottom-up parser, and an utterance-level parser to find interpretations of
sentences that might not be analyzable as complete sentences. Gemini also
includes novel components for recognizing and correcting grammatical
disfluencies, and for doing parse preferences. This paper presents a
component-by-component view of Gemini, providing detailed relevant measurements
of size, efficiency, and performance.
</dc:description>
 <dc:description>Comment: 8 pages, postscript</dc:description>
 <dc:date>1994-07-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407007</dc:identifier>
 <dc:identifier>appeared in 31st ACL, Columbus, Ohio, June 1993, pp. 54-61</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tricolor DAGs for Machine Translation</dc:title>
 <dc:creator>Takeda, Koichi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Machine translation (MT) has recently been formulated in terms of
constraint-based knowledge representation and unification theories, but it is
becoming more and more evident that it is not possible to design a practical MT
system without an adequate method of handling mismatches between semantic
representations in the source and target languages. In this paper, we introduce
the idea of ``information-based'' MT, which is considerably more flexible than
interlingual MT or the conventional transfer-based MT.
</dc:description>
 <dc:description>Comment: 8 pages, Kanji text in the original paper has been romanized</dc:description>
 <dc:date>1994-07-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407008</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Estimating Performance of Pipelined Spoken Language Translation Systems</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Price, Patti</dc:creator>
 <dc:creator>Lyberg, Bertil</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most spoken language translation systems developed to date rely on a
pipelined architecture, in which the main stages are speech recognition,
linguistic analysis, transfer, generation and speech synthesis. When making
projections of error rates for systems of this kind, it is natural to assume
that the error rates for the individual components are independent, making the
system accuracy the product of the component accuracies.
  The paper reports experiments carried out using the SRI-SICS-Telia Research
Spoken Language Translator and a 1000-utterance sample of unseen data. The
results suggest that the naive performance model leads to serious overestimates
of system error rates, since there are in fact strong dependencies between the
components. Predicting the system error rate on the independence assumption by
simple multiplication resulted in a 16\% proportional overestimate for all
utterances, and a 19\% overestimate when only utterances of length 1-10 words
were considered.
</dc:description>
 <dc:description>Comment: 10 pages, Latex source. To appear in Proc. ICSLP '94</dc:description>
 <dc:date>1994-07-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Digalakis, Vassilios</dc:creator>
 <dc:creator>Price, Patti</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A simple and general method is described that can combine different knowledge
sources to reorder N-best lists of hypotheses produced by a speech recognizer.
The method is automatically trainable, acquiring information from both positive
and negative examples. Experiments are described in which it was tested on a
1000-utterance sample of unseen ATIS data.
</dc:description>
 <dc:description>Comment: 13 pages, Latex source. To appear in Proc. HLT '94</dc:description>
 <dc:date>1994-07-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discourse Obligations in Dialogue Processing</dc:title>
 <dc:creator>Traum, David R.</dc:creator>
 <dc:creator>Allen, James F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We show that in modeling social interaction, particularly dialogue, the
attitude of obligation can be a useful adjunct to the popularly considered
attitudes of belief, goal, and intention and their mutual and shared
counterparts. In particular, we show how discourse obligations can be used to
account in a natural manner for the connection between a question and its
answer in dialogue and how obligations can be used along with other parts of
the discourse context to extend the coverage of a dialogue system.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1994-07-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407011</dc:identifier>
 <dc:identifier>In Proceedings of ACL-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Phoneme Recognition Using Acoustic Events</dc:title>
 <dc:creator>Huebener, Kai</dc:creator>
 <dc:creator>Carson-Berndsen, Julie</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a new approach to phoneme recognition using nonsequential
sub--phoneme units. These units are called acoustic events and are
phonologically meaningful as well as recognizable from speech signals. Acoustic
events form a phonologically incomplete representation as compared to
distinctive features. This problem may partly be overcome by incorporating
phonological constraints. Currently, 24 binary events describing manner and
place of articulation, vowel quality and voicing are used to recognize all
German phonemes. Phoneme recognition in this paradigm consists of two steps:
After the acoustic events have been determined from the speech signal, a
phonological parser is used to generate syllable and phoneme hypotheses from
the event lattice. Results obtained on a speaker--dependent corpus are
presented.
</dc:description>
 <dc:description>Comment: 4 pages, to appear at ICSLP'94, PostScript version (compressed and
  uuencoded)</dc:description>
 <dc:date>1994-07-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic
  Representations</dc:title>
 <dc:creator>de Marcken, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an algorithm that acquires words (pairings of phonological forms
and semantic representations) from larger utterances of unsegmented phoneme
sequences and semantic representations. The algorithm maintains from utterance
to utterance only a single coherent dictionary, and learns in the presence of
homonymy, synonymy, and noise. Test results over a corpus of utterances
generated from the Childes database of mother-child interactions are presented.
</dc:description>
 <dc:description>Comment: 12 pages, postscript, to appear in ICGI '94</dc:description>
 <dc:date>1994-07-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Abstract Machine for Typed Feature Structures</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a first step towards the definition of an abstract
machine for linguistic formalisms that are based on typed feature structures,
such as HPSG. The core design of the abstract machine is given in detail,
including the compilation process from a high-level specification language to
the abstract machine language and the implementation of the abstract
instructions. We thus apply methods that were proved useful in computer science
to the study of natural languages: a grammar specified using the formalism is
endowed with an operational semantics. Currently, our machine supports the
unification of simple feature structures, unification of sequences of such
structures, cyclic structures and disjunction.
</dc:description>
 <dc:description>Comment: 38 pages, uuencoded compressed postscript</dc:description>
 <dc:date>1994-07-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Specifying Intonation from Context for Speech Synthesis</dc:title>
 <dc:creator>Prevost, Scott</dc:creator>
 <dc:creator>Steedman, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a theory and a computational implementation for
generating prosodically appropriate synthetic speech in response to database
queries. Proper distinctions of contrast and emphasis are expressed in an
intonation contour that is synthesized by rule under the control of a grammar,
a discourse model, and a knowledge base. The theory is based on Combinatory
Categorial Grammar, a formalism which easily integrates the notions of
syntactic constituency, semantics, prosodic phrasing and information structure.
Results from our current implementation demonstrate the system's ability to
generate a variety of intonational possibilities for a given sentence depending
on the discourse context.
</dc:description>
 <dc:description>Comment: 18 pages</dc:description>
 <dc:date>1994-07-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Role of Cognitive Modeling in Achieving Communicative Intentions</dc:title>
 <dc:creator>Walker, Marilyn</dc:creator>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A discourse planner for (task-oriented) dialogue must be able to make choices
about whether relevant, but optional information (for example, the &quot;satellites&quot;
in an RST-based planner) should be communicated. We claim that effective text
planners must explicitly model aspects of the Hearer's cognitive state, such as
what the hearer is attending to and what inferences the hearer can draw, in
order to make these choices. We argue that a mere representation of the
Hearer's knowledge is inadequate. We support this claim by (1) an analysis of
naturally occurring dialogue, and (2) by simulating the generation of
discourses in a situation in which we can vary the cognitive parameters of the
hearer. Our results show that modeling cognitive state can lead to more
effective discourses (measured with respect to a simple task).
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded compressed ps file</dc:description>
 <dc:date>1994-07-19</dc:date>
 <dc:date>1994-07-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Context-Appropriate Word Orders in Turkish</dc:title>
 <dc:creator>Hoffman, Beryl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Turkish has considerably freer word order than English. The interpretations
of different word orders in Turkish rely on information that describes how a
sentence relates to its discourse context. To capture the syntactic features of
a free word order language, I present an adaptation of Combinatory Categorial
Grammars called {}-CCGs (set-CCGs). In {}-CCGs, a verb's subcategorization
requirements are relaxed so that it requires a set of arguments without
specifying their linear order. I integrate a level of information structure,
representing pragmatic functions such as topic and focus, with {}-CCGs to allow
certain pragmatic distinctions in meaning to influence the word order of a
sentence in a compositional way. Finally, I discuss how this strategy is used
within an implemented generation system which produces Turkish sentences with
context-appropriate word orders in a simple database query task.
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded compressed postscript file, appears in the
  Proceedings of the 7th International Generation Workshop</dc:description>
 <dc:date>1994-07-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Multilingual Documents from a Knowledge Base: The TECHDOC
  Project</dc:title>
 <dc:creator>R&#xf6;sner, Dietmar</dc:creator>
 <dc:creator>Stede, Manfred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  TECHDOC is an implemented system demonstrating the feasibility of generating
multilingual technical documents on the basis of a language-independent
knowledge base. Its application domain is user and maintenance instructions,
which are produced from underlying plan structures representing the activities,
the participating objects with their properties, relations, and so on. This
paper gives a brief outline of the system architecture and discusses some
recent developments in the project: the addition of actual event simulation in
the KB, steps towards a document authoring tool, and a multimodal user
interface. (slightly corrected version of a paper to appear in: COLING 94,
Proceedings)
</dc:description>
 <dc:description>Comment: 5 pages, TEX + PS figure</dc:description>
 <dc:date>1994-07-21</dc:date>
 <dc:date>1994-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tracking Point of View in Narrative</dc:title>
 <dc:creator>Wiebe, Janyce M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Third-person fictional narrative text is composed not only of passages that
objectively narrate events, but also of passages that present characters'
thoughts, perceptions, and inner states. Such passages take a character's
``psychological point of view''. A language understander must determine the
current psychological point of view in order to distinguish the beliefs of the
characters from the facts of the story, to correctly attribute beliefs and
other attitudes to their sources, and to understand the discourse relations
among sentences. Tracking the psychological point of view is not a trivial
problem, because many sentences are not explicitly marked for point of view,
and whether the point of view of a sentence is objective or that of a character
(and if the latter, which character it is) often depends on the context in
which the sentence appears. Tracking the psychological point of view is the
problem addressed in this work. The approach is to seek, by extensive
examinations of naturally-occurring narrative, regularities in the ways that
authors manipulate point of view, and to develop an algorithm that tracks point
of view on the basis of the regularities found. This paper presents this
algorithm, gives demonstrations of an implemented system, and describes the
results of some preliminary empirical studies, which lend support to the
algorithm.
</dc:description>
 <dc:description>Comment: 55 pages, uuencoded compressed ps, appears in Computational
  Linguistics 20:2, pp. 233-287 (electronic version does not reflect all
  copy-editing changes)</dc:description>
 <dc:date>1994-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407019</dc:identifier>
 <dc:identifier>Computational Lingustics 20:2, 233-287</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Sequential Algorithm for Training Text Classifiers</dc:title>
 <dc:creator>Lewis, David D.</dc:creator>
 <dc:creator>Gale, William A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The ability to cheaply train text classifiers is critical to their use in
information retrieval, content analysis, natural language processing, and other
tasks involving data which is partly or fully textual. An algorithm for
sequential sampling during machine learning of statistical classifiers was
developed and tested on a newswire text categorization task. This method, which
we call uncertainty sampling, reduced by as much as 500-fold the amount of
training data that would have to be manually classified to achieve a given
level of effectiveness.
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded, compressed PostScript; Proc. SIGIR-94 LaTex
  available from lewis@research.att.com</dc:description>
 <dc:date>1994-07-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>K-vec: A New Approach for Aligning Parallel Texts</dc:title>
 <dc:creator>Fung, Pascale</dc:creator>
 <dc:creator>Church, Kenneth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Various methods have been proposed for aligning texts in two or more
languages such as the Canadian Parliamentary Debates(Hansards). Some of these
methods generate a bilingual lexicon as a by-product. We present an alternative
alignment strategy which we call K-vec, that starts by estimating the lexicon.
For example, it discovers that the English word &quot;fisheries&quot; is similar to the
French &quot;pe^ches&quot; by noting that the distribution of &quot;fisheries&quot; in the English
text is similar to the distribution of &quot;pe^ches&quot; in the French. K-vec does not
depend on sentence boundaries.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded, compressed PostScript; Proc. COLING-94</dc:description>
 <dc:date>1994-07-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Comparative Discourse Analysis of Parallel Texts</dc:title>
 <dc:creator>van der Eijk, Pim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A quantitative representation of discourse structure can be computed by
measuring lexical cohesion relations among adjacent blocks of text. These
representations have been proposed to deal with sub-topic text segmentation. In
a parallel corpus, similar representations can be derived for versions of a
text in various languages. These can be used for parallel segmentation and as
an alternative measure of text-translation similarity.
</dc:description>
 <dc:description>Comment: 11 pages uuencoded compressed Postscript</dc:description>
 <dc:date>1994-07-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multi-Tape Two-Level Morphology: A Case Study in Semitic Non-linear
  Morphology</dc:title>
 <dc:creator>Kiraz, George</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an implemented multi-tape two-level model capable of
describing Semitic non-linear morphology. The computational framework behind
the current work is motivated by Kay (1987); the formalism presented here is an
extension to the formalism reported by Pulman and Hepple (1993). The objectives
of the current work are: to stay as close as possible, in spirit, to standard
two-level morphology, to stay close to the linguistic description of Semitic
stems, and to present a model which can be used with ease by the Semitist. The
paper illustrates that if finite-state transducers (FSTs) in a standard
two-level morphology model are replaced with multi-tape auxiliary versions
(AFSTs), one can account for Semitic root-and-pattern morphology using high
level notation.
</dc:description>
 <dc:description>Comment: to appear in COLING-94, uuencoded, compressed .ps file, 7 pages</dc:description>
 <dc:date>1994-07-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407024</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>PRINCIPAR---An Efficient, Broad-coverage, Principle-based Parser</dc:title>
 <dc:creator>Lin, Dekang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an efficient, broad-coverage, principle-based parser for English.
The parser has been implemented in C++ and runs on SUN Sparcstations with
X-windows. It contains a lexicon with over 90,000 entries, constructed
automatically by applying a set of extraction and conversion rules to entries
from machine readable dictionaries.
</dc:description>
 <dc:description>Comment: To appear in COLING-94</dc:description>
 <dc:date>1994-07-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Recovering From Parser Failures: A Hybrid Statistical/Symbolic Approach</dc:title>
 <dc:creator>Rose', Carolyn Penstein</dc:creator>
 <dc:creator>Waibel, Alex</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an implementation of a hybrid statistical/symbolic approach to
repairing parser failures in a speech-to-speech translation system. We describe
a module which takes as input a fragmented parse and returns a repaired meaning
representation. It negotiates with the speaker about what the complete meaning
of the utterance is by generating hypotheses about how to fit the fragments of
the partial parse together into a coherent meaning representation. By drawing
upon both statistical and symbolic information, it constrains its repair
hypotheses to those which are both likely and meaningful. Because it updates
its statistical model during use, it improves its performance over time.
</dc:description>
 <dc:date>1994-07-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging and Morphological Disambiguation of Turkish Text</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Kuruoz, Ilker</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Automatic text tagging is an important component in higher level analysis of
text corpora, and its output can be used in many natural language processing
applications. In languages like Turkish or Finnish, with agglutinative
morphology, morphological disambiguation is a very crucial process in tagging,
as the structures of many lexical forms are morphologically ambiguous. This
paper describes a POS tagger for Turkish text based on a full-scale two-level
specification of Turkish morphology that is based on a lexicon of about 24,000
root words. This is augmented with a multi-word and idiomatic construct
recognizer, and most importantly morphological disambiguator based on local
neighborhood constraints, heuristics and limited amount of statistical
information. The tagger also has functionality for statistics compilation and
fine tuning of the morphological analyzer, such as logging erroneous
morphological parses, commonly used roots, etc. Preliminary results indicate
that the tagger can tag about 98-99\% of the texts accurately with very minimal
user intervention. Furthermore for sentences morphologically disambiguated with
the tagger, an LFG parser developed for Turkish, generates, on the average,
50\% less ambiguous parses and parses almost 2.5 times faster. The tagging
functionality is not specific to Turkish, and can be applied to any language
with a proper morphological analysis interface.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of 4th ACL-ANLP Conf. uuencoded gzip'ed
  postscript file, 6 pages</dc:description>
 <dc:date>1994-07-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing as Tree Traversal</dc:title>
 <dc:creator>Gerdemann, Dale</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a unified approach to parsing, in which top-down,
bottom-up and left-corner parsers are related to preorder, postorder and
inorder tree traversals. It is shown that the simplest bottom-up and
left-corner parsers are left recursive and must be converted using an extended
Greibach normal form. With further partial execution, the bottom-up and
left-corner parsers collapse together as in the BUP parser of Matsumoto.
</dc:description>
 <dc:description>Comment: COLING 94 paper, Postscript, compressed and uuencoded</dc:description>
 <dc:date>1994-07-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automated Postediting of Documents</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Chander, Ishwar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Large amounts of low- to medium-quality English texts are now being produced
by machine translation (MT) systems, optical character readers (OCR), and
non-native speakers of English. Most of this text must be postedited by hand
before it sees the light of day. Improving text quality is tedious work, but
its automation has not received much research attention. Anyone who has
postedited a technical report or thesis written by a non-native speaker of
English knows the potential of an automated postediting system. For the case of
MT-generated text, we argue for the construction of postediting modules that
are portable across MT systems, as an alternative to hardcoding improvements
inside any one system. As an example, we have built a complete self-contained
postediting module for the task of article selection (a, an, the) for English
noun phrases. This is a notoriously difficult problem for Japanese-English MT.
Our system contains over 200,000 rules derived automatically from online text
resources. We report on learning algorithms, accuracy, and comparisons with
human performance.
</dc:description>
 <dc:description>Comment: 6 pages, Compressed and uuencoded postscript. To appear: AAAI-94</dc:description>
 <dc:date>1994-07-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building a Large-Scale Knowledge Base for Machine Translation</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Luk, Steve K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Knowledge-based machine translation (KBMT) systems have achieved excellent
results in constrained domains, but have not yet scaled up to newspaper text.
The reason is that knowledge resources (lexicons, grammar rules, world models)
must be painstakingly handcrafted from scratch. One of the hypotheses being
tested in the PANGLOSS machine translation project is whether or not these
resources can be semi-automatically acquired on a very large scale. This paper
focuses on the construction of a large ontology (or knowledge base, or world
model) for supporting KBMT. It contains representations for some 70,000
commonly encountered objects, processes, qualities, and relations. The ontology
was constructed by merging various online dictionaries, semantic networks, and
bilingual resources, through semi-automatic methods. Some of these methods
(e.g., conceptual matching of semantic taxonomies) are broadly applicable to
problems of importing/exporting knowledge from one KB to another. Other methods
(e.g., bilingual matching) allow a knowledge engineer to build up an index to a
KB in a second language, such as Spanish or Japanese.
</dc:description>
 <dc:description>Comment: 6 pages, Compressed and uuencoded postscript. To appear: AAAI-94</dc:description>
 <dc:date>1994-07-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9407030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computing FIRST and FOLLOW Functions for Feature-Theoretic Grammars</dc:title>
 <dc:creator>Trujillo, Arturo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an algorithm for the computation of FIRST and FOLLOW
sets for use with feature-theoretic grammars in which the value of the sets
consists of pairs of feature-theoretic categories. The algorithm preserves as
much information from the grammars as possible, using negative restriction to
define equivalence classes. Addition of a simple data structure leads to an
order of magnitude improvement in execution time over a naive implementation.
</dc:description>
 <dc:description>Comment: 6 pages, COLING 94, compressed, uuencoded PostScript, 93KB</dc:description>
 <dc:date>1994-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9407030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Correct and Efficient Implementation of Appropriateness
  Specifications for Typed Feature Structures</dc:title>
 <dc:creator>Gerdemann, Dale</dc:creator>
 <dc:creator>King, Paul John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we argue that type inferencing incorrectly implements
appropriateness specifications for typed feature structures, promote a
combination of type resolution and unfilling as a correct and efficient
alternative, and consider the expressive limits of this alternative approach.
Throughout, we use feature cooccurence restrictions as illustration and
linguistic motivation.
</dc:description>
 <dc:description>Comment: 5 pages, postscript, compressed and uuencoded (uudecodes to
  gerd_king.ps.Z)</dc:description>
 <dc:date>1994-08-01</dc:date>
 <dc:date>1994-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408001</dc:identifier>
 <dc:identifier>COLING 94 paper</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computational Analyses of Arabic Morphology</dc:title>
 <dc:creator>Kiraz, George A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper demonstrates how a (multi-tape) two-level formalism can be used to
write two-level grammars for Arabic non-linear morphology using a high level,
but computationally tractable, notation. Three illustrative grammars are
provided based on CV-, moraic- and affixational analyses. These are
complemented by a proposal for handling the hitherto computationally untreated
problem of the broken plural. It will be shown that the best grammars for
describing Arabic non-linear morphology are moraic in the case of templatic
stems, and affixational in the case of a-templatic stems. The paper will
demonstrate how the broken plural can be derived under two-level theory via the
`implicit' derivation of the singular.
</dc:description>
 <dc:description>Comment: to appear in Narayanan A., Ditters E. (eds). The Linguistic
  Computation of Arabic. uuencoded, compressed .ps file, 27 pages</dc:description>
 <dc:date>1994-08-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Typed Feature Structures as Descriptions</dc:title>
 <dc:creator>King, Paul John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A description is an entity that can be interpreted as true or false of an
object, and using feature structures as descriptions accrues several
computational benefits. In this paper, I create an explicit interpretation of a
typed feature structure used as a description, define the notion of a
satisfiable feature structure, and create a simple and effective algorithm to
decide if a feature structure is satisfiable.
</dc:description>
 <dc:description>Comment: COLING 94 reserve paper, 5 pages, LaTeX (no .sty exotica)</dc:description>
 <dc:date>1994-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing with Principles and Probabilities</dc:title>
 <dc:creator>Fordham, Andrew</dc:creator>
 <dc:creator>Crocker, Matthew</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper is an attempt to bring together two approaches to language
analysis. The possible use of probabilistic information in principle-based
grammars and parsers is considered, including discussion on some theoretical
and computational problems that arise. Finally a partial implementation of
these ideas is presented, along with some preliminary results from testing on a
small set of sentences.
</dc:description>
 <dc:description>Comment: 10 pages</dc:description>
 <dc:date>1994-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408005</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Modular and Flexible Architecture for an Integrated Corpus Query
  System</dc:title>
 <dc:creator>Christ, Oliver</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper describes the architecture of an integrated and extensible corpus
query system developed at the University of Stuttgart and gives examples of
some of the modules realized within this architecture. The modules form the
core of a corpus workbench. Within the proposed architecture, information
required for the evaluation of queries may be derived from different knowledge
sources (the corpus text, databases, on-line thesauri) and by different means:
either through direct lookup in a database or by calling external tools which
may infer the necessary information at the time of query evaluation. The
information available and the method of information access can be stated
declaratively and individually for each corpus, leading to a flexible,
extensible and modular corpus workbench.
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded gzip'ped PostScript; presented at COMPLEX'94</dc:description>
 <dc:date>1994-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>LHIP: Extended DCGs for Configurable Robust Parsing</dc:title>
 <dc:creator>Ballim, Afzal</dc:creator>
 <dc:creator>Russell, Graham</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present LHIP, a system for incremental grammar development using an
extended DCG formalism. The system uses a robust island-based parsing method
controlled by user-defined performance thresholds.
</dc:description>
 <dc:description>Comment: 10 pages, in Proc. Coling94</dc:description>
 <dc:date>1994-08-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408006</dc:identifier>
 <dc:identifier>Proc. Coling 1994, vol.1 pp.501-507</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Emergent Linguistic Rules from Inducing Decision Trees: Disambiguating
  Discourse Clue Words</dc:title>
 <dc:creator>Siegel, Eric V.</dc:creator>
 <dc:creator>McKeown, Kathleen R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We apply decision tree induction to the problem of discourse clue word sense
disambiguation with a genetic algorithm. The automatic partitioning of the
training set which is intrinsic to decision tree induction gives rise to
linguistically viable rules.
</dc:description>
 <dc:date>1994-08-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408007</dc:identifier>
 <dc:identifier>AAAI94 proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Statistical versus symbolic parsing for captioned-information retrieval</dc:title>
 <dc:creator>Rowe, Neil C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We discuss implementation issues of MARIE-1, a mostly symbolic parser fully
implemented, and MARIE-2, a more statistical parser partially implemented. They
address a corpus of 100,000 picture captions. We argue that the mixed approach
of MARIE-2 should be better for this corpus because its algorithms (not data)
are simpler.
</dc:description>
 <dc:description>Comment: Workshop on the Balancing Act, ACL-94, Las Cruces NM, July 1994</dc:description>
 <dc:date>1994-08-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging accurately -- Don't guess if you know</dc:title>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We discuss combining knowledge-based (or rule-based) and statistical
part-of-speech taggers. We use two mature taggers, ENGCG and Xerox Tagger, to
independently tag the same text and combine the results to produce a fully
disambiguated text. In a 27000 word test sample taken from a previously unseen
corpus we achieve 98.5% accuracy. This paper presents the data in detail. We
describe the problems we encountered in the course of combining the two taggers
and discuss the problem of evaluating taggers.
</dc:description>
 <dc:description>Comment: 6 pages, Proc. ANLP94, uuencoded and gzipped postscript</dc:description>
 <dc:date>1994-08-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Using Selectional Restriction in Language Models for Speech
  Recognition</dc:title>
 <dc:creator>Ueberla, Joerg P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we investigate the use of selectional restriction -- the
constraints a predicate imposes on its arguments -- in a language model for
speech recognition. We use an un-tagged corpus, followed by a public domain
tagger and a very simple finite state machine to obtain verb-object pairs from
unrestricted English text. We then measure the impact the knowledge of the verb
has on the prediction of the direct object in terms of the perplexity of a
cluster-based language model. The results show that even though a clustered
bigram is more useful than a verb-object model, the combination of the two
leads to an improvement over the clustered bigram model.
</dc:description>
 <dc:description>Comment: feedback is welcome to ueberla@cs.sfu.ca</dc:description>
 <dc:date>1994-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Distributional Clustering of English Words</dc:title>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Tishby, Naftali</dc:creator>
 <dc:creator>Lee, Lillian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe and experimentally evaluate a method for automatically clustering
words according to their distribution in particular syntactic contexts.
Deterministic annealing is used to find lowest distortion sets of clusters. As
the annealing parameter increases, existing clusters become unstable and
subdivide, yielding a hierarchical ``soft'' clustering of the data. Clusters
are used as the basis for class models of word coocurrence, and the models
evaluated with respect to held-out test data.
</dc:description>
 <dc:description>Comment: 8 pages, appeared in the proceedings of ACL-93, Columbus, Ohio</dc:description>
 <dc:date>1994-08-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Approximate N-Gram Markov Model for Natural Language Generation</dc:title>
 <dc:creator>Chen, Hsin-Hsi</dc:creator>
 <dc:creator>Lee, Yue-Shi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes an Approximate n-gram Markov Model for bag generation.
Directed word association pairs with distances are used to approximate
(n-1)-gram and n-gram training tables. This model has parameters of word
association model, and merits of both word association model and Markov Model.
The training knowledge for bag generation can be also applied to lexical
selection in machine translation design.
</dc:description>
 <dc:description>Comment: to appear in proceedings of QUALICO-94, 6 pages, uuencoded compressed
  Postscript file; extract with Unix uudecode and uncompress</dc:description>
 <dc:date>1994-08-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Training and Scaling Preference Functions for Disambiguation</dc:title>
 <dc:creator>Alshawi, Hiyan</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an automatic method for weighting the contributions of preference
functions used in disambiguation. Initial scaling factors are derived as the
solution to a least-squares minimization problem, and improvements are then
made by hill-climbing. The method is applied to disambiguating sentences in the
ATIS (Air Travel Information System) corpus, and the performance of the
resulting scaling factors is compared with hand-tuned factors. We then focus on
one class of preference function, those based on semantic lexical collocations.
Experimental results are presented showing that such functions vary
considerably in selecting correct analyses. In particular we define a function
that performs significantly better than ones based on mutual information and
likelihood ratios of lexical associations.
</dc:description>
 <dc:description>Comment: To appear in Computational Linguistics (probably volume 20, December
  94). LaTeX, 21 pages</dc:description>
 <dc:date>1994-08-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Qualitative and Quantitative Models of Speech Translation</dc:title>
 <dc:creator>Alshawi, Hiyan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper compares a qualitative reasoning model of translation with a
quantitative statistical model. We consider these models within the context of
two hypothetical speech translation systems, starting with a logic-based design
and pointing out which of its characteristics are best preserved or eliminated
in moving to the second, quantitative design. The quantitative language and
translation models are based on relations between lexical heads of phrases.
Statistical parameters for structural dependency, lexical transfer, and linear
order are used to select a set of implicit relations between words in a source
utterance, a corresponding set of relations between target language words, and
the most likely translation of the original utterance.
</dc:description>
 <dc:description>Comment: Appeared in proceedings of the ACL workshop &quot;The Balancing Act,
  Combining Symbolic and Statistical Approaches to Language&quot;, Las Cruces NM,
  July 1994. LaTeX, 24 pages</dc:description>
 <dc:date>1994-08-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Experimentally Evaluating Communicative Strategies: The Effect of the
  Task</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Effective problem solving among multiple agents requires a better
understanding of the role of communication in collaboration. In this paper we
show that there are communicative strategies that greatly improve the
performance of resource-bounded agents, but that these strategies are highly
sensitive to the task requirements, situation parameters and agents' resource
limitations. We base our argument on two sources of evidence: (1) an analysis
of a corpus of 55 problem solving dialogues, and (2) experimental simulations
of collaborative problem solving dialogues in an experimental world,
Design-World, where we parameterize task requirements, agents' resources and
communicative strategies.
</dc:description>
 <dc:description>Comment: 8 pages, latex with psfig, lingmacros.sty, available at
  ftp://atlantic.merl.com/pub/walker/aaai94.ps.Z</dc:description>
 <dc:date>1994-08-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408015</dc:identifier>
 <dc:identifier>Proceedings of AAAI 94, Seattle</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Implementing an HPSG theory -- Aspects of the logical architecture,
  the formalization, and the implementation of head-driven phrase structure
  grammars</dc:title>
 <dc:creator>Meurers, Walt Detmar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper presents some aspects involved in the formalization and
implementation of HPSG theories. As basis, the logical setups of Carpenter
(1992) and King (1989, 1994) are briefly compared regarding their usefulness as
basis for HPSGII (Pollard and Sag 1994). The possibilities for expressing HPSG
theories in the HPSGII architecture and in various computational systems (ALE,
Troll, CUF, and TFS) are discussed. Beside a formal characterization of the
possibilities, the paper investigates the specific choices for constraints with
certain linguistic motivations, i.e. the lexicon, structure licencing, and
grammatical principles. An ALE implementation of a theory for German proposed
by Hinrichs and Nakazawa (1994) is used as example and the ALE grammar is
included in the appendix.
</dc:description>
 <dc:description>Comment: paper (56 pages), appendix (28 pages), format: LaTeX, tared,
  compressed, and uuencoded (using uufiles) used: chicago.sty, tree-dvips.sty,
  tree-dvips.pro, psfig.tex, 2up.sty (all available from cmp-lg server)</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reaping the Benefits of Interactive Syntax and Semantics</dc:title>
 <dc:creator>Mahesh, Kavi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Semantic feedback is an important source of information that a parser could
use to deal with local ambiguities in syntax. However, it is difficult to
devise a systematic communication mechanism for interactive syntax and
semantics. In this article, I propose a variant of left-corner parsing to
define the points at which syntax and semantics should interact, an account of
grammatical relations and thematic roles to define the content of the
communication, and a conflict resolution strategy based on independent
preferences from syntax and semantics. The resulting interactive model has been
implemented in a program called COMPERE and shown to account for a wide variety
of psycholinguistic data on structural and lexical ambiguities.
</dc:description>
 <dc:description>Comment: 3 pages, uses latex-acl.sty macro</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408017</dc:identifier>
 <dc:identifier>appeared in ACL-94 Proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Uniform Representations for Syntax-Semantics Arbitration</dc:title>
 <dc:creator>Mahesh, Kavi</dc:creator>
 <dc:creator>Eiselt, Kurt P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Psychological investigations have led to considerable insight into the
working of the human language comprehension system. In this article, we look at
a set of principles derived from psychological findings to argue for a
particular organization of linguistic knowledge along with a particular
processing strategy and present a computational model of sentence processing
based on those principles. Many studies have shown that human sentence
comprehension is an incremental and interactive process in which semantic and
other higher-level information interacts with syntactic information to make
informed commitments as early as possible at a local ambiguity. Early
commitments may be made by using top-down guidance from knowledge of different
types, each of which must be applicable independently of others. Further
evidence from studies of error recovery and delayed decisions points toward an
arbitration mechanism for combining syntactic and semantic information in
resolving ambiguities. In order to account for all of the above, we propose
that all types of linguistic knowledge must be represented in a common form but
must be separable so that they can be applied independently of each other and
integrated at processing time by the arbitrator. We present such a uniform
representation and a computational model called COMPERE based on the
representation and the processing strategy.
</dc:description>
 <dc:description>Comment: 7 pages, uses cogsci94.sty macro</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408018</dc:identifier>
 <dc:identifier>appeared in Cogsci94 Proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building a Parser That can Afford to Interact with Semantics</dc:title>
 <dc:creator>Mahesh, Kavi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Natural language understanding programs get bogged down by the multiplicity
of possible syntactic structures while processing real world texts that human
understanders do not have much difficulty with. In this work, I analyze the
relationships between parsing strategies, the degree of local ambiguity
encountered by them, and semantic feedback to syntax, and propose a parsing
algorithm called {\em Head-Signaled Left Corner Parsing} (HSLC) that minimizes
local ambiguities while supporting interactive syntactic and semantic analysis.
Such a parser has been implemented in a sentence understanding program called
COMPERE.
</dc:description>
 <dc:description>Comment: 2 pages, uses latex-acl.sty macro</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408019</dc:identifier>
 <dc:identifier>appeared in AAAI-94 Proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Having Your Cake and Eating It Too: Autonomy and Interaction in a Model
  of Sentence Processing</dc:title>
 <dc:creator>Eiselt, Kurt P.</dc:creator>
 <dc:creator>Mahesh, Kavi</dc:creator>
 <dc:creator>Holbrook, Jennifer K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Is the human language understander a collection of modular processes
operating with relative autonomy, or is it a single integrated process? This
ongoing debate has polarized the language processing community, with two
fundamentally different types of model posited, and with each camp concluding
that the other is wrong. One camp puts forth a model with separate processors
and distinct knowledge sources to explain one body of data, and the other
proposes a model with a single processor and a homogeneous, monolithic
knowledge source to explain the other body of data. In this paper we argue that
a hybrid approach which combines a unified processor with separate knowledge
sources provides an explanation of both bodies of data, and we demonstrate the
feasibility of this approach with the computational model called COMPERE. We
believe that this approach brings the language processing community
significantly closer to offering human-like language processing systems.
</dc:description>
 <dc:description>Comment: 7 pages, uses aaai.sty macro</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408020</dc:identifier>
 <dc:identifier>appeared in AAAI-93 Proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9408021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Unified Process Model of Syntactic and Semantic Error Recovery in
  Sentence Understanding</dc:title>
 <dc:creator>Holbrook, Jennifer K.</dc:creator>
 <dc:creator>Eiselt, Kurt P.</dc:creator>
 <dc:creator>Mahesh, Kavi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The development of models of human sentence processing has traditionally
followed one of two paths. Either the model posited a sequence of processing
modules, each with its own task-specific knowledge (e.g., syntax and
semantics), or it posited a single processor utilizing different types of
knowledge inextricably integrated into a monolithic knowledge base. Our
previous work in modeling the sentence processor resulted in a model in which
different processing modules used separate knowledge sources but operated in
parallel to arrive at the interpretation of a sentence. One highlight of this
model is that it offered an explanation of how the sentence processor might
recover from an error in choosing the meaning of an ambiguous word. Recent
experimental work by Laurie Stowe strongly suggests that the human sentence
processor deals with syntactic error recovery using a mechanism very much like
that proposed by our model of semantic error recovery. Another way to interpret
Stowe's finding is this: the human sentence processor consists of a single
unified processing module utilizing multiple independent knowledge sources in
parallel. A sentence processor built upon this architecture should at times
exhibit behavior associated with modular approaches, and at other times act
like an integrated system. In this paper we explore some of these ideas via a
prototype computational model of sentence processing called COMPERE, and
propose a set of psychological experiments for testing our theories.
</dc:description>
 <dc:description>Comment: 6 pages, uses cogsci94.sty macro</dc:description>
 <dc:date>1994-08-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9408021</dc:identifier>
 <dc:identifier>appeared in Cogsci-92 Conference Proceedings</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating Knowledge Bases and Statistics in MT</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Chander, Ishwar</dc:creator>
 <dc:creator>Haines, Matthew</dc:creator>
 <dc:creator>Hatzivassiloglou, Vasileios</dc:creator>
 <dc:creator>Hovy, Eduard</dc:creator>
 <dc:creator>Iida, Masayo</dc:creator>
 <dc:creator>Luk, Steve K.</dc:creator>
 <dc:creator>Okumura, Akitoshi</dc:creator>
 <dc:creator>Whitney, Richard</dc:creator>
 <dc:creator>Yamada, Kenji</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We summarize recent machine translation (MT) research at the Information
Sciences Institute of USC, and we describe its application to the development
of a Japanese-English newspaper MT system. Our work aims at scaling up
grammar-based, knowledge-based MT techniques. This scale-up involves the use of
statistical methods, both in acquiring effective knowledge resources and in
making reasonable linguistic choices in the face of knowledge gaps.
</dc:description>
 <dc:description>Comment: 8 pages, compressed, uuencoded postscript</dc:description>
 <dc:date>1994-09-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409001</dc:identifier>
 <dc:identifier>Proc Association for Machine Translation in the Americas (AMTA-94)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Conceptual Association for Compound Noun Analysis</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes research toward the automatic interpretation of compound
nouns using corpus statistics. An initial study aimed at syntactic
disambiguation is presented. The approach presented bases associations upon
thesaurus categories. Association data is gathered from unambiguous cases
extracted from a corpus and is then applied to the analysis of ambiguous
compound nouns. While the work presented is still in progress, a first attempt
to syntactically analyse a test set of 244 examples shows 75% correctness.
Future work is aimed at improving this accuracy and extending the technique to
assign semantic role information, thus producing a complete interpretation.
</dc:description>
 <dc:description>Comment: 3 pages, postscript only, replaced because original postscript
  version incompatible with some printers</dc:description>
 <dc:date>1994-09-06</dc:date>
 <dc:date>1996-09-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409002</dc:identifier>
 <dc:identifier>Proceedings of the Student Session, 32nd Annual Meeting of the
  Association for Computational Linguistics, Las Cruces, NM., 1994 pp337-339</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Probabilistic Model of Compound Nouns</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:creator>Dras, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Compound nouns such as example noun compound are becoming more common in
natural language and pose a number of difficult problems for NLP systems,
notably increasing the complexity of parsing. In this paper we develop a
probabilistic model for syntactically analysing such compounds. The model
predicts compound noun structures based on knowledge of affinities between
nouns, which can be acquired from a corpus. Problems inherent in this
corpus-based approach are addressed: data sparseness is overcome by the use of
semantically motivated word classes and sense ambiguity is explicitly handled
in the model. An implementation based on this model is described in Lauer
(1994) and correctly parses 77% of the test set.
</dc:description>
 <dc:description>Comment: 9 pages, uuencoded compressed postscript, please ignore any undefined
  command error at end</dc:description>
 <dc:date>1994-09-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409003</dc:identifier>
 <dc:identifier>7th Australian Joint Conference on AI, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Experiment on Learning Appropriate Selectional Restrictions from a
  Parsed Corpus</dc:title>
 <dc:creator>Ribas, Francesc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a methodology to extract Selectional Restrictions at a variable
level of abstraction from phrasally analyzed corpora. The method relays in the
use of a wide-coverage noun taxonomy and a statistical measure of the
co-occurrence of linguistic items. Some experimental results about the
performance of the method are provided.
</dc:description>
 <dc:description>Comment: 11 pages</dc:description>
 <dc:date>1994-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409004</dc:identifier>
 <dc:identifier>COLING-94 Proceedings, 769-774</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Focusing for Pronoun Resolution in English Discourse: An Implementation</dc:title>
 <dc:creator>Ersan, Ebru</dc:creator>
 <dc:creator>Akman, Varol</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Anaphora resolution is one of the most active research areas in natural
language processing. This study examines focusing as a tool for the resolution
of pronouns which are a kind of anaphora. Focusing is a discourse phenomenon
like anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and
devised several algorithms to resolve definite anaphora including pronouns. She
presented her theory in a computational framework but did not generally
implement the algorithms. Her algorithms related to focusing and pronoun
resolution are implemented in this thesis. This implementation provides a
better comprehension of the theory both from a conceptual and a computational
point of view. The resulting program is tested on different discourse segments,
and evaluation and analysis of the experiments are presented together with the
statistical results.
</dc:description>
 <dc:description>Comment: iii + 49 pages, compressed, uuencoded Postscript file; revised
  version of the first author's Bilkent M.S. thesis, written under the
  supervision of the second author; notify Akman via e-mail
  (akman@cs.bilkent.edu.tr) or fax (+90-312-266-4126) if you are unable to
  obtain hardcopy, he'll work out something</dc:description>
 <dc:date>1994-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Situated Modeling of Epistemic Puzzles</dc:title>
 <dc:creator>Ersan, Murat</dc:creator>
 <dc:creator>Akman, Varol</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Situation theory is a mathematical theory of meaning introduced by Jon
Barwise and John Perry. It has evoked great theoretical and practical interest
and motivated the framework of a few `computational' systems. PROSIT is the
pioneering work in this direction. Unfortunately, there is a lack of real-life
applications on these systems and this study is a preliminary attempt to remedy
this deficiency. Here, we examine how much PROSIT reflects situation-theoretic
concepts and solve a group of epistemic puzzles using the constructs provided
by this programming language.
</dc:description>
 <dc:description>Comment: iii + 49 pages, compressed, uuencoded Postscript file; revised
  version of the first author's Bilkent M.S. thesis, written under the
  supervision of the second author; notify Akman via e-mail
  (akman@cs.bilkent.edu.tr) or fax (+90-312-266-4126) if you are unable to
  obtain hardcopy, he'll work out something</dc:description>
 <dc:date>1994-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Treating `Free Word Order' in Machine Translation</dc:title>
 <dc:creator>Steinberger, Ralf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In `free word order' languages, every sentence is embedded in its specific
context. Among others, the order of constituents is determined by the
categories `theme', `rheme' and `contrastive focus'. This paper shows how to
recognise and to translate these categories automatically on a sentential
basis, so that sentence embedding can be achieved without having to refer to
the context. Modifier classes, which are traditionally neglected in linguistic
description, are fully covered by the proposed method. (Coling 94, Kyoto, Vol.
I, pages 69-75)
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded compressed postscript file, Coling 94</dc:description>
 <dc:date>1994-09-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing of Spoken Language under Time Constraints</dc:title>
 <dc:creator>Menzel, Wolfgang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Spoken language applications in natural dialogue settings place serious
requirements on the choice of processing architecture. Especially under adverse
phonetic and acoustic conditions parsing procedures have to be developed which
do not only analyse the incoming speech in a time-synchroneous and incremental
manner, but which are able to schedule their resources according to the varying
conditions of the recognition process. Depending on the actual degree of local
ambiguity the parser has to select among the available constraints in order to
narrow down the search space with as little effort as possible.
  A parsing approach based on constraint satisfaction techniques is discussed.
It provides important characteristics of the desired real-time behaviour and
attempts to mimic some of the attention focussing capabilities of the human
speech comprehension mechanism.
</dc:description>
 <dc:description>Comment: 19 pages, LaTeX</dc:description>
 <dc:date>1994-09-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Linguistics Computation, Automatic Model Generation, and Intensions</dc:title>
 <dc:creator>Nourani, Cyrus F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Techniques are presented for defining models of computational linguistics
theories. The methods of generalized diagrams that were developed by this
author for modeling artificial intelligence planning and reasoning are shown to
be applicable to models of computation of linguistics theories. It is shown
that for extensional and intensional interpretations, models can be generated
automatically which assign meaning to computations of linguistics theories for
natural languages.
  Keywords: Computational Linguistics, Reasoning Models, G-diagrams For Models,
Dynamic Model Implementation, Linguistics and Logics For Artificial
Intelligence
</dc:description>
 <dc:description>Comment: The paper is plain text.</dc:description>
 <dc:date>1994-09-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Inducing Probabilistic Grammars by Bayesian Model Merging</dc:title>
 <dc:creator>Stolcke, Andreas</dc:creator>
 <dc:creator>Omohundro, Stephen M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a framework for inducing probabilistic grammars from corpora of
positive samples. First, samples are {\em incorporated} by adding ad-hoc rules
to a working grammar; subsequently, elements of the model (such as states or
nonterminals) are {\em merged} to achieve generalization and a more compact
representation. The choice of what to merge and when to stop is governed by the
Bayesian posterior probability of the grammar given the data, which formalizes
a trade-off between a close fit to the data and a default preference for
simpler models (`Occam's Razor'). The general scheme is illustrated using three
types of probabilistic grammars: Hidden Markov models, class-based $n$-grams,
and stochastic context-free grammars.
</dc:description>
 <dc:description>Comment: To appear in Grammatical Inference and Applications, Second
  International Colloquium on Grammatical Inference; Springer Verlag, 1994. 13
  pages</dc:description>
 <dc:date>1994-09-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Aligning Noisy Parallel Corpora Across Language Groups : Word Pair
  Feature Matching by Dynamic Time Warping</dc:title>
 <dc:creator>Fung, Pascale</dc:creator>
 <dc:creator>McKeown, Kathleen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a new algorithm called DK-vec for aligning pairs of
Asian/Indo-European noisy parallel texts without sentence boundaries. DK-vec
improves on previous alignment algorithms in that it handles better the
non-linear nature of noisy corpora. The algorithm uses frequency, position and
recency information as features for pattern matching. Dynamic Time Warping is
used as the matching technique between word pairs. This algorithm produces a
small bilingual lexicon which provides anchor points for alignment.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded, compressed PostScript</dc:description>
 <dc:date>1994-09-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409011</dc:identifier>
 <dc:identifier>Proc. AMTA-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9409012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards an Automatic Dictation System for Translators: the TransTalk
  Project</dc:title>
 <dc:creator>Dymetman, Marc</dc:creator>
 <dc:creator>Brousseau, Julie</dc:creator>
 <dc:creator>Foster, George</dc:creator>
 <dc:creator>Isabelle, Pierre</dc:creator>
 <dc:creator>Normandin, Yves</dc:creator>
 <dc:creator>Plamondon, Pierre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Professional translators often dictate their translations orally and have
them typed afterwards. The TransTalk project aims at automating the second part
of this process. Its originality as a dictation system lies in the fact that
both the acoustic signal produced by the translator and the source text under
translation are made available to the system. Probable translations of the
source text can be predicted and these predictions used to help the speech
recognition system in its lexical choices. We present the results of the first
prototype, which show a marked improvement in the performance of the speech
recognition task when translation predictions are taken into account.
</dc:description>
 <dc:description>Comment: Published in proceedings of the International Conference on Spoken
  Language Processing (ICSLP) 94. 4 pages, uuencoded compressed latex source
  with 4 postscript figures</dc:description>
 <dc:date>1994-09-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9409012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improving Language Models by Clustering Training Sentences</dc:title>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Many of the kinds of language model used in speech understanding suffer from
imperfect modeling of intra-sentential contextual influences. I argue that this
problem can be addressed by clustering the sentences in a training corpus
automatically into subcorpora on the criterion of entropy reduction, and
calculating separate language model parameters for each cluster. This kind of
clustering offers a way to represent important contextual effects and can
therefore significantly improve the performance of a model. It also offers a
reasonably automatic means to gather evidence on whether a more complex,
context-sensitive model using the same general kind of linguistic information
is likely to reward the effort that would be required to develop it: if
clustering improves the performance of a model, this proves the existence of
further context dependencies, not exploited by the unclustered model. As
evidence for these claims, I present results showing that clustering improves
some models but not others for the ATIS domain. These results are consistent
with other findings for such models, suggesting that the existence or otherwise
of an improvement brought about by clustering is indeed a good pointer to
whether it is worth developing further the unclustered model.
</dc:description>
 <dc:description>Comment: Expanded version of a paper to appear in ANLP-94, Stuttgart. Latex, 7
  pages. Needs latex-acl.sty</dc:description>
 <dc:date>1994-10-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lexikoneintraege fuer deutsche Adverbien (Dictionary Entries for German
  Adverbs)</dc:title>
 <dc:creator>Steinberger, Ralf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Modifiers in general, and adverbs in particular, are neglected categories in
linguistics, and consequently, their treatment in Natural Language Processing
poses problems. In this article, we present the dictionary information for
German adverbs which is necessary to deal with word order, degree modifier
scope and other problems in NLP. We also give evidence for the claim that a
classification according to position classes differs from any semantic
classification.
</dc:description>
 <dc:description>Comment: In German; 10 pages, uuencoded gz-compressed tar file of LaTeX file;
  In: H. Trost (ed), Informatik Xpress 6. Tagungsband der 2. Konferenz
  &quot;Verarbeitung natuerlicher Sprache&quot; KONVENS '94, Wien (Austria), September
  1994, pages: 320-329</dc:description>
 <dc:date>1994-10-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410002</dc:identifier>
 <dc:language>de</dc:language>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Principle Based Semantics for HPSG</dc:title>
 <dc:creator>Frank, A.</dc:creator>
 <dc:creator>Reyle, U.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper presents a constraint based semantic formalism for HPSG. The
advantages of the formlism are shown with respect to a grammar for a fragment
of German that deals with (i) quantifier scope ambiguities triggered by
scrambling and/or movement and (ii) ambiguities that arise from the
collective/distributive distinction of plural NPs. The syntax-semantics
interface directly implements syntactic conditions on quantifier scoping and
distributivity. The construction of semantic representations is guided by
general principles governing the interaction between syntax and semantics. Each
of these principles acts as a constraint to narrow down the set of possible
interpretations of a sentence. Meanings of ambiguous sentences are represented
by single partial representations (so-called U(nderspecified) D(iscourse)
R(epresentation) S(tructure)s) to which further constraints can be added
monotonically to gain more information about the content of a sentence. There
is no need to build up a large number of alternative representations of the
sentence which are then filtered by subsequent discourse and world knowledge.
The advantage of UDRSs is not only that they allow for monotonic incremental
interpretation but also that they are equipped with truth conditions and a
proof theory that allows for inferences to be drawn directly on structures
where quantifier scope is not resolved.
</dc:description>
 <dc:date>1994-10-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Spelling Correction in Agglutinative Languages</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an approach to spelling correction in agglutinative
languages that is based on two-level morphology and a dynamic programming based
search algorithm. Spelling correction in agglutinative languages is
significantly different than in languages like English. The concept of a word
in such languages is much wider that the entries found in a dictionary, owing
to {}~productive word formation by derivational and inflectional affixations.
After an overview of certain issues and relevant mathematical preliminaries, we
formally present the problem and our solution. We then present results from our
experiments with spelling correction in Turkish, a Ural--Altaic agglutinative
language. Our results indicate that we can find the intended correct word in
95\% of the cases and offer it as the first candidate in 74\% of the cases,
when the edit distance is 1.
</dc:description>
 <dc:description>Comment: uuencoded postscript file, poster version to appear in ANLP
  proceedings. (Abstract now fixed)</dc:description>
 <dc:date>1994-10-06</dc:date>
 <dc:date>1994-10-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Centering Approach to Pronouns</dc:title>
 <dc:creator>Brennan, Susan E.</dc:creator>
 <dc:creator>Friedman, Marilyn Walker</dc:creator>
 <dc:creator>Pollard, Carl J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a formalization of the centering approach to
modeling attentional structure in discourse and use it as the basis for an
algorithm to track discourse context and bind pronouns. As described in Grosz,
Joshi and Weinstein (1986), the process of centering attention on entities in
the discourse gives rise to the intersentential transitional states of
continuing, retaining and shifting. We propose an extension to these states
which handles some additional cases of multiple ambiguous pronouns. The
algorithm has been implemented in an HPSG natural language system which serves
as the interface to a database query application.
</dc:description>
 <dc:description>Comment: plain latex but includes psfig.tex, 8 pages with one psfig, published
  in 25th Annual Meeting of the Association of Computational Linguistics, 1987</dc:description>
 <dc:date>1994-10-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410005</dc:identifier>
 <dc:identifier>Association of Computational Linguistics 1987, p. 155-162</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluating Discourse Processing Algorithms</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In order to take steps towards establishing a methodology for evaluating
Natural Language systems, we conducted a case study. We attempt to evaluate two
different approaches to anaphoric processing in discourse by comparing the
accuracy and coverage of two published algorithms for finding the co-specifiers
of pronouns in naturally occurring texts and dialogues. We present the
quantitative results of hand-simulating these algorithms, but this analysis
naturally gives rise to both a qualitative evaluation and recommendations for
performing such evaluations in general. We illustrate the general difficulties
encountered with quantitative evaluation. These are problems with: (a) allowing
for underlying assumptions, (b) determining how to handle underspecifications,
and (c) evaluating the contribution of false positives and error chaining.
</dc:description>
 <dc:description>Comment: plain latex but includes psfig.tex, 11 pages with one psfig,
  published in 27th Annual Meeting of the ACL, 1989</dc:description>
 <dc:date>1994-10-10</dc:date>
 <dc:date>1994-10-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410006</dc:identifier>
 <dc:identifier>Association of Computational Linguistics, 1989, p. 251-262</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Formal Look at Dependency Grammars and Phrase-Structure Grammars, with
  Special Consideration of Word-Order Phenomena</dc:title>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:creator>Joshi, Aravind</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The central role of the lexicon in Meaning-Text Theory (MTT) and other
dependency-based linguistic theories cannot be replicated in linguistic
theories based on context-free grammars (CFGs). We describe Tree Adjoining
Grammar (TAG) as a system that arises naturally in the process of lexicalizing
CFGs. A TAG grammar can therefore be compared directly to an Meaning-Text Model
(MTM). We illustrate this point by discussing the computational complexity of
certain non-projective constructions, and suggest a way of incorporating
locality of word-order definitions into the Surface-Syntactic Component of MTT.
</dc:description>
 <dc:description>Comment: uuencoded compressed ps file, 20 pages</dc:description>
 <dc:date>1994-10-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Recognizing Text Genres with Simple Metrics Using Discriminant Analysis</dc:title>
 <dc:creator>Karlgren, Jussi</dc:creator>
 <dc:creator>Cutting, Douglass</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A simple method for categorizing texts into predetermined text genre
categories using the statistical standard technique of discriminant analysis is
demonstrated with application to the Brown corpus. Discriminant analysis makes
it possible use a large number of parameters that may be specific for a certain
corpus or information stream, and combine them into a small number of
functions, with the parameters weighted on basis of how useful they are for
discriminating text genres. An application to information retrieval is
discussed.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, In proceedings of COLING 94</dc:description>
 <dc:date>1994-10-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lexical Functions and Machine Translation</dc:title>
 <dc:creator>Heylen, Dirk</dc:creator>
 <dc:creator>Maxwell, Kerry G.</dc:creator>
 <dc:creator>Verhagen, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses the lexicographical concept of lexical functions and
their potential exploitation in the development of a machine translation
lexicon designed to handle collocations.
</dc:description>
 <dc:description>Comment: 6 pages, uses named.sty, twocolumn.sty, a4.sty</dc:description>
 <dc:date>1994-10-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>XTAG system - A Wide Coverage Grammar for English</dc:title>
 <dc:creator>Doran, Christy</dc:creator>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Hockey, Beth Ann</dc:creator>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:creator>Zaidel, Martin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents the XTAG system, a grammar development tool based on the
Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic
grammar for English. The various components of the system are discussed and
preliminary evaluation results from the parsing of various corpora are given.
Results from the comparison of XTAG against the IBM statistical parser and the
Alvey Natural Language Tool parser are also given.
</dc:description>
 <dc:description>Comment: ps file. 7 pages</dc:description>
 <dc:date>1994-10-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410010</dc:identifier>
 <dc:identifier>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994, pp. 922-928</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dilemma - An Instant Lexicographer</dc:title>
 <dc:creator>Karlgren, Hans</dc:creator>
 <dc:creator>Karlgren, Jussi</dc:creator>
 <dc:creator>Nordstr&#xf6;m, Magnus</dc:creator>
 <dc:creator>Pettersson, Paul</dc:creator>
 <dc:creator>Wahrol&#xe9;n, Bengt</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Dilemma is intended to enhance quality and increase productivity of expert
human translators by presenting to the writer relevant lexical information
mechanically extracted from comparable existing translations, thus replacing -
or compensating for the absence of - a lexicographer and stand-by terminologist
rather than the translator. Using statistics and crude surface analysis and a
minimum of prior information, Dilemma identifies instances and suggests their
counterparts in parallel source and target texts, on all levels down to
individual words. Dilemma forms part of a tool kit for translation where focus
is on text structure and over-all consistency in large text volumes rather than
on framing sentences, on interaction between many actors in a large project
rather than on retrieval of machine-stored data and on decision making rather
than on application of given rules. In particular, the system has been tuned to
the needs of the ongoing translation of European Community legislation into the
languages of candidate member countries. The system has been demonstrated to
and used by professional translators with promising results.
</dc:description>
 <dc:description>Comment: 3 pages, LaTeX, in proceedings of COLING 94</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Does Baum-Welch Re-estimation Help Taggers?</dc:title>
 <dc:creator>Elworthy, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In part of speech tagging by Hidden Markov Model, a statistical model is used
to assign grammatical categories to words in a text. Early work in the field
relied on a corpus which had been tagged by a human annotator to train the
model. More recently, Cutting {\it et al.} (1992) suggest that training can be
achieved with a minimal lexicon and a limited amount of {\em a priori}
information about probabilities, by using Baum-Welch re-estimation to
automatically refine the model. In this paper, I report two experiments
designed to determine how much manual training information is needed. The first
experiment suggests that initial biasing of either lexical or transition
probabilities is essential to achieve a good accuracy. The second experiment
reveals that there are three distinct patterns of Baum-Welch re-estimation. In
two of the patterns, the re-estimation ultimately reduces the accuracy of the
tagging rather than improving it. The pattern which is applicable can be
predicted from the quality of the initial model and the similarity between the
tagged training corpus (if any) and the corpus to be tagged. Heuristics for
deciding how to use re-estimation in an effective manner are given. The
conclusions are broadly in agreement with those of Merialdo (1994), but give
greater detail about the contributions of different parts of the model.
</dc:description>
 <dc:description>Comment: Uses aclap.sty. Appeared in ANLP 94</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Error Detection in Part of Speech Tagging</dc:title>
 <dc:creator>Elworthy, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A technique for detecting errors made by Hidden Markov Model taggers is
described, based on comparing observable values of the tagging process with a
threshold. The resulting approach allows the accuracy of the tagger to be
improved by accepting a lower efficiency, defined as the proportion of words
which are tagged. Empirical observations are presented which demonstrate the
validity of the technique and suggest how to choose an appropriate threshold.
</dc:description>
 <dc:description>Comment: Postscript. Appeared in NeMLaP 1994</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Freely Available Syntactic Lexicon for English</dc:title>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Martin, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a syntactic lexicon for English that was originally
derived from the Oxford Advanced Learner's Dictionary and the Oxford Dictionary
of Current Idiomatic English, and then modified and augmented by hand. There
are more than 37,000 syntactic entries from all 8 parts of speech. An X-windows
based tool is available for maintaining the lexicon and performing searches. C
and Lisp hooks are also available so that the lexicon can be easily utilized by
parsers and other programs.
</dc:description>
 <dc:description>Comment: Latex file with .eps figure. 8 pages</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410014</dc:identifier>
 <dc:identifier>Proceedings of the International Workshop on Sharable Natural
  Language Resources, Nara, Japan, August 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lexicalization and Grammar Development</dc:title>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Doran, Christy</dc:creator>
 <dc:creator>Becker, Tilman</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a fully lexicalized grammar formalism as a
particularly attractive framework for the specification of natural language
grammars. We discuss in detail Feature-based, Lexicalized Tree Adjoining
Grammars (FB-LTAGs), a representative of the class of lexicalized grammars. We
illustrate the advantages of lexicalized grammars in various contexts of
natural language processing, ranging from wide-coverage grammar development to
parsing and machine translation. We also present a method for compact and
efficient representation of lexicalized trees.
</dc:description>
 <dc:description>Comment: ps file. English w/ German abstract. 10 pages</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410015</dc:identifier>
 <dc:identifier>Proceedings of KONVENS 94, Vienna, Austria, September 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dutch Cross Serial Dependencies in HPSG</dc:title>
 <dc:creator>Rentier, Gerrit</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an analysis of Dutch cross serial dependencies in Head-driven
Phrase Structure Grammar. Arguably, our analysis differs from other analyses in
that we do not refer to `additional' mechanisms (e.g., sequence union, head
wrapping): just standard structure sharing, an immediate dominance schema and a
linear precedence rule.
</dc:description>
 <dc:description>Comment: 5 pages uuencoded compressed PostScript</dc:description>
 <dc:date>1994-10-21</dc:date>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Concurrent Lexicalized Dependency Parsing: The ParseTalk Model</dc:title>
 <dc:creator>Broeker, Norbert</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Schacht, Susanne</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A grammar model for concurrent, object-oriented natural language parsing is
introduced. Complete lexical distribution of grammatical knowledge is achieved
building upon the head-oriented notions of valency and dependency, while
inheritance mechanisms are used to capture lexical generalizations. The
underlying concurrent computation model relies upon the actor paradigm. We
consider message passing protocols for establishing dependency relations and
ambiguity handling.
</dc:description>
 <dc:description>Comment: 90kB, 7pages Postscript</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410017</dc:identifier>
 <dc:identifier>Proc.15th Intl Conference on Computational Linguistics, Kyoto,
  Japan, August 1994, pp.379-385</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Part-of-Speech Tagging with Neural Networks</dc:title>
 <dc:creator>Schmid, Helmut</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Text corpora which are tagged with part-of-speech information are useful in
many areas of linguistic research. In this paper, a new part-of-speech tagging
method based on neural networks (Net- Tagger) is presented and its performance
is compared to that of a HMM-tagger and a trigram-based tagger. It is shown
that the Net- Tagger performs as well as the trigram-based tagger and better
than the HMM-tagger.
</dc:description>
 <dc:description>Comment: 5 pages</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410018</dc:identifier>
 <dc:identifier>Coling-94, 172-176</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Concurrent Lexicalized Dependency Parsing: A Behavioral View on
  ParseTalk Events</dc:title>
 <dc:creator>Schacht, Susanne</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Broeker, Norbert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The behavioral specification of an object-oriented grammar model is
considered. The model is based on full lexicalization, head-orientation via
valency constraints and dependency relations, inheritance as a means for
non-redundant lexicon specification, and concurrency of computation. The
computation model relies upon the actor paradigm, with concurrency entering
through asynchronous message passing between actors. In particular, we here
elaborate on principles of how the global behavior of a lexically distributed
grammar and its corresponding parser can be specified in terms of event type
networks and event networks, resp.
</dc:description>
 <dc:description>Comment: 68kB, 5pages Postscript</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410019</dc:identifier>
 <dc:identifier>Proc.15th Intl Conference on Computational Linguistics, Kyoto,
  Japan, August 1994, pp.498-493</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Construction of a Bilingual Dictionary Intermediated by a Third Language</dc:title>
 <dc:creator>TANAKA, Kumiko</dc:creator>
 <dc:creator>UMEMURA, Kyoji</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  When using a third language to construct a bilingual dictionary, it is
necessary to discriminate equivalencies from inappropriate words derived as a
result of ambiguity in the third language. We propose a method to treat this by
utilizing the structures of dictionaries to measure the nearness of the
meanings of words. The resulting dictionary is a word-to-word bilingual
dictionary of nouns and can be used to refine the entries and equivalencies in
published bilingual dictionaries.
</dc:description>
 <dc:description>Comment: 7 pages ps file compressed and uuencoded</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reference Resolution Using Semantic Patterns in Japanese Newspaper
  Articles</dc:title>
 <dc:creator>Wakao, Takahiro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Reference resolution is one of the important tasks in natural language
processing. In this paper, the author first determines the referents and their
locations of &quot;dousha&quot;, literally meaning &quot;the same company&quot;, which appear in
Japanese newspaper articles. Secondly, three heuristic methods, two of which
use semantic information in text such as company names and their patterns, are
proposed and tested on how accurately they identify the correct referents. The
proposed methods based on semantic patterns show high accuracy for reference
resolution of &quot;dousha&quot; (more than 90\%). This suggests that semantic
pattern-matching methods are effective for reference resolution in newspaper
articles.
</dc:description>
 <dc:description>Comment: 5 pages, Latex, In Proceedings of COLING 94, uses coling.sty</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automated tone transcription</dc:title>
 <dc:creator>Bird, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I report on an investigation into the problem of assigning
tones to pitch contours. The proposed model is intended to serve as a tool for
phonologists working on instrumentally obtained pitch data from tone languages.
Motivation and exemplification for the model is provided by data taken from my
fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and
others, I provide a parametrised F_0 prediction function P which generates F_0
values from a tone sequence, and I explore the asymptotic behaviour of
downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0)
values amounts to finding a tone sequence T such that P(T) {}~= X. This is a
combinatorial optimisation problem, for which two non-deterministic search
techniques are provided: a genetic algorithm and a simulated annealing
algorithm. Finally, two implementations---one for each technique---are
described and then compared using both artificial and real data for sequences
of up to 20 tones. These programs can be adapted to other tone languages by
adjusting the F_0 prediction function.
</dc:description>
 <dc:description>Comment: 12 pages, 4 postscript figures, uses examples.sty, newapa.sty,
  latex-acl.sty, ipamacs.sty</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:date>1994-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410022</dc:identifier>
 <dc:identifier>Proceedings of the First Meeting of the ACL Special</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Korean to English Translation Using Synchronous TAGs</dc:title>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Palmer, Martha</dc:creator>
 <dc:creator>Park, Hyun S.</dc:creator>
 <dc:creator>Joshi, Aravind K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  It is often argued that accurate machine translation requires reference to
contextual knowledge for the correct treatment of linguistic phenomena such as
dropped arguments and accurate lexical selection. One of the historical
arguments in favor of the interlingua approach has been that, since it revolves
around a deep semantic representation, it is better able to handle the types of
linguistic phenomena that are seen as requiring a knowledge-based approach. In
this paper we present an alternative approach, exemplified by a prototype
system for machine translation of English and Korean which is implemented in
Synchronous TAGs. This approach is essentially transfer based, and uses
semantic feature unification for accurate lexical selection of polysemous
verbs. The same semantic features, when combined with a discourse model which
stores previously mentioned entities, can also be used for the recovery of
topicalized arguments. In this paper we concentrate on the translation of
Korean to English.
</dc:description>
 <dc:description>Comment: ps file. 8 pages</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410023</dc:identifier>
 <dc:identifier>Proceedings of AMTA 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Freely Available Wide Coverage Morphological Analyzer for English</dc:title>
 <dc:creator>Karp, Daniel</dc:creator>
 <dc:creator>Schabes, Yves</dc:creator>
 <dc:creator>Zaidel, Martin</dc:creator>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a morphological lexicon for English that handles more
than 317000 inflected forms derived from over 90000 stems. The lexicon is
available in two formats. The first can be used by an implementation of a
two-level processor for morphological analysis. The second, derived from the
first one for efficiency reasons, consists of a disk-based database using a
UNIX hash table facility. We also built an X Window tool to facilitate the
maintenance and browsing of the lexicon. The package is ready to be integrated
into an natural language application such as a parser through hooks written in
Lisp and C.
</dc:description>
 <dc:description>Comment: uuencoded compressed ps file. 5 pages. Contact info has been upated
  from Coling '92 version</dc:description>
 <dc:date>1994-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410024</dc:identifier>
 <dc:identifier>Proceedings of Coling 92</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Syntactic Analysis Of Natural Language Using Linguistic Rules And
  Corpus-based Patterns</dc:title>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:creator>J&#xe4;rvinen, Timo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We are concerned with the syntactic annotation of unrestricted text. We
combine a rule-based analysis with subsequent exploitation of empirical data.
The rule-based surface syntactic analyser leaves some amount of ambiguity in
the output that is resolved using empirical patterns. We have implemented a
system for generating and applying corpus-based patterns. Some patterns
describe the main constituents in the sentence and some the local context of
the each syntactic function. There are several (partly) reduntant patterns, and
the ``pattern'' parser selects analysis of the sentence that matches the
strictest possible pattern(s). The system is applied to an experimental corpus.
We present the results and discuss possible refinements of the method from a
linguistic point of view.
</dc:description>
 <dc:description>Comment: in Proc Coling-94, Vol I, pp. 629-634, Kyoto. Postscript</dc:description>
 <dc:date>1994-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation</dc:title>
 <dc:creator>Brill, Eric</dc:creator>
 <dc:creator>Resnik, Philip</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe a new corpus-based approach to prepositional
phrase attachment disambiguation, and present results comparing performance of
this algorithm with other corpus-based approaches to this problem.
</dc:description>
 <dc:description>Comment: 7 pages, compressed uuencoded postscript</dc:description>
 <dc:date>1994-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410026</dc:identifier>
 <dc:identifier>COLING 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Tagging with Feature Structures</dc:title>
 <dc:creator>Kempe, Andre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The described tagger is based on a hidden Markov model and uses tags composed
of features such as part-of-speech, gender, etc. The contextual probability of
a tag (state transition probability) is deduced from the contextual
probabilities of its feature-value-pairs. This approach is advantageous when
the available training corpus is small and the tag set large, which can be the
case with morphologically rich languages.
</dc:description>
 <dc:description>Comment: Coling-94, 85 KB, 5 pages</dc:description>
 <dc:date>1994-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410027</dc:identifier>
 <dc:identifier>COLING-94, vol.1, pp.161-165, Kyoto, Japan. August 5-9, 1994.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Minimal Change and Bounded Incremental Parsing</dc:title>
 <dc:creator>Wir&#xe9;n, Mats</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Ideally, the time that an incremental algorithm uses to process a change
should be a function of the size of the change rather than, say, the size of
the entire current input. Based on a formalization of ``the set of things
changed'' by an incremental modification, this paper investigates how and to
what extent it is possible to give such a guarantee for a chart-based parsing
framework and discusses the general utility of a minimality notion in
incremental processing.
</dc:description>
 <dc:description>Comment: 7 pages, compressed and uuencoded</dc:description>
 <dc:date>1994-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing</dc:title>
 <dc:creator>Joshi, Aravind K.</dc:creator>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In a lexicalized grammar formalism such as Lexicalized Tree-Adjoining Grammar
(LTAG), each lexical item is associated with at least one elementary structure
(supertag) that localizes syntactic and semantic dependencies. Thus a parser
for a lexicalized grammar must search a large set of supertags to choose the
right ones to combine for the parse of the sentence. We present techniques for
disambiguating supertags using local information such as lexical preference and
local lexical dependencies. The similarity between LTAG and Dependency grammars
is exploited in the dependency model of supertag disambiguation. The
performance results for various models of supertag disambiguation such as
unigram, trigram and dependency-based models are presented.
</dc:description>
 <dc:description>Comment: ps file. 8 pages</dc:description>
 <dc:date>1994-10-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410029</dc:identifier>
 <dc:identifier>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING 94), Kyoto, Japan, August 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Feature-Based TAG in place of multi-component adjunction: Computational
  Implications</dc:title>
 <dc:creator>Hockey, B. A.</dc:creator>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Using feature-based Tree Adjoining Grammar (TAG), this paper presents
linguistically motivated analyses of constructions claimed to require
multi-component adjunction. These feature-based TAG analyses permit parsing of
these constructions using an existing unification-based Earley-style TAG
parser, thus obviating the need for a multi-component TAG parser without
sacrificing linguistic coverage for English.
</dc:description>
 <dc:description>Comment: ps file. 9 pages</dc:description>
 <dc:date>1994-10-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410030</dc:identifier>
 <dc:identifier>Natural Language Processing Pacific Rim Symposium (NLPRS 93)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410031</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards a More User-friendly Correction</dc:title>
 <dc:creator>Genthial, Damien</dc:creator>
 <dc:creator>Courtin, Jacques</dc:creator>
 <dc:creator>Trilan, Jacques Menezo Equipe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We first present our view of detection and correction of syntactic errors. We
then introduce a new correction method, based on heuristic criteria used to
decide which correction should be preferred. Weighting of these criteria leads
to a flexible and parametrable system, which can adapt itself to the user. A
partitioning of the trees based on linguistic criteria: agreement rules, rather
than computational criteria is then necessary. We end by proposing extensions
to lexical correction and to some syntactic errors. Our aim is an adaptable and
user-friendly system capable of automatic correction for some applications.
</dc:description>
 <dc:description>Comment: Postscript file, compressed and uuencoded, 6 pages, published at
  CoLing'94, Kyoto, Japan, August 94</dc:description>
 <dc:date>1994-10-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Planning Argumentative Texts</dc:title>
 <dc:creator>Huang, Xiaorong</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents \proverb\, a text planner for argumentative texts.
\proverb\'s main feature is that it combines global hierarchical planning and
unplanned organization of text with respect to local derivation relations in a
complementary way. The former splits the task of presenting a particular proof
into subtasks of presenting subproofs. The latter simulates how the next
intermediate conclusion to be presented is chosen under the guidance of the
local focus.
</dc:description>
 <dc:description>Comment: Coling94, email: huang@cs.uni-sb.de</dc:description>
 <dc:date>1994-10-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Default Handling in Incremental Generation</dc:title>
 <dc:creator>Harbusch, Karin</dc:creator>
 <dc:creator>Kikui, Gen-ichiro</dc:creator>
 <dc:creator>Kilger, Anne</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Natural language generation must work with insufficient input.
Underspecifications can be caused by shortcomings of the component providing
the input or by the preliminary state of incrementally given input. The paper
aims to escape from such dead-end situations by making assumptions. We discuss
global aspects of default handling. Two problem classes for defaults in the
incremental syntactic generator VM-GEN are presented to substantiate our
discussion.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded LaTeX file</dc:description>
 <dc:date>1994-10-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410033</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9410034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Comparison of Two Smoothing Methods for Word Bigram Models</dc:title>
 <dc:creator>Peto, Linda Bauman</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A COMPARISON OF TWO SMOOTHING METHODS FOR WORD BIGRAM MODELS
  Linda Bauman Peto
  Department of Computer Science
  University of Toronto Abstract Word bigram models estimated from text corpora
require smoothing methods to estimate the probabilities of unseen bigrams. The
deleted estimation method uses the formula:
  Pr(i|j) = lambda f_i + (1-lambda)f_i|j, where f_i and f_i|j are the relative
frequency of i and the conditional relative frequency of i given j,
respectively, and lambda is an optimized parameter. MacKay (1994) proposes a
Bayesian approach using Dirichlet priors, which yields a different formula:
  Pr(i|j) = (alpha/F_j + alpha) m_i + (1 - alpha/F_j + alpha) f_i|j where F_j
is the count of j and alpha and m_i are optimized parameters. This thesis
describes an experiment in which the two methods were trained on a
two-million-word corpus taken from the Canadian _Hansard_ and compared on the
basis of the experimental perplexity that they assigned to a shared test
corpus. The methods proved to be about equally accurate, with MacKay's method
using fewer resources.
</dc:description>
 <dc:description>Comment: M.Sc. thesis, 57 pages, compressed and uucencoded postscript file</dc:description>
 <dc:date>1994-10-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9410034</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Sublanguage Terms: Dictionaries, Usage, and Automatic Classification</dc:title>
 <dc:creator>Losee, Robert M.</dc:creator>
 <dc:creator>Haas, Stephanie W.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The use of terms from natural and social scientific titles and abstracts is
studied from the perspective of sublanguages and their specialized
dictionaries. Different notions of sublanguage distinctiveness are explored.
Objective methods for separating hard and soft sciences are suggested based on
measures of sublanguage use, dictionary characteristics, and sublanguage
distinctiveness. Abstracts were automatically classified with a high degree of
accuracy by using a formula that considers the degree of uniqueness of terms in
each sublanguage. This may prove useful for text filtering or information
retrieval systems.
</dc:description>
 <dc:description>Comment: LaTeX with bibliography file attached</dc:description>
 <dc:date>1994-11-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>CLARE: A Contextual Reasoning and Cooperative Response Framework for the
  Core Language Engine</dc:title>
 <dc:creator>Alshawi, Hiyan</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Crouch, Richard</dc:creator>
 <dc:creator>Pulman, Steve</dc:creator>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Smith, Arnold</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This report describes the research, design and implementation work carried
out in building the CLARE system at SRI International, Cambridge, England.
CLARE was designed as a natural language processing system with facilities for
reasoning and understanding in context and for generating cooperative
responses. The project involved both further development of SRI's Core Language
Engine (Alshawi, 1992, MIT Press) natural language processor and the design and
implementation of new components for reasoning and response generation. The
CLARE system has advanced the state of the art in a wide variety of areas, both
through the use of novel techniques developed on the project, and by extending
the coverage or scale of known techniques. The language components are
application-independent and provide interfaces for the development of new types
of application.
</dc:description>
 <dc:description>Comment: 250 pages, uuencoded compressed tar-ed LaTeX. Written 1992</dc:description>
 <dc:date>1994-11-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adnominal adjectives, code-switching and lexicalized TAG</dc:title>
 <dc:creator>Mahootian, Shahrzad</dc:creator>
 <dc:creator>Santorini, Beatrice</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In codeswitching contexts, the language of a syntactic head determines the
distribution of its complements. Mahootian 1993 derives this generalization by
representing heads as the anchors of elementary trees in a lexicalized TAG.
However, not all codeswitching sequences are amenable to a head-complement
analysis. For instance, adnominal adjectives can occupy positions not available
to them in their own language, and the TAG derivation of such sequences must
use unanchored auxiliary trees. palabras heavy-duty `heavy-duty words'
(Spanish-English; Poplack 1980:584) taste lousy sana `very lousy taste'
(English-Swahili; Myers-Scotton 1993:29, (10)) Given the null hypothesis that
codeswitching and monolingual sequences are derived in an identical manner,
sequences like those above provide evidence that pure lexicalized TAGs are
inadequate for the description of natural language.
</dc:description>
 <dc:description>Comment: e-mails - usmahoot@uxa.ecn.bgu.edu, b-santorini@nwu.edu</dc:description>
 <dc:date>1994-11-02</dc:date>
 <dc:date>1994-11-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Determining Determiner Sequencing: A Syntactic Analysis for English</dc:title>
 <dc:creator>Hockey, Beth Ann</dc:creator>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Previous work on English determiners has primarily concentrated on their
semantics or scoping properties rather than their complex ordering behavior.
The little work that has been done on determiner ordering generally splits
determiners into three subcategories. However, this small number of categories
does not capture the finer distinctions necessary to correctly order
determiners. This paper presents a syntactic account of determiner sequencing
based on eight independently identified semantic features. Complex determiners,
such as genitives, partitives, and determiner modifying adverbials, are also
presented. This work has been implemented as part of XTAG, a wide-coverage
grammar for English based in the Feature-Based, Lexicalized Tree Adjoining
Grammar (FB-LTAG) formalism.
</dc:description>
 <dc:description>Comment: ps file. 4 pages. Proceedings of TAG+3, 1994</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constraining Lexical Selection Across Languages Using TAGs</dc:title>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Palmer, Martha</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Lexical selection in Machine Translation consists of several related
components. Two that have received a lot of attention are lexical mapping from
an underlying concept or lexical item, and choosing the correct
subcategorization frame based on argument structure. Because most MT
applications are small or relatively domain specific, a third component of
lexical selection is generally overlooked - distinguishing between lexical
items that are closely related conceptually. While some MT systems have
proposed using a 'world knowledge' module to decide which word is more
appropriate based on various pragmatic or stylistic constraints, we are
interested in seeing how much we can accomplish using a combination of syntax
and lexical semantics. By using separate ontologies for each language
implemented in FB-LTAGs, we are able to elegantly model the more specific and
language dependent syntactic and semantic distinctions necessary to further
filter the choice of the lexical item.
</dc:description>
 <dc:description>Comment: ps file. 4 pages, Proceedings of TAG+3, 1994</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Status of the XTAG System</dc:title>
 <dc:creator>Doran, Christy</dc:creator>
 <dc:creator>Egedi, Dania</dc:creator>
 <dc:creator>Hockey, Beth Ann</dc:creator>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  XTAG is an ongoing project to develop a wide-coverage grammar for English,
based on the Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG)
formalism. The XTAG system integrates a morphological analyzer, an N-best
part-of-speech tagger, an Early-style parser and an X-window interface, along
with a wide-coverage grammar for English developed using the system. This
system serves as a linguist's workbench for developing FB-LTAG specifications.
This paper presents a description of and recent improvements to the various
components of the XTAG system. It also presents the recent performance of the
wide-coverage grammar on various corpora and compares it against the
performance of other wide-coverage and domain-specific grammars.
</dc:description>
 <dc:description>Comment: uuencoded compressed ps file. 4 pages</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411006</dc:identifier>
 <dc:identifier>Proceedings of TAG+3, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Linguistic Relevance of Quasi-Trees</dc:title>
 <dc:creator>Kroch, Anthony</dc:creator>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We discuss two constructions (long scrambling and ECM verbs) which challenge
most syntactic theories (including traditional TAG approaches) since they seem
to require exceptional mechanisms and postulates. We argue that these
constructions should in fact be analyzed in a similar manner, namely as
involving a verb which selects for a ``defective'' complement. These
complements are defective in that they lack certain Case-assigning abilities
(represented as functional heads). The constructions differ in how many such
abilities are lacking. Following the previous analysis of scrambling of Rambow
(1994), we propose a TAG analysis based on quasi-trees.
</dc:description>
 <dc:description>Comment: 4 pages, uuencoded compressed ps file</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411007</dc:identifier>
 <dc:identifier>In {\em 3e Colloque International sur les Grammaires d'Arbres
  Adjoints (TAG+3)}</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing Free Word-Order Languages in Polynomial Time</dc:title>
 <dc:creator>Becker, Tilman</dc:creator>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a parsing algorithm with polynomial time complexity for a large
subset of V-TAG languages. V-TAG, a variant of multi-component TAG, can handle
free-word order phenomena which are beyond the class LCFRS (which includes
regular TAG). Our algorithm is based on a CYK-style parser for TAGs.
</dc:description>
 <dc:description>Comment: 4 pages, uuencoded compressed ps file</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411008</dc:identifier>
 <dc:identifier>In {\em 3e Colloque International sur les Grammaires d'Arbres
  Adjoints (TAG+3)}</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bootstrapping A Wide-Coverage CCG from FB-LTAG</dc:title>
 <dc:creator>Doran, Christine</dc:creator>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A number of researchers have noted the similarities between LTAGs and CCGs.
Observing this resemblance, we felt that we could make use of the wide-coverage
grammar developed in the XTAG project to build a wide-coverage CCG. To our
knowledge there have been no attempts to construct a large-scale CCG parser
with the lexicon to support it. In this paper, we describe such a system, built
by adapting various XTAG components to CCG. We find that, despite the
similarities between the formalisms, certain parts of the grammatical workload
are distributed differently. In addition, the flexibility of CCG derivations
allows the translated grammar to handle a number of ``non-constituent''
constructions which the XTAG grammar cannot.
</dc:description>
 <dc:description>Comment: ps file. 4 pages, Proceedings of TAG+3, 1994</dc:description>
 <dc:date>1994-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The &quot;Whiteboard&quot; Architecture: a way to integrate heterogeneous
  components of NLP systems</dc:title>
 <dc:creator>Boitet, Christian</dc:creator>
 <dc:creator>Seligman, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a new software architecture for NLP systems made of heterogeneous
components, and demonstrate an architectural prototype we have built at ATR in
the context of Speech Translation.
</dc:description>
 <dc:description>Comment: Postscript, 6 pages</dc:description>
 <dc:date>1994-11-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411010</dc:identifier>
 <dc:identifier>COLING-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Acquiring Knowledge from Encyclopedic Texts</dc:title>
 <dc:creator>Gomez, Fernando</dc:creator>
 <dc:creator>Hull, Richard</dc:creator>
 <dc:creator>Segami, Carlos</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A computational model for the acquisition of knowledge from encyclopedic
texts is described. The model has been implemented in a program, called SNOWY,
that reads unedited texts from {\em The World Book Encyclopedia}, and acquires
new concepts and conceptual relations about topics dealing with the dietary
habits of animals, their classifications and habitats. The program is also able
to answer an ample set of questions about the knowledge that it has acquired.
This paper describes the essential components of this model, namely semantic
interpretation, inferences and representation, and ends with an evaluation of
the performance of the program, a sample of the questions that it is able to
answer, and its relation to other programs of similar nature.
</dc:description>
 <dc:description>Comment: 7 pages, 7 Postscript figures, uses aclap.sty</dc:description>
 <dc:date>1994-11-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411011</dc:identifier>
 <dc:identifier>Proceedings of the Fourth ACL Conference on Applied Natural
  Language Processing, Stuttgart, Germany, October 13-15, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>From Regular to Context Free to Mildly Context Sensitive Tree Rewriting
  Systems: The Path of Child Language Acquisition</dc:title>
 <dc:creator>Frank, Robert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Current syntactic theory limits the range of grammatical variation so
severely that the logical problem of grammar learning is trivial. Yet, children
exhibit characteristic stages in syntactic development at least through their
sixth year. Rather than positing maturational delays, I suggest that
acquisition difficulties are the result of limitations in manipulating
grammatical representations. I argue that the genesis of complex sentences
reflects increasing generative capacity in the systems generating structural
descriptions: conjoined clauses demand only a regular tree rewriting system;
sentential embedding uses a context-free tree substitution grammar;
modification requires TAG, a mildly context-sensitive system.
</dc:description>
 <dc:description>Comment: 4 pages</dc:description>
 <dc:date>1994-11-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411012</dc:identifier>
 <dc:identifier>Appeared in {\em 3e Colloque International sur les grammaires
  d'Arbres Adjoints (TAG+3).}</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Phoneme-level speech and natural language intergration for agglutinative
  languages</dc:title>
 <dc:creator>Kim, Geunbae Lee Jong-Hyeok Lee Kyunghee</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A new tightly coupled speech and natural language integration model is
presented for a TDNN-based large vocabulary continuous speech recognition
system. Unlike the popular n-best techniques developed for integrating mainly
HMM-based speech and natural language systems in word level, which is obviously
inadequate for the morphologically complex agglutinative languages, our model
constructs a spoken language system based on the phoneme-level integration. The
TDNN-CYK spoken language architecture is designed and implemented using the
TDNN-based diphone recognition module integrated with the table-driven
phonological/morphological co-analysis. Our integration model provides a
seamless integration of speech and natural language for connectionist speech
recognition systems especially for morphologically complex languages such as
Korean. Our experiment results show that the speaker-dependent continuous
Eojeol (word) recognition can be integrated with the morphological analysis
with over 80\% morphological analysis success rate directly from the speech
input for the middle-level vocabularies.
</dc:description>
 <dc:description>Comment: 12 pages, Latex Postscript, compressed, uuencoded, will be presented
  in TWLT-8</dc:description>
 <dc:date>1994-11-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatically Identifying Morphological Relations in = Machine-Readable
  Dictionaries</dc:title>
 <dc:creator>Pentheroudakis, Joseph</dc:creator>
 <dc:creator>Vanderwende, Lucy</dc:creator>
 <dc:creator>Corporation, Microsoft</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an automated method for identifying classes of morphologically
related words in an on-line dictionary, and for linking individual senses in
the derived form to one or more senses in the base form by means of
morphological relation attributes. We also present an algorithm for computing a
score reflecting the system=92s certainty in these derivational links; this
computation relies on the content of semantic relations associated with each
sense, which are extracted automatically by parsing each sense definition and
subjecting the parse structure to automated semantic analysis. By processing
the entire set of headwords in the dictionary in this fashion we create a large
set of directed derivational graphs, which can then be accessed by other
components in our broad-coverage NLP system. Spurious or unlikely derivations
are not discarded, but are rather added to the dictionary and assigned a
negative score; this allows the system to handle non-standard uses of these
forms.
</dc:description>
 <dc:description>Comment: PostScript, 19 pages, 250kb; from Proceedings of the 9th Annual
  Conference of the UW Centre for the New OED and Text Research, 1993</dc:description>
 <dc:date>1994-11-07</dc:date>
 <dc:date>1994-11-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing Using Linearly Ordered Phonological Rules</dc:title>
 <dc:creator>Maxwell, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A generate and test algorithm is described which parses a surface form into
one or more lexical entries using linearly ordered phonological rules. This
algorithm avoids the exponential expansion of search space which a naive
parsing algorithm would face by encoding into the form being parsed the
ambiguities which arise during parsing. The algorithm has been implemented and
tested on real language data, and its speed compares favorably with that of a
KIMMO-type parser.
</dc:description>
 <dc:description>Comment: 105kb, 12 pages, published in Computational Phonology: First Meeting
  of the ACL Special Interest Group in Computational Phonology, 1994, pages
  59-70</dc:description>
 <dc:date>1994-11-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Extending DRT with a Focusing Mechanism for Pronominal Anaphora and
  Ellipsis Resolution</dc:title>
 <dc:creator>Abracos, Jose</dc:creator>
 <dc:creator>Lopes, Jose Gabriel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Cormack (1992) proposed a framework for pronominal anaphora resolution. Her
proposal integrates focusing theory (Sidner et al.) and DRT (Kamp and Reyle).
We analyzed this methodology and adjusted it to the processing of Portuguese
texts. The scope of the framework was widened to cover sentences containing
restrictive relative clauses and subject ellipsis. Tests were conceived and
applied to probe the adequacy of proposed modifications when dealing with
processing of current texts.
</dc:description>
 <dc:date>1994-11-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411017</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Comlex Syntax: Building a Computational Lexicon</dc:title>
 <dc:creator>Grishman, Ralph</dc:creator>
 <dc:creator>Macleod, Catherine</dc:creator>
 <dc:creator>Meyers, Adam</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe the design of Comlex Syntax, a computational lexicon providing
detailed syntactic information for approximately 38,000 English headwords. We
consider the types of errors which arise in creating such a lexicon, and how
such errors can be measured and controlled.
</dc:description>
 <dc:date>1994-11-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411018</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Interlanguage Signs and Lexical Transfer Errors</dc:title>
 <dc:creator>Ro, Atle</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A theory of interlanguage (IL) lexicons is outlined, with emphasis on IL
lexical entries, based on the HPSG notion of lexical sign. This theory accounts
for idiosyncratic or lexical transfer of syntactic subcategorisation and idioms
from the first language to the IL. It also accounts for developmental stages in
IL lexical grammar, and grammatical variation in the use of the same lexical
item. The theory offers a tool for robust parsing of lexical transfer errors
and diagnosis of such errors.
</dc:description>
 <dc:description>Comment: Paper presented at COLING-94. 4 pages, Postscript</dc:description>
 <dc:date>1994-11-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Focus on ``only&quot; and ``Not&quot;</dc:title>
 <dc:creator>Ramsay, Allan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Krifka [1993] has suggested that focus should be seen as a means of providing
material for a range of semantic and pragmatic functions to work on, rather
than as a specific semantic or pragmatic function itself. The current paper
describes an implementation of this general idea, and applies it to the
interpretation of {\em only} and {\em not}.
</dc:description>
 <dc:description>Comment: LaTeX bug removed from previous version</dc:description>
 <dc:date>1994-11-11</dc:date>
 <dc:date>1994-11-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411019</dc:identifier>
 <dc:identifier>COLING-94, 881-885</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Extraction in Dutch with Lexical Rules</dc:title>
 <dc:creator>Rentier, Gerrit</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Unbounded dependencies are often modelled by ``traces'' (and ``gap
threading'') in unification-based grammars. Pollard and Sag, however, suggest
an analysis of extraction based on lexical rules, which excludes the notion of
traces (P&amp;S 1994, Chapter 9). In parsing, it suggests a trade of indeterminism
for lexical ambiguity. This paper provides a short introduction to this
approach to extraction with lexical rules, and illustrates the linguistic power
of the approach by applying it to particularly idiosyncratic Dutch extraction
data.
</dc:description>
 <dc:description>Comment: Extension of KONVENS94 publication, 10 pages, PostScript</dc:description>
 <dc:date>1994-11-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Free-ordered CUG on Chemical Abstract Machine</dc:title>
 <dc:creator>Tojo, Satoshi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a paradigm for concurrent natural language generation. In order to
represent grammar rules distributively, we adopt categorial unification grammar
(CUG) where each category owns its functional type. We augment typed lambda
calculus with several new combinators, to make the order of lambda-conversions
free for partial / local processing. The concurrent calculus is modeled with
Chemical Abstract Machine. We show an example of a Japanese causative auxiliary
verb that requires a drastic rearrangement of case domination.
</dc:description>
 <dc:description>Comment: COLING 94, five pages, one PS file of figure `t.eps'</dc:description>
 <dc:date>1994-11-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adaptive Sentence Boundary Disambiguation</dc:title>
 <dc:creator>Palmer, David D.</dc:creator>
 <dc:creator>Hearst, Marti A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Labeling of sentence boundaries is a necessary prerequisite for many natural
language processing tasks, including part-of-speech tagging and sentence
alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate
them most systems use brittle, special-purpose regular expression grammars and
exception rules. As an alternative, we have developed an efficient, trainable
algorithm that uses a lexicon with part-of-speech probabilities and a
feed-forward neural network. After training for less than one minute, the
method correctly labels over 98.5\% of sentence boundaries in a corpus of over
27,000 sentence-boundary marks. We show the method to be efficient and easily
adaptable to different text genres, including single-case texts.
</dc:description>
 <dc:description>Comment: This is a Latex version of the previously submitted ps file
  (formatted as a uuencoded gz-compressed .tar file created by csh script). The
  software from the work described in this paper is available by contacting
  dpalmer@cs.berkeley.edu</dc:description>
 <dc:date>1994-11-16</dc:date>
 <dc:date>1994-11-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411022</dc:identifier>
 <dc:identifier>Proceedings of ANLP 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Abstract Generation based on Rhetorical Structure Extraction</dc:title>
 <dc:creator>Ono, Kenji</dc:creator>
 <dc:creator>Sumita, Kazuo</dc:creator>
 <dc:creator>Research, Seiji Miike</dc:creator>
 <dc:creator>Center, Development</dc:creator>
 <dc:creator>1, Toshiba Corporation Komukai-Toshiba-cho</dc:creator>
 <dc:creator>Saiwai-ku</dc:creator>
 <dc:creator>Kawasaki</dc:creator>
 <dc:creator>210</dc:creator>
 <dc:creator>Japan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We have developed an automatic abstract generation system for Japanese
expository writings based on rhetorical structure extraction. The system first
extracts the rhetorical structure, the compound of the rhetorical relations
between sentences, and then cuts out less important parts in the extracted
structure to generate an abstract of the desired length.
  Evaluation of the generated abstract showed that it contains at maximum 74\%
of the most important sentences of the original text. The system is now
utilized as a text browser for a prototypical interactive document retrieval
system.
</dc:description>
 <dc:description>Comment: 5 pages including 2 eps Figure, using epsbox.sty, art10.sty</dc:description>
 <dc:date>1994-11-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reverse Queries in DATR</dc:title>
 <dc:creator>Langer, Hagen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  DATR is a declarative representation language for lexical information and as
such, in principle, neutral with respect to particular processing strategies.
Previous DATR compiler/interpreter systems support only one access strategy
that closely resembles the set of inference rules of the procedural semantics
of DATR (Evans &amp; Gazdar 1989a). In this paper we present an alternative access
strategy (reverse query strategy) for a non-trivial subset of DATR.
</dc:description>
 <dc:description>Comment: PostScript, 7 pages</dc:description>
 <dc:date>1994-11-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multi-Dimensional Inheritance</dc:title>
 <dc:creator>Erbach, Gregor</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we present an alternative approach to multiple inheritance for
typed feature structures. In our approach, a feature structure can be
associated with several types coming from different hierarchies (dimensions).
In case of multiple inheritance, a type has supertypes from different
hierarchies. We contrast this approach with approaches based on a single type
hierarchy where a feature structure has only one unique most general type, and
multiple inheritance involves computation of greatest lower bounds in the
hierarchy. The proposed approach supports current linguistic analyses in
constraint-based formalisms like HPSG, inheritance in the lexicon, and
knowledge representation for NLP systems. Finally, we show that
multi-dimensional inheritance hierarchies can be compiled into a Prolog term
representation, which allows to compute the conjunction of two types
efficiently by Prolog term unification.
</dc:description>
 <dc:description>Comment: 9 pages, styles: a4,figfont,eepic,epsf</dc:description>
 <dc:date>1994-11-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411025</dc:identifier>
 <dc:identifier>Proceedings of KONVENS 94 (ed. H. Trost), Vienna, pp. 102-111</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Manipulating Human-oriented Dictionaries with very simple tools</dc:title>
 <dc:creator>Gaschler, Jean</dc:creator>
 <dc:creator>Lafourcade, Mathieu</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a methodology for building and manipulating
human-oriented dictionaries. This methodology has been applied in the
construction of a French-English-Malay dictionary which has been obtained by
&quot;crossing&quot; semi-automatically two bilingual dictionaries. We use only Microsoft
Word, a specialized language for writing transcriptors and a small but powerful
dictionary tool.
</dc:description>
 <dc:description>Comment: Postscript, 4 pages</dc:description>
 <dc:date>1994-11-18</dc:date>
 <dc:date>1994-11-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Classifier Assignment by Corpus-based Approach</dc:title>
 <dc:creator>Sornlertlamvanich, Virach</dc:creator>
 <dc:creator>Pantachat, Wantanee</dc:creator>
 <dc:creator>Meknavin, Surapant</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an algorithm for selecting an appropriate classifier word
for a noun. In Thai language, it frequently happens that there is fluctuation
in the choice of classifier for a given concrete noun, both from the point of
view of the whole spe ech community and individual speakers. Basically, there
is no exect rule for classifier selection. As far as we can do in the
rule-based approach is to give a default rule to pick up a corresponding
classifier of each noun. Registration of classifier for each noun is limited to
the type of unit classifier because other types are open due to the meaning of
representation. We propose a corpus-based method (Biber, 1993; Nagao, 1993;
Smadja, 1993) which generates Noun Classifier Associations (NCA) to overcome
the problems in classifier assignment and semantic construction of noun phrase.
The NCA is created statistically from a large corpus and recomposed under
concept hierarchy constraints and frequency of occurrences.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded gzip compressed Postscript file; COLING-94, Vol.1,
  pp.556-561</dc:description>
 <dc:date>1994-11-20</dc:date>
 <dc:date>1995-10-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Speech-Language Interface in the Spoken Language Translator</dc:title>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The Spoken Language Translator is a prototype for practically useful systems
capable of translating continuous spoken language within restricted domains.
The prototype system translates air travel (ATIS) queries from spoken English
to spoken Swedish and to French. It is constructed, with as few modifications
as possible, from existing pieces of speech and language processing software.
The speech recognizer and language understander are connected by a fairly
conventional pipelined N-best interface. This paper focuses on the ways in
which the language processor makes intelligent use of the sentence hypotheses
delivered by the recognizer. These ways include (1) producing modified
hypotheses to reflect the possible presence of repairs in the uttered word
sequence; (2) fast parsing with a version of the grammar automatically
specialized to the more frequent constructions in the training corpus; and (3)
allowing syntactic and semantic factors to interact with acoustic ones in the
choice of a meaning structure for translation, so that the acoustically
preferred hypothesis is not always selected even if it is within linguistic
coverage.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX. Published: Proceedings of TWLT-8, December 1994</dc:description>
 <dc:date>1994-11-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Probabilistic Context-Free Parsing Algorithm that Computes
  Prefix Probabilities</dc:title>
 <dc:creator>Stolcke, Andreas</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an extension of Earley's parser for stochastic context-free
grammars that computes the following quantities given a stochastic context-free
grammar and an input string: a) probabilities of successive prefixes being
generated by the grammar; b) probabilities of substrings being generated by the
nonterminals, including the entire string being generated by the grammar; c)
most likely (Viterbi) parse of the string; d) posterior expected number of
applications of each grammar production, as required for reestimating rule
probabilities. (a) and (b) are computed incrementally in a single left-to-right
pass over the input. Our algorithm compares favorably to standard bottom-up
parsing methods for SCFGs in that it works efficiently on sparse grammars by
making use of Earley's top-down control structure. It can process any
context-free rule format without conversion to some normal form, and combines
computations for (a) through (d) in a single algorithm. Finally, the algorithm
has simple extensions for processing partially bracketed inputs, and for
finding partial parses and their likelihoods on ungrammatical inputs.
</dc:description>
 <dc:description>Comment: 45 pages. Slightly shortened version to appear in Computational
  Linguistics 21</dc:description>
 <dc:date>1994-11-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Complexity of Scrambling: A New Twist to the Competence - Performance
  Distinction</dc:title>
 <dc:creator>Joshi, Aravind K</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we discuss the following issue: How do we decide whether a
certain property of language is a competence property or a performance
property? Our claim is that the answer to this question is not given a-priori.
The answer depends on the formal devices (formal grammars and machines)
available to us for describing language. We discuss this issue in the context
of the complexity of processing of center embedding (of relative clauses in
English) and scrambling (in German, for example) from arbitrary depths of
embedding.
</dc:description>
 <dc:description>Comment: ps, 4 pages, Proceedings of TAG+3, 1994</dc:description>
 <dc:date>1994-11-29</dc:date>
 <dc:date>1994-11-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Generation of Technical Documentation</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:creator>Mellish, Chris</dc:creator>
 <dc:creator>Levine, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Natural-language generation (NLG) techniques can be used to automatically
produce technical documentation from a domain knowledge base and linguistic and
contextual models. We discuss this application of NLG technology from both a
technical and a usefulness (costs and benefits) perspective. This discussion is
based largely on our experiences with the IDAS documentation-generation
project, and the reactions various interested people from industry have had to
IDAS. We hope that this summary of our experiences with IDAS and the lessons we
have learned from it will be beneficial for other researchers who wish to build
technical-documentation generation systems.
</dc:description>
 <dc:description>Comment: uuencoded compressed tar file, with LaTeX source and ps figures. Will
  appear in APPLIED ARTIFICIAL INTELLIGENCE journal, volume 9 (1995)</dc:description>
 <dc:date>1994-11-29</dc:date>
 <dc:date>1994-11-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9411032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Has a Consensus NL Generation Architecture Appeared, and is it
  Psycholinguistically Plausible?</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I survey some recent applications-oriented NL generation systems, and claim
that despite very different theoretical backgrounds, these systems have a
remarkably similar architecture in terms of the modules they divide the
generation process into, the computations these modules perform, and the way
the modules interact with each other. I also compare this `consensus
architecture' among applied NLG systems with psycholinguistic knowledge about
how humans speak, and argue that at least some aspects of the consensus
architecture seem to be in agreement with what is known about human language
production, despite the fact that psycholinguistic plausibility was not in
general a goal of the developers of the surveyed systems.
</dc:description>
 <dc:description>Comment: uuencoded compressed tar file, containing LaTeX source and two style
  files. This paper appeared in the 1994 International NLG workshop</dc:description>
 <dc:date>1994-11-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9411032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dependency Grammar and the Parsing of Chinese Sentences</dc:title>
 <dc:creator>Lai, Bong Yeung Tom</dc:creator>
 <dc:creator>Huang, Changning</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Dependency Grammar has been used by linguists as the basis of the syntactic
components of their grammar formalisms. It has also been used in natural
language parsing. In China, attempts have been made to use this grammar
formalism to parse Chinese sentences using corpus-based techniques. This paper
reviews the properties of Dependency Grammar as embodied in four axioms for the
well-formedness conditions for dependency structures. It is shown that allowing
multiple governors as done by some followers of this formalism is unnecessary.
The practice of augmenting Dependency Grammar with functional labels is also
discussed in the light of building functional structures when the sentence is
parsed. This will also facilitate semantic interpretation.
</dc:description>
 <dc:description>Comment: PostScript. 8 pages. Contains bitmap figures and will therefore print
  slowly. Will appear in Proceedings of 1994 Joint Conference of 8th ACLIC and
  2nd PacFoCoL, Kyoto, Aug. 10-12, 1994</dc:description>
 <dc:date>1994-12-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>N-Gram Cluster Identification During Empirical Knowledge Representation
  Generation</dc:title>
 <dc:creator>Collier, Robin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an overview of current research concerning knowledge
extraction from technical texts. In particular, the use of empirical techniques
during the identification and generation of a semantic representation is
considered. A key step is the discovery of useful n-grams and correlations
between clusters of these n-grams.
</dc:description>
 <dc:date>1994-12-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Extended Clustering Algorithm for Statistical Language Models</dc:title>
 <dc:creator>Ueberla, Joerg P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Statistical language models frequently suffer from a lack of training data.
This problem can be alleviated by clustering, because it reduces the number of
free parameters that need to be trained. However, clustered models have the
following drawback: if there is ``enough'' data to train an unclustered model,
then the clustered variant may perform worse. On currently used language
modeling corpora, e.g. the Wall Street Journal corpus, how do the performances
of a clustered and an unclustered model compare? While trying to address this
question, we develop the following two ideas. First, to get a clustering
algorithm with potentially high performance, an existing algorithm is extended
to deal with higher order N-grams. Second, to make it possible to cluster large
amounts of training data more efficiently, a heuristic to speed up the
algorithm is presented. The resulting clustering algorithm can be used to
cluster trigrams on the Wall Street Journal corpus and the language models it
produces can compete with existing back-off models. Especially when there is
only little training data available, the clustered models clearly outperform
the back-off models.
</dc:description>
 <dc:description>Comment: 27 pages, latex, comments welcome</dc:description>
 <dc:date>1994-12-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Knowledge Representation for Lexical Semantics: Is Standard First Order
  Logic Enough?</dc:title>
 <dc:creator>Light, Marc</dc:creator>
 <dc:creator>Schubert, Lenhart</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Natural language understanding applications such as interactive planning and
face-to-face translation require extensive inferencing. Many of these
inferences are based on the meaning of particular open class words. Providing a
representation that can support such lexically-based inferences is a primary
concern of lexical semantics. The representation language of first order logic
has well-understood semantics and a multitude of inferencing systems have been
implemented for it. Thus it is a prime candidate to serve as a lexical
semantics representation. However, we argue that FOL, although a good starting
point, needs to be extended before it can efficiently and concisely support all
the lexically-based inferences needed.
</dc:description>
 <dc:description>Comment: Presented at the &quot;Future of the Dictionary&quot; workshop, Grenoble,
  France (October, 1994), 12 pages PostScript</dc:description>
 <dc:date>1994-12-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Segmenting speech without a lexicon: The roles of phonotactics and
  speech source</dc:title>
 <dc:creator>Cartwright, Timothy Andrew</dc:creator>
 <dc:creator>Brent, Michael R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Infants face the difficult problem of segmenting continuous speech into words
without the benefit of a fully developed lexicon. Several sources of
information in speech might help infants solve this problem, including prosody,
semantic correlations and phonotactics. Research to date has focused on
determining to which of these sources infants might be sensitive, but little
work has been done to determine the potential usefulness of each source. The
computer simulations reported here are a first attempt to measure the
usefulness of distributional and phonotactic information in segmenting phoneme
sequences. The algorithms hypothesize different segmentations of the input into
words and select the best hypothesis according to the Minimum Description
Length principle. Our results indicate that while there is some useful
information in both phoneme distributions and phonotactic rules, the
combination of both sources is most useful.
</dc:description>
 <dc:description>Comment: Uses wsuipa font package and latex-acl.sty</dc:description>
 <dc:date>1994-12-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Robust stochastic parsing using the inside-outside algorithm</dc:title>
 <dc:creator>Briscoe</dc:creator>
 <dc:creator>Ted</dc:creator>
 <dc:creator>Waegner</dc:creator>
 <dc:creator>Nick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper describes a parser of sequences of (English) part-of-speech labels
which utilises a probabilistic grammar trained using the inside-outside
algorithm. The initial (meta)grammar is defined by a linguist and further rules
compatible with metagrammatical constraints are automatically generated. During
training, rules with very low probability are rejected yielding a wide-coverage
parser capable of ranking alternative analyses. A series of corpus-based
experiments describe the parser's performance.
</dc:description>
 <dc:description>Comment: Revised and updated version of paper from AAAI Workshop on
  Probabilistically-based Natural Language Processing Techniques, 1992, 16
  pages, uuencoded, compressed postscript</dc:description>
 <dc:date>1994-12-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Coupling Phonology and Phonetics in a Constraint-Based Gestural Model</dc:title>
 <dc:creator>Walther, Markus</dc:creator>
 <dc:creator>Kroeger, Bernd J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An implemented approach which couples a constraint-based phonology component
with an articulatory speech synthesizer is proposed. Articulatory gestures
ensure a tight connection between both components, as they comprise both
physical-phonetic and phonological aspects. The phonological modelling of e.g.
syllabification and phonological processes such as German final devoicing is
expressed in the constraint logic programming language CUF. Extending CUF by
arithmetic constraints allows the simultaneous description of both phonology
and phonetics. Thus declarative lexicalist theories of grammar such as HPSG may
be enriched up to the level of detailed phonetic realisation. Initial acoustic
demonstrations show that our approach is in principle capable of synthesizing
full utterances in a linguistically motivated fashion.
</dc:description>
 <dc:description>Comment: English version of the German original: Walther, Markus and Kroeger,
  Bernd J. (1994): Phonologie-Phonetikkopplung in einem constraint- basierten
  gesturalen Modell. In: Harald Trost (ed.), Proceedings KONVENS '94, Vienna.
  10 pages, gzip'ed, uuencoded postscript</dc:description>
 <dc:date>1994-12-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9412008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Analysis of Japanese Compound Nouns using Collocational Information</dc:title>
 <dc:creator>Yosiyuki, Kobayasi</dc:creator>
 <dc:creator>Takenobu, Takunaga</dc:creator>
 <dc:creator>Hozumi, Tanaka</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Analyzing compound nouns is one of the crucial issues for natural language
processing systems, in particular for those systems that aim at a wide coverage
of domains. In this paper, we propose a method to analyze structures of
Japanese compound nouns by using both word collocations statistics and a
thesaurus. An experiment is conducted with 160,000 word collocations to analyze
compound nouns of with an average length of 4.9 characters. The accuracy of
this method is about 80%.
</dc:description>
 <dc:description>Comment: COLING'94 papar, Technical Paper at Tokyo Institute of Technology,
  6pages and LaTeX</dc:description>
 <dc:date>1994-12-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9412008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501001</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using default inheritance to describe LTAG</dc:title>
 <dc:creator>Evans, Roger</dc:creator>
 <dc:creator>Gazdar, Gerald</dc:creator>
 <dc:creator>Weir, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present the results of an investigation into how the set of elementary
trees of a Lexicalized Tree Adjoining Grammar can be represented in the lexical
knowledge representation language DATR (Evans &amp; Gazdar 1989a,b). The LTAG under
consideration is based on the one described in Abeille et al. (1990). Our
approach is similar to that of Vijay-Shanker &amp; Schabes (1992) in that we
formulate an inheritance hierarchy that efficiently encodes the elementary
trees. However, rather than creating a new representation formalism for this
task, we employ techniques of established utility in other lexically-oriented
frameworks. In particular, we show how DATR's default mechanism can be used to
eliminate the need for a non-immediate dominance relation in the descriptions
of the surface LTAG entries. This allows us to embed the tree structures in the
feature theory in a manner reminiscent of HPSG subcategorisation frames, and
hence express lexical rules as relations over feature structures.
</dc:description>
 <dc:date>1995-01-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9501001</dc:identifier>
 <dc:identifier>Proceedings of TAG+3, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>NL Understanding with a Grammar of Constructions</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:creator>Szummer, Marcin</dc:creator>
 <dc:creator>Jarecki, Stanislaw</dc:creator>
 <dc:creator>Johnson, David E.</dc:creator>
 <dc:creator>Morgenstern, Leora</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an approach to natural language understanding based on a
computable grammar of constructions. A &quot;construction&quot; consists of a set of
features of form and a description of meaning in a context. A grammar is a set
of constructions. This kind of grammar is the key element of Mincal, an
implemented natural language, speech-enabled interface to an on-line calendar
system. The system consists of a NL grammar, a parser, an on-line calendar, a
domain knowledge base (about dates, times and meetings), an application
knowledge base (about the calendar), a speech recognizer, a speech generator,
and the interfaces between those modules. We claim that this architecture
should work in general for spoken interfaces in small domains. In this paper we
present two novel aspects of the architecture: (a) the use of constructions,
integrating descriptions of form, meaning and context into one whole; and (b)
the separation of domain knowledge from application knowledge. We describe the
data structures for encoding constructions, the structure of the knowledge
bases, and the interactions of the key modules of the system.
</dc:description>
 <dc:description>Comment: appeared in Proc. Coling'94, Kyoto, Japan, 1994; 5 postscript pages;
  email to wlodz@watson.ibm.com</dc:description>
 <dc:date>1995-01-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9501002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501003</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An HPSG Parser Based on Description Logics</dc:title>
 <dc:creator>Quantz, J. Joachim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I present a parser based on Description Logics (DL) for a
German HPSG -style fragment. The specified parser relies mainly on the
inferential capabilities of the underlying DL system. Given a preferential
default extension for DL disambiguation is achieved by choosing the parse
containing a qualitatively minimal number of exceptions.
</dc:description>
 <dc:date>1995-01-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9501003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501004</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lexical Knowledge Representation in an Intelligent Dictionary Help
  System</dc:title>
 <dc:creator>Agirre, E.</dc:creator>
 <dc:creator>Arregi, X.</dc:creator>
 <dc:creator>Artola, X.</dc:creator>
 <dc:creator>de Ilarraza, A. Diaz</dc:creator>
 <dc:creator>Sarasola, K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The frame-based knowledge representation model adopted in IDHS (Intelligent
Dictionary Help System) is described in this paper. It is used to represent the
lexical knowledge acquired automatically from a conventional dictionary.
Moreover, the enrichment processes that have been performed on the Dictionary
Knowledge Base and the dynamic exploitation of this knowledge - both based on
the exploitation of the properties of lexical semantic relations - are also
described.
</dc:description>
 <dc:description>Comment: 8 pages, postscript file, originally written in Mac Word</dc:description>
 <dc:date>1995-01-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9501004</dc:identifier>
 <dc:identifier>Proceedings of COLING 94, Vol. 1, 544-550.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9501005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Tool for Collecting Domain Dependent Sortal Constraints From Corpora</dc:title>
 <dc:creator>Andry, Francois</dc:creator>
 <dc:creator>Gawron, Mark</dc:creator>
 <dc:creator>Dowding, John</dc:creator>
 <dc:creator>Moore, Robert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe a tool designed to generate semi-automatically the
sortal constraints specific to a domain to be used in a natural language (NL)
understanding system. This tool is evaluated using the SRI Gemini NL
understanding system in the ATIS domain.
</dc:description>
 <dc:description>Comment: COLING 94's paper - Latex document - 6 pages</dc:description>
 <dc:date>1995-01-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9501005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Interlingual Lexical Organisation for Multilingual Lexical Databases in
  NADIA</dc:title>
 <dc:creator>Serasset, Gilles</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a lexical organisation for multilingual lexical databases (MLDB).
This organisation is based on acceptions (word-senses). We detail this lexical
organisation and show a mock-up built to experiment with it. We also present
our current work in defining and prototyping a specialised system for the
management of acception-based MLDB. Keywords: multilingual lexical database,
acception, linguistic structure.
</dc:description>
 <dc:description>Comment: 5 pages, Macintosh Postscript, published in COLING-94, pp. 278-282</dc:description>
 <dc:date>1995-02-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Unification-Based Natural Language Grammars</dc:title>
 <dc:creator>Osborne, Miles</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  When parsing unrestricted language, wide-covering grammars often
undergenerate. Undergeneration can be tackled either by sentence correction, or
by grammar correction. This thesis concentrates upon automatic grammar
correction (or machine learning of grammar) as a solution to the problem of
undergeneration. Broadly speaking, grammar correction approaches can be
classified as being either {\it data-driven}, or {\it model-based}. Data-driven
learners use data-intensive methods to acquire grammar. They typically use
grammar formalisms unsuited to the needs of practical text processing and
cannot guarantee that the resulting grammar is adequate for subsequent semantic
interpretation. That is, data-driven learners acquire grammars that generate
strings that humans would judge to be grammatically ill-formed (they {\it
overgenerate}) and fail to assign linguistically plausible parses. Model-based
learners are knowledge-intensive and are reliant for success upon the
completeness of a {\it model of grammaticality}. But in practice, the model
will be incomplete. Given that in this thesis we deal with undergeneration by
learning, we hypothesise that the combined use of data-driven and model-based
learning would allow data-driven learning to compensate for model-based
learning's incompleteness, whilst model-based learning would compensate for
data-driven learning's unsoundness. We describe a system that we have used to
test the hypothesis empirically. The system combines data-driven and
model-based learning to acquire unification-based grammars that are more
suitable for practical text parsing. Using the Spoken English Corpus as data,
and by quantitatively measuring undergeneration, overgeneration and parse
plausibility, we show that this hypothesis is correct.
</dc:description>
 <dc:description>Comment: DPhil thesis, self-unpacking latex file, 114 pages with 33 pages of
  appendices.</dc:description>
 <dc:date>1995-02-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502003</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>ProFIT: Prolog with Features, Inheritance and Templates</dc:title>
 <dc:creator>Erbach, Gregor</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  ProFIT is an extension of Standard Prolog with Features, Inheritance and
Templates. ProFIT allows the programmer or grammar developer to declare an
inheritance hierarchy, features and templates. Sorted feature terms can be used
in ProFIT programs together with Prolog terms to provide a clearer description
language for linguistic structures. ProFIT compiles all sorted feature terms
into a Prolog term representation, so that the built-in Prolog term unification
can be used for the unification of sorted feature structures, and no special
unification algorithm is needed. ProFIT programs are compiled into Prolog
programs, so that no meta-interpreter is needed for their execution. ProFIT
thus provides a direct step from grammars developed with sorted feature terms
to Prolog programs usable for practical NLP systems.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, eaclap.sty</dc:description>
 <dc:date>1995-02-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502004</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bottom-Up Earley Deduction</dc:title>
 <dc:creator>Erbach, Gregor</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a bottom-up variant of Earley deduction. Bottom-up deduction is
preferable to top-down deduction because it allows incremental processing (even
for head-driven grammars), it is data-driven, no subsumption check is needed,
and preference values attached to lexical items can be used to guide best-first
search. We discuss the scanning step for bottom-up Earley deduction and
indexing schemes that help avoid useless deduction steps.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX, eaclap.sty</dc:description>
 <dc:date>1995-02-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502004</dc:identifier>
 <dc:identifier>Proceedings of COLING 94, pages 796-802</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502005</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Off-line Optimization for Earley-style HPSG Processing</dc:title>
 <dc:creator>Minnen, Guido</dc:creator>
 <dc:creator>Gerdemann, Dale</dc:creator>
 <dc:creator>Goetz, Thilo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A novel approach to HPSG based natural language processing is described that
uses an off-line compiler to automatically prime a declarative grammar for
generation or parsing, and inputs the primed grammar to an advanced
Earley-style processor. This way we provide an elegant solution to the problems
with empty heads and efficient bidirectional processing which is illustrated
for the special case of HPSG generation. Extensive testing with a large HPSG
grammar revealed some important constraints on the form of the grammar.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX (avm.sty, eaclap.sty and tree-dvips)</dc:description>
 <dc:date>1995-02-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502005</dc:identifier>
 <dc:identifier>Proceedings EACL 95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502006</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Rapid Development of Morphological Descriptions for Full Language
  Processing Systems</dc:title>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I describe a compiler and development environment for feature-augmented
two-level morphology rules integrated into a full NLP system. The compiler is
optimized for a class of languages including many or most European ones, and
for rapid development and debugging of descriptions of new languages. The key
design decision is to compose morphophonological and morphosyntactic
information, but not the lexicon, when compiling the description. This results
in typical compilation times of about a minute, and has allowed a reasonably
full, feature-based description of French inflectional morphology to be
developed in about a month by a linguist new to the system.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX (2.09 preferred); eaclap.sty; Procs of Euro ACL-95</dc:description>
 <dc:date>1995-02-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502007</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Utilization of a Lexicon for Spelling Correction in Modern Greek</dc:title>
 <dc:creator>Vagelatos, A.</dc:creator>
 <dc:creator>Triantopoulou, T.</dc:creator>
 <dc:creator>Tsalidis, C.</dc:creator>
 <dc:creator>Christodoulakis, D.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present an interactive spelling correction system for Modern
Greek. The entire system is based on a morphological lexicon. Emphasis is given
to the development of the lexicon, especially as far as storage economy, speed
efficiency and dictionary coverage is concerned. Extensive research was
conducted from both the computer engineering and linguisting fields, in order
to describe inflectional morphology as economically as possible.
</dc:description>
 <dc:description>Comment: 5 pages, PS File, gzip compressed, uuencoded, to be presented at
  SAC95, ACM Computing Week, Nashville, USA.</dc:description>
 <dc:date>1995-02-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502008</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Robust and Efficient Three-Layered Dialogue Component for a
  Speech-to-Speech Translation System</dc:title>
 <dc:creator>Alexandersson, Jan</dc:creator>
 <dc:creator>Maier, Elisabeth</dc:creator>
 <dc:creator>Reithinger, Norbert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present the dialogue component of the speech-to-speech translation system
VERBMOBIL. In contrast to conventional dialogue systems it mediates the
dialogue while processing maximally 50% of the dialogue in depth. Special
requirements like robustness and efficiency lead to a 3-layered hybrid
architecture for the dialogue module, using statistics, an automaton and a
planner. A dialogue memory is constructed incrementally.
</dc:description>
 <dc:description>Comment: Postscript file, compressed and uuencoded, 15 pages, to appear in
  Proceedings of EACL-95, Dublin.</dc:description>
 <dc:date>1995-02-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502009</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Learning More Appropriate Selectional Restrictions</dc:title>
 <dc:creator>Ribas, Francesc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present some variations affecting the association measure and thresholding
on a technique for learning Selectional Restrictions from on-line corpora. It
uses a wide-coverage noun taxonomy and a statistical measure to generalize the
appropriate semantic classes. Evaluation measures for the Selectional
Restrictions learning task are discussed. Finally, an experimental evaluation
of these variations is reported.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX (eaclap.sty)</dc:description>
 <dc:date>1995-02-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502009</dc:identifier>
 <dc:identifier>Proceedings EACL-95, Ireland</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502010</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>NPtool, a detector of English noun phrases</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  NPtool is a fast and accurate system for extracting noun phrases from English
texts for the purposes of e.g. information retrieval, translation unit
discovery, and corpus studies. After a general introduction, the system
architecture is presented in outline. Then follows an examination of a recently
written Constraint Syntax. An evaluation report concludes the paper.
</dc:description>
 <dc:description>Comment: uuencoded and gzipped .ps, 10 pages.</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502011</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Specifying a shallow grammatical representation for parsing purposes</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:creator>Jarvinen, Timo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Is it possible to specify a grammatical representation (descriptors and their
application guidelines) to such a degree that it can be consistently applied by
different grammarians e.g. for producing a benchmark corpus for parser
evaluation? Arguments for and against have been given, but very little
empirical evidence. In this article we report on a double-blind experiment with
a surface-oriented morphosyntactic grammatical representation used in a
large-scale English parser. We argue that a consistently applicable
representation for morphology and also shallow syntax can be specified. A
grammatical representation with a near-100% coverage of running text can be
specified with a reasonable effort, especially if the representation is based
on structural distinctions (i.e. it is structurally resolvable).
</dc:description>
 <dc:description>Comment: EACL95, uuencoded and gzipped .ps</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502012</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A syntax-based part-of-speech analyser</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  There are two main methodologies for constructing the knowledge base of a
natural language analyser: the linguistic and the data-driven. Recent
state-of-the-art part-of-speech taggers are based on the data-driven approach.
Because of the known feasibility of the linguistic rule-based approach at
related levels of description, the success of the data-driven approach in
part-of-speech analysis may appear surprising. In this paper, a case is made
for the syntactic nature of part-of-speech tagging. A new tagger of English
that uses only linguistic distributional rules is outlined and empirically
evaluated. Tested against a benchmark corpus of 38,000 words of previously
unseen text, this syntax-based system reaches an accuracy of above 99%.
Compared to the 95-97% accuracy of its best competitors, this result suggests
the feasibility of the linguistic approach also in part-of-speech analysis.
</dc:description>
 <dc:description>Comment: EACL95, uuencoded and gzipped .ps. (Bibliographic mistake corrected.)</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:date>1995-02-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ambiguity resolution in a reductionistic parser</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We are concerned with dependency-oriented morphosyntactic parsing of running
text. While a parsing grammar should avoid introducing structurally
unresolvable distinctions in order to optimise on the accuracy of the parser,
it also is beneficial for the grammarian to have as expressive a structural
representation available as possible. In a reductionistic parsing system this
policy may result in considerable ambiguity in the input; however, even massive
ambiguity can be tackled efficiently with an accurate parsing description and
effective parsing technology.
</dc:description>
 <dc:description>Comment: EACL93, .ps</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502014</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ellipsis and Quantification: a substitutional approach</dc:title>
 <dc:creator>Crouch, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper describes a substitutional approach to ellipsis resolution giving
comparable results to Dalrymple, Shieber and Pereira (1991), but without the
need for order-sensitive interleaving of quantifier scoping and ellipsis
resolution. It is argued that the order-independence results from viewing
semantic interpretation as building a description of a semantic composition,
instead of the more common view of interpretation as actually performing the
composition
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX uses eaclap.sty; Procs of Euro ACL-95</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502015</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Semantics of Resource Sharing in Lexical-Functional Grammar</dc:title>
 <dc:creator>Kehler, Andrew</dc:creator>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Lamping, John</dc:creator>
 <dc:creator>Saraswat, Vijay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We argue that the resource sharing that is commonly manifest in semantic
accounts of coordination is instead appropriately handled in terms of
structure-sharing in LFG f-structures. We provide an extension to the previous
account of LFG semantics (Dalrymple et al., 1993b) according to which
dependencies between f-structures are viewed as resources; as a result a
one-to-one correspondence between uses of f-structures and meanings is
maintained. The resulting system is sufficiently restricted in cases where
other approaches overgenerate; the very property of resource-sensitivity for
which resource sharing appears to be problematic actually provides explanatory
advantages over systems that more freely replicate resources during derivation.
</dc:description>
 <dc:description>Comment: 8 pages, to appear in EACL-95. Requires eaclap.sty, tree-dvips.sty,
  tree-dvips.pro, lingmacros.sty, dgmacros.tex, lfgmacros.tex. Comments
  welcome.</dc:description>
 <dc:date>1995-02-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502015</dc:identifier>
 <dc:identifier>Proceedings of EACL-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502016</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Higher-order Linear Logic Programming of Categorial Deduction</dc:title>
 <dc:creator>Morrill, Glyn</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We show how categorial deduction can be implemented in higher-order (linear)
logic programming, thereby realising parsing as deduction for the associative
and non-associative Lambek calculi. This provides a method of solution to the
parsing problem of Lambek categorial grammar applicable to a variety of its
extensions.
</dc:description>
 <dc:description>Comment: 8 pages LaTeX, uses eaclap.sty, to appear EACL95</dc:description>
 <dc:date>1995-02-14</dc:date>
 <dc:date>1995-02-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502017</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deterministic Consistency Checking of LP Constraints</dc:title>
 <dc:creator>Manandhar, Suresh</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We provide a constraint based computational model of linear precedence as
employed in the HPSG grammar formalism. An extended feature logic which adds a
wide range of constraints involving precedence is described. A sound, complete
and terminating deterministic constraint solving procedure is given.
Deterministic computational model is achieved by weakening the logic such that
it is sufficient for linguistic applications involving word-order.
</dc:description>
 <dc:description>Comment: EACL'95, 8 pages, LaTeX, eepic.sty, epsf.sty, eaclap.sty, figures -
  tar-ed, gzip-ed, uuencode-d</dc:description>
 <dc:date>1995-02-14</dc:date>
 <dc:date>1995-02-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502018</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Algorithms for Analysing the Temporal Structure of Discourse</dc:title>
 <dc:creator>Hitzeman, Janet</dc:creator>
 <dc:creator>Moens, Marc</dc:creator>
 <dc:creator>Grover, Claire</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a method for analysing the temporal structure of a discourse
which takes into account the effects of tense, aspect, temporal adverbials and
rhetorical structure and which minimises unnecessary ambiguity in the temporal
structure. It is part of a discourse grammar implemented in Carpenter's ALE
formalism. The method for building up the temporal structure of the discourse
combines constraints and preferences: we use constraints to reduce the number
of possible structures, exploiting the HPSG type hierarchy and unification for
this purpose; and we apply preferences to choose between the remaining options
using a temporal centering mechanism. We end by recommending that an
underspecified representation of the structure using these techniques be used
to avoid generating the temporal/rhetorical structure until higher-level
information can be used to disambiguate.
</dc:description>
 <dc:description>Comment: EACL '95, 8 pages, 1 eps picture, tar-ed, compressed, uuencoded, uses
  eaclap.sty, a4wide.sty, epsf.tex</dc:description>
 <dc:date>1995-02-15</dc:date>
 <dc:date>1995-02-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating &quot;Free&quot; Word Order Syntax and Information Structure</dc:title>
 <dc:creator>Hoffman, Beryl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a combinatory categorial formalism called Multiset-CCG
that can capture the syntax and interpretation of ``free'' word order in
languages such as Turkish. The formalism compositionally derives the
predicate-argument structure and the information structure (e.g. topic, focus)
of a sentence in parallel, and uniformly handles word order variation among the
arguments and adjuncts within a clause, as well as in complex clauses and
across clause boundaries.
</dc:description>
 <dc:description>Comment: 8 pages PostScript in EACL 95</dc:description>
 <dc:date>1995-02-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502020</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Formalization and Parsing of Typed Unification-Based ID/LP Grammars</dc:title>
 <dc:creator>Morawietz, Frank</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper defines unification based ID/LP grammars based on typed feature
structures as nonterminals and proposes a variant of Earley's algorithm to
decide whether a given input sentence is a member of the language generated by
a particular typed unification ID/LP grammar. A solution to the problem of the
nonlocal flow of information in unification ID/LP grammars as discussed in
Seiffert (1991) is incorporated into the algorithm. At the same time, it tries
to connect this technical work with linguistics by presenting an example of the
problem resulting from HPSG approaches to linguistics (Hinrichs and Nakasawa
1994, Richter and Sailer 1995) and with computational linguistics by drawing
connections from this approach to systems implementing HPSG, especially the
TROLL system, Gerdemann et al. (forthcoming).
</dc:description>
 <dc:description>Comment: paper (81 pages), appendix (17 pages, Prolog code), format: .ps
  compressed and uuencoded</dc:description>
 <dc:date>1995-02-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Tractable Extension of Linear Indexed Grammars</dc:title>
 <dc:creator>Keller, Bill</dc:creator>
 <dc:creator>Weir, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  It has been shown that Linear Indexed Grammars can be processed in polynomial
time by exploiting constraints which make possible the extensive use of
structure-sharing. This paper describes a formalism that is more powerful than
Linear Indexed Grammar, but which can also be processed in polynomial time
using similar techniques. The formalism, which we refer to as Partially Linear
PATR manipulates feature structures rather than stacks.
</dc:description>
 <dc:description>Comment: 8 pages LaTeX, uses eaclap.sty, to appear in EACL-95</dc:description>
 <dc:date>1995-02-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502022</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stochastic HPSG</dc:title>
 <dc:creator>Brew, Chris</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we provide a probabilistic interpretation for typed feature
structures very similar to those used by Pollard and Sag. We begin with a
version of the interpretation which lacks a treatment of re-entrant feature
structures, then provide an extended interpretation which allows them. We
sketch algorithms allowing the numerical parameters of our probabilistic
interpretations of HPSG to be estimated from corpora.
</dc:description>
 <dc:description>Comment: 7 pages, eaclap, Proceedings of EACL-95</dc:description>
 <dc:date>1995-02-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502023</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Splitting the Reference Time: Temporal Anaphora and Quantification in
  DRT</dc:title>
 <dc:creator>Nelken, Rani</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:creator>.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an analysis of temporal anaphora in sentences which
contain quantification over events, within the framework of Discourse
Representation Theory. The analysis in (Partee 1984) of quantified sentences,
introduced by a temporal connective, gives the wrong truth-conditions when the
temporal connective in the subordinate clause is &quot;before&quot; or &quot;after&quot;. This
problem has been previously analyzed in (de Swart 1991) as an instance of the
proportion problem, and given a solution from a Generalized Quantifier
approach. By using a careful distinction between the different notions of
reference time, based on (Kamp and Reyle 1993), we propose a solution to this
problem, within the framework of DRT. We show some applications of this
solution to additional temporal anaphora phenomena in quantified sentences.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, uses eaclap.sty, to appear in Proceedings of EACL-95</dc:description>
 <dc:date>1995-02-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502024</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Robust Parser Based on Syntactic Information</dc:title>
 <dc:creator>Lee, Kong Joo</dc:creator>
 <dc:creator>Kweon, Cheol Jung</dc:creator>
 <dc:creator>Seo, Jungyun</dc:creator>
 <dc:creator>Kim, Gil Chang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we propose a robust parser which can parse extragrammatical
sentences. This parser can recover them using only syntactic information. It
can be easily modified and extended because it utilize only syntactic
information.
</dc:description>
 <dc:description>Comment: 6 pages LaTeX, uses eaclap.sty, to appear in EACL-95.</dc:description>
 <dc:date>1995-02-20</dc:date>
 <dc:date>1995-02-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Principle Based Semantics for HPSG</dc:title>
 <dc:creator>Frank, Anette</dc:creator>
 <dc:creator>Reyle, Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper presents a constraint based semantic formalism for HPSG. The
syntax-semantics interface directly implements syntactic conditions on
quantifier scoping and distributivity. The construction of semantic
representations is guided by general principles governing the interaction
between syntax and semantics. Each of these principles acts as a constraint to
narrow down the set of possible interpretations of a sentence. Meanings of
ambiguous sentences are represented by single partial representations
(so-called U(nderspecified) D(iscourse) R(epresentation) S(tructure)s) to which
further constraints can be added monotonically to gain more information about
the content of a sentence. There is no need to build up a large number of
alternative representations of the sentence which are then filtered by
subsequent discourse and world knowledge. The advantage of UDRSs is not only
that they allow for monotonic incremental interpretation but also that they are
equipped with truth conditions and a proof theory that allows for inferences to
be drawn directly on structures where quantifier scope is not resolved.
</dc:description>
 <dc:description>Comment: EACL-95</dc:description>
 <dc:date>1995-02-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Reasoning with Ambiguities</dc:title>
 <dc:creator>Reyle, Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper adresses the problem of reasoning with ambiguities. Semantic
representations are presented that leave scope relations between quantifiers
and/or other operators unspecified. Truth conditions are provided for these
representations and different consequence relations are judged on the basis of
intuitive correctness. Finally inference patterns are presented that operate
directly on these underspecified structures, i.e. do not rely on any
translation into the set of their disambiguations.
</dc:description>
 <dc:description>Comment: EACL 1995</dc:description>
 <dc:date>1995-02-21</dc:date>
 <dc:date>1995-02-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502027</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards an Account of Extraposition in HPSG</dc:title>
 <dc:creator>Keller, Frank</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper investigates the syntax of extraposition in the HPSG framework. We
present English and German data (partly taken from corpora), and provide an
analysis using lexical rules and a nonlocal dependency. The condition for
binding this dependency is formulated relative to the antecedent of the
extraposed phrase, which entails that no fixed site for extraposition exists.
Our analysis accounts for the interaction of extraposition with fronting and
coordination, and predicts constraints on multiple extraposition.
</dc:description>
 <dc:description>Comment: 6 pages Postscript; use uudecode and gunzip to decode</dc:description>
 <dc:date>1995-02-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502027</dc:identifier>
 <dc:identifier>Proceedings of the EACL-95, Dublin</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502028</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Lexical Acquisition via Constraint Solving</dc:title>
 <dc:creator>Pedersen, Ted</dc:creator>
 <dc:creator>Chen, Weidong</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a method to automatically acquire the syntactic and
semantic classifications of unknown words. Our method reduces the search space
of the lexical acquisition problem by utilizing both the left and the right
context of the unknown word. Link Grammar provides a convenient framework in
which to implement our method.
</dc:description>
 <dc:description>Comment: 6 pages, AAAI 1995 Spring Symposium Series</dc:description>
 <dc:date>1995-02-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502029</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Topic Identification in Discourse</dc:title>
 <dc:creator>Chen, Kuang-hua</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes a corpus-based language model for topic identification.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus. The
word association norms are based on three factors: 1) word importance, 2) pair
co-occurrence, and 3) distance. They are trained on the paragraph and sentence
levels for noun-noun and noun-verb pairs, respectively. Under the topic
coherence postulation, the nouns that have the strongest connectivities with
the other nouns and verbs in the discourse form the preferred topic set. The
collocational semantics then is used to identify the topics from paragraphs and
to discuss the topic shift phenomenon among paragraphs.
</dc:description>
 <dc:description>Comment: 5 pages, uuencoded and compressed postscript file (EACL 95)</dc:description>
 <dc:date>1995-02-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502030</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bi-directional memory-based dialog translation: The KEMDT approach</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Jung, Hanmin</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A bi-directional Korean/English dialog translation system is designed and
implemented using the memory-based translation technique. The system KEMDT
(Korean/English Memory-based Dialog Translation system) can perform Korean to
English, and English to Korean translation using unified memory network and
extended marker passing algorithm. We resolve the word order variation and
frequent word omission problems in Korean by classifying the concept sequence
element in four different types and extending the marker-
passing-based-translation algorithm. Unlike the previous memory-based
translation systems, the KEMDT system develops the bilingual memory network and
the unified bi-directional marker passing translation algorithm. For efficient
language specific processing, we separate the morphological processors from the
memory-based translator. The KEMDT technology provides a hierarchical memory
network and an efficient marker-based control for the recent example-based MT
paradigm.
</dc:description>
 <dc:description>Comment: latex postscript with psfig, 7 pages, to be presented at pacific
  association for computational lingusitics conference (pacling95)</dc:description>
 <dc:date>1995-02-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502031</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Cooperative Error Handling and Shallow Processing</dc:title>
 <dc:creator>Bowden, Tanya</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper is concerned with the detection and correction of sub-sentential
English text errors. Previous spelling programs, unless restricted to a very
small set of words, have operated as post-processors. And to date, grammar
checkers and other programs which deal with ill-formed input usually step
directly from spelling considerations to a full-scale parse, assuming a
complete sentence. Work described below is aimed at evaluating the
effectiveness of shallow (sub-sentential) processing and the feasibility of
cooperative error checking, through building and testing appropriately an
error-processing system. A system under construction is outlined which
incorporates morphological checks (using new two-level error rules) over a
directed letter graph, tag positional trigrams and partial parsing. Intended
testing is discussed.
</dc:description>
 <dc:description>Comment: EACL 95 (student session)</dc:description>
 <dc:date>1995-02-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502032</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An NLP Approach to a Specific Type of Texts: Car Accident Reports</dc:title>
 <dc:creator>Estival, Dominique</dc:creator>
 <dc:creator>Gayral, Francoise</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The work reported here is the result of a study done within a larger project
on the ``Semantics of Natural Languages'' viewed from the field of Artificial
Intelligence and Computational Linguistics. In this project, we have chosen a
corpus of insurance claim reports. These texts deal with a relatively
circumscribed domain, that of road traffic, thereby limiting the
extra-linguistic knowledge necessary to understand them. Moreover, these texts
present a number of very specific characteristics, insofar as they are written
in a quasi-institutional setting which imposes many constraints on their
production. We first determine what these constraints are in order to then show
how they provide the writer with the means to create as succint a text as
possible, and in a symmetric way, how they provide the reader with the means to
interpret the text and to distinguish between its factual and argumentative
aspects.
</dc:description>
 <dc:description>Comment: 20 pages</dc:description>
 <dc:date>1995-02-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502033</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation
  Process</dc:title>
 <dc:creator>Azzam, Saliha</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper concerns both anaphora resolution and prepositional phrase (PP)
attachment that are the most frequent ambiguities in natural language
processing. Several methods have been proposed to deal with each phenomenon
separately, however none of proposed systems has considered the way of dealing
both phenomena. We tackle this issue, proposing an algorithm to co-ordinate the
treatment of these two problems efficiently, i.e., the aim is also to exploit
at each step all the results that each component can provide.
</dc:description>
 <dc:description>Comment: EACL 95 (student session)(re-revised, minor changes: correction of
  affiliation)</dc:description>
 <dc:date>1995-02-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502033</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Grouping Words Using Statistical Context</dc:title>
 <dc:creator>Huckle, Christopher C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper (cmp-lg/yymmnnn) has been accepted for publication in the student
session of EACL-95. It outlines ongoing work using statistical and unsupervised
neural network methods for clustering words in untagged corpora. Such
approaches are of interest when attempting to understand the development of
human intuitive categorization of language as well as for trying to improve
computational methods in natural language understanding. Some preliminary
results using a simple statistical approach are described, along with work
using an unsupervised neural network to distinguish between the sense classes
into which words fall.
</dc:description>
 <dc:description>Comment: This file should be converted using uudecode followed by uncompress.
  It is in PostScript format, and 3 pages in length</dc:description>
 <dc:date>1995-02-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502034</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incorporating &quot;Unconscious Reanalysis&quot; into an Incremental, Monotonic
  Parser</dc:title>
 <dc:creator>Sturt, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an implementation based on a recent model in the
psycholinguistic literature. We define a parsing operation which allows the
reanalysis of dependencies within an incremental and monotonic processing
architecture, and discuss search strategies for its application in a
head-initial language (English) and a head-final language (Japanese).
</dc:description>
 <dc:description>Comment: standard Latex209 6 pages</dc:description>
 <dc:date>1995-02-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502035</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502036</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Literal Movement Grammars</dc:title>
 <dc:creator>Groenink, Annius V.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Literal movement grammars (LMGs) provide a general account of extraposition
phenomena through an attribute mechanism allowing top-down displacement of
syntactical information. LMGs provide a simple and efficient treatment of
complex linguistic phenomena such as cross-serial dependencies in German and
Dutch---separating the treatment of natural language into a parsing phase
closely resembling traditional context-free treatment, and a disambiguation
phase which can be carried out using matching, as opposed to full unification
employed in most current grammar formalisms of linguistical relevance.
</dc:description>
 <dc:description>Comment: 8 pages, Postscript. To be presented at EACL95, March 1995, Dublin</dc:description>
 <dc:date>1995-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502036</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502037</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A State-Transition Grammar for Data-Oriented Parsing</dc:title>
 <dc:creator>Tugwell, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a grammar formalism designed for use in data-oriented
approaches to language processing. The formalism is best described as a
right-linear indexed grammar extended in linguistically interesting ways. The
paper goes on to investigate how a corpus pre-parsed with this formalism may be
processed to provide a probabilistic language model for use in the parsing of
fresh texts.
</dc:description>
 <dc:description>Comment: Latex 2e, 6 pages, EACL 95 student session</dc:description>
 <dc:date>1995-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502037</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502038</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Implementation and evaluation of a German HMM for POS disambiguation</dc:title>
 <dc:creator>Feldweg, Helmut</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A German language model for the Xerox HMM tagger is presented. This model's
performance is compared with two other German taggers with partial parameter
re-estimation and full adaption of parameters from pre-tagged corpora. The
ambiguity types resolved by this model are analysed and compared to ambiguity
types of English and French. Finally, the model's error types are described. I
argue that although the overall performance of these models for German is
comparable to results for English and French, a more exact analysis
demonstrates important differences in the types of disambiguation involved for
German.
</dc:description>
 <dc:description>Comment: 6 pages, uses eaclap.sty, EACL SIGDAT workshop 1995</dc:description>
 <dc:date>1995-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502038</dc:identifier>
 <dc:identifier>Proceedings of the ACL SIGDAT Workshop, Dublin 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9502039</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multilingual Sentence Categorization according to Language</dc:title>
 <dc:creator>Giguet, Emmanuel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe an approach to sentence categorization which has
the originality to be based on natural properties of languages with no training
set dependency. The implementation is fast, small, robust and textual errors
tolerant. Tested for french, english, spanish and german discrimination, the
system gives very interesting results, achieving in one test 99.4% correct
assignments on real sentences.
  The resolution power is based on grammatical words (not the most common
words) and alphabet. Having the grammatical words and the alphabet of each
language at its disposal, the system computes for each of them its likelihood
to be selected. The name of the language having the optimum likelihood will tag
the sentence --- but non resolved ambiguities will be maintained. We will
discuss the reasons which lead us to use these linguistic facts and present
several directions to improve the system's classification performance.
  Categorization sentences with linguistic properties shows that difficult
problems have sometimes simple solutions.
</dc:description>
 <dc:description>Comment: 4 pages --- LaTeX</dc:description>
 <dc:date>1995-02-28</dc:date>
 <dc:date>1995-03-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9502039</dc:identifier>
 <dc:identifier>Eacl 95 SIGDAT Workshop : From text to tags</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503001</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using a Corpus for Teaching Turkish Morphology</dc:title>
 <dc:creator>Guvenir, H. Altay</dc:creator>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper reports on the preliminary phase of our ongoing research towards
developing an intelligent tutoring environment for Turkish grammar. One of the
components of this environment is a corpus search tool which, among other
aspects of the language, will be used to present the learner sample sentences
along with their morphological analyses. Following a brief introduction to the
Turkish language and its morphology, the paper describes the morphological
analysis and ambiguity resolution used to construct the corpus used in the
search tool. Finally, implementation issues and details involving the user
interface of the tool are discussed.
</dc:description>
 <dc:description>Comment: uuencoded gzip'ed postscript file. Appeared in Proceedings of TWLT-7,
  University of Twente, The Netherlands, June 1994. Software described is
  available at ftp://ftp.cs.bilkent.edu.tr/pub/Turklang/corpus-search/</dc:description>
 <dc:date>1995-03-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computational dialectology in Irish Gaelic</dc:title>
 <dc:creator>Kessler, Brett</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Dialect groupings can be discovered objectively and automatically by cluster
analysis of phonetic transcriptions such as those found in a linguistic atlas.
The first step in the analysis, the computation of linguistic distance between
each pair of sites, can be computed as Levenshtein distance between phonetic
strings. This correlates closely with the much more laborious technique of
determining and counting isoglosses, and is more accurate than the more
familiar metric of computing Hamming distance based on whether vocabulary
entries match. In the actual clustering step, traditional agglomerative
clustering works better than the top-down technique of partitioning around
medoids. When agglomerative clustering of phonetic string comparison distances
is applied to Gaelic, reasonable dialect boundaries are obtained, corresponding
to national and (within Ireland) provincial boundaries.
</dc:description>
 <dc:date>1995-03-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503003</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging French -- comparing a statistical and a constraint-based method</dc:title>
 <dc:creator>Chanod, Jean-Pierre</dc:creator>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we compare two competing approaches to part-of-speech tagging,
statistical and constraint-based disambiguation, using French as our test
language. We imposed a time limit on our experiment: the amount of time spent
on the design of our constraint system was about the same as the time we used
to train and test the easy-to-implement statistical model. We describe the two
systems and compare the results. The accuracy of the statistical method is
reasonably good, comparable to taggers for English. But the constraint-based
tagger seems to be superior even with the limited time we allowed ourselves for
rule development.
</dc:description>
 <dc:description>Comment: in Proceedings of EACL-95, uuencoded gzipped postscript</dc:description>
 <dc:date>1995-03-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503003</dc:identifier>
 <dc:identifier>Seventh Conference of the European Chapter of the ACL (EACL95).
  149-156. ACL, Dublin, 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Creating a tagset, lexicon and guesser for a French tagger</dc:title>
 <dc:creator>Chanod, Jean-Pierre</dc:creator>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We earlier described two taggers for French, a statistical one and a
constraint-based one. The two taggers have the same tokeniser and morphological
analyser. In this paper, we describe aspects of this work concerned with the
definition of the tagset, the building of the lexicon, derived from an existing
two-level morphological analyser, and the definition of a lexical transducer
for guessing unknown words.
</dc:description>
 <dc:description>Comment: aclap.sty</dc:description>
 <dc:date>1995-03-02</dc:date>
 <dc:date>1995-04-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503004</dc:identifier>
 <dc:identifier>ACL SIGDAT workshop: From Texts To Tags: Issues In Multilingual
  Language Analysis. 58-64. University College Dublin, Ireland, 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503005</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A specification language for Lexical Functional Grammars</dc:title>
 <dc:creator>Blackburn, Patrick</dc:creator>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper defines a language L for specifying LFG grammars. This enables
constraints on LFG's composite ontology (c-structures synchronised with
f-structures) to be stated directly; no appeal to the LFG construction
algorithm is needed. We use L to specify schemata annotated rules and the LFG
uniqueness, completeness and coherence principles. Broader issues raised by
this work are noted and discussed.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX uses eaclap.sty; Procs of Euro ACL-95</dc:description>
 <dc:date>1995-03-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503006</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>ParseTalk about Sentence- and Text-Level Anaphora</dc:title>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We provide a unified account of sentence-level and text-level anaphora within
the framework of a dependency-based grammar model. Criteria for anaphora
resolution within sentence boundaries rephrase major concepts from GB's binding
theory, while those for text-level anaphora incorporate an adapted version of a
Grosz-Sidner-style focus model.
</dc:description>
 <dc:description>Comment: in Proceedings of EACL-95, uuencoded and gzipped postscript (see also
  technical Report at
  http://www.coling.uni-freiburg.de:80/forschung/papers/eacl95.ps)</dc:description>
 <dc:date>1995-03-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503007</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Semantics of Motion</dc:title>
 <dc:creator>Sablayrolles, Pierre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a semantic study of motion complexes (ie. of a
motion verb followed by a spatial preposition). We focus on the spatial and the
temporal intrinsic semantic properties of the motion verbs, on the one hand,
and of the spatial prepositions, on the other hand. Then, we address the
problem of combining these basic semantics in order to formally and
automatically derive the spatiotemporal semantics of a motion complex from the
spatiotemporal properties of its components.
</dc:description>
 <dc:description>Comment: 3 pages, uses eaclap.sty</dc:description>
 <dc:date>1995-03-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503007</dc:identifier>
 <dc:identifier>Proceedings of the EACL95 Dublin</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503008</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ellipsis and Higher-Order Unification</dc:title>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Shieber, Stuart M.</dc:creator>
 <dc:creator>Pereira, Fernando C. N.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a new method for characterizing the interpretive possibilities
generated by elliptical constructions in natural language. Unlike previous
analyses, which postulate ambiguity of interpretation or derivation in the full
clause source of the ellipsis, our analysis requires no such hidden ambiguity.
Further, the analysis follows relatively directly from an abstract statement of
the ellipsis interpretation problem. It predicts correctly a wide range of
interactions between ellipsis and other semantic phenomena such as quantifier
scope and bound anaphora. Finally, although the analysis itself is stated
nonprocedurally, it admits of a direct computational method for generating
interpretations.
</dc:description>
 <dc:description>Comment: 54 pages</dc:description>
 <dc:date>1995-03-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503008</dc:identifier>
 <dc:identifier>Linguistics and Philosophy 14(4):399-452</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Distributional Part-of-Speech Tagging</dc:title>
 <dc:creator>Schuetze, Hinrich</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an algorithm for tagging words whose part-of-speech
properties are unknown. Unlike previous work, the algorithm categorizes word
tokens in context instead of word types. The algorithm is evaluated on the
Brown Corpus.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1995-03-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503009</dc:identifier>
 <dc:identifier>EACL 95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503010</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Corpus-based Method for Automatic Identification of Support Verbs for
  Nominalizations</dc:title>
 <dc:creator>Grefenstette, Gregory</dc:creator>
 <dc:creator>Teufel, Simone</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Nominalization is a highly productive phenomena in most languages. The
process of nominalization ejects a verb from its syntactic role into a nominal
position. The original verb is often replaced by a semantically emptied support
verb (e.g., &quot;make a proposal&quot;). The choice of a support verb for a given
nominalization is unpredictable, causing a problem for language learners as
well as for natural language processing systems. We present here a method of
discovering support verbs from an untagged corpus via low-level syntactic
processing and comparison of arguments attached to verbal forms and potential
nominalized forms. The result of the process is a list of potential support
verbs for the nominalized form of a given predicate.
</dc:description>
 <dc:description>Comment: EACL'95</dc:description>
 <dc:date>1995-03-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503011</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improving Statistical Language Model Performance with Automatically
  Generated Word Hierarchies</dc:title>
 <dc:creator>McMahon, John</dc:creator>
 <dc:creator>Smith, F. J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An automatic word classification system has been designed which processes
word unigram and bigram frequency statistics extracted from a corpus of natural
language utterances. The system implements a binary top-down form of word
clustering which employs an average class mutual information metric. Resulting
classifications are hierarchical, allowing variable class granularity. Words
are represented as structural tags --- unique $n$-bit numbers the most
significant bit-patterns of which incorporate class information. Access to a
structural tag immediately provides access to all classification levels for the
corresponding word. The classification system has successfully revealed some of
the structure of English, from the phonemic to the semantic level. The system
has been compared --- directly and indirectly --- with other recent word
classification systems. Class based interpolated language models have been
constructed to exploit the extra information supplied by the classifications
and some experiments have shown that the new models improve model performance.
</dc:description>
 <dc:description>Comment: 17 Page Paper. Self-extracting PostScript File</dc:description>
 <dc:date>1995-03-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503012</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Note on Zipf's Law, Natural Languages, and Noncoding DNA regions</dc:title>
 <dc:creator>Niyogi, Partha</dc:creator>
 <dc:creator>Berwick, Robert C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Quantitative Biology</dc:subject>
 <dc:description>  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the
basis of Zipf rank frequency data that noncoding DNA sequence regions are more
like natural languages than coding regions. We argue on the contrary that an
empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to
natural languages. Although DNA is a presumably an ``organized system of
signs'' in Mandelbrot's (1961) sense, an observation of statistical features of
the sort presented in the Mantegna et al. paper does not shed light on the
similarity between DNA's ``grammar'' and natural language grammars, just as the
observation of exact Zipf-like behavior cannot distinguish between the
underlying processes of tossing an $M$ sided die or a finite-state branching
process.
</dc:description>
 <dc:description>Comment: compressed uuencoded postscript file: 14 pages</dc:description>
 <dc:date>1995-03-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503013</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incremental Interpretation: Applications, Theory, and Relationship to
  Dynamic Semantics</dc:title>
 <dc:creator>Milward, David</dc:creator>
 <dc:creator>Cooper, Robin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Why should computers interpret language incrementally? In recent years
psycholinguistic evidence for incremental interpretation has become more and
more compelling, suggesting that humans perform semantic interpretation before
constituent boundaries, possibly word by word. However, possible computational
applications have received less attention. In this paper we consider various
potential applications, in particular graphical interaction and dialogue. We
then review the theoretical and computational tools available for mapping from
fragments of sentences to fully scoped semantic representations. Finally, we
tease apart the relationship between dynamic semantics and incremental
interpretation.
</dc:description>
 <dc:description>Comment: Procs. of COLING 94, LaTeX (2.09 preferred), 8 pages</dc:description>
 <dc:date>1995-03-13</dc:date>
 <dc:date>1995-03-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503014</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Non-Constituent Coordination: Theory and Practice</dc:title>
 <dc:creator>Milward, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Despite the large amount of theoretical work done on non-constituent
coordination during the last two decades, many computational systems still
treat coordination using adapted parsing strategies, in a similar fashion to
the SYSCONJ system developed for ATNs. This paper reviews the theoretical
literature, and shows why many of the theoretical accounts actually have worse
coverage than accounts based on processing. Finally, it shows how processing
accounts can be described formally and declaratively in terms of Dynamic
Grammars.
</dc:description>
 <dc:description>Comment: Procs. of COLING 94, LaTeX (2.09 preferred), 7 pages</dc:description>
 <dc:date>1995-03-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503015</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incremental Interpretation of Categorial Grammar</dc:title>
 <dc:creator>Milward, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper describes a parser for Categorial Grammar which provides fully word
by word incremental interpretation. The parser does not require fragments of
sentences to form constituents, and thereby avoids problems of spurious
ambiguity. The paper includes a brief discussion of the relationship between
basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar
and the Lambek Calculus. It also includes a discussion of some of the issues
which arise when parsing lexicalised grammars, and the possibilities for using
statistical techniques for tuning to particular languages.
</dc:description>
 <dc:description>Comment: Procs. of EACL 95, LaTeX (2.09 preferred), eaclap.sty, 9 pages</dc:description>
 <dc:date>1995-03-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503016</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural Language Interfaces to Databases - An Introduction</dc:title>
 <dc:creator>Androutsopoulos, I.</dc:creator>
 <dc:creator>Ritchie, G. D.</dc:creator>
 <dc:creator>Thanisch, P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper is an introduction to natural language interfaces to databases
(NLIDBs). A brief overview of the history of NLIDBs is first given. Some
advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to
formal query languages, form-based interfaces, and graphical interfaces. An
introduction to some of the linguistic problems NLIDBs have to confront
follows, for the benefit of readers less familiar with computational
linguistics. The discussion then moves on to NLIDB architectures, portability
issues, restricted natural language input systems (including menu-based
NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of
NLIDB research are then presented, namely database updates, meta-knowledge
questions, temporal questions, and multi-modal NLIDBs. The paper ends with
reflections on the current state of the art.
</dc:description>
 <dc:description>Comment: 50 pages, uuencoded compressed tar file, containing LaTeX code and
  .eps figures. Uses a4wide.sty. (No changes in the text. Fixed problem with
  epsf macro. Use the epsf.sty included in the tar file, not the epsf.sty of
  the cmp-lg server.)</dc:description>
 <dc:date>1995-03-14</dc:date>
 <dc:date>1995-03-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503016</dc:identifier>
 <dc:identifier>Natural Language Engineering 1:1, 29-81</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503017</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Redundancy in Collaborative Dialogue</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In dialogues in which both agents are autonomous, each agent deliberates
whether to accept or reject the contributions of the current speaker. A speaker
cannot simply assume that a proposal or an assertion will be accepted. However,
an examination of a corpus of naturally-occurring problem-solving dialogues
shows that agents often do not explicitly indicate acceptance or rejection.
Rather the speaker must infer whether the hearer understands and accepts the
current contribution based on indirect evidence provided by the hearer's next
dialogue contribution. In this paper, I propose a model of the role of
informationally redundant utterances in providing evidence to support
inferences about mutual understanding and acceptance. The model (1) requires a
theory of mutual belief that supports mutual beliefs of various strengths; (2)
explains the function of a class of informationally redundant utterances that
cannot be explained by other accounts; and (3) contributes to a theory of
dialogue by showing how mutual beliefs can be inferred in the absence of the
master-slave assumption.
</dc:description>
 <dc:description>Comment: 8 pages, lingmacros, reformatted version of Coling92 paper</dc:description>
 <dc:date>1995-03-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503017</dc:identifier>
 <dc:identifier>Fourteenth International Conference on Computational Linguistics,
  Nantes, 1992</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503018</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discourse and Deliberation: Testing a Collaborative Strategy</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A discourse strategy is a strategy for communicating with another agent.
Designing effective dialogue systems requires designing agents that can choose
among discourse strategies. We claim that the design of effective strategies
must take cognitive factors into account, propose a new method for testing the
hypothesized factors, and present experimental results on an effective strategy
for supporting deliberation. The proposed method of computational dialogue
simulation provides a new empirical basis for computational linguistics.
</dc:description>
 <dc:description>Comment: 8 pages, psfig.sty, lingmacros.sty, reformatted version of Coling94
  paper</dc:description>
 <dc:date>1995-03-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503018</dc:identifier>
 <dc:identifier>Fifteenth International Conference on Computational Linguistics,
  Kyoto</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503019</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>SATZ - An Adaptive Sentence Segmentation System</dc:title>
 <dc:creator>Palmer, David D.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper provides a detailed description of the sentence segmentation
system first introduced in cmp-lg/9411022. It provides results of systematic
experiments involving sentence boundary determination, including context size,
lexicon size, and single-case texts. Also included are the results of
successfully adapting the system to German and French. The source code for the
system is available as a compressed tar file at
ftp://cs-tr.CS.Berkeley.EDU/pub/cstr/satz.tar.Z .
</dc:description>
 <dc:description>Comment: 30 pages, uuencoded compressed PostScript file (about 170k)</dc:description>
 <dc:date>1995-03-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503020</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Different Issues in the Design of a Lemmatizer/Tagger for Basque</dc:title>
 <dc:creator>Aduriz, I.</dc:creator>
 <dc:creator>Alegria, I.</dc:creator>
 <dc:creator>Arriola, J. M.</dc:creator>
 <dc:creator>Artola, X.</dc:creator>
 <dc:creator>A., Diaz de Illarraza</dc:creator>
 <dc:creator>Ezeiza, N.</dc:creator>
 <dc:creator>Gojenola, K.</dc:creator>
 <dc:creator>Maritxalar, M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents relevant issues that have been considered in the design
of a general purpose lemmatizer/tagger for Basque (EUSLEM). The
lemmatizer/tagger is conceived as a basic tool necessary for other linguistic
applications. It uses the lexical data base and the morphological analyzer
previously developed and implemented. Due to the characteristics of the
language, the tagset here proposed in structured in for levels, so that each
level is a refinement of the previous one in the sense that it adds more
detailed information. We will focus on the problems found in designing this
tagset and on the strategies for morphological disambiguation that will be
used.
</dc:description>
 <dc:description>Comment: in Proceedings of SIGDAT-95 (EACL-Dublin), 6 pages, uuencoded and
  gz-compressed postscript</dc:description>
 <dc:date>1995-03-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Note on the Complexity of Restricted Attribute-Value Grammars</dc:title>
 <dc:creator>Torenvliet, Leen</dc:creator>
 <dc:creator>Trautwein, Marten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The recognition problem for attribute-value grammars (AVGs) was shown to be
undecidable by Johnson in 1988. Therefore, the general form of AVGs is of no
practical use. In this paper we study a very restricted form of AVG, for which
the recognition problem is decidable (though still NP-complete), the R-AVG. We
show that the R-AVG formalism captures all of the context free languages and
more, and introduce a variation on the so-called `off-line parsability
constraint', the `honest parsability constraint', which lets different types of
R-AVG coincide precisely with well-known time complexity classes.
</dc:description>
 <dc:description>Comment: 18 pages, also available by (1) anonymous ftp at
  ftp://ftp.fwi.uva.nl/pub/theory/illc/researchReports/CT-95-02.ps.gz ; (2) WWW
  from http://www.fwi.uva.nl/~mtrautwe/</dc:description>
 <dc:date>1995-03-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Assessing Complexity Results in Feature Theories</dc:title>
 <dc:creator>Trautwein, Marten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we assess the complexity results of formalisms that describe
the feature theories used in computational linguistics. We show that from these
complexity results no immediate conclusions can be drawn about the complexity
of the recognition problem of unification grammars using these feature
theories. On the one hand, the complexity of feature theories does not provide
an upper bound for the complexity of such unification grammars.
  On the other hand, the complexity of feature theories need not provide a
lower bound. Therefore, we argue for formalisms that describe actual
unification grammars instead of feature theories. Thus the complexity results
of these formalisms judge upon the hardness of unification grammars in
computational linguistics.
</dc:description>
 <dc:description>Comment: 16 pages, includes 3 Postscript figures, uses epsf.sty, also
  available by (1) anonymous ftp at
  ftp://ftp.fwi.uva.nl/pub/theory/illc/researchReports/LP-95-01.ps.gz (2) WWW
  from http://www.fwi.uva.nl/~mtrautwe/ This version differs slightly from the
  original technical Report. Some non-standard style-files are removed from
  this version.</dc:description>
 <dc:date>1995-03-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A fast partial parse of natural language sentences using a connectionist
  method</dc:title>
 <dc:creator>Lyon, Caroline</dc:creator>
 <dc:creator>Dickerson, Bob</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The pattern matching capabilities of neural networks can be used to locate
syntactic constituents of natural language. This paper describes a fully
automated hybrid system, using neural nets operating within a grammatic
framework. It addresses the representation of language for connectionist
processing, and describes methods of constraining the problem size. The
function of the network is briefly explained, and results are given.
</dc:description>
 <dc:description>Comment: 8 pages, 3 Postscript figures, uses eaclap.sty it needs LaTeX2e and
  uses begin{filecontents*} to write out the 3 PostScript figures.</dc:description>
 <dc:date>1995-03-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>From compositional to systematic semantics</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We prove a theorem stating that any semantics can be encoded as a
compositional semantics, which means that, essentially, the standard definition
of compositionality is formally vacuous. We then show that when compositional
semantics is required to be &quot;systematic&quot; (that is, the meaning function cannot
be arbitrary, but must belong to some class), it is possible to distinguish
between compositional and non-compositional semantics. As a result, we believe
that the paper clarifies the concept of compositionality and opens a
possibility of making systematic formal comparisons of different systems of
grammars.
</dc:description>
 <dc:description>Comment: 11 pp. Latex.;</dc:description>
 <dc:date>1995-03-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503024</dc:identifier>
 <dc:identifier>Linguistics and Philosophy(17):329-342, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9503025</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Co-occurrence Vectors from Corpora vs. Distance Vectors from
  Dictionaries</dc:title>
 <dc:creator>Niwa, Yoshiki</dc:creator>
 <dc:creator>Nitta, Yoshihiko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A comparison was made of vectors derived by using ordinary co-occurrence
statistics from large text corpora and of vectors derived by measuring the
inter-word distances in dictionary definitions. The precision of word sense
disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal
(20M total words) was higher than that by using distance vectors from the
Collins English Dictionary (60K head words + 1.6M definition words). However,
other experimental results suggest that distance vectors contain some different
semantic information from co-occurrence vectors.
</dc:description>
 <dc:description>Comment: 6 pages, appeared in the Proc. of COLING94 (pp. 304-309).</dc:description>
 <dc:date>1995-04-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9503025</dc:identifier>
 <dc:identifier>COLING94, 304-309.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic processing proper names in texts</dc:title>
 <dc:creator>Wolinski, Francis</dc:creator>
 <dc:creator>Vichot, Frantz</dc:creator>
 <dc:creator>Dillet, Bruno</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows first the problems raised by proper names in natural
language processing. Second, it introduces the knowledge representation
structure we use based on conceptual graphs. Then it explains the techniques
which are used to process known and unknown proper names. At last, it gives the
performance of the system and the further works we intend to deal with.
</dc:description>
 <dc:description>Comment: 10 Postscript pages (compress, uuencode)</dc:description>
 <dc:date>1995-04-03</dc:date>
 <dc:date>1995-04-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagset Design and Inflected Languages</dc:title>
 <dc:creator>Elworthy, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An experiment designed to explore the relationship between tagging accuracy
and the nature of the tagset is described, using corpora in English, French and
Swedish. In particular, the question of internal versus external criteria for
tagset design is considered, with the general conclusion that external
(linguistic) criteria should be followed. Some problems associated with tagging
unknown words in inflected languages are briefly considered.
</dc:description>
 <dc:description>Comment: EACL-94 SIGDAT paper. Includes large tables and figures, which may
  upset some version of LaTeX. Revised version to correct a minor typo!</dc:description>
 <dc:date>1995-04-03</dc:date>
 <dc:date>1995-04-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Collaborating on Referring Expressions</dc:title>
 <dc:creator>Heeman, Peter A.</dc:creator>
 <dc:creator>Hirst, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a computational model of how conversational participants
collaborate in order to make a referring action successful. The model is based
on the view of language as goal-directed behavior. We propose that the content
of a referring expression can be accounted for by the planning paradigm. Not
only does this approach allow the processes of building referring expressions
and identifying their referents to be captured by plan construction and plan
inference, it also allows us to account for how participants clarify a
referring expression by using meta-actions that reason about and manipulate the
plan derivation that corresponds to the referring expression. To account for
how clarification goals arise and how inferred clarification plans affect the
agent, we propose that the agents are in a certain state of mind, and that this
state includes an intention to achieve the goal of referring and a plan that
the agents are currently considering. It is this mental state that sanctions
the adoption of goals and the acceptance of inferred plans, and so acts as a
link between understanding and generation.
</dc:description>
 <dc:description>Comment: 32 pages, 2 figures, to appear in Computation Linguistics 21-3</dc:description>
 <dc:date>1995-04-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Computational Treatment of HPSG Lexical Rules as Covariation in
  Lexical Entries</dc:title>
 <dc:creator>Meurers, Walt Detmar</dc:creator>
 <dc:creator>Minnen, Guido</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a compiler which translates a set of HPSG lexical rules and their
interaction into definite relations used to constrain lexical entries. The
compiler ensures automatic transfer of properties unchanged by a lexical rule.
Thus an operational semantics for the full lexical rule mechanism as used in
HPSG linguistics is provided. Program transformation techniques are used to
advance the resulting encoding. The final output constitutes a computational
counterpart of the linguistic generalizations captured by lexical rules and
allows ``on the fly'' application.
</dc:description>
 <dc:description>Comment: 15 pages, uuencoded and compressed postscript to appear in
  Proceedings of the 5th Int. Workshop on Natural Language Understanding and
  Logic Programming. Lisbon, Portugal. 1995</dc:description>
 <dc:date>1995-04-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constraint Logic Programming for Natural Language Processing</dc:title>
 <dc:creator>Blache, Philippe</dc:creator>
 <dc:creator>Hathout, Nabil</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes an evaluation of the adequacy of the constraint logic
programming paradigm for natural language processing. Theoretical aspects of
this question have been discussed in several works. We adopt here a pragmatic
point of view and our argumentation relies on concrete solutions. Using actual
contraints (in the CLP sense) is neither easy nor direct. However, CLP can
improve parsing techniques in several aspects such as concision, control,
efficiency or direct representation of linguistic formalism. This discussion is
illustrated by several examples and the presentation of an HPSG parser.
</dc:description>
 <dc:description>Comment: 15 pages, uuencoded and compressed postscript to appear in
  Proceedings of the 5th Int. Workshop on Natural Language Understanding and
  Logic Programming. Lisbon, Portugal. 1995</dc:description>
 <dc:date>1995-04-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Cues and control in Expert-Client Dialogues</dc:title>
 <dc:creator>Whittaker, Steve</dc:creator>
 <dc:creator>Stenton, Phil</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We conducted an empirical analysis into the relation between control and
discourse structure. We applied control criteria to four dialogues and
identified 3 levels of discourse structure. We investigated the mechanism for
changing control between these structures and found that utterance type and not
cue words predicted shifts of control. Participants used certain types of
signals when discourse goals were proceeding successfully but resorted to
interruptions when they were not.
</dc:description>
 <dc:description>Comment: 8 pages, latex</dc:description>
 <dc:date>1995-04-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504006</dc:identifier>
 <dc:identifier>Proceedings of the 26th Annual Meeting of the Association of
  Computational Linguistics, 1988</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Mixed Initiative in Dialogue: An Investigation into Discourse
  Segmentation</dc:title>
 <dc:creator>Walker, Marilyn</dc:creator>
 <dc:creator>Whittaker, Steve</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Conversation between two people is usually of mixed-initiative, with control
over the conversation being transferred from one person to another. We apply a
set of rules for the transfer of control to 4 sets of dialogues consisting of a
total of 1862 turns. The application of the control rules lets us derive
domain-independent discourse structures. The derived structures indicate that
initiative plays a role in the structuring of discourse. In order to explore
the relationship of control and initiative to discourse processes like
centering, we analyze the distribution of four different classes of anaphora
for two data sets. This distribution indicates that some control segments are
hierarchically related to others. The analysis suggests that discourse
participants often mutually agree to a change of topic. We also compared
initiative in Task Oriented and Advice Giving dialogues and found that both
allocation of control and the manner in which control is transferred is
radically different for the two dialogue types. These differences can be
explained in terms of collaborative planning principles.
</dc:description>
 <dc:description>Comment: 8 pages, latex</dc:description>
 <dc:date>1995-04-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504007</dc:identifier>
 <dc:identifier>Proceedings of the 28th Annual Meeting of the Association of
  Computational Linguistics, 1990</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>SKOPE: A connectionist/symbolic architecture of spoken Korean processing</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Spoken language processing requires speech and natural language integration.
Moreover, spoken Korean calls for unique processing methodology due to its
linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic
spoken Korean processing engine, which emphasizes that: 1) connectionist and
symbolic techniques must be selectively applied according to their relative
strength and weakness, and 2) the linguistic characteristics of Korean must be
fully considered for phoneme recognition, speech and language integration, and
morphological/syntactic processing. The design and implementation of SKOPE
demonstrates how connectionist/symbolic hybrid architectures can be constructed
for spoken agglutinative language processing. Also SKOPE presents many novel
ideas for speech and language processing. The phoneme recognition,
morphological analysis, and syntactic analysis experiments show that SKOPE is a
viable approach for the spoken Korean processing.
</dc:description>
 <dc:description>Comment: 8 pages, latex, use aaai.sty &amp; aaai.bst, bibfile: nlpsp.bib, to be
  presented at IJCAI95 workshops on new approaches to learning for natural
  language processing</dc:description>
 <dc:date>1995-04-07</dc:date>
 <dc:date>1995-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Abstract Machine for Typed Feature Structures</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an abstract machine for linguistic formalisms that are
based on typed feature structures, such as HPSG. The core design of the
abstract machine is given in detail, including the compilation process from a
high-level language to the abstract machine language and the implementation of
the abstract instructions. The machine's engine supports the unification of
typed, possibly cyclic, feature structures. A separate module deals with
control structures and instructions to accommodate parsing for phrase structure
grammars. We treat the linguistic formalism as a high-level declarative
programming language, applying methods that were proved useful in computer
science to the study of natural languages: a grammar specified using the
formalism is endowed with an operational semantics.
</dc:description>
 <dc:description>Comment: Self-contained LaTeX, 15 pages, to appear in NLULP95</dc:description>
 <dc:date>1995-04-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504009</dc:identifier>
 <dc:identifier>Proc. 5th Int. Workshop on Natural Language Understanding and
  Logic Programming, Lisbon, May 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>MAXIMUM LIKELIHOOD AND MINIMUM ENTROPY IDENTIFICATION OF GRAMMARS</dc:title>
 <dc:creator>Collet, P.</dc:creator>
 <dc:creator>Galves, A.</dc:creator>
 <dc:creator>Lopes, A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Using the Thermodynamic Formalism, we introduce a Gibbsian model for the
identification of regular grammars based only on positive evidence. This model
mimics the natural language acquisition procedure driven by prosody which is
here represented by the thermodynamical potential. The statistical question we
face is how to estimate the incidenc e matrix of a subshift of finite type from
a sample produced by a Gibbs state whose potential is known. The model
acquaints for both the robustness of t he language acquisition procedure and
language changes. The probabilistic appr oach we use avoids invoking ad-hoc
restrictions as Berwick's Subset Principle.
</dc:description>
 <dc:description>Comment: 12 pages, ps, gzip, uuencoded</dc:description>
 <dc:date>1995-04-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Processing Model for Free Word Order Languages</dc:title>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:creator>Joshi, Aravind K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Like many verb-final languages, Germn displays considerable word-order
freedom: there is no syntactic constraint on the ordering of the nominal
arguments of a verb, as long as the verb remains in final position. This effect
is referred to as ``scrambling'', and is interpreted in transformational
frameworks as leftward movement of the arguments. Furthermore, arguments from
an embedded clause may move out of their clause; this effect is referred to as
``long-distance scrambling''. While scrambling has recently received
considerable attention in the syntactic literature, the status of long-distance
scrambling has only rarely been addressed. The reason for this is the
problematic status of the data: not only is long-distance scrambling highly
dependent on pragmatic context, it also is strongly subject to degradation due
to processing constraints. As in the case of center-embedding, it is not
immediately clear whether to assume that observed unacceptability of highly
complex sentences is due to grammatical restrictions, or whether we should
assume that the competence grammar does not place any restrictions on
scrambling (and that, therefore, all such sentences are in fact grammatical),
and the unacceptability of some (or most) of the grammatically possible word
orders is due to processing limitations. In this paper, we will argue for the
second view by presenting a processing model for German.
</dc:description>
 <dc:description>Comment: 23 pages, uuencoded compressed ps file. In {\em Perspectives on
  Sentence Processing}, C. Clifton, Jr., L. Frazier and K. Rayner, editors.
  Lawrence Erlbaum Associates, 1994</dc:description>
 <dc:date>1995-04-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Linear Logic for Meaning Assembly</dc:title>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Lamping, John</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Saraswat, Vijay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Semantic theories of natural language associate meanings with utterances by
providing meanings for lexical items and rules for determining the meaning of
larger units given the meanings of their parts. Meanings are often assumed to
combine via function application, which works well when constituent structure
trees are used to guide semantic composition. However, we believe that the
functional structure of Lexical-Functional Grammar is best used to provide the
syntactic information necessary for constraining derivations of meaning in a
cross-linguistically uniform format. It has been difficult, however, to
reconcile this approach with the combination of meanings by function
application. In contrast to compositional approaches, we present a deductive
approach to assembling meanings, based on reasoning with constraints, which
meshes well with the unordered nature of information in the functional
structure. Our use of linear logic as a `glue' for assembling meanings allows
for a coherent treatment of the LFG requirements of completeness and coherence
as well as of modification and quantification.
</dc:description>
 <dc:description>Comment: 19 pages, uses lingmacros.sty, fullname.sty, tree-dvips.sty,
  latexsym.sty, requires the new version of Latex</dc:description>
 <dc:date>1995-04-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504012</dc:identifier>
 <dc:identifier>Proceedings of CLNLP, Edinburgh, April 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>NLG vs. Templates</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  One of the most important questions in applied NLG is what benefits (or
`value-added', in business-speak) NLG technology offers over template-based
approaches. Despite the importance of this question to the applied NLG
community, however, it has not been discussed much in the research NLG
community, which I think is a pity. In this paper, I try to summarize the
issues involved and recap current thinking on this topic. My goal is not to
answer this question (I don't think we know enough to be able to do so), but
rather to increase the visibility of this issue in the research community, in
the hope of getting some input and ideas on this very important question. I
conclude with a list of specific research areas I would like to see more work
in, because I think they would increase the `value-added' of NLG over
templates.
</dc:description>
 <dc:description>Comment: Uuencoded compressed tar file, containing LaTeX source and a style
  file. This paper will appear in the 1995 European NL Generation Workshop</dc:description>
 <dc:date>1995-04-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>LexGram - a practical categorial grammar formalism -</dc:title>
 <dc:creator>Koenig, Esther</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present the LexGram system, an amalgam of (Lambek) categorial grammar and
Head Driven Phrase Structure Grammar (HPSG), and show that the grammar
formalism it implements is a well-structured and useful tool for actual grammar
development.
</dc:description>
 <dc:description>Comment: 16 pages</dc:description>
 <dc:date>1995-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504014</dc:identifier>
 <dc:identifier>Proc.CLNLP95, Edinburgh</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Estimating Lexical Priors for Low-Frequency Syncretic Forms</dc:title>
 <dc:creator>Baayen, Harald</dc:creator>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Given a previously unseen form that is morphologically n-ways ambiguous, what
is the best estimator for the lexical prior probabilities for the various
functions of the form? We argue that the best estimator is provided by
computing the relative frequencies of the various functions among the hapax
legomena --- the forms that occur exactly once in a corpus. This result has
important implications for the development of stochastic morphological taggers,
especially when some initial hand-tagging of a corpus is required: For
predicting lexical priors for very low-frequency morphologically ambiguous
types (most of which would not occur in any given corpus) one should
concentrate on tagging a good representative sample of the hapax legomena,
rather than extensively tagging words of all frequency ranges.
</dc:description>
 <dc:description>Comment: Submitted to Computational Linguistics</dc:description>
 <dc:date>1995-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Memoization of Top Down Parsing</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses the relationship between memoized top-down recognizers
and chart parsers. It presents a version of memoization suitable for
continuation-passing style programs. When applied to a simple formalization of
a top-down recognizer it yields a terminating parser.
</dc:description>
 <dc:description>Comment: uuencoded, compressed postscript file</dc:description>
 <dc:date>1995-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Uniform Treatment of Pragmatic Inferences in Simple and Complex
  Utterances and Sequences of Utterances</dc:title>
 <dc:creator>Marcu, Daniel</dc:creator>
 <dc:creator>Hirst, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Drawing appropriate defeasible inferences has been proven to be one of the
most pervasive puzzles of natural language processing and a recurrent problem
in pragmatics. This paper provides a theoretical framework, called ``stratified
logic'', that can accommodate defeasible pragmatic inferences. The framework
yields an algorithm that computes the conversational, conventional, scalar,
clausal, and normal state implicatures; and the presuppositions that are
associated with utterances. The algorithm applies equally to simple and complex
utterances and sequences of utterances.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX Source. To appear in the Proceedings of ACL-95.
  Requires aclap.sty file</dc:description>
 <dc:date>1995-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Implemented Formalism for Computing Linguistic Presuppositions and
  Existential Commitments</dc:title>
 <dc:creator>Marcu, Daniel</dc:creator>
 <dc:creator>Hirst, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We rely on the strength of linguistic and philosophical perspectives in
constructing a framework that offers a unified explanation for presuppositions
and existential commitment. We use a rich ontology and a set of methodological
principles that embed the essence of Meinong's philosophy and Grice's
conversational principles into a stratified logic, under an unrestricted
interpretation of the quantifiers. The result is a logical formalism that
yields a tractable computational method that uniformly calculates all the
presuppositions of a given utterance, including the existential ones.
</dc:description>
 <dc:description>Comment: 10 pages, LaTeX Source. Requires iwcs.sty file. Ignore LaTeX warning
  messages (Proceedings of the International Workshop on Computational
  Semantics, Tilburg, The Netherlands, pages 141--150, December 1994.)</dc:description>
 <dc:date>1995-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Formalism and an Algorithm for Computing Pragmatic Inferences and
  Detecting Infelicities</dc:title>
 <dc:creator>Marcu, Daniel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Since Austin introduced the term ``infelicity'', the linguistic literature
has been flooded with its use, but no formal or computational explanation has
been given for it. This thesis provides one for those infelicities that occur
when a pragmatic inference is cancelled.
  Our contribution assumes the existence of a finer grained taxonomy with
respect to pragmatic inferences. It is shown that if one wants to account for
the natural language expressiveness, one should distinguish between pragmatic
inferences that are felicitous to defeat and pragmatic inferences that are
infelicitously defeasible. Thus, it is shown that one should consider at least
three types of information: indefeasible, felicitously defeasible, and
infelicitously defeasible. The cancellation of the last of these determines the
pragmatic infelicities.
  A new formalism has been devised to accommodate the three levels of
information, called ``stratified logic''. Within it, we are able to express
formally notions such as ``utterance U presupposes P'' or ``utterance U is
infelicitous''. Special attention is paid to the implications that our work has
in solving some well-known existential philosophical puzzles. The formalism
yields an algorithm for computing interpretations for utterances, for
determining their associated presuppositions, and for signalling infelicitous
utterances that has been implemented in Common Lisp. The algorithm applies
equally to simple and complex utterances and sequences of utterances.
</dc:description>
 <dc:description>Comment: 132 pages, LaTeX Source. Master Thesis, October 1994. Requires
  epsf.sty, ut-thesis.sty, named.sty, headerfooter.sty, my-macros.sty files</dc:description>
 <dc:date>1995-04-25</dc:date>
 <dc:date>1995-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504020</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computational Interpretations of the Gricean Maxims in the Generation of
  Referring Expressions</dc:title>
 <dc:creator>Dale, Robert</dc:creator>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We examine the problem of generating definite noun phrases that are
appropriate referring expressions; i.e, noun phrases that (1) successfully
identify the intended referent to the hearer whilst (2) not conveying to her
any false conversational implicatures (Grice, 1975). We review several possible
computational interpretations of the conversational implicature maxims, with
different computational costs, and argue that the simplest may be the best,
because it seems to be closest to what human speakers do. We describe our
recommended algorithm in detail, along with a specification of the resources a
host system must provide in order to make use of the algorithm, and an
implementation used in the natural language generation component of the IDAS
system.
  This paper will appear in the the April--June 1995 issue of Cognitive
Science, and is made available on cmp-lg with the permission of Ablex, the
publishers of that journal.
</dc:description>
 <dc:description>Comment: 29 pages, compressed PS file</dc:description>
 <dc:date>1995-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Phonological Derivation in Optimality Theory</dc:title>
 <dc:creator>Ellison, T. Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Optimality Theory is a constraint-based theory of phonology which allows
constraints to be violated. Consequently, implementing the theory presents
problems for declarative constraint-based processing frameworks. On the basis
of two regularity assumptions, that candidate sets are regular and that
constraints can be modelled by transducers, this paper presents and proves
correct algorithms for computing the action of constraints, and hence deriving
surface forms.
</dc:description>
 <dc:description>Comment: 7 pages</dc:description>
 <dc:date>1995-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504021</dc:identifier>
 <dc:identifier>Coling 94:1007-1013 (Vol II)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constraints, Exceptions and Representations</dc:title>
 <dc:creator>Ellison, T. Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows that default-based phonologies have the potential to capture
morphophonological generalisations which cannot be captured by non-defaul
theories. In achieving this result, I offer a characterisation of
Underspecification Theory and Optimality Theory in terms of their methods for
ordering defaults. The result means that machine learning techniques for
building non-defualt analyses may not provide a suitable basis for
morphophonological analysis.
</dc:description>
 <dc:description>Comment: 9 pages postscript</dc:description>
 <dc:date>1995-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504022</dc:identifier>
 <dc:identifier>(Proc of ACL SIGPHON First Meeting)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>TAKTAG: Two-phase learning method for hybrid statistical/rule-based
  part-of-speech disambiguation</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:creator>Shin, Sanghyun</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Both statistical and rule-based approaches to part-of-speech (POS)
disambiguation have their own advantages and limitations. Especially for
Korean, the narrow windows provided by hidden markov model (HMM) cannot cover
the necessary lexical and long-distance dependencies for POS disambiguation. On
the other hand, the rule-based approaches are not accurate and flexible to new
tag-sets and languages. In this regard, the statistical/rule-based hybrid
method that can take advantages of both approaches is called for the robust and
flexible POS disambiguation. We present one of such method, that is, a
two-phase learning architecture for the hybrid statistical/rule-based POS
disambiguation, especially for Korean. In this method, the statistical learning
of morphological tagging is error-corrected by the rule-based learning of Brill
[1992] style tagger. We also design the hierarchical and flexible Korean
tag-set to cope with the multiple tagging applications, each of which requires
different tag-set. Our experiments show that the two-phase learning method can
overcome the undesirable features of solely HMM-based or solely rule-based
tagging, especially for morphologically complex Korean.
</dc:description>
 <dc:description>Comment: 10pages, latex, named.sty &amp; named.bst, use psfig figures, submitted</dc:description>
 <dc:date>1995-04-26</dc:date>
 <dc:date>1995-05-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Morphographemic Model for Error Correction in Nonconcatenative Strings</dc:title>
 <dc:creator>Bowden, Tanya</dc:creator>
 <dc:creator>Kiraz, George Anton</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces a spelling correction system which integrates
seamlessly with morphological analysis using a multi-tape formalism. Handling
of various Semitic error problems is illustrated, with reference to Arabic and
Syriac examples. The model handles errors vocalisation, diacritics, phonetic
syncopation and morphographemic idiosyncrasies, in addition to Damerau errors.
A complementary correction strategy for morphologically sound but
morphosyntactically ill-formed words is outlined.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX</dc:description>
 <dc:date>1995-04-27</dc:date>
 <dc:date>1995-04-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discourse Processing of Dialogues with Multiple Threads</dc:title>
 <dc:creator>Rose', Carolyn Penstein</dc:creator>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:creator>Levin, Lori S.</dc:creator>
 <dc:creator>Van Ess-Dykema, Carol</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we will present our ongoing work on a plan-based discourse
processor developed in the context of the Enthusiast Spanish to English
translation system as part of the JANUS multi-lingual speech-to-speech
translation system. We will demonstrate that theories of discourse which
postulate a strict tree structure of discourse on either the intentional or
attentional level are not totally adequate for handling spontaneous dialogues.
We will present our extension to this approach along with its implementation in
our plan-based discourse processor. We will demonstrate that the implementation
of our approach outperforms an implementation based on the strict tree
structure approach.
</dc:description>
 <dc:description>Comment: 8 pages, compressed, uuencoded postscript. If you have trouble
  printing the postscript, please send mail to cprose@lcl.cmu.edu</dc:description>
 <dc:date>1995-04-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504025</dc:identifier>
 <dc:identifier>Proceedings of the 33rd Annual Meeting of the Association for
  Computational Linguistics, MIT, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The intersection of Finite State Automata and Definite Clause Grammars</dc:title>
 <dc:creator>van Noord, Gertjan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Bernard Lang defines parsing as the calculation of the intersection of a FSA
(the input) and a CFG. Viewing the input for parsing as a FSA rather than as a
string combines well with some approaches in speech understanding systems, in
which parsing takes a word lattice as input (rather than a word string).
Furthermore, certain techniques for robust parsing can be modelled as finite
state transducers. In this paper we investigate how we can generalize this
approach for unification grammars. In particular we will concentrate on how we
might the calculation of the intersection of a FSA and a DCG. It is shown that
existing parsing algorithms can be easily extended for FSA inputs. However, we
also show that the termination properties change drastically: we show that it
is undecidable whether the intersection of a FSA and a DCG is empty (even if
the DCG is off-line parsable). Furthermore we discuss approaches to cope with
the problem.
</dc:description>
 <dc:description>Comment: 7 pages. Requires pictexwd package. To appear in ACL 95</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504026</dc:identifier>
 <dc:identifier>Proceedings of the 33rd ACL. Boston 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Generation Algorithm for Lexicalist MT</dc:title>
 <dc:creator>Poznanski, Victor</dc:creator>
 <dc:creator>Beaven, John L.</dc:creator>
 <dc:creator>Whitelock, Pete</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The lexicalist approach to Machine Translation offers significant advantages
in the development of linguistic descriptions. However, the Shake-and-Bake
generation algorithm of (Whitelock, COLING-92) is NP-complete. We present a
polynomial time algorithm for lexicalist MT generation provided that sufficient
information can be transferred to ensure more determinism.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of ACL-95</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:date>1995-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Memoization of Coroutined Constraints</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:creator>D&#xf6;rre, Jochen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Some linguistic constraints cannot be effectively resolved during parsing at
the location in which they are most naturally introduced. This paper shows how
constraints can be propagated in a memoizing parser (such as a chart parser) in
much the same way that variable bindings are, providing a general treatment of
constraint coroutining in memoization. Prolog code for a simple application of
our technique to Bouma and van Noord's (1994) categorial grammar analysis of
Dutch is provided.
</dc:description>
 <dc:description>Comment: To appear in The Proceedings of ACL '95, uses aclap.sty</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:date>1995-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504029</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Quantifiers, Anaphora, and Intensionality</dc:title>
 <dc:creator>Dalrymple, Mary</dc:creator>
 <dc:creator>Lamping, John</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:creator>Saraswat, Vijay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The relationship between Lexical-Functional Grammar (LFG) {\em functional
structures} (f-structures) for sentences and their semantic interpretations can
be expressed directly in a fragment of linear logic in a way that correctly
explains the constrained interactions between quantifier scope ambiguity, bound
anaphora and intensionality. This deductive approach to semantic interpretaion
obviates the need for additional mechanisms, such as Cooper storage, to
represent the possible scopes of a quantified NP, and explains the interactions
between quantified NPs, anaphora and intensional verbs such as `seek'. A single
specification in linear logic of the argument requirements of intensional verbs
is sufficient to derive the correct reading predictions for intensional-verb
clauses both with nonquantified and with quantified direct objects. In
particular, both de dicto and de re readings are derived for quantified
objects. The effects of type-raising or quantifying-in rules in other
frameworks here just follow as linear-logic theorems.
  While our approach resembles current categorial approaches in important ways,
it differs from them in allowing the greater type flexibility of categorial
semantics while maintaining a precise connection to syntax. As a result, we are
able to provide derivations for certain readings of sentences with intensional
verbs and complex direct objects that are not derivable in current purely
categorial accounts of the syntax-semantics interface.
</dc:description>
 <dc:description>Comment: 41 pages, uses lingmacros.sty, fullname.sty, lfgmacros.tex,
  tree-dvips.sty, tree-dvips.pro and attached macros. Extends and revises
  cmp-lg/9404009 and cmp-lg/9404010</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Statistical Decision-Tree Models for Parsing</dc:title>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Syntactic natural language parsers have shown themselves to be inadequate for
processing highly-ambiguous large-vocabulary text, as is evidenced by their
poor performance on domains like the Wall Street Journal, and by the movement
away from parsing-based approaches to text-processing in general. In this
paper, I describe SPATTER, a statistical parser based on decision-tree learning
techniques which constructs a complete parse for every sentence and achieves
accuracy rates far better than any published result. This work is based on the
following premises: (1) grammars are too complex and detailed to develop
manually for most interesting domains; (2) parsing models must rely heavily on
lexical and contextual information to analyze sentences accurately; and (3)
existing {$n$}-gram modeling techniques are inadequate for parsing models. In
experiments comparing SPATTER with IBM's computer manuals parser, SPATTER
significantly outperforms the grammar-based parser. Evaluating SPATTER against
the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures,
SPATTER achieves 86\% precision, 86\% recall, and 1.3 crossing brackets per
sentence for sentences of 40 words or less, and 91\% precision, 90\% recall,
and 0.5 crossing brackets for sentences between 10 and 20 words in length.
</dc:description>
 <dc:description>Comment: uses aclap.sty, psfig.tex (v1.9), postscript figures</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504030</dc:identifier>
 <dc:identifier>Proceedings of the 33rd Annual Meeting of the ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Error-tolerant Finite State Recognition with Applications to
  Morphological Analysis and Spelling Correction</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Error-tolerant recognition enables the recognition of strings that deviate
mildly from any string in the regular set recognized by the underlying finite
state recognizer. Such recognition has applications in error-tolerant
morphological processing, spelling correction, and approximate string matching
in information retrieval. After a description of the concepts and algorithms
involved, we give examples from two applications: In the context of
morphological analysis, error-tolerant recognition allows misspelled input word
forms to be corrected, and morphologically analyzed concurrently. We present an
application of this to error-tolerant analysis of agglutinative morphology of
Turkish words. The algorithm can be applied to morphological analysis of any
language whose morphology is fully captured by a single (and possibly very
large) finite state transducer, regardless of the word formation processes and
morphographemic phenomena involved. In the context of spelling correction,
error-tolerant recognition can be used to enumerate correct candidate forms
from a given misspelled string within a certain edit distance. Again, it can be
applied to any language with a word list comprising all inflected forms, or
whose morphology is fully described by a finite state transducer. We present
experimental results for spelling correction for a number of languages. These
results indicate that such recognition works very efficiently for candidate
generation in spelling correction for many European languages such as English,
Dutch, French, German, Italian (and others) with very large word lists of root
and inflected forms (some containing well over 200,000 forms), generating all
candidate solutions within 10 to 45 milliseconds (with edit distance 1) on a
SparcStation 10/41. For spelling correction in Turkish, error-tolerant
</dc:description>
 <dc:description>Comment: Replaces 9504031. gzipped, uuencoded postscript file. To appear in
  Computational Linguistics Volume 22 No:1, 1996, Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/clpaper9512.ps.z</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:date>1995-07-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Replace Operator</dc:title>
 <dc:creator>Karttunen, Lauri</dc:creator>
 <dc:creator>Centre, Rank Xerox Research</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces to the calculus of regular expressions a replace
operator, -&gt;, and defines a set of replacement expressions that concisely
encode several alternate variations of the operation.
  The basic case is unconditional obligatory replacement:
  UPPER -&gt; LOWER
  Conditional versions of replacement, such as,
  UPPER -&gt; LOWER || LEFT _ RIGHT constrain the operation by left and right
contexts. UPPER, LOWER, LEFT, and RIGHT may be regular expressions of any
complexity.
  Replace expressions denote regular relations. The replace operator is defined
in terms of other regular expression operators using techniques introduced by
Ronald M. Kaplan and Martin Kay in &quot;Regular Models of Phonological Rule
Systems&quot; (Computational Linguistics 20:3 331-378. 1994).
</dc:description>
 <dc:description>Comment: To appear in the Proceedings of ACL-95. 8 pages, PostScript</dc:description>
 <dc:date>1995-04-29</dc:date>
 <dc:date>1995-05-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Corpus Statistics Meet the Noun Compound: Some Empirical Results</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A variety of statistical methods for noun compound analysis are implemented
and compared. The results support two main conclusions. First, the use of
conceptual association not only enables a broad coverage, but also improves the
accuracy. Second, an analysis model based on dependency grammar is
substantially more accurate than one based on deepest constituents, even though
the latter is more prevalent in the literature.
</dc:description>
 <dc:description>Comment: 8 pages, 5 figures, uses modified version of aclap.sty, replaced
  because old version failed to TeX properly</dc:description>
 <dc:date>1995-04-28</dc:date>
 <dc:date>1996-09-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504033</dc:identifier>
 <dc:identifier>Proceedings of the 33rd Annual Meeting of the Association for
  Computational Linguistics, Boston, MA., 1995 pp47-54</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9504034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bayesian Grammar Induction for Language Modeling</dc:title>
 <dc:creator>Chen, Stanley F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a corpus-based induction algorithm for probabilistic context-free
grammars. The algorithm employs a greedy heuristic search within a Bayesian
framework, and a post-pass using the Inside-Outside algorithm. We compare the
performance of our algorithm to n-gram models and the Inside-Outside algorithm
in three language modeling tasks. In two of the tasks, the training data is
generated by a probabilistic context-free grammar and in both tasks our
algorithm outperforms the other techniques. The third task involves
naturally-occurring data, and in this task our algorithm does not perform as
well as n-gram models but vastly outperforms the Inside-Outside algorithm.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, uses aclap.sty</dc:description>
 <dc:date>1995-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9504034</dc:identifier>
 <dc:identifier>Proc. 33rd Annual Meeting of the ACL, p. 228-235, Cambridge, MA
  1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Response Generation in Collaborative Negotiation</dc:title>
 <dc:creator>Chu-Carroll, Jennifer</dc:creator>
 <dc:creator>Carberry, Sandra</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In collaborative planning activities, since the agents are autonomous and
heterogeneous, it is inevitable that conflicts arise in their beliefs during
the planning process. In cases where such conflicts are relevant to the task at
hand, the agents should engage in collaborative negotiation as an attempt to
square away the discrepancies in their beliefs. This paper presents a
computational strategy for detecting conflicts regarding proposed beliefs and
for engaging in collaborative negotiation to resolve the conflicts that warrant
resolution. Our model is capable of selecting the most effective aspect to
address in its pursuit of conflict resolution in cases where multiple conflicts
arise, and of selecting appropriate evidence to justify the need for such
modification. Furthermore, by capturing the negotiation process in a recursive
Propose-Evaluate-Modify cycle of actions, our model can successfully handle
embedded negotiation subdialogues.
</dc:description>
 <dc:description>Comment: 9 pages, 1 Postscript figure, requires aclap.sty and epsf.sty. To
  appear in ACL-95</dc:description>
 <dc:date>1995-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>New Techniques for Context Modeling</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:creator>Thomas, Robert G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce three new techniques for statistical language models: extension
modeling, nonmonotonic contexts, and the divergence heuristic. Together these
techniques result in language models that have few states, even fewer
parameters, and low message entropies. For example, our techniques achieve a
message entropy of 1.97 bits/char on the Brown corpus using only 89,325
parameters. In contrast, the character 4-gram model requires more than 250
times as many parameters in order to achieve a message entropy of only 2.47
bits/char. The fact that our model performs significantly better while using
vastly fewer parameters indicates that it is a better probability model of
natural language text.
</dc:description>
 <dc:description>Comment: 8 pages, to appear in Proc. ACL 1995</dc:description>
 <dc:date>1995-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compiling HPSG type constraints into definite clause programs</dc:title>
 <dc:creator>Goetz, Thilo</dc:creator>
 <dc:creator>Meurers, Walt Detmar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a new approach to HPSG processing: compiling HPSG grammars
expressed as type constraints into definite clause programs. This provides a
clear and computationally useful correspondence between linguistic theories and
their implementation. The compiler performs off-line constraint inheritance and
code optimization. As a result, we are able to efficiently process with HPSG
grammars without having to hand-translate them into definite clause or phrase
structure based systems.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded compressed postscript</dc:description>
 <dc:date>1995-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505003</dc:identifier>
 <dc:identifier>Proceedings of ACL-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DATR Theories and DATR Models</dc:title>
 <dc:creator>Keller, Bill</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Evans and Gazdar introduced DATR as a simple, non-monotonic language for
representing natural language lexicons. Although a number of implementations of
DATR exist, the full language has until now lacked an explicit, declarative
semantics. This paper rectifies the situation by providing a mathematical
semantics for DATR. We present a view of DATR as a language for defining
certain kinds of partial functions by cases. The formal model provides a
transparent treatment of DATR's notion of global context. It is shown that
DATR's default mechanism can be accounted for by interpreting value descriptors
as families of values indexed by paths.
</dc:description>
 <dc:description>Comment: 8 pages, LATEX, uses aclap.sty, Procs. ACL95</dc:description>
 <dc:date>1995-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Syntactic Rules and Tags with Genetic Algorithms for
  Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules</dc:title>
 <dc:creator>Losee, Robert M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The grammars of natural languages may be learned by using genetic algorithms
that reproduce and mutate grammatical rules and part-of-speech tags, improving
the quality of later generations of grammatical components. Syntactic rules are
randomly generated and then evolve; those rules resulting in improved parsing
and occasionally improved retrieval and filtering performance are allowed to
further propagate. The LUST system learns the characteristics of the language
or sublanguage used in document abstracts by learning from the document
rankings obtained from the parsed abstracts. Unlike the application of
traditional linguistic rules to retrieval and filtering applications, LUST
develops grammatical structures and tags without the prior imposition of some
common grammatical assumptions (e.g., part-of-speech assumptions), producing
grammars that are empirically based and are optimized for this particular
application.
</dc:description>
 <dc:description>Comment: latex document, postscript figures not included. Accepted for
  publication in Information Processing and Management</dc:description>
 <dc:date>1995-05-02</dc:date>
 <dc:date>1995-05-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Treating Coordination with Datalog Grammars</dc:title>
 <dc:creator>Dahl, Veronica</dc:creator>
 <dc:creator>Tarau, Paul</dc:creator>
 <dc:creator>Moreno, Lidia</dc:creator>
 <dc:creator>Palomar, Manolo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In previous work we studied a new type of DCGs, Datalog grammars, which are
inspired on database theory. Their efficiency was shown to be better than that
of their DCG counterparts under (terminating) OLDT-resolution. In this article
we motivate a variant of Datalog grammars which allows us a meta-grammatical
treatment of coordination. This treatment improves in some respects over
previous work on coordination in logic grammars, although more research is
needed for testing it in other respects.
</dc:description>
 <dc:date>1995-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing a Flexible Word Order Language</dc:title>
 <dc:creator>Pericliev, Vladimir</dc:creator>
 <dc:creator>Grigorov, Alexander</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A logic formalism is presented which increases the expressive power of the
ID/LP format of GPSG by enlarging the inventory of ordering relations and
extending the domain of their application to non-siblings. This allows a
concise, modular and declarative statement of intricate word order
regularities.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, in proceedings of COLING-94</dc:description>
 <dc:date>1995-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Conciseness through Aggregation in Text Generation</dc:title>
 <dc:creator>Shaw, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Aggregating different pieces of similar information is necessary to generate
concise and easy to understand reports in technical domains. This paper
presents a general algorithm that combines similar messages in order to
generate one or more coherent sentences for them. The process is not as trivial
as might be expected. Problems encountered are briefly described.
</dc:description>
 <dc:description>Comment: 3 pages, LaTex, uses aclap.sty, student sessions of ACL95</dc:description>
 <dc:date>1995-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505008</dc:identifier>
 <dc:identifier>student sessions ACL 95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compilation of HPSG to TAG</dc:title>
 <dc:creator>Kasper, Robert</dc:creator>
 <dc:creator>Kiefer, Bernd</dc:creator>
 <dc:creator>Netter, Klaus</dc:creator>
 <dc:creator>Vijay-Shanker, K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an implemented compilation algorithm that translates HPSG into
lexicalized feature-based TAG, relating concepts of the two theories. While
HPSG has a more elaborated principle-based theory of possible phrase
structures, TAG provides the means to represent lexicalized structures more
explicitly. Our objectives are met by giving clear definitions that determine
the projection of structures from the lexicon, and identify maximal
projections, auxiliary trees and foot nodes.
</dc:description>
 <dc:description>Comment: 8 pages, aclap.sty, appears in ACL-95, Note: LaTeX'ing this file
  requires a modified LaTeX with an increased &quot;semantic nest size.&quot;</dc:description>
 <dc:date>1995-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagset Reduction Without Information Loss</dc:title>
 <dc:creator>Brants, Thorsten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A technique for reducing a tagset used for n-gram part-of-speech
disambiguation is introduced and evaluated in an experiment. The technique
ensures that all information that is provided by the original tagset can be
restored from the reduced one. This is crucial, since we are interested in the
linguistically motivated tags for part-of-speech disambiguation. The reduced
tagset needs fewer parameters for its statistical model and allows more
accurate parameter estimation. Additionally, there is a slight but not
significant improvement of tagging accuracy.
</dc:description>
 <dc:description>Comment: 3 pages, LaTeX, to appear in proceedings of ACL-95, student session</dc:description>
 <dc:date>1995-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluation of Semantic Clusters</dc:title>
 <dc:creator>Agarwal, Rajeev</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Semantic clusters of a domain form an important feature that can be useful
for performing syntactic and semantic disambiguation. Several attempts have
been made to extract the semantic clusters of a domain by probabilistic or
taxonomic techniques. However, not much progress has been made in evaluating
the obtained semantic clusters. This paper focuses on an evaluation mechanism
that can be used to evaluate semantic clusters produced by a system against
those provided by human experts.
</dc:description>
 <dc:description>Comment: 3 pages; Self-contained LaTeX; Uses aclap.sty; To appear in ACL-95
  student session</dc:description>
 <dc:date>1995-05-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Symbolic and Surgical Acquisition of Terms through Variation</dc:title>
 <dc:creator>Jacquemin, Christian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Terminological acquisition is an important issue in learning for NLP due to
the constant terminological renewal through technological changes. Terms play a
key role in several NLP-activities such as machine translation, automatic
indexing or text understanding. In opposition to classical once-and-for-all
approaches, we propose an incremental process for terminological enrichment
which operates on existing reference lists and large corpora. Candidate terms
are acquired by extracting variants of reference terms through {\em FASTR}, a
unification-based partial parser. As acquisition is performed within specific
morpho-syntactic contexts (coordinations, insertions or permutations of
compounds), rich conceptual links are learned together with candidate terms. A
clustering of terms related through coordination yields classes of conceptually
close terms while graphs resulting from insertions denote generic/specific
relations. A graceful degradation of the volume of acquisition on partial
initial lists confirms the robustness of the method to incomplete data.
</dc:description>
 <dc:description>Comment: 8 pages compressed uuencoded latex, uses aaai.sty, 1 figure .eps To
  appear in Proceedings Workshop &quot;New Approaches to Learning for NLP&quot; at
  IJCAI'95</dc:description>
 <dc:date>1995-05-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Utilizing Statistical Dialogue Act Processing in Verbmobil</dc:title>
 <dc:creator>Reithinger, Norbert</dc:creator>
 <dc:creator>Maier, Elisabeth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we present a statistical approach for dialogue act processing
in the dialogue component of the speech-to-speech translation system \vm.
Statistics in dialogue processing is used to predict follow-up dialogue acts.
As an application example we show how it supports repair when unexpected
dialogue states occur.
</dc:description>
 <dc:description>Comment: 6 pages; compressed and uuencoded postscript file; to appear in
  ACL-95</dc:description>
 <dc:date>1995-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compositionality for Presuppositions over Tableaux</dc:title>
 <dc:creator>Gervas, Pablo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Tableaux originate as a decision method for a logical language. They can also
be extended to obtain a structure that spells out all the information in a set
of sentences in terms of truth value assignments to atomic formulas that appear
in them. This approach is pursued here. Over such a structure, compositional
rules are provided for obtaining the presuppositions of a logical statement
from its atomic subformulas and their presuppositions. The rules are based on
classical logic semantics and they are shown to model the behaviour of
presuppositions observed in natural language sentences built with {\em if
\ldots then}, {\em and} and {\em or}. The advantages of this method over
existing frameworks for presuppositions are discussed.
</dc:description>
 <dc:date>1995-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Analysis of Complex Diagrams using Constraint-Based Parsing</dc:title>
 <dc:creator>Futrelle, Robert P.</dc:creator>
 <dc:creator>Nikolakis, Nikos</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes substantial advances in the analysis (parsing) of
diagrams using constraint grammars. The addition of set types to the grammar
and spatial indexing of the data make it possible to efficiently parse real
diagrams of substantial complexity. The system is probably the first to
demonstrate efficient diagram parsing using grammars that easily be retargeted
to other domains. The work assumes that the diagrams are available as a flat
collection of graphics primitives: lines, polygons, circles, Bezier curves and
text. This is appropriate for future electronic documents or for vectorized
diagrams converted from scanned images. The classes of diagrams that we have
analyzed include x,y data graphs and genetic diagrams drawn from the biological
literature, as well as finite state automata diagrams (states and arcs). As an
example, parsing a four-part data graph composed of 133 primitives required 35
sec using Macintosh Common Lisp on a Macintosh Quadra 700.
</dc:description>
 <dc:description>Comment: 9 pages, Postscript, no fonts, compressed, uuencoded. Composed in
  MSWord 5.1a for the Mac. To appear in ICDAR '95. Other versions at
  ftp://ftp.ccs.neu.edu/pub/people/futrelle</dc:description>
 <dc:date>1995-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Pattern Matching method for finding Noun and Proper Noun Translations
  from Noisy Parallel Corpora</dc:title>
 <dc:creator>Fung, Pascale</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a pattern matching method for compiling a bilingual lexicon of
nouns and proper nouns from unaligned, noisy parallel texts of
Asian/Indo-European language pairs. Tagging information of one language is
used. Word frequency and position information for high and low frequency words
are represented in two different vector forms for pattern matching. New anchor
point finding and noise elimination techniques are introduced. We obtained a
73.1\% precision. We also show how the results can be used in the compilation
of domain-specific noun phrases.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded compressed postscript file. To appear in the
  Proceedings of the 33rd ACL</dc:description>
 <dc:date>1995-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Robust Parsing of Spoken Dialogue Using Contextual Knowledge and
  Recognition Probabilities</dc:title>
 <dc:creator>Hanrieder, Gerhard</dc:creator>
 <dc:creator>Goerz, Guenther</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe the linguistic processor of a spoken dialogue
system. The parser receives a word graph from the recognition module as its
input. Its task is to find the best path through the graph. If no complete
solution can be found, a robust mechanism for selecting multiple partial
results is applied. We show how the information content rate of the results can
be improved if the selection is based on an integrated quality score combining
word recognition scores and context-dependent semantic predictions. Results of
parsing word graphs with and without predictions are reported.
</dc:description>
 <dc:description>Comment: 4 pages, LaTex source, 3 PostScript figures, uses epsf.sty and
  ETRW.sty, to appear in Proceedings of ESCA Workshop on Spoken Dialogue
  Systems, Denmark, May 30-June 2</dc:description>
 <dc:date>1995-05-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Acquiring a Lexicon from Unsegmented Speech</dc:title>
 <dc:creator>de Marcken, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present work-in-progress on the machine acquisition of a lexicon from
sentences that are each an unsegmented phone sequence paired with a primitive
representation of meaning. A simple exploratory algorithm is described, along
with the direction of current work and a discussion of the relevance of the
problem for child language acquisition and computer speech recognition.
</dc:description>
 <dc:description>Comment: Postscript, 3-pages, to appear in ACL '95 Student Session</dc:description>
 <dc:date>1995-05-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Measuring semantic complexity</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We define {\em semantic complexity} using a new concept of {\em meaning
automata}. We measure the semantic complexity of understanding of prepositional
phrases, of an &quot;in depth understanding system&quot;, and of a natural language
interface to an on-line calendar. We argue that it is possible to measure some
semantic complexities of natural language processing systems before building
them, and that systems that exhibit relatively complex behavior can be built
from semantically simple components.
</dc:description>
 <dc:description>Comment: 11 pp. Latex.. To appear in Proc. BISFAI'95, The Fourth Bar-Ilan
  Symposium on Foundations of Artificial Intelligence, June 20-22, 1995,
  Ramat-Gan and Jerusalem, Israel. Correspondence to wlodz@watson.ibm.com</dc:description>
 <dc:date>1995-05-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>CRYSTAL: Inducing a Conceptual Dictionary</dc:title>
 <dc:creator>Soderland, Stephen</dc:creator>
 <dc:creator>Fisher, David</dc:creator>
 <dc:creator>Aseltine, Jonathan</dc:creator>
 <dc:creator>Lehnert, Wendy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  One of the central knowledge sources of an information extraction system is a
dictionary of linguistic patterns that can be used to identify the conceptual
content of a text. This paper describes CRYSTAL, a system which automatically
induces a dictionary of &quot;concept-node definitions&quot; sufficient to identify
relevant information from a training corpus. Each of these concept-node
definitions is generalized as far as possible without producing errors, so that
a minimum number of dictionary entries cover the positive training instances.
Because it tests the accuracy of each proposed definition, CRYSTAL can often
surpass human intuitions in creating reliable extraction rules.
</dc:description>
 <dc:description>Comment: 6 pages, Postscript, IJCAI-95
  http://ciir.cs.umass.edu/info/psfiles/tepubs/tepubs.html</dc:description>
 <dc:date>1995-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improving the Efficiency of a Generation Algorithm for Shake and Bake
  Machine Translation Using Head-Driven Phrase Structure Grammar</dc:title>
 <dc:creator>Popowich, Fred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A Shake and Bake machine translation algorithm for Head-Driven Phrase
Structure Grammar is introduced based on the algorithm proposed by Whitelock
for unification categorial grammar. The translation process is then analysed to
determine where the potential sources of inefficiency reside, and some
proposals are introduced which greatly improve the efficiency of the generation
algorithm. Preliminary empirical results from tests involving a small grammar
are presented, and suggestions for greater improvement to the algorithm are
provided.
</dc:description>
 <dc:description>Comment: 12 pages, uuencoded and compressed Postscript. To appear in
  Proceedings of the Fifth International Workshop on Natural Language and Logic
  Programming (NLULP5)</dc:description>
 <dc:date>1995-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating One-Anaphoric Expressions: Where Does the Decision Lie?</dc:title>
 <dc:creator>Dale, Robert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most natural language generation systems embody mechanisms for choosing
whether to subsequently refer to an already-introduced entity by means of a
pronoun or a definite noun phrase. Relatively few systems, however, consider
referring to entites by means of one-anaphoric expressions such as
\lingform{the small green one}. This paper looks at what is involved in
generating referring expressions of this type. Consideration of how to fit this
capability into a standard algorithm for referring expression generation leads
us to suggest a revision of some of the assumptions that underlie existing
approaches. We demonstrate the usefulness of our approach to one-anaphora
generation in the context of a simple database interface application, and make
some observations about the impact of this approach on referring expression
generation more generally.
</dc:description>
 <dc:description>Comment: 10 pages, compressed PS file; This paper appears in the Working
  Papers of Pacling-II, Brisbane, Australia, April 19--22 1995.</dc:description>
 <dc:date>1995-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Some Novel Applications of Explanation-Based Learning to Parsing
  Lexicalized Tree-Adjoining Grammars</dc:title>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:creator>Joshi, Aravind</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present some novel applications of Explanation-Based
Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars. The
novel aspects are (a) immediate generalization of parses in the training set,
(b) generalization over recursive structures and (c) representation of
generalized parses as Finite State Transducers. A highly impoverished parser
called a ``stapler'' has also been introduced. We present experimental results
using EBL for different corpora and architectures to show the effectiveness of
our approach.
</dc:description>
 <dc:description>Comment: uuencoded postscript file</dc:description>
 <dc:date>1995-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505023</dc:identifier>
 <dc:identifier>ACL 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Exploring the role of Punctuation in Parsing Natural Text</dc:title>
 <dc:creator>Jones, Bernard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Few, if any, current NLP systems make any significant use of punctuation.
Intuitively, a treatment of punctuation seems necessary to the analysis and
production of text. Whilst this has been suggested in the fields of discourse
structure, it is still unclear whether punctuation can help in the syntactic
field. This investigation attempts to answer this question by parsing some
corpus-based material with two similar grammars --- one including rules for
punctuation, the other ignoring it. The punctuated grammar significantly
out-performs the unpunctuated one, and so the conclusion is that punctuation
can play a useful role in syntactic processing.
</dc:description>
 <dc:description>Comment: 5 pages, LaTeX (2.09 preferred), COLING-94 paper</dc:description>
 <dc:date>1995-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Multiple Knowledge Sources for Discourse Segmentation</dc:title>
 <dc:creator>Litman, Diane J.</dc:creator>
 <dc:creator>Passonneau, Rebecca J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We predict discourse segment boundaries from linguistic features of
utterances, using a corpus of spoken narratives as data. We present two methods
for developing segmentation algorithms from training data: hand tuning and
machine learning. When multiple types of features are used, results approach
human performance on an independent test set (both methods), and using
cross-validation (machine learning).
</dc:description>
 <dc:description>Comment: 8 pages. Self-contained latex source. To appear in Proceedings of the
  33rd ACL, 1995. (This replacement version revised so that no lines exceed 80
  characters.)</dc:description>
 <dc:date>1995-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging the Teleman Corpus</dc:title>
 <dc:creator>Brants, Thorsten</dc:creator>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Experiments were carried out comparing the Swedish Teleman and the English
Susanne corpora using an HMM-based and a novel reductionistic statistical
part-of-speech tagger. They indicate that tagging the Teleman corpus is the
more difficult task, and that the performance of the two different taggers is
comparable.
</dc:description>
 <dc:description>Comment: 14 pages, LaTeX, to appear in Proceedings of the 10th Nordic
  Conference of Computational Linguistics, Helsinki, Finland, 1995</dc:description>
 <dc:date>1995-05-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Quantifier Scope and Constituency</dc:title>
 <dc:creator>Park, Jong C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Traditional approaches to quantifier scope typically need stipulation to
exclude readings that are unavailable to human understanders. This paper shows
that quantifier scope phenomena can be precisely characterized by a semantic
representation constrained by surface constituency, if the distinction between
referential and quantificational NPs is properly observed. A CCG implementation
is described and compared to other approaches.
</dc:description>
 <dc:description>Comment: 8 pages, compressed and uuencoded postscript file, ACL-95</dc:description>
 <dc:date>1995-05-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>D-Tree Grammars</dc:title>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:creator>Vijay-Shanker, K.</dc:creator>
 <dc:creator>Weir, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  DTG are designed to share some of the advantages of TAG while overcoming some
of its limitations. DTG involve two composition operations called subsertion
and sister-adjunction. The most distinctive feature of DTG is that, unlike TAG,
there is complete uniformity in the way that the two DTG operations relate
lexical items: subsertion always corresponds to complementation and
sister-adjunction to modification. Furthermore, DTG, unlike TAG, can provide a
uniform analysis for em wh-movement in English and Kashmiri, despite the fact
that the em wh element in Kashmiri appears in sentence-second position, and not
sentence-initial position as in English.
</dc:description>
 <dc:description>Comment: Latex source, needs aclap.sty, 8 pages, to appear in ACL-95</dc:description>
 <dc:date>1995-05-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Mapping Scrambled Korean Sentences into English Using Synchronous TAGs</dc:title>
 <dc:creator>Park, Hyun S.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Synchronous Tree Adjoining Grammars can be used for Machine Translation.
However, translating a free order language such as Korean to English is
complicated. I present a mechanism to translate scrambled Korean sentences into
English by combining the concepts of Multi-Component TAGs (MC-TAGs) and
Synchronous TAGs (STAGs).
</dc:description>
 <dc:description>Comment: uuencoded compressed ps file. 3 pages. To appear ACL95</dc:description>
 <dc:date>1995-05-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Encoding Lexicalized Tree Adjoining Grammars with a Nonmonotonic
  Inheritance Hierarchy</dc:title>
 <dc:creator>Evans, Roger</dc:creator>
 <dc:creator>Gazdar, Gerald</dc:creator>
 <dc:creator>Weir, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows how DATR, a widely used formal language for lexical
knowledge representation, can be used to define an LTAG lexicon as an
inheritance hierarchy with internal lexical rules. A bottom-up featural
encoding is used for LTAG trees and this allows lexical rules to be implemented
as covariation constraints within feature structures. Such an approach
eliminates the considerable redundancy otherwise associated with an LTAG
lexicon.
</dc:description>
 <dc:description>Comment: Latex source, needs aclap.sty, 8 pages</dc:description>
 <dc:date>1995-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505030</dc:identifier>
 <dc:identifier>Proceedings of ACL95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505031</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Compactness of Construction Grammars</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an argument for {\em construction grammars} based on the minimum
description length (MDL) principle (a formal version of the Ockham Razor). The
argument consists in using linguistic and computational evidence in setting up
a formal model, and then applying the MDL principle to prove its superiority
with respect to alternative models. We show that construction-based
representations are at least an order of magnitude more compact that the
corresponding lexicalized representations of the same linguistic data.
  The result is significant for our understanding of the relationship between
syntax and semantics, and consequently for choosing NLP architectures. For
instance, whether the processing should proceed in a pipeline from syntax to
semantics to pragmatics, and whether all linguistic information should be
combined in a set of constraints. From a broader perspective, this paper does
not only argue for a certain model of processing, but also provides a
methodology for determining advantages of different approaches to NLP.
</dc:description>
 <dc:description>Comment: 10 pp. Latex. Correspondence to wlodz@watson.ibm.com</dc:description>
 <dc:date>1995-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505032</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Context and ontology in understanding of dialogs</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a model of NLP in which ontology and context are directly included
in a grammar. The model is based on the concept of {\em construction},
consisting of a set of features of form, a set of semantic and pragmatic
conditions describing its application context, and a description of its
meaning. In this model ontology is embedded into the grammar; e.g. the
hierarchy of {\it np} constructions is based on the corresponding ontology.
Ontology is also used in defining contextual parameters; e.g. $\left[
current\_question \ time(\_) \right] $.
  A parser based on this model allowed us to build a set of dialog
understanding systems that include an on-line calendar, a banking machine, and
an insurance quote system. The proposed approach is an alternative to the
standard &quot;pipeline&quot; design of morphology-syntax-semantics-pragmatics; the
account of meaning conforms to our intuitions about compositionality, but there
is no homomorphism from syntax to semantics.
</dc:description>
 <dc:description>Comment: 7 pp. Latex (documentstyle[ijcai89,named]). Proc. IJCAI'95 Workshop
  on Context in NLP. Montreal, Aug.1995. Correspondence to wlodz@watson.ibm.com</dc:description>
 <dc:date>1995-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>User-Defined Nonmonotonicity in Unification-Based Formalisms</dc:title>
 <dc:creator>Stromback, Lena</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A common feature of recent unification-based grammar formalisms is that they
give the user the ability to define his own structures. However, this
possibility is mostly limited and does not include nonmonotonic operations. In
this paper we show how nonmonotonic operations can also be user-defined by
applying default logic (Reiter 1980) and generalizing previous results on
nonmonotonic sorts (Young &amp; Rounds 1993).
</dc:description>
 <dc:description>Comment: 7 pages, uses aclap.sty and acl.bst</dc:description>
 <dc:date>1995-05-16</dc:date>
 <dc:date>1995-05-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505033</dc:identifier>
 <dc:identifier>ACL-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semantic Ambiguity and Perceived Ambiguity</dc:title>
 <dc:creator>Poesio, Massimo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I explore some of the issues that arise when trying to establish a connection
between the underspecification hypothesis pursued in the NLP literature and
work on ambiguity in semantics and in the psychological literature. A theory of
underspecification is developed `from the first principles', i.e., starting
from a definition of what it means for a sentence to be semantically ambiguous
and from what we know about the way humans deal with ambiguity. An
underspecified language is specified as the translation language of a grammar
covering sentences that display three classes of semantic ambiguity: lexical
ambiguity, scopal ambiguity, and referential ambiguity. The expressions of this
language denote sets of senses. A formalization of defeasible reasoning with
underspecified representations is presented, based on Default Logic. Some
issues to be confronted by such a formalization are discussed.
</dc:description>
 <dc:description>Comment: Latex, 47 pages. Uses tree-dvips.sty, lingmacros.sty, fullname.sty</dc:description>
 <dc:date>1995-05-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505034</dc:identifier>
 <dc:identifier>K. van Deemter and S. Peters (eds.), Semantic ambiguity and
  Underspecification, CSLI, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505035</identifier>
 <datestamp>2016-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Development of a Spanish Version of the Xerox Tagger</dc:title>
 <dc:creator>Le&#xf3;n, Fernando S&#xe1;nchez</dc:creator>
 <dc:creator>Serrano, Amalio F. Nieto</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes work performed withing the CRATER ({\em C}orpus {\em
R}esources {\em A}nd {\em T}erminology {\em E}xt{\em R}action, MLAP-93/20)
project, funded by the Commission of the European Communities. In particular,
it addresses the issue of adapting the Xerox Tagger to Spanish in order to tag
the Spanish version of the ITU (International Telecommunications Union) corpus.
The model implemented by this tagger is briefly presented along with some
modifications performed on it in order to use some parameters not
probabilistically estimated. Initial decisions, like the tagset, the lexicon
and the training corpus are also discussed. Finally, results are presented and
the benefits of the {\em mixed model} justified.
</dc:description>
 <dc:description>Comment: 13 pages</dc:description>
 <dc:date>1995-05-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505035</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating Gricean and Attentional Constraints</dc:title>
 <dc:creator>Passonneau, Rebecca J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper concerns how to generate and understand discourse anaphoric noun
phrases. I present the results of an analysis of all discourse anaphoric noun
phrases (N=1,233) in a corpus of ten narrative monologues, where the choice
between a definite pronoun or phrasal NP conforms largely to Gricean
constraints on informativeness. I discuss Dale and Reiter's [To appear] recent
model and show how it can be augmented for understanding as well as generating
the range of data presented here. I argue that integrating centering [Grosz et
al., 1983] [Kameyama, 1985] with this model can be applied uniformly to
discourse anaphoric pronouns and phrasal NPs. I conclude with a hypothesis for
addressing the interaction between local and global discourse processing.
</dc:description>
 <dc:description>Comment: 7 pages. Self-contained latex source. Uses ijcai95.sty and named.bst.
  To appear in IJCAI 1995</dc:description>
 <dc:date>1995-05-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505036</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Identifying Word Translations in Non-Parallel Texts</dc:title>
 <dc:creator>Rapp, Reinhard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Common algorithms for sentence and word-alignment allow the automatic
identification of word translations from parallel texts. This study suggests
that the identification of word translations should also be possible with
non-parallel and even unrelated texts. The method proposed is based on the
assumption that there is a correlation between the patterns of word
co-occurrences in texts of different languages.
</dc:description>
 <dc:description>Comment: 3 pages, requires aclap.sty and epic.sty</dc:description>
 <dc:date>1995-05-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505037</dc:identifier>
 <dc:identifier>Proceedings of ACL-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ubiquitous Talker: Spoken Language Interaction with Real World Objects</dc:title>
 <dc:creator>Nagao, Katashi</dc:creator>
 <dc:creator>Rekimoto, Jun</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Augmented reality is a research area that tries to embody an electronic
information space within the real world, through computational devices. A
crucial issue within this area, is the recognition of real world objects or
situations.
  In natural language processing, it is much easier to determine
interpretations of utterances, even if they are ill-formed, when the context or
situation is fixed. We therefore introduce robust, natural language processing
into a system of augmented reality with situation awareness. Based on this
idea, we have developed a portable system, called the Ubiquitous Talker. This
consists of an LCD display that reflects the scene at which a user is looking
as if it is a transparent glass, a CCD camera for recognizing real world
objects with color-bar ID codes, a microphone for recognizing a human voice and
a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its
user with some information related to a recognized object, by using the display
and voice. It also accepts requests or questions as voice inputs. The user
feels as if he/she is talking with the object itself through the system.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX file with PostScript files, to appear in Proc.
  IJCAI-95, also available from http://www.csl.sony.co.jp/person/nagao.html</dc:description>
 <dc:date>1995-05-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505038</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505039</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating efficient belief models for task-oriented dialogues</dc:title>
 <dc:creator>Taylor, Jasper</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We have shown that belief modelling for dialogue can be simplified if the
assumption is made that the participants are cooperating, i.e., they are not
committed to any goals requiring deception. In such domains, there is no need
to maintain individual representations of deeply nested beliefs; instead, three
specific types of belief can be used to summarize all the states of nested
belief that can exist about a domain entity.
  Here, we set out to design a ``compiler'' for belief models. This system will
accept as input a description of agents' interactions with a task domain
expressed in a fully-expressive belief logic with non-monotonic and temporal
extensions. It generates an operational belief model for use in that domain,
sufficient for the requirements of cooperative dialogue, including the
negotiation of complex domain plans. The compiled model incorporates the belief
simplification mentioned above, and also uses a simplified temporal logic of
belief based on the restricted circumstances under which beliefs can change.
  We shall review the motivation for creating such a system, and introduce a
general procedure for taking a logical specification for a domain and procesing
it into an operational model. We shall then discuss the specific changes that
are made during this procedure for limiting the level of abstraction at which
the concepts of belief nesting, default reasoning and time are expressed.
Finally we shall go through a worked example relating to the Map Task, a simple
cooperative problem-solving exercise.
</dc:description>
 <dc:description>Comment: 17 pages, 1 postscript figure. Paper presented at CLNLP workshop,
  Edinburgh, April 3-5, 1995</dc:description>
 <dc:date>1995-05-23</dc:date>
 <dc:date>1995-08-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505039</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505040</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Text Chunking using Transformation-Based Learning</dc:title>
 <dc:creator>Ramshaw, Lance A.</dc:creator>
 <dc:creator>Marcus, Mitchell P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Eric Brill introduced transformation-based learning and showed that it can do
part-of-speech tagging with fairly high accuracy. The same method can be
applied at a higher level of textual interpretation for locating chunks in the
tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is
convenient to view chunking as a tagging problem by encoding the chunk
structure in new tags attached to each word. In automatic tests using
Treebank-derived data, this technique achieved recall and precision rates of
roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that
partition the sentence. Some interesting adaptations to the
transformation-based learning approach are also suggested by this application.
</dc:description>
 <dc:description>Comment: 13 pages, LaTeX2e, 1 included figure</dc:description>
 <dc:date>1995-05-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505040</dc:identifier>
 <dc:identifier>ACL Third Workshop on Very Large Corpora, June 1995, pp. 82-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505041</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Descriptive Complexity, Language Complexity, and GB</dc:title>
 <dc:creator>Rogers, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce $L^2_{K,P}$, a monadic second-order language for reasoning about
trees which characterizes the strongly Context-Free Languages in the sense that
a set of finite trees is definable in $L^2_{K,P}$ iff it is (modulo a
projection) a Local Set---the set of derivation trees generated by a CFG. This
provides a flexible approach to establishing language-theoretic complexity
results for formalisms that are based on systems of well-formedness constraints
on trees. We demonstrate this technique by sketching two such results for
Government and Binding Theory. First, we show that {\em free-indexation\/}, the
mechanism assumed to mediate a variety of agreement and binding relationships
in GB, is not definable in $L^2_{K,P}$ and therefore not enforcible by CFGs.
Second, we show how, in spite of this limitation, a reasonably complete GB
account of English can be defined in $L^2_{K,P}$. Consequently, the language
licensed by that account is strongly context-free. We illustrate some of the
issues involved in establishing this result by looking at the definition, in
$L^2_{K,P}$, of chains. The limitations of this definition provide some insight
into the types of natural linguistic principles that correspond to higher
levels of language complexity. We close with some speculation on the possible
significance of these results for generative linguistics.
</dc:description>
 <dc:description>Comment: To appear in Specifying Syntactic Structures, papers from the Logic,
  Structures, and Syntax workshop, Amsterdam, Sept. 1994. LaTeX source with
  nine included postscript figures</dc:description>
 <dc:date>1995-05-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505041</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505042</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Robust Parsing Based on Discourse Information: Completing partial parses
  of ill-formed sentences on the basis of discourse information</dc:title>
 <dc:creator>Nasukawa, Tetsuya</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In a consistent text, many words and phrases are repeatedly used in more than
one sentence. When an identical phrase (a set of consecutive words) is repeated
in different sentences, the constituent words of those sentences tend to be
associated in identical modification patterns with identical parts of speech
and identical modifiee-modifier relationships. Thus, when a syntactic parser
cannot parse a sentence as a unified structure, parts of speech and
modifiee-modifier relationships among morphologically identical words in
complete parses of other sentences within the same text provide useful
information for obtaining partial parses of the sentence. In this paper, we
describe a method for completing partial parses by maintaining consistency
among morphologically identical words within the same text as regards their
part of speech and their modifiee-modifier relationship. The experimental
results obtained by using this method with technical documents offer good
prospects for improving the accuracy of sentence analysis in a broad-coverage
natural language processing system such as a machine translation system.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of ACL-95, 8 pages, 4 Postscript figures,
  uses aclap.sty and epsbox.sty</dc:description>
 <dc:date>1995-05-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505042</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505043</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Decision Trees for Coreference Resolution</dc:title>
 <dc:creator>McCarthy, Joseph F.</dc:creator>
 <dc:creator>Lehnert, Wendy G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes RESOLVE, a system that uses decision trees to learn how
to classify coreferent phrases in the domain of business joint ventures. An
experiment is presented in which the performance of RESOLVE is compared to the
performance of a manually engineered set of rules for the same task. The
results show that decision trees achieve higher performance than the rules in
two of three evaluation metrics developed for the coreference task. In addition
to achieving better performance than the rules, RESOLVE provides a framework
that facilitates the exploration of the types of knowledge that are useful for
solving the coreference problem.
</dc:description>
 <dc:description>Comment: 6 pages; LaTeX source; 1 uuencoded compressed EPS file (separate);
  uses ijcai95.sty, named.bst, epsf.tex; to appear in Proc. IJCAI '95</dc:description>
 <dc:date>1995-05-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505043</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505044</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best
  Translation Lexicons</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows how to induce an N-best translation lexicon from a bilingual
text corpus using statistical properties of the corpus together with four
external knowledge sources. The knowledge sources are cast as filters, so that
any subset of them can be cascaded in a uniform framework. A new objective
evaluation measure is used to compare the quality of lexicons induced with
different filter cascades. The best filter cascades improve lexicon quality by
up to 137% over the plain vanilla statistical method, and approach human
performance. Drastically reducing the size of the training corpus has a much
smaller impact on lexicon quality when these knowledge sources are used. This
makes it practical to train on small hand-built corpora for language pairs
where large bilingual corpora are unavailable. Moreover, three of the four
filters prove useful even when used with large training corpora.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of the Third Workshop on Very Large Corpora,
  15 pages, uuencoded compressed PostScript</dc:description>
 <dc:date>1995-05-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505044</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9505045</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Hybrid Transfer in an English-French Spoken Language Translator</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Bouillon, Pierrette</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper argues the importance of high-quality translation for spoken
language translation systems. It describes an architecture suitable for rapid
development of high-quality limited-domain translation systems, which has been
implemented within an advanced prototype English to French spoken language
translator. The focus of the paper is the hybrid transfer model which combines
unification-based rules and a set of trainable statistical preferences;
roughly, rules encode domain-independent grammatical information and
preferences encode domain-dependent distributional information. The preferences
are trained from sets of examples produced by the system, which have been
annotated by human judges as correct or incorrect. An experiment is described
in which the model was tested on a 2000 utterance sample of previously unseen
data.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX (2.09 preferred); eaclap.sty; Procs of IA '95
  (Montpellier, France)</dc:description>
 <dc:date>1995-05-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9505045</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ma(r)king concessions in English and German</dc:title>
 <dc:creator>Grote, Brigitte</dc:creator>
 <dc:creator>Lenke, Nils</dc:creator>
 <dc:creator>Stede, Manfred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In order to generate cohesive discourse, many of the relations holding
between text segments need to be signalled to the reader by means of cue words,
or {\em discourse markers}. Programs usually do this in a simplistic way, e.g.,
by using one marker per relation. In reality, however, language offers a very
wide range of markers from which informed choices should be made. In order to
account for the variety and to identify the parameters governing the choices,
detailled linguistic analyses are necessary. We worked with one area of
discourse relations, the Concession family, identified its underlying
pragmatics and semantics, and undertook extensive corpus studies to examine the
range of markers used in both English and German. On the basis of an initial
classification of these markers, we propose a generation model for producing
bilingual text that can incorporate marker choice into its overall decision
framework.
</dc:description>
 <dc:description>Comment: 23 pages, uuencoded compressed postscript</dc:description>
 <dc:date>1995-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506001</dc:identifier>
 <dc:identifier>Proc. of the 6th European Workshop on Natural Language Generation,
  NL-Leiden, May 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Weak subsumption Constraints for Type Diagnosis: An Incremental
  Algorithm</dc:title>
 <dc:creator>Mueller, Martin</dc:creator>
 <dc:creator>Niehren, Joachim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce constraints necessary for type checking a higher-order
concurrent constraint language, and solve them with an incremental algorithm.
Our constraint system extends rational unification by constraints x$\subseteq$
y saying that ``$x$ has at least the structure of $y$'', modelled by a weak
instance relation between trees. This notion of instance has been carefully
chosen to be weaker than the usual one which renders semi-unification
undecidable. Semi-unification has more than once served to link unification
problems arising from type inference and those considered in computational
linguistics. Just as polymorphic recursion corresponds to subsumption through
the semi-unification problem, our type constraint problem corresponds to weak
subsumption of feature graphs in linguistics. The decidability problem for
\WhatsIt for feature graphs has been settled by
D\&quot;orre~\cite{Doerre:WeakSubsumption:94}. \nocite{RuppRosnerJohnson:94} In
contrast to D\&quot;orre's, our algorithm is fully incremental and does not refer to
finite state automata. Our algorithm also is a lot more flexible. It allows a
number of extensions (records, sorts, disjunctive types, type declarations, and
others) which make it suitable for type inference of a full-fledged programming
language.
</dc:description>
 <dc:description>Comment: Presented at CLNLP'95. An improved version is available under the
  name &quot;A Type is a Type is a Type&quot; from the Authors</dc:description>
 <dc:date>1995-06-02</dc:date>
 <dc:date>1995-11-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Syllable parsing in English and French</dc:title>
 <dc:creator>Hammond, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I argue that Optimality Theory provides for an explanatory
model of syllabic parsing in English and French. The argument is based on
psycholinguistic facts that have been mysterious up to now. This argument is
further buttressed by the computational implementation developed here. This
model is important for several reasons. First, it provides a demonstration of
how OT can be used in a performance domain. Second, it suggests a new
relationship between phonological theory and psycholinguistics. (Code in Perl
is included and a WWW-interface is running at
http://mayo.douglass.arizona.edu.)
</dc:description>
 <dc:description>Comment: postscript (mac), unix-compressed, mac(!)-uuencoded</dc:description>
 <dc:date>1995-06-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Higher-Order Logic Programming for Semantic Interpretation of
  Coordinate Constructs</dc:title>
 <dc:creator>Kulick, Seth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Many theories of semantic interpretation use lambda-term manipulation to
compositionally compute the meaning of a sentence. These theories are usually
implemented in a language such as Prolog that can simulate lambda-term
operations with first-order unification. However, for some interesting cases,
such as a Combinatory Categorial Grammar account of coordination constructs,
this can only be done by obscuring the underlying linguistic theory with the
``tricks'' needed for implementation. This paper shows how the use of abstract
syntax permitted by higher-order logic programming allows an elegant
implementation of the semantics of Combinatory Categorial Grammar, including
its handling of coordination constructs.
</dc:description>
 <dc:description>Comment: 7 pages, ACL-95, uses aclap.sty</dc:description>
 <dc:date>1995-06-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Support Tool for Tagset Mapping</dc:title>
 <dc:creator>Teufel, Simone</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Many different tagsets are used in existing corpora; these tagsets vary
according to the objectives of specific projects (which may be as far apart as
robust parsing vs. spelling correction). In many situations, however, one would
like to have uniform access to the linguistic information encoded in corpus
annotations without having to know the classification schemes in detail. This
paper describes a tool which maps unstructured morphosyntactic tags to a
constraint-based, typed, configurable specification language, a ``standard
tagset''. The mapping relies on a manually written set of mapping rules, which
is automatically checked for consistency. In certain cases, unsharp mappings
are unavoidable, and noise, i.e. groups of word forms {\sl not} conforming to
the specification, will appear in the output of the mapping. The system
automatically detects such noise and informs the user about it. The tool has
been tested with rules for the UPenn tagset \cite{up} and the SUSANNE tagset
\cite{garside}, in the framework of the EAGLES\footnote{LRE project EAGLES, cf.
\cite{eagles}.} validation phase for standardised tagsets for European
languages.
</dc:description>
 <dc:description>Comment: EACL-Sigdat 95, contains 4 ps figures (minor graphic changes)</dc:description>
 <dc:date>1995-06-08</dc:date>
 <dc:date>1995-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Extraction of Tagset Mappings from Parallel-Annotated Corpora</dc:title>
 <dc:creator>Hughes, John</dc:creator>
 <dc:creator>Souter, Clive</dc:creator>
 <dc:creator>Atwell, Eric</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes some of the recent work of project AMALGAM (automatic
mapping among lexico-grammatical annotation models). We are investigating ways
to map between the leading corpus annotation schemes in order to improve their
resuability. Collation of all the included corpora into a single large
annotated corpus will provide a more detailed language model to be developed
for tasks such as speech and handwriting recognition. In particular, we focus
here on a method of extracting mappings from corpora that have been annotated
according to more than one annotation scheme.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, uses EACL95 style file: aclap.sty</dc:description>
 <dc:date>1995-06-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Features and Agreement</dc:title>
 <dc:creator>Bayer, Sam</dc:creator>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper compares the consistency-based account of agreement phenomena in
`unification-based' grammars with an implication-based account based on a
simple feature extension to Lambek Categorial Grammar (LCG). We show that the
LCG treatment accounts for constructions that have been recognized as
problematic for `unification-based' treatments.
</dc:description>
 <dc:date>1995-06-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506008</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>CLiFF Notes: Research in the Language, Information and Computation
  Laboratory of the University of Pennsylvania</dc:title>
 <dc:creator>Editors</dc:creator>
 <dc:creator>:</dc:creator>
 <dc:creator>Stone, Matthew</dc:creator>
 <dc:creator>Levison, Libby</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Short abstracts by computational linguistics researchers at the University of
Pennsylvania describing ongoing individual and joint projects.
</dc:description>
 <dc:description>Comment: Annual Research Survey. 112 pages. uuencoded compressed postscript.
  Available as http://www.cis.upenn.edu/~cliff-group/94/cliffnotes.html</dc:description>
 <dc:date>1995-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Filling Knowledge Gaps in a Broad-Coverage Machine Translation System</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Chander, Ishwar</dc:creator>
 <dc:creator>Haines, Matthew</dc:creator>
 <dc:creator>Hatzivassiloglou, Vasileios</dc:creator>
 <dc:creator>Hovy, Eduard</dc:creator>
 <dc:creator>Iida, Masayo</dc:creator>
 <dc:creator>Luk, Steve K.</dc:creator>
 <dc:creator>Whitney, Richard</dc:creator>
 <dc:creator>Yamada, Kenji</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Knowledge-based machine translation (KBMT) techniques yield high quality in
domains with detailed semantic models, limited vocabulary, and controlled input
grammar. Scaling up along these dimensions means acquiring large knowledge
resources. It also means behaving reasonably when definitive knowledge is not
yet available. This paper describes how we can fill various KBMT knowledge
gaps, often using robust statistical techniques. We describe quantitative and
qualitative results from JAPANGLOSS, a broad-coverage Japanese-English MT
system.
</dc:description>
 <dc:description>Comment: 7 pages, Compressed and uuencoded postscript. To appear: IJCAI-95</dc:description>
 <dc:date>1995-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Two-level, Many-Paths Generation</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Hatzivassiloglou, Vasileios</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Large-scale natural language generation requires the integration of vast
amounts of knowledge: lexical, grammatical, and conceptual. A robust generator
must be able to operate well even when pieces of knowledge are missing. It must
also be robust against incomplete or inaccurate inputs. To attack these
problems, we have built a hybrid generator, in which gaps in symbolic knowledge
are filled by statistical methods. We describe algorithms and show experimental
results. We also discuss how the hybrid generation model can be used to
simplify current generators and enhance their portability, even when perfect
knowledge is in principle obtainable.
</dc:description>
 <dc:description>Comment: 9 pages, Compressed and uuencoded postscript. To appear: ACL-95</dc:description>
 <dc:date>1995-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506011</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Unification-Based Glossing</dc:title>
 <dc:creator>Hatzivassiloglou, Vasileios</dc:creator>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an approach to syntax-based machine translation that combines
unification-style interpretation with statistical processing. This approach
enables us to translate any Japanese newspaper article into English, with
quality far better than a word-for-word translation. Novel ideas include the
use of feature structures to encode word lattices and the use of unification to
compose and manipulate lattices. Unification also allows us to specify abstract
features that delay target-language synthesis until enough source-language
information is assembled. Our statistical component enables us to search
efficiently among competing translations and locate those with high English
fluency.
</dc:description>
 <dc:description>Comment: 8 pages, Compressed and uuencoded postscript. To appear: IJCAI-95</dc:description>
 <dc:date>1995-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Presenting Punctuation</dc:title>
 <dc:creator>White, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Until recently, punctuation has received very little attention in the
linguistics and computational linguistics literature. Since the publication of
Nunberg's (1990) monograph on the topic, however, punctuation has seen its
stock begin to rise: spurred in part by Nunberg's ground-breaking work, a
number of valuable inquiries have been subsequently undertaken, including Hovy
and Arens (1991), Dale (1991), Pascual (1993), Jones (1994), and Briscoe
(1994). Continuing this line of research, I investigate in this paper how
Nunberg's approach to presenting punctuation (and other formatting devices)
might be incorporated into NLG systems. Insofar as the present paper focuses on
the proper syntactic treatment of punctuation, it differs from these other
subsequent works in that it is the first to examine this issue from the
generation perspective.
</dc:description>
 <dc:description>Comment: compressed uuencoded PostScript, 19 pages; Word 6.0 doc available
  upon request from mike@cogentex.com</dc:description>
 <dc:date>1995-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506012</dc:identifier>
 <dc:identifier>In Proceedings of the Fifth European Workshop on Natural Language
  Generation, Leiden, The Netherlands, pp. 107--125.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Study of the Context(s) in a Specific Type of Texts: Car Accident
  Reports</dc:title>
 <dc:creator>Estival, Dominique</dc:creator>
 <dc:creator>Gayral, Francoise</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses the issue of defining context, and more specifically the
different contexts needed for understanding a particular type of texts. The
corpus chosen is homogeneous and allows us to determine characteristic
properties of the texts from which certain inferences can be drawn by the
reader. These characteristic properties come from the real world domain
(K-context), the type of events the texts describe (F-context) and the genre of
the texts (E-context). Together, these three contexts provide elements for the
resolution of anaphoric expressions and for several types of disambiguation. We
show in particular that the argumentation aspect of these texts is an essential
part of the context and explains some of the inferences that can be drawn.
</dc:description>
 <dc:description>Comment: 9 pages, in Proceedings of the Workshop on &quot;Context in Natural
  Language Processing&quot;, IJCAI'95, Montreal. requires `ijcai89.sty', `named.sty'
  and `named.bst'</dc:description>
 <dc:date>1995-06-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506014</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Inducing Features of Random Fields</dc:title>
 <dc:creator>Della Pietra, S.</dc:creator>
 <dc:creator>Della Pietra, V.</dc:creator>
 <dc:creator>Lafferty, J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a technique for constructing random fields from a set of training
samples. The learning paradigm builds increasingly complex fields by allowing
potential functions, or features, that are supported by increasingly large
subgraphs. Each feature has a weight that is trained by minimizing the
Kullback-Leibler divergence between the model and the empirical distribution of
the training data. A greedy algorithm determines how features are incrementally
added to the field and an iterative scaling algorithm is used to estimate the
optimal values of the weights.
  The statistical modeling techniques introduced in this paper differ from
those common to much of the natural language processing literature since there
is no probabilistic finite state or push-down automaton on which the model is
built. Our approach also differs from the techniques common to the computer
vision literature in that the underlying random fields are non-Markovian and
have a large number of parameters that must be estimated. Relations to other
learning approaches including decision trees and Boltzmann machines are given.
As a demonstration of the method, we describe its application to the problem of
automatic word classification in natural language processing.
  Key words: random field, Kullback-Leibler divergence, iterative scaling,
divergence geometry, maximum entropy, EM algorithm, statistical learning,
clustering, word morphology, natural language processing
</dc:description>
 <dc:description>Comment: 34 pages, compressed postscript</dc:description>
 <dc:date>1995-06-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Ambiguity in the Acquisition of Lexical Information</dc:title>
 <dc:creator>Vanderwende, Lucy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an approach to the automatic identification of lexical
information in on-line dictionaries. This approach uses bootstrapping
techniques, specifically so that ambiguity in the dictionary text can be
treated properly. This approach consists of processing an on-line dictionary
multiple times, each time refining the lexical information previously acquired
and identifying new lexical information. The strength of this approach is that
lexical information can be acquired from definitions which are syntactically
ambiguous, given that information acquired during the first pass can be used to
improve the syntactic analysis of definitions in subsequent passes. In the
context of a lexical knowledge base, the types of lexical information that need
to be represented cannot be viewed as a fixed set, but rather as a set that
will change given the resources of the lexical knowledge base and the
requirements of analysis systems which access it.
</dc:description>
 <dc:date>1995-06-14</dc:date>
 <dc:date>1995-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Indefeasible Semantics and Defeasible Pragmatics</dc:title>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An account of utterance interpretation in discourse needs to face the issue
of how the discourse context controls the space of interacting preferences.
Assuming a discourse processing architecture that distinguishes the grammar and
pragmatics subsystems in terms of monotonic and nonmonotonic inferences, I will
discuss how independently motivated default preferences interact in the
interpretation of intersentential pronominal anaphora. In the framework of a
general discourse processing model that integrates both the grammar and
pragmatics subsystems, I will propose a fine structure of the preferential
interpretation in pragmatics in terms of defeasible rule interactions. The
pronoun interpretation preferences that serve as the empirical ground draw from
the survey data specifically obtained for the present purpose.
</dc:description>
 <dc:description>Comment: 29 pages, self-contained LaTeX source. To appear in Kanazawa, M., C.
  Pinon, and H. de Swart, eds., Quantifiers, Deduction, and Context. Stanford,
  CA: CSLI</dc:description>
 <dc:date>1995-06-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Effect of Pitch Accenting on Pronoun Referent Resolution</dc:title>
 <dc:creator>Cahn, Janet</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  By strictest interpretation, theories of both centering and intonational
meaning fail to predict the existence of pitch accented pronominals. Yet they
occur felicitously in spoken discourse. To explain this, I emphasize the dual
functions served by pitch accents, as markers of both propositional
(semantic/pragmatic) and attentional salience. This distinction underlies my
proposals about the attentional consequences of pitch accents when applied to
pronominals, in particular, that while most pitch accents may weaken or
reinforce a cospecifier's status as the center of attention, a contrastively
stressed pronominal may force a shift, even when contraindicated by textual
features.
</dc:description>
 <dc:description>Comment: 3 pages, uses aclap.sty. In Proceedings of the ACL, 1995</dc:description>
 <dc:date>1995-06-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506017</dc:identifier>
 <dc:identifier>Proceedings of the ACL, 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intelligent Voice Prosthesis: Converting Icons into Natural Language
  Sentences</dc:title>
 <dc:creator>Vaillant, Pascal</dc:creator>
 <dc:creator>Checler, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The Intelligent Voice Prosthesis is a communication tool which reconstructs
the meaning of an ill-structured sequence of icons or symbols, and expresses
this meaning into sentences of a Natural Language (French). It has been
developed for the use of people who cannot express themselves orally in natural
language, and further, who are not able to comply to grammatical rules such as
those of natural language. We describe how available corpora of iconic
communication by children with Cerebral Palsy has led us to implement a simple
and relevant semantic description of the symbol lexicon. We then show how a
unification-based, bottom-up semantic analysis allows the system to uncover the
meaning of the user's utterances by computing proper dependencies between the
symbols. The result of the analysis is then passed to a lexicalization module
which chooses the right words of natural language to use, and builds a
linguistic semantic network. This semantic network is then generated into
French sentences via hierarchization into trees, using a lexicalized Tree
Adjoining Grammar. Finally we describe the modular, customizable interface
which has been developed for this system.
</dc:description>
 <dc:description>Comment: Montpellier'95. 11 pages, 6 Encapsulated Postscript figures, uses
  a4.sty and epsf.sty</dc:description>
 <dc:date>1995-06-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506019</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Review of Charniak's &quot;Statistical Language Learning&quot;</dc:title>
 <dc:creator>Magerman, David M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This article is an in-depth review of Eugene Charniak's book, &quot;Statistical
Language Learning&quot;. The review evaluates the appropriateness of the book as an
introductory text for statistical language learning for a variety of audiences.
It also includes an extensive bibliography of articles and papers which might
be used as a supplement to this book for learning or teaching statistical
language modeling.
</dc:description>
 <dc:date>1995-06-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506019</dc:identifier>
 <dc:identifier>Computational Linguistics 21:1, 103-111</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>GLR-Parsing of Word Lattices Using a Beam Search Method</dc:title>
 <dc:creator>Staab, Steffen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an approach that allows the efficient integration of
speech recognition and language understanding using Tomita's generalized
LR-parsing algorithm. For this purpose the GLRP-algorithm is revised so that an
agenda mechanism can be used to control the flow of computation of the parsing
process. This new approach is used to integrate speech recognition and speech
understanding incrementally with a beam search method. These considerations
have been implemented and tested on ten word lattices.
</dc:description>
 <dc:description>Comment: 4 pages, 61K postscript, compressed, uuencoded, Eurospeech 9/95,
  Madrid</dc:description>
 <dc:date>1995-06-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Prepositional Phrase Attachment through a Backed-Off Model</dc:title>
 <dc:creator>Collins, Michael</dc:creator>
 <dc:creator>Brooks, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Recent work has considered corpus-based or statistical approaches to the
problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb
phrases of the form {v np1 p np2} are resolved through a model which considers
values of the four head words (v, n1, p and n2). This paper shows that the
problem is analogous to n-gram language models in speech recognition, and that
one of the most common methods for language modeling, the backed-off estimate,
is applicable. Results on Wall Street Journal data of 84.5% accuracy are
obtained using this method. A surprising result is the importance of low-count
events - ignoring events which occur less than 5 times in training data reduces
performance to 81.6%.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of the Third Workshop on Very Large Corpora,
  12 pages, LaTeX</dc:description>
 <dc:date>1995-06-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506022</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Deriving Procedural and Warning Instructions from Device and Environment
  Models</dc:title>
 <dc:creator>Ansari, Daniel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This study is centred on the generation of instructions for household
appliances. We show how knowledge about a device, together with knowledge about
the environment, can be used for reasoning about instructions. The information
communicated by the instructions can be planned from a version of the knowledge
of the artifact and environment. We present the latter, which we call the {\it
planning knowledge}, in the form of axioms in the {\it situation calculus}.
This planning knowledge formally characterizes the behaviour of the artifact,
and it is used to produce a basic plan of actions that the device and user take
to accomplish a given goal. We explain how both procedural and warning
instructions can be generated from this basic plan.
  In order to partially justify that instruction generation can be automated
from a formal device design specification, we assume that the planning
knowledge is {\it derivable\/} from the device and world knowledge.
</dc:description>
 <dc:description>Comment: 63 pages, uses authdate.sty</dc:description>
 <dc:date>1995-06-23</dc:date>
 <dc:date>1995-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Empirical Discovery in Linguistics</dc:title>
 <dc:creator>Pericliev, Vladimir</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A discovery system for detecting correspondences in data is described, based
on the familiar induction methods of J. S. Mill. Given a set of observations,
the system induces the ``causally'' related facts in these observations. Its
application to empirical linguistic discovery is described.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX, in Working Notes of AAAI Spring Symposium Series,
  Symposium: Systematic Methods of Scientific Discovery, March 27-29, Stanford
  University</dc:description>
 <dc:date>1995-06-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Approach to Proper Name Tagging for German</dc:title>
 <dc:creator>Thielen, Christine</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an incremental method for the tagging of proper names in
German newspaper texts. The tagging is performed by the analysis of the
syntactic and textual contexts of proper names together with a morphological
analysis. The proper names selected by this process supply new contexts which
can be used for finding new proper names, and so on. This procedure was applied
to a small German corpus (50,000 words) and correctly disambiguated 65% of the
capitalized words, which should improve when it is applied to a very large
corpus.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, 2 uuencoded tar-compressed eps-figures added,
  EACL-SIGDAT 95</dc:description>
 <dc:date>1995-06-28</dc:date>
 <dc:date>1995-06-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Categorial Framework for Composition in Multiple Linguistic Domains</dc:title>
 <dc:creator>Bozsahin, Cem</dc:creator>
 <dc:creator>Gocmen, Elvan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a computational framework for a grammar architecture in
which different linguistic domains such as morphology, syntax, and semantics
are treated not as separate components but compositional domains. Word and
phrase formation are modeled as uniform processes contributing to the
derivation of the semantic form. The morpheme, as well as the lexeme, has
lexical representation in the form of semantic content, tactical constraints,
and phonological realization. The motivation for this work is to handle
morphology-syntax interaction (e.g., valency change in causatives,
subcategorization imposed by case-marking affixes) in an incremental way. The
model is based on Combinatory Categorial Grammars.
</dc:description>
 <dc:description>Comment: 7 pages LaTeX, CSNLP-95 (Dublin), uses {a4,avm,lingmacros}.sty</dc:description>
 <dc:date>1995-06-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506025</dc:identifier>
 <dc:identifier>Proceedings of the Fourth Int Conf on Cognitive Science of NLP,</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9506026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Computational Approach to Aspectual Composition</dc:title>
 <dc:creator>White, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, I argue, contrary to the prevailing opinion in the linguistics
and philosophy literature, that a sortal approach to aspectual composition can
indeed be explanatory. In support of this view, I develop a synthesis of
competing proposals by Hinrichs, Krifka and Jackendoff which takes Jackendoff's
cross-cutting sortal distinctions as its point of departure. To show that the
account is well-suited for computational purposes, I also sketch an implemented
calculus of eventualities which yields many of the desired inferences. Further
details on both the model-theoretic semantics and the implementation can be
found in (White, 1994).
</dc:description>
 <dc:description>Comment: 15 pages</dc:description>
 <dc:date>1995-07-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9506026</dc:identifier>
 <dc:identifier>In Workshop Notes of the 5th International Workshop TSM 95 (Time,
  Space and Movement: Meaning and Knowledge in the Sensible World), Chateau de
  Bonas, Gascony, France, 23-27 June 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Constraint Categorial Grammars</dc:title>
 <dc:creator>Damas, Luis</dc:creator>
 <dc:creator>Moreira, Nelma</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although unification can be used to implement a weak form of
$\beta$-reduction, several linguistic phenomena are better handled by using
some form of $\lambda$-calculus. In this paper we present a higher order
feature description calculus based on a typed $\lambda$-calculus. We show how
the techniques used in \CLG for resolving complex feature constraints can be
efficiently extended. \CCLG is a simple formalism, based on categorial
grammars, designed to test the practical feasibility of such a calculus.
</dc:description>
 <dc:description>Comment: 12 pages, 5 Postscript figures, uses llncs.sty and epsfig.sty. To
  appear in Proceedings of EPIA'95, 7th Portuguese Conference on Artificial
  Intelligence, Funchal- Madeira Island, Portugal 3-6 October , 1995</dc:description>
 <dc:date>1995-07-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507002</identifier>
 <datestamp>2016-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A framework for lexical representation</dc:title>
 <dc:creator>Go&#xf1;i, Jos&#xe9; M.</dc:creator>
 <dc:creator>Gonz&#xe1;lez, Jos&#xe9; C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a unification-based lexical platform designed for
highly inflected languages (like Roman ones). A formalism is proposed for
encoding a lemma-based lexical source, well suited for linguistic
generalizations. From this source, we automatically generate an allomorph
indexed dictionary, adequate for efficient processing. A set of software tools
have been implemented around this formalism: access libraries, morphological
processors, etc.
</dc:description>
 <dc:description>Comment: 9 pages</dc:description>
 <dc:date>1995-07-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507002</dc:identifier>
 <dc:identifier>AI95: 15th International Conference. Language Engineering 95
  (Montpellier, France), pp. 243-252.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507003</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Robust Processing of Natural Language</dc:title>
 <dc:creator>Menzel, Wolfgang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Previous approaches to robustness in natural language processing usually
treat deviant input by relaxing grammatical constraints whenever a successful
analysis cannot be provided by ``normal'' means. This schema implies, that
error detection always comes prior to error handling, a behaviour which hardly
can compete with its human model, where many erroneous situations are treated
without even noticing them.
  The paper analyses the necessary preconditions for achieving a higher degree
of robustness in natural language processing and suggests a quite different
approach based on a procedure for structural disambiguation. It not only offers
the possibility to cope with robustness issues in a more natural way but
eventually might be suited to accommodate quite different aspects of robust
behaviour within a single framework.
</dc:description>
 <dc:description>Comment: 16 pages, LaTeX, uses pstricks.sty, pstricks.tex, pstricks.pro,
  pst-node.sty, pst-node.tex, pst-node.pro. To appear in: Proc. KI-95, 19th
  German Conference on Artificial Intelligence, Bielefeld (Germany), Lecture
  Notes in Computer Science, Springer 1995</dc:description>
 <dc:date>1995-07-13</dc:date>
 <dc:date>1995-07-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507004</identifier>
 <datestamp>2016-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>GRAMPAL: A Morphological Processor for Spanish implemented in Prolog</dc:title>
 <dc:creator>Moreno, Antonio</dc:creator>
 <dc:creator>Go&#xf1;i, Jos&#xe9; M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A model for the full treatment of Spanish inflection for verbs, nouns and
adjectives is presented. This model is based on feature unification and it
relies upon a lexicon of allomorphs both for stems and morphemes. Word forms
are built by the concatenation of allomorphs by means of special contextual
features. We make use of standard Definite Clause Grammars (DCG) included in
most Prolog implementations, instead of the typical finite-state approach. This
allows us to take advantage of the declarativity and bidirectionality of Logic
Programming for NLP.
  The most salient feature of this approach is simplicity: A really
straightforward rule and lexical components. We have developed a very simple
model for complex phenomena.
  Declarativity, bidirectionality, consistency and completeness of the model
are discussed: all and only correct word forms are analysed or generated, even
alternative ones and gaps in paradigms are preserved. A Prolog implementation
has been developed for both analysis and generation of Spanish word forms. It
consists of only six DCG rules, because our {\em lexicalist\/} approach --i.e.
most information is in the dictionary. Although it is quite efficient, the
current implementation could be improved for analysis by using the non logical
features of Prolog, especially in word segmentation and dictionary access.
</dc:description>
 <dc:description>Comment: 11 pages</dc:description>
 <dc:date>1995-07-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507004</dc:identifier>
 <dc:identifier>GULP-PRODE95: Joint Conference on Declarative Programming, Marina
  di Vietri, Salerno (Italy). September, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Comparative Ellipsis and Variable Binding</dc:title>
 <dc:creator>Lerner, Jan</dc:creator>
 <dc:creator>Pinkal, Manfred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we discuss the question whether phrasal comparatives should be
given a direct interpretation, or require an analysis as elliptic
constructions, and answer it with Yes and No. The most adequate analysis of
wide reading attributive (WRA) comparatives seems to be as cases of ellipsis,
while a direct (but asymmetric) analysis fits the data for narrow scope
attributive comparatives. The question whether it is a syntactic or a semantic
process which provides the missing linguistic material in the complement of WRA
comparatives is also given a complex answer: Linguistic context is accessed by
combining a reconstruction operation and a mechanism of anaphoric reference.
The analysis makes only few and straightforward syntactic assumptions. In part,
this is made possible because the use of Generalized Functional Application as
a semantic operation allows us to model semantic composition in a flexible way.
</dc:description>
 <dc:description>Comment: Postscript, 15 pages</dc:description>
 <dc:date>1995-07-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507005</dc:identifier>
 <dc:identifier>to appear in SALT V Proceedings, Cornell University,</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Transfer in a Connectionist Model of the Acquisition of Morphology</dc:title>
 <dc:creator>Gasser, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The morphological systems of natural languages are replete with examples of
the same devices used for multiple purposes: (1) the same type of morphological
process (for example, suffixation for both noun case and verb tense) and (2)
identical morphemes (for example, the same suffix for English noun plural and
possessive). These sorts of similarity would be expected to convey advantages
on language learners in the form of transfer from one morphological category to
another. Connectionist models of morphology acquisition have been faulted for
their supposed inability to represent phonological similarity across
morphological categories and hence to facilitate transfer. This paper describes
a connectionist model of the acquisition of morphology which is shown to
exhibit transfer of this type. The model treats the morphology acquisition
problem as one of learning to map forms onto meanings and vice versa. As the
network learns these mappings, it makes phonological generalizations which are
embedded in connection weights. Since these weights are shared by different
morphological categories, transfer is enabled. In a set of experiments with
artificial stimuli, networks were trained first on one morphological task
(e.g., tense) and then on a second (e.g., number). It is shown that in the
context of suffixation, prefixation, and template rules, the second task is
facilitated when the second category either makes use of the same forms or the
same general process type (e.g., prefixation) as the first.
</dc:description>
 <dc:description>Comment: 21 pages, uuencoded compressed Postscript</dc:description>
 <dc:date>1995-07-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507007</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Algorithm for Surface Generation</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A method is given that &quot;inverts&quot; a logic grammar and displays it from the
point of view of the logical form, rather than from that of the word string.
LR-compiling techniques are used to allow a recursive-descent generation
algorithm to perform &quot;functor merging&quot; much in the same way as an LR parser
performs prefix merging.
  This is an improvement on the semantic-head-driven generator that results in
a much smaller search space. The amount of semantic lookahead can be varied,
and appropriate tradeoff points between table size and resulting nondeterminism
can be found automatically.
</dc:description>
 <dc:description>Comment: Uuencoded compressed PostScript format</dc:description>
 <dc:date>1995-07-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507007</dc:identifier>
 <dc:identifier>IJCAI 95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507008</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Constraint-based Case Frame Lexicon Architecture</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Yilmaz, Okan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In Turkish, (and possibly in many other languages) verbs often convey several
meanings (some totally unrelated) when they are used with subjects, objects,
oblique objects, adverbial adjuncts, with certain lexical, morphological, and
semantic features, and co-occurrence restrictions. In addition to the usual
sense variations due to selectional restrictions on verbal arguments, in most
cases, the meaning conveyed by a case frame is idiomatic and not compositional,
with subtle constraints. In this paper, we present an approach to building a
constraint-based case frame lexicon for use in natural language processing in
Turkish, whose prototype we have implemented under the TFS system developed at
Univ. of Stuttgart.
  A number of observations that we have made on Turkish have indicated that we
need something beyond the traditional transitive and intransitive distinction,
and utilize a framework where verb valence is considered as the obligatory
co-existence of an arbitrary subset of possible arguments along with the
obligatory exclusion of certain others, relative to a verb sense. Additional
morphological lexical and semantic constraints on the syntactic constituents
organized as a 5-tier constraint hierarchy, are utilized to map a given
syntactic structure case-fraame to a specific verb sense.
</dc:description>
 <dc:description>Comment: gzipped, uuencoded postscipt file, 11 pages. To be presented at the
  ESSLLI Workshop -- The Computational Lexicon. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1995/BU-CEIS-9511.ps.z</dc:description>
 <dc:date>1995-07-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Specifying Logic Programs in Controlled Natural Language</dc:title>
 <dc:creator>Fuchs, Norbert E.</dc:creator>
 <dc:creator>Schwitter, Rolf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Writing specifications for computer programs is not easy since one has to
take into account the disparate conceptual worlds of the application domain and
of software development. To bridge this conceptual gap we propose controlled
natural language as a declarative and application-specific specification
language. Controlled natural language is a subset of natural language that can
be accurately and efficiently processed by a computer, but is expressive enough
to allow natural usage by non-specialists. Specifications in controlled natural
language are automatically translated into Prolog clauses, hence become formal
and executable. The translation uses a definite clause grammar (DCG) enhanced
by feature structures. Inter-text references of the specification, e.g.
anaphora, are resolved with the help of discourse representation theory (DRT).
The generated Prolog clauses are added to a knowledge base. We have implemented
a prototypical specification system that successfully processes the
specification of a simple automated teller machine.
</dc:description>
 <dc:description>Comment: 16 pages, compressed, uuencoded Postscript, published in Proceedings
  CLNLP 95, COMPULOGNET/ELSNET/EAGLES Workshop on Computational Logic for
  Natural Language Processing, Edinburgh, April 3-5, 1995</dc:description>
 <dc:date>1995-07-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507009</dc:identifier>
 <dc:identifier>Proceedings CLNLP 95, COMPULOGNET/ELSNET/EAGLES</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On-line Learning of Binary Lexical Relations Using Two-dimensional
  Weighted Majority Algorithms</dc:title>
 <dc:creator>Abe, Naoki</dc:creator>
 <dc:creator>Li, Hang</dc:creator>
 <dc:creator>Nakamura, Atsuyoshi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We consider the problem of learning a certain type of lexical semantic
knowledge that can be expressed as a binary relation between words, such as the
so-called sub-categorization of verbs (a verb-noun relation) and the compound
noun phrase relation (a noun-noun relation). Specifically, we view this problem
as an on-line learning problem in the sense of Littlestone's learning model in
which the learner's goal is to minimize the total number of prediction
mistakes. In the computational learning theory literature, Goldman, Rivest and
Schapire and subsequently Goldman and Warmuth have considered the on-line
learning problem for binary relations R : X * Y -&gt; {0, 1} in which one of the
domain sets X can be partitioned into a relatively small number of types,
namely clusters consisting of behaviorally indistinguishable members of X. In
this paper, we extend this model and suppose that both of the sets X, Y can be
partitioned into a small number of types, and propose a host of prediction
algorithms which are two-dimensional extensions of Goldman and Warmuth's
weighted majority type algorithm proposed for the original model. We apply
these algorithms to the learning problem for the `compound noun phrase'
relation, in which a noun is related to another just in case they can form a
noun phrase together. Our experimental results show that all of our algorithms
out-perform Goldman and Warmuth's algorithm. We also theoretically analyze the
performance of one of our algorithms, in the form of an upper bound on the
worst case number of prediction mistakes it makes.
</dc:description>
 <dc:description>Comment: 9 pages, uuencoded compressed postscript</dc:description>
 <dc:date>1995-07-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507010</dc:identifier>
 <dc:identifier>Proc. of The 12th Int. Conf. on Machine Learning, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generalizing Case Frames Using a Thesaurus and the MDL Principle</dc:title>
 <dc:creator>Li, Hang</dc:creator>
 <dc:creator>Abe, Naoki</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address the problem of automatically acquiring case-frame patterns from
large corpus data. In particular, we view this problem as the problem of
estimating a (conditional) distribution over a partition of words, and propose
a new generalization method based on the MDL (Minimum Description Length)
principle. In order to assist with the efficiency, our method makes use of an
existing thesaurus and restricts its attention on those partitions that are
present as `cuts' in the thesaurus tree, thus reducing the generalization
problem to that of estimating the `tree cut models' of the thesaurus. We then
give an efficient algorithm which provably obtains the optimal tree cut model
for the given frequency data, in the sense of MDL. We have used the case-frame
patterns obtained using our method to resolve pp-attachment ambiguity.Our
experimental results indicate that our method improves upon or is at least as
effective as existing methods.
</dc:description>
 <dc:description>Comment: 11 pages, uuencoded compressed postscript, a revised version</dc:description>
 <dc:date>1995-07-24</dc:date>
 <dc:date>1996-03-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507011</dc:identifier>
 <dc:identifier>Proc. of Recent Advances in Natural Language Processing, 239-248,
  1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Grammar Formalism and Cross-Serial Dependencies</dc:title>
 <dc:creator>Burheim, Tore</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  First we define a unification grammar formalism called the Tree Homomorphic
Feature Structure Grammar. It is based on Lexical Functional Grammar (LFG), but
has a strong restriction on the syntax of the equations. We then show that this
grammar formalism defines a full abstract family of languages, and that it is
capable of describing cross-serial dependencies of the type found in Swiss
German.
</dc:description>
 <dc:description>Comment: 19 pages uuencodet gnu-compressed PostScript format. A previous
  version of this paper is printed in the proceedings from the joint
  ELSNET/COMPULOG-NET/EAGLES workshop Computational Logic for Natural Language
  Processing (CLNLP95) in Edinburgh in April 1995</dc:description>
 <dc:date>1995-07-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Indexed Languages and Unification Grammars</dc:title>
 <dc:creator>Burheim, Tore</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Indexed languages are interesting in computational linguistics because they
are the least class of languages in the Chomsky hierarchy that has not been
shown not to be adequate to describe the string set of natural language
sentences. We here define a class of unification grammars that exactly describe
the class of indexed languages.
</dc:description>
 <dc:description>Comment: 16 pages uuencodet gnu-compressed PostScript format. Also in
  Proceedings of the 10th Nordic Conference of Computational Linguistics,
  NODALIDA-95, Helsinki, 1995</dc:description>
 <dc:date>1995-07-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9507014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Co-Indexing Labelled DRSs to Represent and Reason with Ambiguities</dc:title>
 <dc:creator>Reyle, Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper addresses the problem of representing ambiguities in a way that
allows for monotonic disambiguation and for direct deductive computation. The
paper focuses on an extension of the formalism of underspecified DRSs to
ambiguities introduced by plural NPs. It deals with the collective/distributive
distinction, and also with generic and cumulative readings. In addition it
provides a systematic account for an underspecified treatment of plural pronoun
resolution.
</dc:description>
 <dc:description>Comment: gzipped ps-file. To appear in: Stanley Peters, Kees van Deemter
  (1995): Semantic Ambiguity and Underspecification, CSLI Publications,
  Stanford</dc:description>
 <dc:date>1995-07-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9507014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bridging as Coercive Accommodation</dc:title>
 <dc:creator>Bos, Johan</dc:creator>
 <dc:creator>Buitelaar, Paul</dc:creator>
 <dc:creator>Mineur, Anne-Marie</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we discuss the notion of &quot;bridging&quot; in Discourse Representation
Theory as a tool to account for discourse referents that have only been
established implicitly, through the lexical semantics of other referents. In
doing so, we use ideas from Generative Lexicon theory, to introduce antecedents
for anaphoric expressions that cannot be &quot;linked&quot; to a proper antecedent, but
that do not need to be &quot;accommodated&quot; because they have some connection to the
network of discourse referents that is already established.
</dc:description>
 <dc:description>Comment: LaTeX file, 16 pages, uses named.sty. Paper presented at CLNLP
  workshop, Edinburgh, April 3-5, 1995</dc:description>
 <dc:date>1995-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Compositional Treatment of Polysemous Arguments in Categorial Grammar</dc:title>
 <dc:creator>Mineur, Anne-Marie</dc:creator>
 <dc:creator>Buitelaar, Paul</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We discuss an extension of the standard logical rules (functional application
and abstraction) in Categorial Grammar (CG), in order to deal with some
specific cases of polysemy. We borrow from Generative Lexicon theory which
proposes the mechanism of {\em coercion}, next to a rich nominal lexical
semantic structure called {\em qualia structure}.
  In a previous paper we introduced coercion into the framework of {\em
sign-based} Categorial Grammar and investigated its impact on traditional
Fregean compositionality. In this paper we will elaborate on this idea, mostly
working towards the introduction of a new semantic dimension. Where in current
versions of sign-based Categorial Grammar only two representations are derived:
a prosodic one (form) and a logical one (modelling), here we introduce also a
more detaled representation of the lexical semantics. This extra knowledge will
serve to account for linguistic phenomena like {\em metonymy\/}.
</dc:description>
 <dc:description>Comment: LaTeX file, 19 pages, uses pubsmacs, pubsbib, pubsarticle, leqno</dc:description>
 <dc:date>1995-08-02</dc:date>
 <dc:date>1995-09-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Robust Parsing Algorithm For Link Grammars</dc:title>
 <dc:creator>Grinberg, Dennis</dc:creator>
 <dc:creator>Lafferty, John</dc:creator>
 <dc:creator>Sleator, Daniel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a robust parsing algorithm based on the link grammar
formalism for parsing natural languages. Our algorithm is a natural extension
of the original dynamic programming recognition algorithm which recursively
counts the number of linkages between two words in the input sentence. The
modified algorithm uses the notion of a null link in order to allow a
connection between any pair of adjacent words, regardless of their dictionary
definitions. The algorithm proceeds by making three dynamic programming passes.
In the first pass, the input is parsed using the original algorithm which
enforces the constraints on links to ensure grammaticality. In the second pass,
the total cost of each substring of words is computed, where cost is determined
by the number of null links necessary to parse the substring. The final pass
counts the total number of parses with minimal cost. All of the original
pruning techniques have natural counterparts in the robust algorithm. When used
together with memoization, these techniques enable the algorithm to run
efficiently with cubic worst-case complexity. We have implemented these ideas
and tested them by parsing the Switchboard corpus of conversational English.
This corpus is comprised of approximately three million words of text,
corresponding to more than 150 hours of transcribed speech collected from
telephone conversations restricted to 70 different topics. Although only a
small fraction of the sentences in this corpus are &quot;grammatical&quot; by standard
criteria, the robust link grammar parser is able to extract relevant structure
for a large portion of the sentences. We present the results of our experiments
using this system, including the analyses of selected and random sentences from
the corpus.
</dc:description>
 <dc:description>Comment: 17 pages, compressed postscript</dc:description>
 <dc:date>1995-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing English with a Link Grammar</dc:title>
 <dc:creator>Sleator, Daniel D. K.</dc:creator>
 <dc:creator>Temperley, Davy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We develop a formal grammatical system called a link grammar, show how
English grammar can be encoded in such a system, and give algorithms for
efficiently parsing with a link grammar. Although the expressive power of link
grammars is equivalent to that of context free grammars, encoding natural
language grammars appears to be much easier with the new system. We have
written a program for general link parsing and written a link grammar for the
English language. The performance of this preliminary system -- both in the
breadth of English phenomena that it captures and in the computational
resources used -- indicates that the approach may have practical uses as well
as linguistic significance. Our program is written in C and may be obtained
through the internet.
</dc:description>
 <dc:description>Comment: 91 pages, compressed postscript</dc:description>
 <dc:date>1995-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Matching Technique in Example-Based Machine Translation</dc:title>
 <dc:creator>Cranias, Lambros</dc:creator>
 <dc:creator>Papageorgiou, Harris</dc:creator>
 <dc:creator>Piperidis, Stelios</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses an important problem in Example-Based Machine
Translation (EBMT), namely how to measure similarity between a sentence
fragment and a set of stored examples. A new method is proposed that measures
similarity according to both surface structure and content. A second
contribution is the use of clustering to make retrieval of the best matching
example from the database more efficient. Results on a large number of test
cases from the CELEX database are presented.
</dc:description>
 <dc:description>Comment: 5 pages,LaTeX uses aclap.sty</dc:description>
 <dc:date>1995-08-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT</dc:title>
 <dc:creator>Trujillo, Arturo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper presents a prototype lexicalist Machine Translation system (based
on the so-called `Shake-and-Bake' approach of Whitelock (1992) consisting of an
analysis component, a dynamic bilingual lexicon, and a generation component,
and shows how it is applied to a range of MT problems. Multi-Lexeme
translations are handled through bi-lexical rules which map bilingual lexical
signs into new bilingual lexical signs. It is argued that much translation can
be handled by equating translationally equivalent lists of lexical signs,
either directly in the bilingual lexicon, or by deriving them through
bi-lexical rules. Lexical semantic information organized as Qualia structures
(Pustejovsky 1991) is used as a mechanism for restricting the domain of the
rules.
</dc:description>
 <dc:description>Comment: 21 pages; tarred, compressed, uuencoded Latex. In `Proceedings of the
  Sixth International Conference on Theoretical and Methodological Issues in
  Machine Translation', Leuven, Belgium, July 1995, pp. 48--66</dc:description>
 <dc:date>1995-08-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Dynamic Approach to Rhythm in Language: Toward a Temporal Phonology</dc:title>
 <dc:creator>Port, Robert</dc:creator>
 <dc:creator>Cummins, Fred</dc:creator>
 <dc:creator>Gasser, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  It is proposed that the theory of dynamical systems offers appropriate tools
to model many phonological aspects of both speech production and perception. A
dynamic account of speech rhythm is shown to be useful for description of both
Japanese mora timing and English timing in a phrase repetition task. This
orientation contrasts fundamentally with the more familiar symbolic approach to
phonology, in which time is modeled only with sequentially arrayed symbols. It
is proposed that an adaptive oscillator offers a useful model for perceptual
entrainment (or `locking in') to the temporal patterns of speech production.
This helps to explain why speech is often perceived to be more regular than
experimental measurements seem to justify. Because dynamic models deal with
real time, they also help us understand how languages can differ in their
temporal detail---contributing to foreign accents, for example. The fact that
languages differ greatly in their temporal detail suggests that these effects
are not mere motor universals, but that dynamical models are intrinsic
components of the phonological characterization of language.
</dc:description>
 <dc:description>Comment: 31 pages; compressed, uuencoded Postscript</dc:description>
 <dc:date>1995-08-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Constraint-Based Lambek Calculi</dc:title>
 <dc:creator>Doerre, Jochen</dc:creator>
 <dc:creator>Manandhar, Suresh</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We explore the consequences of layering a Lambek proof system over an
arbitrary (constraint) logic. A simple model-theoretic semantics for our hybrid
language is provided for which a particularly simple combination of Lambek's
and the proof system of the base logic is complete. Furthermore the proof
system for the underlying base logic can be assumed to be a black box. The
essential reasoning needed to be performed by the black box is that of {\em
entailment checking}. Assuming feature logic as the base logic entailment
checking amounts to a {\em subsumption} test which is a well-known quasi-linear
time decidable problem.
</dc:description>
 <dc:description>Comment: uuencoded gzipped ps file, 97k, 18 pages</dc:description>
 <dc:date>1995-08-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Labelled Analytic Theorem Proving Environment for Categorial Grammar</dc:title>
 <dc:creator>Luz-Filho, Saturnino F.</dc:creator>
 <dc:creator>Sturt, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a system for the investigation of computational properties of
categorial grammar parsing based on a labelled analytic tableaux theorem
prover. This proof method allows us to take a modular approach, in which the
basic grammar can be kept constant, while a range of categorial calculi can be
captured by assigning different properties to the labelling algebra. The
theorem proving strategy is particularly well suited to the treatment of
categorial grammar, because it allows us to distribute the computational cost
between the algorithm which deals with the grammatical types and the algebraic
checker which constrains the derivation.
</dc:description>
 <dc:description>Comment: 11 pages, LaTeX2e, uses examples.sty and a4wide.sty</dc:description>
 <dc:date>1995-08-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508009</dc:identifier>
 <dc:identifier>To appear in the Proceedings of IWPT-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Heuristics and Parse Ranking</dc:title>
 <dc:creator>Srinivas, B.</dc:creator>
 <dc:creator>Doran, Christine</dc:creator>
 <dc:creator>Kulick, Seth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  There are currently two philosophies for building grammars and parsers --
Statistically induced grammars and Wide-coverage grammars. One way to combine
the strengths of both approaches is to have a wide-coverage grammar with a
heuristic component which is domain independent but whose contribution is tuned
to particular domains. In this paper, we discuss a three-stage approach to
disambiguation in the context of a lexicalized grammar, using a variety of
domain independent heuristic techniques. We present a training algorithm which
uses hand-bracketed treebank parses to set the weights of these heuristics. We
compare the performance of our grammar against the performance of the IBM
statistical grammar, using both untrained and trained weights for the
heuristics.
</dc:description>
 <dc:description>Comment: uuencoded compressed ps file. A4 format. 10 pages</dc:description>
 <dc:date>1995-08-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508010</dc:identifier>
 <dc:identifier>International Workshop on Parsing Technologies (IWPT 95)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Use of Knowledge Preconditions in Language Processing</dc:title>
 <dc:creator>Lochbaum, Karen E.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  If an agent does not possess the knowledge needed to perform an action, it
may privately plan to obtain the required information on its own, or it may
involve another agent in the planning process by engaging it in a dialogue. In
this paper, we show how the requirements of knowledge preconditions can be used
to account for information-seeking subdialogues in discourse. We first present
an axiomatization of knowledge preconditions for the SharedPlan model of
collaborative activity (Grosz &amp; Kraus, 1993), and then provide an analysis of
information-seeking subdialogues within a general framework for discourse
processing. In this framework, SharedPlans and relationships among them are
used to model the intentional component of Grosz and Sidner's (1986) theory of
discourse structure.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX, uses ijcai95.sty, postscript figures</dc:description>
 <dc:date>1995-08-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508011</dc:identifier>
 <dc:identifier>Proceedings of IJCAI-95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9508012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Natural Law of Succession</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Consider the problem of multinomial estimation. You are given an alphabet of
k distinct symbols and are told that the i-th symbol occurred exactly n_i times
in the past. On the basis of this information alone, you must now estimate the
conditional probability that the next symbol will be i. In this report, we
present a new solution to this fundamental problem in statistics and
demonstrate that our solution outperforms standard approaches, both in theory
and in practice.
</dc:description>
 <dc:description>Comment: 23 pages</dc:description>
 <dc:date>1995-08-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9508012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>How much is enough?: Data requirements for statistical NLP</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I explore a number of issues in the analysis of data
requirements for statistical NLP systems. A preliminary framework for viewing
such systems is proposed and a sample of existing works are compared within
this framework. The first steps toward a theory of data requirements are made
by establishing some results relevant to bounding the expected error rate of a
class of simplified statistical language learners as a function of the volume
of training data.
</dc:description>
 <dc:description>Comment: 9 pages</dc:description>
 <dc:date>1995-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9509001</dc:identifier>
 <dc:identifier>2nd Conference of the Pacific Association for Computational
  Linguistics, Brisbane, Australia</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Conserving Fuel in Statistical Language Learning: Predicting Data
  Requirements</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I address the practical concern of predicting how much training
data is sufficient for a statistical language learning system. First, I briefly
review earlier results and show how these can be combined to bound the expected
accuracy of a mode-based learner as a function of the volume of training data.
I then develop a more accurate estimate of the expected accuracy function under
the assumption that inputs are uniformly distributed. Since this estimate is
expensive to compute, I also give a close but cheaply computable approximation
to it. Finally, I report on a series of simulations exploring the effects of
inputs that are not uniformly distributed. Although these results are based on
simplistic assumptions, they are a tentative step toward a useful theory of
data requirements for SLL systems.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1995-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9509002</dc:identifier>
 <dc:identifier>Eighth Australian Joint Conference on Artificial Intelligence,
  Canberra, 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Cluster Expansions and Iterative Scaling for Maximum Entropy Language
  Models</dc:title>
 <dc:creator>Lafferty, John D.</dc:creator>
 <dc:creator>Suhm, Bernhard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The maximum entropy method has recently been successfully introduced to a
variety of natural language applications. In each of these applications,
however, the power of the maximum entropy method is achieved at the cost of a
considerable increase in computational requirements. In this paper we present a
technique, closely related to the classical cluster expansion from statistical
mechanics, for reducing the computational demands necessary to calculate
conditional maximum entropy language models.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded and compressed postscript</dc:description>
 <dc:date>1995-09-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9509003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Development and Migration of Concepts from Donor to Borrower
  Disciplines: Sublanguage Term Use in Hard &amp; Soft Sciences</dc:title>
 <dc:creator>Losee, Robert M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Academic disciplines, often divided into hard and soft sciences, may be
understood as &quot;donor disciplines&quot; if they produce more concepts than they
borrow from other disciplines, or &quot;borrower disciplines&quot; if they import more
than they originate. Terms used to describe these concepts can be used to
distinguish between hard and soft, donor and borrower, as well as individual
discipline-specific sublanguages. Using term frequencies, the birth, growth,
death, and migration of concepts and their associated terms are examined.
</dc:description>
 <dc:description>Comment: uuencoded compressed postscript file</dc:description>
 <dc:date>1995-09-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9509004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9509005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>ParseTalk about Textual Ellipsis</dc:title>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A hybrid methodology for the resolution of text-level ellipsis is presented
in this paper. It incorporates conceptual proximity criteria applied to
ontologically well-engineered domain knowledge bases and an approach to
centering based on functional topic/comment patterns. We state text grammatical
predicates for ellipsis and then turn to the procedural aspects of their
evaluation within the framework of an actor-based implementation of a lexically
distributed parser.
</dc:description>
 <dc:description>Comment: 11 pages, uuencoded compressed PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/ranlp95.ps)</dc:description>
 <dc:date>1995-09-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9509005</dc:identifier>
 <dc:identifier>RANLP 95: Proc. of the Intl. Conf. on Recent Advances in Natural
  Language Processing. Tzigov Chark, Bulgaria, Sep. 14-16 1995, pp.62-72.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>POS Tagging Using Relaxation Labelling</dc:title>
 <dc:creator>Padro, Lluis</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Relaxation labelling is an optimization technique used in many fields to
solve constraint satisfaction problems. The algorithm finds a combination of
values for a set of variables such that satisfies -to the maximum possible
degree- a set of given constraints. This paper describes some experiments
performed applying it to POS tagging, and the results obtained. It also ponders
the possibility of applying it to word sense disambiguation.
</dc:description>
 <dc:description>Comment: compressed &amp; uuencoded postscript file. Paper length: 39 pages</dc:description>
 <dc:date>1995-10-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Chinese Text Processing Technique for the Processing of Sanskrit
  Based Indian Languages: Maximum Resource Utilization and Maximum
  Compatibility</dc:title>
 <dc:creator>Hasan, Md Maruf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Chinese text processing systems are using Double Byte Coding , while almost
all existing Sanskrit Based Indian Languages have been using Single Byte coding
for text processing. Through observation, Chinese Information Processing
Technique has already achieved great technical development both in east and
west. In contrast,Indian Languages are being processed by computer, more or
less, for word processing purpose. This paper mainly emphasizes the method of
processing Indian languages from a Computational Linguistic point of view. An
overall design method is illustrated in this paper.This method concentrated on
maximum resource utilization and compatibility: the ultimate goal is to have a
Multiplatform Multilingual System. Keywords Text Procrssing, Multilingual Text
Processing, Chinese Language Processing, Indian Language Processing, Character
Coding.
</dc:description>
 <dc:description>Comment: It may take longer time to print this file in a postscript printer. A
  Microsoft Word version of this paper can be provided on request. Interested
  people can obtain our Bengali fonts. Please send mail to the author</dc:description>
 <dc:date>1995-09-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Proposal for Word Sense Disambiguation using Conceptual Distance</dc:title>
 <dc:creator>Agirre, Eneko</dc:creator>
 <dc:creator>Rigau, German</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a method for the resolution of lexical ambiguity and its
automatic evaluation over the Brown Corpus. The method relies on the use of the
wide-coverage noun taxonomy of WordNet and the notion of conceptual distance
among concepts, captured by a Conceptual Density formula developed for this
purpose. This fully automatic method requires no hand coding of lexical
entries, hand tagging of text nor any kind of training process. The results of
the experiment have been automatically evaluated against SemCor, the
sense-tagged version of the Brown Corpus.
</dc:description>
 <dc:description>Comment: Postscript version. 7 pages</dc:description>
 <dc:date>1995-10-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510003</dc:identifier>
 <dc:identifier>1st Intl. Conf. on recent Advances in NLP. Bulgaria. 1995.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510004</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Disambiguating bilingual nominal entries against WordNet</dc:title>
 <dc:creator>Rigau, German</dc:creator>
 <dc:creator>Agirre, Eneko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper explores the acquisition of conceptual knowledge from bilingual
dictionaries (French/English, Spanish/English and English/Spanish) using a
pre-existing broad coverage Lexical Knowledge Base (LKB) WordNet. Bilingual
nominal entries are disambiguated agains WordNet, therefore linking the
bilingual dictionaries to WordNet yielding a multilingual LKB (MLKB). The
resulting MLKB has the same structure as WordNet, but some nodes are attached
additionally to disambiguated vocabulary of other languages.
  Two different, complementary approaches are explored. In one of the
approaches each entry of the dictionary is taken in turn, exploiting the
information in the entry itself. The inferential capability for disambiguating
the translation is given by Semantic Density over WordNet. In the other
approach, the bilingual dictionary was merged with WordNet, exploiting mainly
synonymy relations. Each of the approaches was used in a different dictionary.
  Both approaches attain high levels of precision on their own, showing that
disambiguating bilingual nominal entries, and therefore linking bilingual
dictionaries to WordNet is a feasible task.
</dc:description>
 <dc:description>Comment: Postscrip version. 12 pages</dc:description>
 <dc:date>1995-10-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510004</dc:identifier>
 <dc:identifier>Workshop On The Computational Lexicon - ESSLLI 95.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Developing and Evaluating a Probabilistic LR Parser of Part-of-Speech
  and Punctuation Labels</dc:title>
 <dc:creator>Briscoe, Ted</dc:creator>
 <dc:creator>Carroll, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an approach to robust domain-independent syntactic parsing of
unrestricted naturally-occurring (English) input. The technique involves
parsing sequences of part-of-speech and punctuation labels using a
unification-based grammar coupled with a probabilistic LR parser. We describe
the coverage of several corpora using this grammar and report the results of a
parsing experiment using probabilities derived from bracketed training data. We
report the first substantial experiments to assess the contribution of
punctuation to deriving an accurate syntactic analysis, by parsing identical
texts both with and without naturally-occurring punctuation marks.
</dc:description>
 <dc:description>Comment: 11 pages, standard LaTeX</dc:description>
 <dc:date>1995-10-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510005</dc:identifier>
 <dc:identifier>4th International Workshop on Parsing Technologies (IWPT-95),
  48-58</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incorporating Discourse Aspects in English -- Polish MT: Towards Robust
  Implementation</dc:title>
 <dc:creator>Stys, Malgorzata E.</dc:creator>
 <dc:creator>Zemke, Stefan S.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The main aim of translation is an accurate transfer of meaning so that the
result is not only grammatically and lexically correct but also communicatively
adequate. This paper stresses the need for discourse analysis the aim of which
is to preserve the communicative meaning in English--Polish machine
translation. Unlike English, which is a positional language with word order
grammatically determined, Polish displays a strong tendency to order
constituents according to their degree of salience, so that the most
informationally salient elements are placed towards the end of the clause
regardless of their grammatical function. The Centering Theory developed for
tracking down given information units in English and the Theory of Functional
Sentence Perspective predicting informativeness of subsequent constituents
provide theoretical background for this work. The notion of {\em center} is
extended to accommodate not only for pronominalisation and exact reiteration
but also for definiteness and other center pointing constructs. Center
information is additionally graded and applicable to all primary constituents
in a given utterance. This information is used to order the post-transfer
constituents correctly, relying on statistical regularities and some syntactic
clues.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded and compressed postscript file (presented at
  Recent Advances in NLP 95)</dc:description>
 <dc:date>1995-10-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Identification of Support Verbs: A Step Towards a Definition
  of Semantic Weight</dc:title>
 <dc:creator>Dras, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Current definitions of notions of lexical density and semantic weight are
based on the division of words into closed and open classes, and on intuition.
This paper develops a computationally tractable definition of semantic weight,
concentrating on what it means for a word to be semantically light; the
definition involves looking at the frequency of a word in particular syntactic
constructions which are indicative of lightness. Verbs such as &quot;make&quot; and
&quot;take&quot;, when they function as support verbs, are often considered to be
semantically light. To test our definition, we carried out an experiment based
on that of Grefenstette and Teufel (1995), where we automatically identify
light instances of these words in a corpus; this was done by incorporating our
frequency-related definition of semantic weight into a statistical approach
similar to that of Grefenstette and Teufel. The results show that this is a
plausible definition of semantic lightness for verbs, which can possibly be
extended to defining semantic lightness for other classes of words.
</dc:description>
 <dc:description>Comment: 8 pages, standard LaTeX (replaced to fix LaTeX style used)</dc:description>
 <dc:date>1995-10-25</dc:date>
 <dc:date>1995-10-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9510008</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Toward an MT System without Pre-Editing --- Effects of New Methods in
  ALT-J/E ---</dc:title>
 <dc:creator>Ikehara, Satoru</dc:creator>
 <dc:creator>Shirai, Satoshi</dc:creator>
 <dc:creator>Yokoo, Akio</dc:creator>
 <dc:creator>Nakaiwa, Hiromi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Recently, several types of Japanese-to-English machine translation systems
have been developed, but all of them require an initial process of rewriting
the original text into easily translatable Japanese. Therefore these systems
are unsuitable for translating information that needs to be speedily
disseminated. To overcome this limitation, a Multi-Level Translation Method
based on the Constructive Process Theory has been proposed. This paper
describes the benefits of using this method in the Japanese-to-English machine
translation system ALT-J/E.
  In comparison with conventional compositional methods, the Multi-Level
Translation Method emphasizes the importance of the meaning contained in
expression structures as a whole. It is shown to be capable of translating
typical written Japanese based on the meaning of the text in its context, with
comparative ease. We are now hopeful of carrying out useful machine translation
with no manual pre-editing.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX, optional Japanese commented out, uses twocolumn.sty,
  a4wide.sty, lsalike.sty, gb4e.sty</dc:description>
 <dc:date>1995-10-31</dc:date>
 <dc:date>1995-11-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9510008</dc:identifier>
 <dc:identifier>Proceedings of MT Summit III, 1991, 101-106.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Countability and Number in Japanese-to-English Machine Translation</dc:title>
 <dc:creator>Bond, Francis</dc:creator>
 <dc:creator>Ogura, Kentaro</dc:creator>
 <dc:creator>Ikehara, Satoru</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a heuristic method that uses information in the Japanese
text along with knowledge of English countability and number stored in transfer
dictionaries to determine the countability and number of English noun phrases.
Incorporating this method into the machine translation system ALT-J/E, helped
to raise the percentage of noun phrases generated with correct use of articles
and number from 65% to 73%.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX, uses twocolumn.sty, 11pt, lsalike.sty, gb4e.sty</dc:description>
 <dc:date>1995-11-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511001</dc:identifier>
 <dc:identifier>Proceedings of the 15th International Conference on Computational
  Linguistics (COLING'94), pp 32--38.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Letting the Cat out of the Bag: Generation for Shake-and-Bake MT</dc:title>
 <dc:creator>Brew, Chris</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Describes an algorithm for the generation phase of a Shake-and-Bake Machine
Translation system. Since the problem is NP-complete, it is unlikely that the
algorithm will be efficient in all cases, but for the cases tested it offers an
improvement over Whitelock's previously published algorithm. The work was
carried out while the author was employed at Sharp Laboratories of Europe Ltd.
</dc:description>
 <dc:description>Comment: 9 pages, published in proceedings of COLING-92, gzipped postscript</dc:description>
 <dc:date>1995-11-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Effect of Resource Limits and Task Complexity on Collaborative
  Planning in Dialogue</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows how agents' choice in communicative action can be designed
to mitigate the effect of their resource limits in the context of particular
features of a collaborative planning task. I first motivate a number of
hypotheses about effective language behavior based on a statistical analysis of
a corpus of natural collaborative planning dialogues. These hypotheses are then
tested in a dialogue testbed whose design is motivated by the corpus analysis.
Experiments in the testbed examine the interaction between (1) agents' resource
limits in attentional capacity and inferential capacity; (2) agents' choice in
communication; and (3) features of communicative tasks that affect task
difficulty such as inferential complexity, degree of belief coordination
required, and tolerance for errors. The results show that good algorithms for
communication must be defined relative to the agents' resource limits and the
features of the task. Algorithms that are inefficient for inferentially simple,
low coordination or fault-tolerant tasks are effective when tasks require
coordination or complex inferences, or are fault-intolerant. The results
provide an explanation for the occurrence of utterances in human dialogues
that, prima facie, appear inefficient, and provide the basis for the design of
effective algorithms for communicative choice for resource limited agents.
</dc:description>
 <dc:description>Comment: 64 pages, uses psfig, lingmacros, named</dc:description>
 <dc:date>1995-11-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511003</dc:identifier>
 <dc:identifier>Artificial Intelligence Journal 85(1-2), pp. 181-243, 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An investigation into the correlation of cue phrases, unfilled pauses
  and the structuring of spoken discourse</dc:title>
 <dc:creator>Cahn, Janet</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Expectations about the correlation of cue phrases, the duration of unfilled
pauses and the structuring of spoken discourse are framed in light of Grosz and
Sidner's theory of discourse and are tested for a directions-giving dialogue.
The results suggest that cue phrase and discourse structuring tasks may align,
and show a correlation for pause length and some of the modifications that
speakers can make to discourse structure.
</dc:description>
 <dc:description>Comment: 12 pages, uufile'd source includes 3 .sty files - darpasls.sty,
  times.sty, psfonts.sty</dc:description>
 <dc:date>1995-11-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511004</dc:identifier>
 <dc:identifier>Proceedings of the IRCS Workshop on Prosody in Natural Speech,
  University of Pennsylvania. (1992) 19-30</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Chart-driven Connectionist Categorial Parsing of Spoken Korean</dc:title>
 <dc:creator>Lee, WonIl</dc:creator>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  While most of the speech and natural language systems which were developed
for English and other Indo-European languages neglect the morphological
processing and integrate speech and natural language at the word level, for the
agglutinative languages such as Korean and Japanese, the morphological
processing plays a major role in the language processing since these languages
have very complex morphological phenomena and relatively simple syntactic
functionality. Obviously degenerated morphological processing limits the usable
vocabulary size for the system and word-level dictionary results in exponential
explosion in the number of dictionary entries. For the agglutinative languages,
we need sub-word level integration which leaves rooms for general morphological
processing. In this paper, we developed a phoneme-level integration model of
speech and linguistic processings through general morphological analysis for
agglutinative languages and a efficient parsing scheme for that integration.
Korean is modeled lexically based on the categorial grammar formalism with
unordered argument and suppressed category extensions, and chart-driven
connectionist parsing method is introduced.
</dc:description>
 <dc:description>Comment: 6 pages, Postscript file, Proceedings of ICCPOL'95</dc:description>
 <dc:date>1995-11-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Disambiguating Noun Groupings with Respect to WordNet Senses</dc:title>
 <dc:creator>Resnik, Philip</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Word groupings useful for language processing tasks are increasingly
available, as thesauri appear on-line, and as distributional word clustering
techniques improve. However, for many tasks, one is interested in relationships
among word {\em senses}, not words. This paper presents a method for automatic
sense disambiguation of nouns appearing within sets of related nouns --- the
kind of data one finds in on-line thesauri, or as the output of distributional
clustering algorithms. Disambiguation is performed with respect to WordNet
senses, which are fairly fine-grained; however, the method also permits the
assignment of higher-level WordNet categories rather than sense labels. The
method is illustrated primarily by example, though results of a more rigorous
evaluation are also presented.
</dc:description>
 <dc:description>Comment: LaTeX, 16 pages, uses breakcites.sty, authdate.sty</dc:description>
 <dc:date>1995-11-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511006</dc:identifier>
 <dc:identifier>Proceedings of the 3rd Workshop on Very Large Corpora, MIT, 30
  June 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9511007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy</dc:title>
 <dc:creator>Resnik, Philip</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a new measure of semantic similarity in an IS-A taxonomy,
based on the notion of information content. Experimental evaluation suggests
that the measure performs encouragingly well (a correlation of r = 0.79 with a
benchmark set of human similarity judgments, with an upper bound of r = 0.90
for human subjects performing the same task), and significantly better than the
traditional edge counting approach (r = 0.66).
</dc:description>
 <dc:description>Comment: 6 pages, 2 postscript figures, uses ijcai95.sty</dc:description>
 <dc:date>1995-11-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9511007</dc:identifier>
 <dc:identifier>Proceedings of the 14th International Joint Conference on
  Artificial Intelligence</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Analysis of the Arabic Broken Plural and Diminutive</dc:title>
 <dc:creator>Kiraz, George A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper demonstrates how the challenging problem of the Arabic broken
plural and diminutive can be handled under a multi-tape two-level model, an
extension to two-level morphology.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded compressed .ps file, ICEMCO-96: 5th Inter. Conf.
  and Exhibition on Multi-Lingual Computing, Cambridge</dc:description>
 <dc:date>1995-12-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9512001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Unsupervised Acquisition of a Lexicon from Continuous Speech</dc:title>
 <dc:creator>de Marcken, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an unsupervised learning algorithm that acquires a
natural-language lexicon from raw speech. The algorithm is based on the optimal
encoding of symbol sequences in an MDL framework, and uses a hierarchical
representation of language that overcomes many of the problems that have
stymied previous grammar-induction procedures. The forward mapping from symbol
sequences to the speech stream is modeled using features based on articulatory
gestures. We present results on the acquisition of lexicons and language models
from raw speech, text, and phonetic transcripts, and demonstrate that our
algorithm compares very favorably to other reported results with respect to
segmentation performance and statistical efficiency.
</dc:description>
 <dc:description>Comment: 27 page technical report</dc:description>
 <dc:date>1995-12-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9512002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Limited Attention and Discourse Structure</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This squib examines the role of limited attention in a theory of discourse
structure and proposes a model of attentional state that relates current
hierarchical theories of discourse structure to empirical evidence about human
discourse processing capabilities. First, I present examples that are not
predicted by Grosz and Sidner's stack model of attentional state. Then I
consider an alternative model of attentional state, the cache model, which
accounts for the examples, and which makes particular processing predictions.
Finally I suggest a number of ways that future research could distinguish the
predictions of the cache model and the stack model.
</dc:description>
 <dc:description>Comment: 9 pages, uses twoside,cl,lingmacros</dc:description>
 <dc:date>1995-12-18</dc:date>
 <dc:date>1996-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9512003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural language processing: she needs something old and something new
  (maybe something borrowed and something blue, too)</dc:title>
 <dc:creator>Jones, Karen Sparck</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Given the present state of work in natural language processing, this address
argues first, that advance in both science and applications requires a revival
of concern about what language is about, broadly speaking the world; and
second, that an attack on the summarising task, which is made ever more
important by the growth of electronic text resources and requires an
understanding of the role of large-scale discourse structure in marking
important text content, is a good way forward.
</dc:description>
 <dc:description>Comment: Presidential Address, 1994, Association for Computational Linguistics</dc:description>
 <dc:date>1995-12-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9512004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9512005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Term Encoding of Typed Feature Structures</dc:title>
 <dc:creator>Gerdemann, Dale</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an approach to Prolog-style term encoding of typed
feature structures. The type feature structures to be encoded are constrained
by appropriateness conditions as in Carpenter's ALE system. But unlike ALE, we
impose a further independently motivated closed-world assumption. This
assumption allows us to apply term encoding in cases that were problematic for
previous approaches. In particular, previous approaches have ruled out multiple
inheritance and further specification of feature-value declarations on
subtypes. In the present approach, these spececial cases can be handled as
well, though with some increase in complexity. For grammars without multiple
inheritance and specification of feature values, the encoding presented here
reduces to that of previous approaches.
</dc:description>
 <dc:description>Comment: 12 pages Latex2e, ecltree, epic, eepic, tree-dvips</dc:description>
 <dc:date>1995-12-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9512005</dc:identifier>
 <dc:identifier>Proceedings of the Fourth International Workshop on Parsing
  Technologies, pp. 89-98, 1995</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Inference of DATR Theories</dc:title>
 <dc:creator>Barg, Petra</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an approach for the automatic acquisition of linguistic
knowledge from unstructured data. The acquired knowledge is represented in the
lexical knowledge representation language DATR. A set of transformation rules
that establish inheritance relationships and a default-inference algorithm make
up the basis components of the system. Since the overall approach is not
restricted to a special domain, the heuristic inference strategy uses criteria
to evaluate the quality of a DATR theory, where different domains may require
different criteria. The system is applied to the linguistic learning task of
German noun inflection.
</dc:description>
 <dc:description>Comment: Latex 10 pages, 1 Postscript figure. To appear in H.-H. Bock, W.
  Polasek (eds.) Data Analysis and Information Systems: Statistical and
  conceptual approaches (Proceedings of the 19th Annual Conference of the
  Gesellschaft fuer Klassifikation e.V., University of Basel), Springer Verlag,
  pp. 506-515</dc:description>
 <dc:date>1996-01-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generic rules and non-constituent coordination</dc:title>
 <dc:creator>Gonzalo, Julio</dc:creator>
 <dc:creator>Solias, Teresa</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a metagrammatical formalism, {\em generic rules}, to give a
default interpretation to grammar rules. Our formalism introduces a process of
{\em dynamic binding} interfacing the level of pure grammatical knowledge
representation and the parsing level. We present an approach to non-constituent
coordination within categorial grammars, and reformulate it as a generic rule.
This reformulation is context-free parsable and reduces drastically the search
space associated to the parsing task for such phenomena.
</dc:description>
 <dc:description>Comment: latex2e, 12 pages, uses tree-dvips.sty. Appeared in IWPT'95</dc:description>
 <dc:date>1996-01-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601002</dc:identifier>
 <dc:identifier>IV International Workshop on Parsing Technologies (IWPT 95)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601003</identifier>
 <datestamp>2008-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Report of the Study Group on Assessment and Evaluation</dc:title>
 <dc:creator>Crouch, Richard</dc:creator>
 <dc:creator>Gaizauskas, Robert</dc:creator>
 <dc:creator>Netter, Klaus</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This is an interim report discussing possible guidelines for the assessment
and evaluation of projects developing speech and language systems. It was
prepared at the request of the European Commission DG XIII by an ad hoc study
group, and is now being made available in the form in which it was submitted to
the Commission. However, the report is not an official European Commission
document, and does not reflect European Commission policy, official or
otherwise.
  After a discussion of terminology, the report focusses on combining
user-centred and technology-centred assessment, and on how meaningful
comparisons can be made of a variety of systems performing different tasks for
different domains. The report outlines the kind of infra-structure that might
be required to support comparative assessment and evaluation of heterogenous
projects, and also the results of a questionnaire concerning different
approaches to evaluation.
</dc:description>
 <dc:description>Comment: 83 pages</dc:description>
 <dc:date>1996-01-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Similarity between Words Computed by Spreading Activation on an English
  Dictionary</dc:title>
 <dc:creator>Kozima, Hideki</dc:creator>
 <dc:creator>Furugori, Teiji</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes a method for measuring semantic similarity between words
as a new tool for text analysis. The similarity is measured on a semantic
network constructed systematically from a subset of the English dictionary,
LDOCE (Longman Dictionary of Contemporary English). Spreading activation on the
network can directly compute the similarity between any two words in the
Longman Defining Vocabulary, and indirectly the similarity of all the other
words in LDOCE. The similarity represents the strength of lexical cohesion or
semantic relation, and also provides valuable information about similarity and
coherence of texts.
</dc:description>
 <dc:description>Comment: 8 pages, uufiles (paper.tex, eacl93.sty, named.bst)</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601004</dc:identifier>
 <dc:identifier>Proceedings of EACL-93 (Utrecht), pp.232-239, 1993.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Text Segmentation Based on Similarity between Words</dc:title>
 <dc:creator>Kozima, Hideki</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes a new indicator of text structure, called the lexical
cohesion profile (LCP), which locates segment boundaries in a text. A text
segment is a coherent scene; the words in a segment are linked together via
lexical cohesion relations. LCP records mutual similarity of words in a
sequence of text. The similarity of words, which represents their cohesiveness,
is computed using a semantic network. Comparison with the text segments marked
by a number of subjects shows that LCP closely correlates with the human
judgments. LCP may provide valuable information for resolving anaphora and
ellipsis.
</dc:description>
 <dc:description>Comment: 3 pages, uufiles (paper.tex, acl.sty, bezier.sty)</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601005</dc:identifier>
 <dc:identifier>Proceedings of ACL-93 (Ohio), pp.286-288, 1993.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Possessive Pronouns as Determiners in Japanese-to-English Machine
  Translation</dc:title>
 <dc:creator>Bond, Francis</dc:creator>
 <dc:creator>Ogura, Kentaro</dc:creator>
 <dc:creator>Ikehara, Satoru</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Possessive pronouns are used as determiners in English when no equivalent
would be used in a Japanese sentence with the same meaning. This paper proposes
a heuristic method of generating such possessive pronouns even when there is no
equivalent in the Japanese. The method uses information about the use of
possessive pronouns in English treated as a lexical property of nouns, in
addition to contextual information about noun phrase referentiality and the
subject and main verb of the sentence that the noun phrase appears in. The
proposed method has been implemented in NTT Communication Science Laboratories'
Japanese-to-English machine translation system ALT-J/E. In a test set of 6,200
sentences, the proposed method increased the number of noun phrases with
appropriate possessive pronouns generated, by 263 to 609, at the cost of
generating 83 noun phrases with inappropriate possessive pronouns.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX, uses twocolumn.sty, lsalike.sty</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Context-Sensitive Measurement of Word Distance by Adaptive Scaling of a
  Semantic Space</dc:title>
 <dc:creator>Kozima, Hideki</dc:creator>
 <dc:creator>Ito, Akira</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper proposes a computationally feasible method for measuring
context-sensitive semantic distance between words. The distance is computed by
adaptive scaling of a semantic space. In the semantic space, each word in the
vocabulary V is represented by a multi-dimensional vector which is obtained
from an English dictionary through a principal component analysis. Given a word
set C which specifies a context for measuring word distance, each dimension of
the semantic space is scaled up or down according to the distribution of C in
the semantic space. In the space thus transformed, distance between words in V
becomes dependent on the context C. An evaluation through a word prediction
task shows that the proposed measurement successfully extracts the context of a
text.
</dc:description>
 <dc:description>Comment: 8 pages, single LaTeX file</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:date>1996-06-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Noun Phrase Reference in Japanese-to-English Machine Translation</dc:title>
 <dc:creator>Bond, Francis</dc:creator>
 <dc:creator>Ogura, Kentaro</dc:creator>
 <dc:creator>Kawaoka, Tsukasa</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows the necessity of distinguishing different referential uses
of noun phrases in machine translation. We argue that differentiating between
the generic, referential and ascriptive uses of noun phrases is the minimum
necessary to generate articles and number correctly when translating from
Japanese to English. Heuristics for determining these differences are proposed
for a Japanese-to-English machine translation system. Finally the results of
using the proposed heuristics are shown to have raised the percentage of noun
phrases generated with correct use of articles and number in the
Japanese-to-English machine translation system ALT-J/E from 65% to 77%.
</dc:description>
 <dc:description>Comment: 12 pages, LaTeX, uses 11pt, a4wide, lsalike.sty</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A General Architecture for Language Engineering (GATE) - a new approach
  to Language Engineering R&amp;D</dc:title>
 <dc:creator>Cunningham, Hamish</dc:creator>
 <dc:creator>Gaizauskas, Robert J.</dc:creator>
 <dc:creator>Wilks, Yorick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This report argues for the provision of a common software infrastructure for
NLP systems. Current trends in Language Engineering research are reviewed as
motivation for this infrastructure, and relevant recent work discussed. A
freely-available system called GATE is described which builds on this work.
</dc:description>
 <dc:description>Comment: 52 page technical report, LaTeX 2e source</dc:description>
 <dc:date>1996-01-23</dc:date>
 <dc:date>1996-01-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601010</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing with Typed Feature Structures</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we provide for parsing with respect to grammars expressed in a
general TFS-based formalism, a restriction of ALE. Our motivation being the
design of an abstract (WAM-like) machine for the formalism, we consider parsing
as a computational process and use it as an operational semantics to guide the
design of the control structures for the abstract machine.
  We emphasize the notion of abstract typed feature structures (AFSs) that
encode the essential information of TFSs and define unification over AFSs
rather than over TFSs. We then introduce an explicit construct of multi-rooted
feature structures (MRSs) that naturally extend TFSs and use them to represent
phrasal signs as well as grammar rules. We also employ abstractions of MRSs and
give the mathematical foundations needed for manipulating them. We then present
a simple bottom-up chart parser as a model for computation: grammars written in
the TFS-based formalism are executed by the parser. Finally, we show that the
parser is correct.
</dc:description>
 <dc:description>Comment: PostScript, 15 pages; Proc. 4th Intl. Workshop on Parsing
  Technologies, Prague, September 1995</dc:description>
 <dc:date>1996-01-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9601011</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing with Typed Feature Structures</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we provide for parsing with respect to grammars expressed in a
general TFS-based formalism, a restriction of ALE. Our motivation being the
design of an abstract (WAM-like) machine for the formalism, we consider parsing
as a computational process and use it as an operational semantics to guide the
design of the control structures for the abstract machine.
  We emphasize the notion of abstract typed feature structures (AFSs) that
encode the essential information of TFSs and define unification over AFSs
rather than over TFSs. We then introduce an explicit construct of multi-rooted
feature structures (MRSs) that naturally extend TFSs and use them to represent
phrasal signs as well as grammar rules. We also employ abstractions of MRSs and
give the mathematical foundations needed for manipulating them. We formally
define grammars and the languages they generate, and then describe a model for
computation that corresponds to bottom-up chart parsing: grammars written in
the TFS-based formalism are executed by the parser. We show that the
computation is correct with respect to the independent definition. Finally, we
discuss the class of grammars for which computations terminate and prove that
termination can be guaranteed for off-line parsable grammars.
</dc:description>
 <dc:description>Comment: PostScript, 29 pages</dc:description>
 <dc:date>1996-01-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9601011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>How Part-of-Speech Tags Affect Text Retrieval and Filtering Performance</dc:title>
 <dc:creator>Losee, Robert M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Natural language processing (NLP) applied to information retrieval (IR) and
filtering problems may assign part-of-speech tags to terms and, more generally,
modify queries and documents. Analytic models can predict the performance of a
text filtering system as it incorporates changes suggested by NLP, allowing us
to make precise statements about the average effect of NLP operations on IR.
Here we provide a model of retrieval and tagging that allows us to both compute
the performance change due to syntactic parsing and to allow us to understand
what factors affect performance and how. In addition to a prediction of
performance with tags, upper and lower bounds for retrieval performance are
derived, giving the best and worst effects of including part-of-speech tags.
Empirical grounds for selecting sets of tags are considered.
</dc:description>
 <dc:description>Comment: uuencoded and compressed postscript</dc:description>
 <dc:date>1996-02-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9602001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Situations and Computation: An Overview of Recent Research</dc:title>
 <dc:creator>TIN, Erkan</dc:creator>
 <dc:creator>AKMAN, Varol</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Serious thinking about the computational aspects of situation theory is just
starting. There have been some recent proposals in this direction (viz. PROSIT
and ASTL), with varying degrees of divergence from the ontology of the theory.
We believe that a programming environment incorporating bona fide
situation-theoretic constructs is needed and describe our very recent BABY-SIT
implementation. A detailed critical account of PROSIT and ASTL is also offered
in order to compare our system with these pioneering and influential
frameworks.
</dc:description>
 <dc:description>Comment: 30 pages, also published in the University of Tuebingen Technical
  Report Series</dc:description>
 <dc:date>1996-02-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9602002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Text Windows and Phrases Differing by Discipline, Location in Document,
  and Syntactic Structure</dc:title>
 <dc:creator>Losee, Robert M.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Knowledge of window style, content, location and grammatical structure may be
used to classify documents as originating within a particular discipline or may
be used to place a document on a theory versus practice spectrum. This
distinction is also studied here using the type-token ratio to differentiate
between sublanguages. The statistical significance of windows is computed,
based on the the presence of terms in titles, abstracts, citations, and section
headers, as well as binary independent (BI) and inverse document frequency
(IDF) weightings. The characteristics of windows are studied by examining their
within window density (WWD) and the S concentration (SC), the concentration of
terms from various document fields (e.g. title, abstract) in the fulltext. The
rate of window occurrences from the beginning to the end of document fulltext
differs between academic fields. Different syntactic structures in sublanguages
are examined, and their use is considered for discriminating between specific
academic disciplines and, more generally, between theory versus practice or
knowledge versus applications oriented documents.
</dc:description>
 <dc:description>Comment: uuencoded and compressed postscript</dc:description>
 <dc:date>1996-02-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9602003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9602004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Assessing agreement on classification tasks: the kappa statistic</dc:title>
 <dc:creator>Carletta, Jean</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Currently, computational linguists and cognitive scientists working in the
area of discourse and dialogue argue that their subjective judgments are
reliable using several different statistics, none of which are easily
interpretable or comparable to each other. Meanwhile, researchers in content
analysis have already experienced the same difficulties and come up with a
solution in the kappa statistic. We discuss what is wrong with reliability
measures as they are currently used for discourse and dialogue work in
computational linguistics and cognitive science, and argue that we would be
better off as a field adopting techniques from content analysis.
</dc:description>
 <dc:description>Comment: 9 pages</dc:description>
 <dc:date>1996-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9602004</dc:identifier>
 <dc:identifier>Computational Lingustics 22:2 (1996 forthcoming)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Speech Recognition by Composition of Weighted Finite Automata</dc:title>
 <dc:creator>Pereira, Fernando C. N.</dc:creator>
 <dc:creator>Riley, Michael D.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a general framework based on weighted finite automata and weighted
finite-state transducers for describing and implementing speech recognizers.
The framework allows us to represent uniformly the information sources and data
structures used in recognition, including context-dependent units,
pronunciation dictionaries, language models and lattices. Furthermore, general
but efficient algorithms can used for combining information sources in actual
recognizers and for optimizing their application. In particular, a single
composition algorithm is used both to combine in advance information sources
such as language models and dictionaries, and to combine acoustic observations
and information sources dynamically during recognition.
</dc:description>
 <dc:description>Comment: 24 pages, uses psfig.sty</dc:description>
 <dc:date>1996-03-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Finite-State Approximation of Phrase-Structure Grammars</dc:title>
 <dc:creator>Pereira, Fernando C. N.</dc:creator>
 <dc:creator>Wright, Rebecca N.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Phrase-structure grammars are effective models for important syntactic and
semantic aspects of natural languages, but can be computationally too demanding
for use as language models in real-time speech recognition. Therefore,
finite-state models are used instead, even though they lack expressive power.
To reconcile those two alternatives, we designed an algorithm to compute
finite-state approximations of context-free grammars and
context-free-equivalent augmented phrase-structure grammars. The approximation
is exact for certain context-free grammars generating regular languages,
including all left-linear and right-linear context-free grammars. The algorithm
has been used to build finite-state language models for limited-domain speech
recognition tasks.
</dc:description>
 <dc:description>Comment: 24 pages, uses psfig.sty; revised and extended version of the 1991
  ACL meeting paper with the same title</dc:description>
 <dc:date>1996-03-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Attempto Controlled English (ACE)</dc:title>
 <dc:creator>Fuchs, Norbert E.</dc:creator>
 <dc:creator>Schwitter, Rolf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Attempto Controlled English (ACE) allows domain specialists to interactively
formulate requirements specifications in domain concepts. ACE can be accurately
and efficiently processed by a computer, but is expressive enough to allow
natural usage. The Attempto system translates specification texts in ACE into
discourse representation structures and optionally into Prolog. Translated
specification texts are incrementally added to a knowledge base. This knowledge
base can be queried in ACE for verification, and it can be executed for
simulation, prototyping and validation of the specification.
</dc:description>
 <dc:description>Comment: 13 pages, compressed, uuencoded Postscript, to be presented at CLAW
  96, The First International Workshop on Controlled Language Applications,
  Katholieke Universiteit Leuven, 26-27 March 1996</dc:description>
 <dc:date>1996-03-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Attempto - From Specifications in Controlled Natural Language towards
  Executable Specifications</dc:title>
 <dc:creator>Schwitter, Rolf</dc:creator>
 <dc:creator>Fuchs, Norbert E.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Deriving formal specifications from informal requirements is difficult since
one has to take into account the disparate conceptual worlds of the application
domain and of software development. To bridge the conceptual gap we propose
controlled natural language as a textual view on formal specifications in
logic. The specification language Attempto Controlled English (ACE) is a subset
of natural language that can be accurately and efficiently processed by a
computer, but is expressive enough to allow natural usage. The Attempto system
translates specifications in ACE into discourse representation structures and
into Prolog. The resulting knowledge base can be queried in ACE for
verification, and it can be executed for simulation, prototyping and validation
of the specification.
</dc:description>
 <dc:description>Comment: 15 pages, compressed, uuencoded Postscript, to be presented at EMISA
  Workshop 'Naturlichsprachlicher Entwurf von Informationssystemen -
  Grundlagen, Methoden, Werkzeuge, Anwendungen', May 28-30, 1996, Ev. Akademie
  Tutzing</dc:description>
 <dc:date>1996-03-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrated speech and morphological processing in a connectionist
  continuous speech understanding for Korean</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A new tightly coupled speech and natural language integration model is
presented for a TDNN-based continuous possibly large vocabulary speech
recognition system for Korean. Unlike popular n-best techniques developed for
integrating mainly HMM-based speech recognition and natural language processing
in a {\em word level}, which is obviously inadequate for morphologically
complex agglutinative languages, our model constructs a spoken language system
based on a {\em morpheme-level} speech and language integration. With this
integration scheme, the spoken Korean processing engine (SKOPE) is designed and
implemented using a TDNN-based diphone recognition module integrated with a
Viterbi-based lexical decoding and symbolic phonological/morphological
co-analysis. Our experiment results show that the speaker-dependent continuous
{\em eojeol} (Korean word) recognition and integrated morphological analysis
can be achieved with over 80.6% success rate directly from speech inputs for
the middle-level vocabularies.
</dc:description>
 <dc:description>Comment: latex source with a4 style, 15 pages, to be published in computer
  processing of oriental language journal</dc:description>
 <dc:date>1996-03-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9603006</identifier>
 <datestamp>2016-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Extraction of V-N-Collocations from Text Corpora: A Feasibility Study
  for German</dc:title>
 <dc:creator>Breidt, Elisabeth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The usefulness of a statistical approach suggested by Church et al. (1991) is
evaluated for the extraction of verb-noun (V-N) collocations from German text
corpora. Some problematic issues of that method arising from properties of the
German language are discussed and various modifications of the method are
considered that might improve extraction results for German. The precision and
recall of all variant methods is evaluated for V-N collocations containing
support verbs, and the consequences for further work on the extraction of
collocations from German corpora are discussed.
  With a sufficiently large corpus (&gt;= 6 mio. word-tokens), the average error
rate of wrong extractions can be reduced to 2.2% (97.8% precision) with the
most restrictive method, however with a loss in data of almost 50% compared to
a less restrictive method with still 87.6% precision. Depending on the goal to
be achieved, emphasis can be put on a high recall for lexicographic purposes or
on high precision for automatic lexical acquisition, in each case unfortunately
leading to a decrease of the corresponding other variable. Low recall can still
be acceptable if very large corpora (i.e. 50 - 100 million words) are available
or if corpora for special domains are used in addition to the data found in
machine readable (collocation) dictionaries.
</dc:description>
 <dc:description>Comment: 12 pages, revised version of paper presented at 1st ACL-Workshop on
  Very Large Corpora, Columbus, Ohio, June 1993</dc:description>
 <dc:date>1996-03-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9603006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Hand-crafted Rules and Unsupervised Learning in
  Constraint-based Morphological Disambiguation</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Tur, Gokhan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a constraint-based morphological disambiguation approach
that is applicable languages with complex morphology--specifically
agglutinative languages with productive inflectional and derivational
morphological phenomena. In certain respects, our approach has been motivated
by Brill's recent work, but with the observation that his transformational
approach is not directly applicable to languages like Turkish. Our system
combines corpus independent hand-crafted constraint rules, constraint rules
that are learned via unsupervised learning from a training corpus, and
additional statistical information from the corpus to be morphologically
disambiguated. The hand-crafted rules are linguistically motivated and tuned to
improve precision without sacrificing recall. The unsupervised learning process
produces two sets of rules: (i) choose rules which choose morphological parses
of a lexical item satisfying constraint effectively discarding other parses,
and (ii) delete rules, which delete parses satisfying a constraint. Our
approach also uses a novel approach to unknown word processing by employing a
secondary morphological processor which recovers any relevant inflectional and
derivational information from a lexical item whose root is unknown. With this
approach, well below 1 percent of the tokens remains as unknown in the texts we
have experimented with. Our results indicate that by combining these
hand-crafted,statistical and learned information sources, we can attain a
recall of 96 to 97 percent with a corresponding precision of 93 to 94 percent,
and ambiguity of 1.02 to 1.03 parses per token.
</dc:description>
 <dc:description>Comment: gzipped and uuencoded postscript, 13 pages. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/emnlp.ps.z</dc:description>
 <dc:date>1996-04-11</dc:date>
 <dc:date>1996-04-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Constraint-based Case Frame Lexicon</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Yilmaz, Okan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a constraint-based case frame lexicon architecture for
bi-directional mapping between a syntactic case frame and a semantic frame. The
lexicon uses a semantic sense as the basic unit and employs a multi-tiered
constraint structure for the resolution of syntactic information into the
appropriate senses and/or idiomatic usage. Valency changing transformations
such as morphologically marked passivized or causativized forms are handled via
lexical rules that manipulate case frames templates. The system has been
implemented in a typed-feature system and applied to Turkish.
</dc:description>
 <dc:description>Comment: gzipped, uuencoded postscript, 6 pages. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/coling96-ccl.ps.z ; To Appear in
  Proceedings of COLING 96, Copenhaged, Denmark, August 1996</dc:description>
 <dc:date>1996-04-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Error-tolerant Tree Matching</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an efficient algorithm for retrieving from a database of
trees, all trees that match a given query tree approximately, that is, within a
certain error tolerance. It has natural language processing applications in
searching for matches in example-based translation systems, and retrieval from
lexical databases containing entries of complex feature structures. The
algorithm has been implemented on SparcStations, and for large randomly
generated synthetic tree databases (some having tens of thousands of trees) it
can associatively search for trees with a small error, in a matter of tenths of
a second to few seconds.
</dc:description>
 <dc:description>Comment: gzipped and uuencoded postscript, 5 pages. Minor fix in one of the
  figures. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/coling96-ettm.ps.z</dc:description>
 <dc:date>1996-04-11</dc:date>
 <dc:date>1996-04-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Apportioning Development Effort in a Probabilistic LR Parsing System
  through Evaluation</dc:title>
 <dc:creator>Carroll, John</dc:creator>
 <dc:creator>Briscoe, Ted</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an implemented system for robust domain-independent syntactic
parsing of English, using a unification-based grammar of part-of-speech and
punctuation labels coupled with a probabilistic LR parser. We present
evaluations of the system's performance along several different dimensions;
these enable us to assess the contribution that each individual part is making
to the success of the system as a whole, and thus prioritise the effort to be
devoted to its further enhancement. Currently, the system is able to parse
around 80% of sentences in a substantial corpus of general text containing a
number of distinct genres. On a random sample of 250 such sentences the system
has a mean crossing bracket rate of 0.71 and recall and precision of 83% and
84% respectively when evaluated against manually-disambiguated analyses.
</dc:description>
 <dc:description>Comment: 10 pages, 1 Postscript figure. To Appear in Proceedings of the
  Conference on Empirical Methods in Natural Language Processing, University of
  Pennsylvania, May 1996</dc:description>
 <dc:date>1996-04-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604004</dc:identifier>
 <dc:identifier>Conference on Empirical Methods in Natural Language Processing
  (EMNLP-96), 92-100</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Better Language Models with Model Merging</dc:title>
 <dc:creator>Brants, Thorsten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper investigates model merging, a technique for deriving Markov models
from text or speech corpora. Models are derived by starting with a large and
specific model and by successively combining states to build smaller and more
general models. We present methods to reduce the time complexity of the
algorithm and report on experiments on deriving language models for a speech
recognition task. The experiments show the advantage of model merging over the
standard bigram approach. The merged model assigns a lower perplexity to the
test set and uses considerably fewer states.
</dc:description>
 <dc:description>Comment: LaTeX, 9 pages. In Proceedings of EMNLP-96, Philadelphia, PA</dc:description>
 <dc:date>1996-04-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Role of the Gricean Maxims in the Generation of Referring
  Expressions</dc:title>
 <dc:creator>Dale, Robert</dc:creator>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Grice's maxims of conversation [Grice 1975] are framed as directives to be
followed by a speaker of the language. This paper argues that, when considered
from the point of view of natural language generation, such a characterisation
is rather misleading, and that the desired behaviour falls out quite naturally
if we view language generation as a goal-oriented process. We argue this
position with particular regard to the generation of referring expressions.
</dc:description>
 <dc:description>Comment: LaTeX file, needs aaai.sty (available from the cmp-lg macro library).
  This paper was presented at the 1996 AAAI Spring Symposium on Computational
  Models of Conversational Implicature</dc:description>
 <dc:date>1996-04-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Collocational Grammar</dc:title>
 <dc:creator>Freeman, Robert John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A perspective of statistical language models which emphasizes their
collocational aspect is advocated. It is suggested that strings be generalized
in terms of classes of relationships instead of classes of objects. The single
most important characteristic of such a model is a mechanism for comparing
patterns. When patterns are fully generalized a natural definition of syntactic
class emerges as a subset of relational class. These collocational syntactic
classes should be an unambiguous partition of traditional syntactic classes.
</dc:description>
 <dc:description>Comment: 7 pages, uuencoded gzipped Postscript</dc:description>
 <dc:date>1996-04-18</dc:date>
 <dc:date>1996-04-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Algorithms for Parsing the DOP Model</dc:title>
 <dc:creator>Goodman, Joshua</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Excellent results have been reported for Data-Oriented Parsing (DOP) of
natural language texts (Bod, 1993). Unfortunately, existing algorithms are both
computationally intensive and difficult to implement. Previous algorithms are
expensive due to two factors: the exponential number of rules that must be
generated and the use of a Monte Carlo parsing algorithm. In this paper we
solve the first problem by a novel reduction of the DOP model to a small,
equivalent probabilistic context-free grammar. We solve the second problem by a
novel deterministic parsing strategy that maximizes the expected number of
correct constituents, rather than the probability of a correct parse tree.
Using the optimizations, experiments yield a 97% crossing brackets rate and 88%
zero crossing brackets rate. This differs significantly from the results
reported by Bod, and is comparable to results from a duplication of Pereira and
Schabes's (1992) experiment on the same data. We show that Bod's results are at
least partially due to an extremely fortuitous choice of test data, and
partially due to using cleaner data than other researchers.
</dc:description>
 <dc:description>Comment: 10 pages</dc:description>
 <dc:date>1996-04-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604008</dc:identifier>
 <dc:identifier>Proceedings of the Conference on Empirical Methods in Natural
  Language Processing, May 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Another Facet of LIG Parsing</dc:title>
 <dc:creator>Boullier, Pierre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present a new parsing algorithm for linear indexed grammars
(LIGs) in the same spirit as the one described in (Vijay-Shanker and Weir,
1993) for tree adjoining grammars. For a LIG $L$ and an input string $x$ of
length $n$, we build a non ambiguous context-free grammar whose sentences are
all (and exclusively) valid derivation sequences in $L$ which lead to $x$. We
show that this grammar can be built in ${\cal O}(n^6)$ time and that individual
parses can be extracted in linear time with the size of the extracted parse
tree. Though this ${\cal O}(n^6)$ upper bound does not improve over previous
results, the average case behaves much better. Moreover, practical parsing
times can be decreased by some statically performed computations.
</dc:description>
 <dc:description>Comment: LaTex, 8 pages. To appear in Proceedings of ACL'96, Univ. of
  California, Santa Cruz, June 1996</dc:description>
 <dc:date>1996-04-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Off-line Constraint Propagation for Efficient HPSG Processing</dc:title>
 <dc:creator>Meurers, Walt Detmar</dc:creator>
 <dc:creator>Minnen, Guido</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We investigate the use of a technique developed in the constraint programming
community called constraint propagation to automatically make a HPSG theory
more specific at those places where linguistically motivated underspecification
would lead to inefficient processing. We discuss two concrete HPSG examples
showing how off-line constraint propagation helps improve processing
efficiency.
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded gzipped Postscript</dc:description>
 <dc:date>1996-04-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604010</dc:identifier>
 <dc:identifier>Proceedings HPSG/TALN Conference, Marseille, France, May 20-22</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multi-level post-processing for Korean character recognition using
  morphological analysis and linguistic evaluation</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:creator>Yoo, JinHee</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most of the post-processing methods for character recognition rely on
contextual information of character and word-fragment levels. However, due to
linguistic characteristics of Korean, such low-level information alone is not
sufficient for high-quality character-recognition applications, and we need
much higher-level contextual information to improve the recognition results.
This paper presents a domain independent post-processing technique that
utilizes multi-level morphological, syntactic, and semantic information as well
as character-level information. The proposed post-processing system performs
three-level processing: candidate character-set selection, candidate eojeol
(Korean word) generation through morphological analysis, and final single
eojeol-sequence selection by linguistic evaluation. All the required linguistic
information and probabilities are automatically acquired from a statistical
corpus analysis. Experimental results demonstrate the effectiveness of our
method, yielding error correction rate of 80.46%, and improved recognition rate
of 95.53% from before-post-processing rate 71.2% for single best-solution
selection.
</dc:description>
 <dc:description>Comment: latex with a4, epsfig style, 21 pages, 11 postscript figures,
  accepted in pattern recognition journal</dc:description>
 <dc:date>1996-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>SemHe: A Generalised Two-Level System</dc:title>
 <dc:creator>Kiraz, George Anton</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a generalised two-level implementation which can handle
linear and non-linear morphological operations. An algorithm for the
interpretation of multi-tape two-level rules is described. In addition, a
number of issues which arise when developing non-linear grammars are discussed
with examples from Syriac.
</dc:description>
 <dc:description>Comment: uuencoded Z-compressed .tar file created by csh script uufiles. 8
  pages. To appear in Proceedings of ACL'96, Univ. of California, Santa Cruz,
  June 1996</dc:description>
 <dc:date>1996-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Syntactic Analyses for Parallel Grammars: Auxiliaries and Genitive NPs</dc:title>
 <dc:creator>Butt, Miriaam</dc:creator>
 <dc:creator>Fortman, Christian</dc:creator>
 <dc:creator>Rohrer, Christian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper focuses on two disparate aspects of German syntax from the
perspective of parallel grammar development. As part of a cooperative project,
we present an innovative approach to auxiliaries and multiple genitive NPs in
German. The LFG-based implementation presented here avoids unnessary structural
complexity in the representation of auxiliaries by challenging the traditional
analysis of auxiliaries as raising verbs. The approach developed for multiple
genitive NPs provides a more abstract, language independent representation of
genitives associated with nominalized verbs. Taken together, the two approaches
represent a step towards providing uniformly applicable treatments for
differing languages, thus lightening the burden for machine translation.
</dc:description>
 <dc:description>Comment: To Appear in Proceedings of COLING'96 Copenhagen, Denmark, August
  1996</dc:description>
 <dc:date>1996-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The importance of being lazy -- using lazy evaluation to process queries
  to HPSG grammars</dc:title>
 <dc:creator>G&#xf6;tz, Thilo</dc:creator>
 <dc:creator>Meurers, Walt Detmar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Linguistic theories formulated in the architecture of {\sc hpsg} can be very
precise and explicit since {\sc hpsg} provides a formally well-defined setup.
However, when querying a faithful implementation of such an explicit theory,
the large data structures specified can make it hard to see the relevant
aspects of the reply given by the system. Furthermore, the system spends much
time applying constraints which can never fail just to be able to enumerate
specific answers. In this paper we want to describe lazy evaluation as the
result of an off-line compilation technique. This method of evaluation can be
used to answer queries to an {\sc hpsg} system so that only the relevant
aspects are checked and output.
</dc:description>
 <dc:description>Comment: uuencoded, gzipped postscript. In proceedings of TALN and HPSG 96,
  Marseille, France</dc:description>
 <dc:date>1996-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computing Prosodic Morphology</dc:title>
 <dc:creator>Kiraz, George Anton</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper establishes a framework under which various aspects of prosodic
morphology, such as templatic morphology and infixation, can be handled under
two-level theory using an implemented multi-tape two-level model. The paper
provides a new computational analysis of root-and-pattern morphology based on
prosody.
</dc:description>
 <dc:description>Comment: uuencoded Z-compressed .tar file created by csh script uufiles. 6
  pages. To appear in coling-96</dc:description>
 <dc:date>1996-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Processing Metonymy: a Domain-Model Heuristic Graph Traversal Approach</dc:title>
 <dc:creator>Bouaud, Jacques</dc:creator>
 <dc:creator>Bachimont, Bruno</dc:creator>
 <dc:creator>Zweigenbaum, Pierre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address here the treatment of metonymic expressions from a knowledge
representation perspective, that is, in the context of a text understanding
system which aims to build a conceptual representation from texts according to
a domain model expressed in a knowledge representation formalism.
  We focus in this paper on the part of the semantic analyser which deals with
semantic composition. We explain how we use the domain model to handle metonymy
dynamically, and more generally, to underlie semantic composition, using the
knowledge descriptions attached to each concept of our ontology as a kind of
concept-level, multiple-role qualia structure.
  We rely for this on a heuristic path search algorithm that exploits the
graphic aspects of the conceptual graphs formalism. The methods described have
been implemented and applied on French texts in the medical domain.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, one encapsulated PostScript figure, uses colap.sty
  (included) and epsf.sty (available from the cmp-lg macro library). To appear
  in Coling-96</dc:description>
 <dc:date>1996-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast Parsing using Pruning and Grammar Specialization</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We show how a general grammar may be automatically adapted for fast parsing
of utterances from a specific domain by means of constituent pruning and
grammar specialization based on explanation-based learning. These methods
together give an order of magnitude increase in speed, and the coverage loss
entailed by grammar specialization is reduced to approximately half that
reported in previous work. Experiments described here suggest that the loss of
coverage has been reduced to the point where it no longer causes significant
performance degradation in the context of a real application.
</dc:description>
 <dc:description>Comment: 8 pages LaTeX, ACL-96, needs aclap.sty; see also
  http://www.cam.sri.com/</dc:description>
 <dc:date>1996-04-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Measure of a Model</dc:title>
 <dc:creator>Bruce, Rebecca</dc:creator>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:creator>Pedersen, Ted</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes measures for evaluating the three determinants of how
well a probabilistic classifier performs on a given test set. These
determinants are the appropriateness, for the test set, of the results of (1)
feature selection, (2) formulation of the parametric form of the model, and (3)
parameter estimation. These are part of any model formulation procedure, even
if not broken out as separate steps, so the tradeoffs explored in this paper
are relevant to a wide variety of methods. The measures are demonstrated in a
large experiment, in which they are used to analyze the results of roughly 300
classifiers that perform word-sense disambiguation.
</dc:description>
 <dc:description>Comment: 12 pages, uuencoded compressed postscript file</dc:description>
 <dc:date>1996-04-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604018</dc:identifier>
 <dc:identifier>In Proceedings of the Empirical Methods in Natural Language
  Processing Conference, May 1996, Philadelphia, PA</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Magic for Filter Optimization in Dynamic Bottom-up Processing</dc:title>
 <dc:creator>Minnen, Guido</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Off-line compilation of logic grammars using Magic allows an incorporation of
filtering into the logic underlying the grammar. The explicit definite clause
characterization of filtering resulting from Magic compilation allows processor
independent and logically clean optimizations of dynamic bottom-up processing
with respect to goal-directedness. Two filter optimizations based on the
program transformation technique of Unfolding are discussed which are of
practical and theoretical interest.
</dc:description>
 <dc:description>Comment: 8 pages LaTeX (uses aclap.sty)</dc:description>
 <dc:date>1996-04-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604019</dc:identifier>
 <dc:identifier>Proceedings of ACL 96, Santa Cruz, USA, June 23-28</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Translating into Free Word Order Languages</dc:title>
 <dc:creator>Hoffman, Beryl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, I discuss machine translation of English text into Turkish, a
relatively ``free'' word order language. I present algorithms that determine
the topic and the focus of each target sentence (using salience (Centering
Theory), old vs. new information, and contrastiveness in the discourse model)
in order to generate the contextually appropriate word orders in the target
language.
</dc:description>
 <dc:description>Comment: COLING 1996, latex, 6 pages, colap = aclap = eaclap.sty</dc:description>
 <dc:date>1996-04-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Extended Dependency Structures and their Formal Interpretation</dc:title>
 <dc:creator>Dymetman, Marc</dc:creator>
 <dc:creator>Copperman, Max</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe two ``semantically-oriented'' dependency-structure formalisms,
U-forms and S-forms. U-forms have been previously used in machine translation
as interlingual representations, but without being provided with a formal
interpretation. S-forms, which we introduce in this paper, are a scoped version
of U-forms, and we define a compositional semantics mechanism for them. Two
types of semantic composition are basic: complement incorporation and modifier
incorporation. Binding of variables is done at the time of incorporation,
permitting much flexibility in composition order and a simple account of the
semantic effects of permuting several incorporations.
</dc:description>
 <dc:description>Comment: uuencoded gz-compressed .tar file created by csh script uufiles. 17
  pages. To appear in Proceedings of Coling-96. (Change from original
  submission: increased portability)</dc:description>
 <dc:date>1996-04-29</dc:date>
 <dc:date>1996-05-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Unsupervised Learning of Word-Category Guessing Rules</dc:title>
 <dc:creator>Mikheev, Andrei</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Words unknown to the lexicon present a substantial problem to part-of-speech
tagging. In this paper we present a technique for fully unsupervised
statistical acquisition of rules which guess possible parts-of-speech for
unknown words. Three complementary sets of word-guessing rules are induced from
the lexicon and a raw corpus: prefix morphological rules, suffix morphological
rules and ending-guessing rules. The learning was performed on the Brown Corpus
data and rule-sets, with a highly competitive performance, were produced and
compared with the state-of-the-art.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX (aclap.sty for ACL-96); Proceedings of ACL-96 Santa
  Cruz, USA; also see cmp-lg/9604025</dc:description>
 <dc:date>1996-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Model-Theoretic Framework for Theories of Syntax</dc:title>
 <dc:creator>Rogers, James</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A natural next step in the evolution of constraint-based grammar formalisms
from rewriting formalisms is to abstract fully away from the details of the
grammar mechanism---to express syntactic theories purely in terms of the
properties of the class of structures they license. By focusing on the
structural properties of languages rather than on mechanisms for generating or
checking structures that exhibit those properties, this model-theoretic
approach can offer simpler and significantly clearer expression of theories and
can potentially provide a uniform formalization, allowing disparate theories to
be compared on the basis of those properties. We discuss $\LKP$, a monadic
second-order logical framework for such an approach to syntax that has the
distinctive virtue of being superficially expressive---supporting direct
statement of most linguistically significant syntactic properties---but having
well-defined strong generative capacity---languages are definable in $\LKP$ iff
they are strongly context-free. We draw examples from the realms of GPSG and
GB.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of the 34th Annual Meeting of the
  Association for Computational Linguistics. Uses aclap.sty. 7 Pages</dc:description>
 <dc:date>1996-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Connectivity in Bag Generation</dc:title>
 <dc:creator>Trujillo, Arturo</dc:creator>
 <dc:creator>Berry, Simon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a pruning technique which can be used to reduce the
number of paths searched in rule-based bag generators of the type proposed by
\cite{poznanskietal95} and \cite{popowich95}. Pruning the search space in these
generators is important given the computational cost of bag generation. The
technique relies on a connectivity constraint between the semantic indices
associated with each lexical sign in a bag. Testing the algorithm on a range of
sentences shows reductions in the generation time and the number of edges
constructed.
</dc:description>
 <dc:description>Comment: Latex, 6 pages, needs colap.sty. To appear in COLING-96</dc:description>
 <dc:date>1996-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Part-of-Speech Guessing Rules from Lexicon: Extension to
  Non-Concatenative Operations</dc:title>
 <dc:creator>Mikheev, Andrei</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  One of the problems in part-of-speech tagging of real-word texts is that of
unknown to the lexicon words. In Mikheev (ACL-96 cmp-lg/9604022), a technique
for fully unsupervised statistical acquisition of rules which guess possible
parts-of-speech for unknown words was proposed. One of the over-simplification
assumed by this learning technique was the acquisition of morphological rules
which obey only simple concatenative regularities of the main word with an
affix. In this paper we extend this technique to the non-concatenative cases of
suffixation and assess the gain in the performance.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX (colap.sty for COLING-96); to appear in Proceedings of
  COLING-96</dc:description>
 <dc:date>1996-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9604026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards a Workbench for Acquisition of Domain Knowledge from Natural
  Language</dc:title>
 <dc:creator>Mikheev, Andrei</dc:creator>
 <dc:creator>Finch, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe an architecture and functionality of main
components of a workbench for an acquisition of domain knowledge from large
text corpora. The workbench supports an incremental process of corpus analysis
starting from a rough automatic extraction and organization of lexico-semantic
regularities and ending with a computer supported analysis of extracted data
and a semi-automatic refinement of obtained hypotheses. For doing this the
workbench employs methods from computational linguistics, information retrieval
and knowledge engineering. Although the workbench is currently under
implementation some of its components are already implemented and their
performance is illustrated with samples from engineering for a medical domain.
</dc:description>
 <dc:description>Comment: 8 pages, compressed postscript; Proceedings of EACL-95 Dublin,
  Ireland</dc:description>
 <dc:date>1996-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9604026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compiling a Partition-Based Two-Level Formalism</dc:title>
 <dc:creator>Grimley-Evans, Edmund</dc:creator>
 <dc:creator>Kiraz, George Anton</dc:creator>
 <dc:creator>Pulman, Stephen G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an algorithm for the compilation of a two (or more)
level orthographic or phonological rule notation into finite state transducers.
The notation is an alternative to the standard one deriving from Koskenniemi's
work: it is believed to have some practical descriptive advantages, and is
quite widely used, but has a different interpretation. Efficient interpreters
exist for the notation, but until now it has not been clear how to compile to
equivalent automata in a transparent way. The present paper shows how to do
this, using some of the conceptual tools provided by Kaplan and Kay's regular
relations calculus.
</dc:description>
 <dc:description>Comment: Uuencoded gz-compressed .tar file created by csh script uufiles,
  needs colap.sty and psfig (available from the cmp-lg macro library). 6 pages,
  to appear in COLING-96</dc:description>
 <dc:date>1996-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building Natural-Language Generation Systems</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This is a very short paper that briefly discusses some of the tasks that NLG
systems perform. It is of no research interest, but I have occasionally found
it useful as a way of introducing NLG to potential project collaborators who
know nothing about the field.
</dc:description>
 <dc:description>Comment: Standard LaTeX. A 3-page paper of no research interest, but
  occasionally useful in helping to explain applied NLG to people with little
  knowledge of the field. Presented at the AIPE workshop in Glasgow</dc:description>
 <dc:date>1996-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Yet Another Paper about Partial Verb Phrase Fronting in German</dc:title>
 <dc:creator>M&#xfc;ller, Stefan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I describe a very simple HPSG analysis for partial verb phrase fronting. I
will argue that the presented account is more adequate than others made during
the past years because it allows the description of constituents in fronted
positions with their modifier remaining in the non-fronted part of the
sentence. A problem with ill-formed signs that are admitted by all HPSG
accounts for partial verb phrase fronting known so far will be explained and a
solution will be suggested that uses the difference between combinatoric
relations of signs and their representation in word order domains.
</dc:description>
 <dc:description>Comment: COLING 1996, gz-compressed .tar file</dc:description>
 <dc:date>1996-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Higher-Order Coloured Unification and Natural Language Semantics</dc:title>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:creator>Kohlhase, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we show that Higher-Order Coloured Unification - a form of
unification developed for automated theorem proving - provides a general theory
for modeling the interface between the interpretation process and other sources
of linguistic, non semantic information. In particular, it provides the general
theory for the Primary Occurrence Restriction which (Dalrymple, Shieber and
Pereira, 1991)'s analysis called for.
</dc:description>
 <dc:description>Comment: 9 pages, LateX file, uses aclap.sty, To appear in Proceedings of
  ACL96</dc:description>
 <dc:date>1996-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Focus and Higher-Order Unification</dc:title>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:creator>Kohlhase, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Pulman has shown that Higher--Order Unification (HOU) can be used to model
the interpretation of focus. In this paper, we extend the unification--based
approach to cases which are often seen as a test--bed for focus theory:
utterances with multiple focus operators and second occurrence expressions. We
then show that the resulting analysis favourably compares with two prominent
theories of focus (namely, Rooth's Alternative Semantics and Krifka's
Structured Meanings theory) in that it correctly generates interpretations
which these alternative theories cannot yield. Finally, we discuss the formal
properties of the approach and argue that even though HOU need not terminate,
for the class of unification--problems dealt with in this paper, HOU avoids
this shortcoming and is in fact computationally tractable.
</dc:description>
 <dc:description>Comment: 6 pages, Latex file, uses colap.sty, to appear in Proceedings of
  COLING 96</dc:description>
 <dc:date>1996-05-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Active Constraints for a Direct Interpretation of HPSG</dc:title>
 <dc:creator>Blache, Philippe</dc:creator>
 <dc:creator>Paquelin, Jean-Louis</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we characterize the properties of a direct interpretation of
HPSG and present the advantages of this approach. High-level programming
languages constitute in this perspective an efficient solution: we show how a
multi-paradigm approach, containing in particular constraint logic programming,
offers mechanims close to that of the theory and preserves its fundamental
properties. We take the example of LIFE and describe the implementation of the
main HPSG mechanisms.
</dc:description>
 <dc:description>Comment: Uuencoded gz-compressed .tar, in Proceedings of TALN-HPSG'96</dc:description>
 <dc:date>1996-05-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Resolving Anaphors in Embedded Sentences</dc:title>
 <dc:creator>Azzam, Saliha</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose an algorithm to resolve anaphors, tackling mainly the problem of
intrasentential antecedents. We base our methodology on the fact that such
antecedents are likely to occur in embedded sentences. Sidner's focusing
mechanism is used as the basic algorithm in a more complete approach. The
proposed algorithm has been tested and implemented as a part of a conceptual
analyser, mainly to process pronouns. Details of an evaluation are given.
</dc:description>
 <dc:description>Comment: 7 pages, to appear in Proceedings of ACL 1996. LaTeX source, 1 EPS
  figure, needs aclap.sty</dc:description>
 <dc:date>1996-05-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tactical Generation in a Free Constituent Order Language</dc:title>
 <dc:creator>Hakkani, Dilek Zeynep</dc:creator>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Cicekli, Ilyas</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes tactical generation in Turkish, a free constituent order
language, in which the order of the constituents may change according to the
information structure of the sentences to be generated. In the absence of any
information regarding the information structure of a sentence (i.e., topic,
focus, background, etc.), the constituents of the sentence obey a default
order, but the order is almost freely changeable, depending on the constraints
of the text flow or discourse. We have used a recursively structured finite
state machine for handling the changes in constituent order, implemented as a
right-linear grammar backbone. Our implementation environment is the GenKit
system, developed at Carnegie Mellon University--Center for Machine
Translation. Morphological realization has been implemented using an external
morphological analysis/generation component which performs concrete morpheme
selection and handles morphographemic processes.
</dc:description>
 <dc:description>Comment: gzipped, uuencoded postscript file</dc:description>
 <dc:date>1996-05-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605008</dc:identifier>
 <dc:identifier>Proceedings of 1996 International Workshop on Natural Language
  Generation</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning similarity-based word sense disambiguation from sparse data</dc:title>
 <dc:creator>Karov, Yael</dc:creator>
 <dc:creator>Edelman, Shimon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a method for automatic word sense disambiguation using a text
corpus and a machine-readable dictionary (MRD). The method is based on word
similarity and context similarity measures. Words are considered similar if
they appear in similar contexts; contexts are similar if they contain similar
words. The circularity of this definition is resolved by an iterative,
converging process, in which the system learns from the corpus a set of typical
usages for each of the senses of the polysemous word listed in the MRD. A new
instance of a polysemous word is assigned the sense associated with the typical
usage most similar to its context. Experiments show that this method performs
well, and can learn even from very sparse training data.
</dc:description>
 <dc:description>Comment: To appear in the Fourth Workshop on Very Large Corpora, 1996,
  Copenhagen. 18 pages. (revised, format change only)</dc:description>
 <dc:date>1996-05-05</dc:date>
 <dc:date>1996-07-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Best-First Surface Realization</dc:title>
 <dc:creator>Busemann, Stephan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Current work in surface realization concentrates on the use of general,
abstract algorithms that interpret large, reversible grammars. Only little
attention has been paid so far to the many small and simple applications that
require coverage of a small sublanguage at different degrees of sophistication.
The system TG/2 described in this paper can be smoothly integrated with deep
generation processes, it integrates canned text, templates, and context-free
rules into a single formalism, it allows for both textual and tabular output,
and it can be parameterized according to linguistic preferences. These features
are based on suitably restricted production system techniques and on a generic
backtracking regime.
</dc:description>
 <dc:description>Comment: 10 pages, LaTeX source, one EPS figure</dc:description>
 <dc:date>1996-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Counting Coordination Categorially</dc:title>
 <dc:creator>Cremers, Crit</dc:creator>
 <dc:creator>Hijzelendoorn, Maarten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a way of reducing the complexity of parsing free
coordination. It lives on the Coordinative Count Invariant, a property of
derivable sequences in occurrence-sensitive categorial grammar. This invariant
can be exploited to cut down deterministically the search space for coordinated
sentences to minimal fractions. The invariant is based on inequalities, which
is shown to be the best one can get in the presence of coordination without
proper parsing. It is implemented in a categorial parser for Dutch. Some
results of applying the invariant to the parsing of coordination in this parser
are presented.
</dc:description>
 <dc:description>Comment: 7 pages, PostScript</dc:description>
 <dc:date>1996-05-06</dc:date>
 <dc:date>1996-05-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A New Statistical Parser Based on Bigram Lexical Dependencies</dc:title>
 <dc:creator>Collins, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a new statistical parser which is based on probabilities
of dependencies between head-words in the parse tree. Standard bigram
probability estimation techniques are extended to calculate probabilities of
dependencies between pairs of words. Tests using Wall Street Journal data show
that the method performs at least as well as SPATTER (Magerman 95, Jelinek et
al 94), which has the best published results for a statistical parser on this
task. The simplicity of the approach means the model trains on 40,000 sentences
in under 15 minutes. With a beam search strategy parsing speed can be improved
to over 200 sentences a minute with negligible loss in accuracy.
</dc:description>
 <dc:description>Comment: 8 pages, to appear in Proceedings of ACL 96. Uuencoded gz-compressed
  postscript file created by csh script uufiles</dc:description>
 <dc:date>1996-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Dependencies between Case Frame Slots</dc:title>
 <dc:creator>Li, Hang</dc:creator>
 <dc:creator>Abe, Naoki</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address the problem of automatically acquiring case frame patterns
(selectional patterns) from large corpus data. In particular, we propose a
method of learning dependencies between case frame slots. We view the problem
of learning case frame patterns as that of learning multi-dimensional discrete
joint distributions, where random variables represent case slots. We then
formalize the dependencies between case slots as the probabilistic dependencies
between these random variables. Since the number of parameters in a
multi-dimensional joint distribution is exponential, it is infeasible to
accurately estimate them in practice. To overcome this difficulty, we settle
with approximating the target joint distribution by the product of low order
component distributions, based on corpus data. In particular we propose to
employ an efficient learning algorithm based on the MDL principle to realize
this task. Our experimental results indicate that for certain classes of verbs,
the accuracy achieved in a disambiguation experiment is improved by using the
acquired knowledge of dependencies.
</dc:description>
 <dc:description>Comment: 12 pages, LaTex source, 2 eps figures, uses colap.sty and epsf.sty ;
  an extended abstract of this paper appeared in Proc. of COLING'96, page 10-15</dc:description>
 <dc:date>1996-05-12</dc:date>
 <dc:date>1996-09-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Clustering Words with the MDL Principle</dc:title>
 <dc:creator>Li, Hang</dc:creator>
 <dc:creator>Abe, Naoki</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address the problem of automatically constructing a thesaurus
(hierarchically clustering words) based on corpus data. We view the problem of
clustering words as that of estimating a joint distribution over the Cartesian
product of a partition of a set of nouns and a partition of a set of verbs, and
propose an estimation algorithm using simulated annealing with an energy
function based on the Minimum Description Length (MDL) Principle. We
empirically compared the performance of our method based on the MDL Principle
against that of one based on the Maximum Likelihood Estimator, and found that
the former outperforms the latter. We also evaluated the method by conducting
pp-attachment disambiguation experiments using an automatically constructed
thesaurus. Our experimental results indicate that we can improve accuracy in
disambiguation by using such a thesaurus.
</dc:description>
 <dc:description>Comment: 11 pages, LaTex source, 4 eps figures, uses colap.sty and epsf.sty;
  an extended abstract of this paper appeared in Proc. of COLING'96, page 4-9</dc:description>
 <dc:date>1996-05-12</dc:date>
 <dc:date>1996-09-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adapting the Core Language Engine to French and Spanish</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Bouillon, Pierrette</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe how substantial domain-independent language-processing systems
for French and Spanish were quickly developed by manually adapting an existing
English-language system, the SRI Core Language Engine. We explain the
adaptation process in detail, and argue that it provides a fairly general
recipe for converting a grammar-based system for English into a corresponding
one for a Romance language.
</dc:description>
 <dc:description>Comment: 9 pages, aclap.sty; to appear in NLP+IA 96; see also
  http://www.cam.sri.com/</dc:description>
 <dc:date>1996-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing for Semidirectional Lambek Grammar is NP-Complete</dc:title>
 <dc:creator>Doerre, Jochen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We study the computational complexity of the parsing problem of a variant of
Lambek Categorial Grammar that we call {\em semidirectional}. In
semidirectional Lambek calculus $\SDL$ there is an additional non-directional
abstraction rule allowing the formula abstracted over to appear anywhere in the
premise sequent's left-hand side, thus permitting non-peripheral extraction.
$\SDL$ grammars are able to generate each context-free language and more than
that. We show that the parsing problem for semidirectional Lambek Grammar is
NP-complete by a reduction of the 3-Partition problem.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX source (uses aclap.sty, tree-dvips.{sty,pro})</dc:description>
 <dc:date>1996-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605016</dc:identifier>
 <dc:identifier>Proceedings ACL '96 (Santa Cruz)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Chart Generator for Shake and Bake Machine Translation</dc:title>
 <dc:creator>Popowich, Fred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A generation algorithm based on an active chart parsing algorithm is
introduced which can be used in conjunction with a Shake and Bake machine
translation system. A concise Prolog implementation of the algorithm is
provided, and some performance comparisons with a shift-reduce based algorithm
are given which show the chart generator is much more efficient for generating
all possible sentences from an input specification.
</dc:description>
 <dc:description>Comment: 12 pages, uuencoded, unix compressed postscript, to appear in the
  Proceedings of AI'96, the 11th Canadian Conference on Artificial Intelligence</dc:description>
 <dc:date>1996-05-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Tabular LR Parsing</dc:title>
 <dc:creator>Nederhof, Mark-Jan</dc:creator>
 <dc:creator>Satta, Giorgio</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We give a new treatment of tabular LR parsing, which is an alternative to
Tomita's generalized LR algorithm. The advantage is twofold. Firstly, our
treatment is conceptually more attractive because it uses simpler concepts,
such as grammar transformations and standard tabulation techniques also know as
chart parsing. Secondly, the static and dynamic complexity of parsing, both in
space and time, is significantly reduced.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty</dc:description>
 <dc:date>1996-05-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605018</dc:identifier>
 <dc:identifier>Proceedings ACL '96 (Santa Cruz)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Noun-Phrase Analysis in Unrestricted Text for Information Retrieval</dc:title>
 <dc:creator>Evans, David A.</dc:creator>
 <dc:creator>Zhai, Chengxiang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Information retrieval is an important application area of natural-language
processing where one encounters the genuine challenge of processing large
quantities of unrestricted natural-language text. This paper reports on the
application of a few simple, yet robust and efficient noun-phrase analysis
techniques to create better indexing phrases for information retrieval. In
particular, we describe a hybrid approach to the extraction of meaningful
(continuous or discontinuous) subcompounds from complex noun phrases using both
corpus statistics and linguistic heuristics. Results of experiments show that
indexing based on such extracted subcompounds improves both recall and
precision in an information retrieval system. The noun-phrase analysis
techniques are also potentially useful for book indexing and automatic
thesaurus extraction.
</dc:description>
 <dc:description>Comment: 8 pages, gzipped, uuencoded Postscript file, to appear in ACL'96</dc:description>
 <dc:date>1996-05-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605019</dc:identifier>
 <dc:identifier>Proceedings of the 34th Annual Meeting of Association for
  Computational Linguistics, Santa Cruz, California, June 24-28, 1996. 17-24.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Where Defaults Don't Help: the Case of the German Plural System</dc:title>
 <dc:creator>Nakisa, Ramin Charles</dc:creator>
 <dc:creator>Hahn, Ulrike</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The German plural system has become a focal point for conflicting theories of
language, both linguistic and cognitive. We present simulation results with
three simple classifiers - an ordinary nearest neighbour algorithm, Nosofsky's
`Generalized Context Model' (GCM) and a standard, three-layer backprop network
- predicting the plural class from a phonological representation of the
singular in German. Though these are absolutely `minimal' models, in terms of
architecture and input information, they nevertheless do remarkably well. The
nearest neighbour predicts the correct plural class with an accuracy of 72% for
a set of 24,640 nouns from the CELEX database. With a subset of 8,598
(non-compound) nouns, the nearest neighbour, the GCM and the network score
71.0%, 75.0% and 83.5%, respectively, on novel items. Furthermore, they
outperform a hybrid, `pattern-associator + default rule', model, as proposed by
Marcus et al. (1995), on this data set.
</dc:description>
 <dc:description>Comment: In proceedings of the 18th annual meeting of the Cognitive Science
  Society. 6 pages, 4 postscript figures. cmp-lg has trouble handling PS fonts
  under NFSS, so the postscript version is 6 pages in postscript Times Roman
  and the LaTeX source produces 7 pages of Computer Modern. Just add times to
  the \usepackage command if your system can handle it</dc:description>
 <dc:date>1996-05-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Functional Centering</dc:title>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Based on empirical evidence from a free word order language (German) we
propose a fundamental revision of the principles guiding the ordering of
discourse entities in the forward-looking centers within the centering model.
We claim that grammatical role criteria should be replaced by indicators of the
functional information structure of the utterances, i.e., the distinction
between context-bound and unbound discourse elements. This claim is backed up
by an empirical evaluation of functional centering.
</dc:description>
 <dc:description>Comment: 8 pages, uuencoded compressed PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/acl96.ps.gz)</dc:description>
 <dc:date>1996-05-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605021</dc:identifier>
 <dc:identifier>Proceedings of ACL '96 (Santa Cruz)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Processing Complex Sentences in the Centering Framework</dc:title>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We extend the centering model for the resolution of intra-sentential anaphora
and specify how to handle complex sentences. An empirical evaluation indicates
that the functional information structure guides the search for an antecedent
within the sentence.
</dc:description>
 <dc:description>Comment: 3 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/acl96-student.ps.gz)</dc:description>
 <dc:date>1996-05-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605022</dc:identifier>
 <dc:identifier>Proceedings of ACL '96 (Santa Cruz), Student Session</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Simple Transformation for Offline-Parsable Grammars and its
  Termination Properties</dc:title>
 <dc:creator>Dymetman, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present, in easily reproducible terms, a simple transformation for
offline-parsable grammars which results in a provably terminating parsing
program directly top-down interpretable in Prolog. The transformation consists
in two steps: (1) removal of empty-productions, followed by: (2) left-recursion
elimination. It is related both to left-corner parsing (where the grammar is
compiled, rather than interpreted through a parsing program, and with the
advantage of guaranteed termination in the presence of empty productions) and
to the Generalized Greibach Normal Form for DCGs (with the advantage of
implementation simplicity).
</dc:description>
 <dc:description>Comment: Latex. 5 pages. Appeared in Coling-94 Proceedings</dc:description>
 <dc:date>1996-05-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Terminological Knowledge Representation Languages to Manage
  Linguistic Resources</dc:title>
 <dc:creator>Jordan, Pamela W.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I examine how terminological languages can be used to manage linguistic data
during NL research and development. In particular, I consider the lexical
semantics task of characterizing semantic verb classes and show how the
language can be extended to flag inconsistencies in verb class definitions,
identify the need for new verb classes, and identify appropriate linguistic
hypotheses for a new verb's behavior.
</dc:description>
 <dc:description>Comment: 3 pages, ACL96 Student Session, uses aclap.sty</dc:description>
 <dc:date>1996-05-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605024</dc:identifier>
 <dc:identifier>Proceedings of ACL 96, Santa Cruz, USA, June 23-28</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Conceptual Reasoning Approach to Textual Ellipsis</dc:title>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Markert, Katja</dc:creator>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a hybrid text understanding methodology for the resolution of
textual ellipsis. It integrates conceptual criteria (based on the
well-formedness and conceptual strength of role chains in a terminological
knowledge base) and functional constraints reflecting the utterances'
information structure (based on the distinction between context-bound and
unbound discourse elements). The methodological framework for text ellipsis
resolution is the centering model that has been adapted to these constraints.
</dc:description>
 <dc:description>Comment: 5 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/ecai96.ps.gz)</dc:description>
 <dc:date>1996-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605025</dc:identifier>
 <dc:identifier>ECAI '96: Proc. of 12th European Conference on Artificial
  Intelligence. Budapest, Aug 12-16 1996, pp.572-576</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Trading off Completeness for Efficiency --- The \textsc{ParseTalk}
  Performance Grammar Approach to Real-World Text Parsing</dc:title>
 <dc:creator>Neuhaus, Peter</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We argue for a performance-based design of natural language grammars and
their associated parsers in order to meet the constraints posed by real-world
natural language understanding. This approach incorporates declarative and
procedural knowledge about language and language use within an object-oriented
specification framework. We discuss several message passing protocols for
real-world text parsing and provide reasons for sacrificing completeness of the
parse in favor of efficiency.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded gzipped PS file</dc:description>
 <dc:date>1996-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605026</dc:identifier>
 <dc:identifier>Proceedings of FLAIRS '96 (Key West)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Restricted Parallelism in Object-Oriented Lexical Parsing</dc:title>
 <dc:creator>Neuhaus, Peter</dc:creator>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an approach to parallel natural language parsing which is based on
a concurrent, object-oriented model of computation. A depth-first, yet
incomplete parsing algorithm for a dependency grammar is specified and several
restrictions on the degree of its parallelization are discussed.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded gzipped PS file</dc:description>
 <dc:date>1996-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605027</dc:identifier>
 <dc:identifier>Proceedings of COLING '96 (Copenhagen)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept
  Accuracy</dc:title>
 <dc:creator>Boros, M.</dc:creator>
 <dc:creator>Eckert, W.</dc:creator>
 <dc:creator>Gallwitz, F.</dc:creator>
 <dc:creator>Goerz, G.</dc:creator>
 <dc:creator>Hanrieder, G.</dc:creator>
 <dc:creator>Niemann, H.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe an approach to automatic evaluation of both the
speech recognition and understanding capabilities of a spoken dialogue system
for train time table information. We use word accuracy for recognition and
concept accuracy for understanding performance judgement. Both measures are
calculated by comparing these modules' output with a correct reference answer.
We report evaluation results for a spontaneous speech corpus with about 10000
utterances. We observed a nearly linear relationship between word accuracy and
concept accuracy.
</dc:description>
 <dc:description>Comment: 4 pages PS, Latex2e source importing 2 eps figures, uses icslp.cls,
  caption.sty, psfig.sty; to appear in the Proceedings of the Fourth
  International Conference on Spoken Language Processing (ICSLP 96)</dc:description>
 <dc:date>1996-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Word Association Norms Using Tree Cut Pair Models</dc:title>
 <dc:creator>Abe, Naoki</dc:creator>
 <dc:creator>Li, Hang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We consider the problem of learning co-occurrence information between two
word categories, or more in general between two discrete random variables
taking values in a hierarchically classified domain. In particular, we consider
the problem of learning the `association norm' defined by A(x,y)=p(x,
y)/(p(x)*p(y)), where p(x, y) is the joint distribution for x and y and p(x)
and p(y) are marginal distributions induced by p(x, y). We formulate this
problem as a sub-task of learning the conditional distribution p(x|y), by
exploiting the identity p(x|y) = A(x,y)*p(x). We propose a two-step estimation
method based on the MDL principle, which works as follows: It first estimates
p(x) as p1 using MDL, and then estimates p(x|y) for a fixed y by applying MDL
on the hypothesis class of {A * p1 | A \in B} for some given class B of
representations for association norm. The estimation of A is therefore obtained
as a side-effect of a near optimal estimation of p(x|y). We then apply this
general framework to the problem of acquiring case-frame patterns. We assume
that both p(x) and A(x, y) for given y are representable by a model based on a
classification that exists within an existing thesaurus tree as a `cut,' and
hence p(x|y) is represented as the product of a pair of `tree cut models.' We
then devise an efficient algorithm that implements our general strategy. We
tested our method by using it to actually acquire case-frame patterns and
conducted disambiguation experiments using the acquired knowledge. The
experimental results show that our method improves upon existing methods.
</dc:description>
 <dc:description>Comment: 10 pages, LaTex source, 6 eps figures, uses ml94.sty and epsf.sty; to
  appear in the 13th Int. Conf. on Machine Learning (ICML'96)</dc:description>
 <dc:date>1996-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incremental Centering and Center Ambiguity</dc:title>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we present a model of anaphor resolution within the framework
of the centering model. The consideration of an incremental processing mode
introduces the need to manage structural ambiguity at the center level. Hence,
the centering framework is further refined to account for local and global
parsing ambiguities which propagate up to the level of center representations,
yielding moderately adapted data structures for the centering algorithm.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded gzipped PS file (see also Technical Report at:
  http://www.coling.uni-freiburg.de/public/papers/cogsci96-center.ps.gz)</dc:description>
 <dc:date>1996-05-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605030</dc:identifier>
 <dc:identifier>CogSci '96: Proc. of 18th Annual Conference of the Cognitive
  Science Society. La Jolla, Ca., Jul 12-15 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605031</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Algorithms for Parsing the DOP Model? A Reply to Joshua
  Goodman</dc:title>
 <dc:creator>Bod, Rens</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This note is a reply to Joshua Goodman's paper &quot;Efficient Algorithms for
Parsing the DOP Model&quot; (Goodman, 1996; cmp-lg/9604008). In his paper, Goodman
makes a number of claims about (my work on) the Data-Oriented Parsing model
(Bod, 1992-1996). This note shows that some of these claims must be mistaken.
</dc:description>
 <dc:description>Comment: 5 pages, Postscript file</dc:description>
 <dc:date>1996-05-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605031</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Synchronous Models of Language</dc:title>
 <dc:creator>Rambow, Owen</dc:creator>
 <dc:creator>Satta, Giorgio</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In synchronous rewriting, the productions of two rewriting systems are paired
and applied synchronously in the derivation of a pair of strings. We present a
new synchronous rewriting system and argue that it can handle certain phenomena
that are not covered by existing synchronous systems. We also prove some
interesting formal/computational properties of our system.
</dc:description>
 <dc:description>Comment: 8 pages uuencoded gzipped ps file</dc:description>
 <dc:date>1996-05-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Notes on LR Parser Design</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The design of an LR parser based on interleaving the atomic symbol processing
of a context-free backbone grammar with the full constraints of the underlying
unification grammar is described. The parser employs a set of reduced
constraints derived from the unification grammar in the LR parsing step. Gap
threading is simulated to reduce the applicability of empty productions.
</dc:description>
 <dc:description>Comment: 5 pages, uuncoded, gzipped PostScript</dc:description>
 <dc:date>1996-05-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605033</dc:identifier>
 <dc:identifier>Coling 94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Handling Sparse Data by Successive Abstraction</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A general, practical method for handling sparse data that avoids held-out
data and iterative reestimation is derived from first principles. It has been
tested on a part-of-speech tagging task and outperformed (deleted)
interpolation with context-independent weights, even when the latter used a
globally optimal parameter setting determined a posteriori.
</dc:description>
 <dc:description>Comment: 6 pages, uuencoded, gzipped PostScript</dc:description>
 <dc:date>1996-05-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605034</dc:identifier>
 <dc:identifier>Coling 96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Example-Based Optimization of Surface-Generation Tables</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A method is given that &quot;inverts&quot; a logic grammar and displays it from the
point of view of the logical form, rather than from that of the word string.
LR-compiling techniques are used to allow a recursive-descent generation
algorithm to perform &quot;functor merging&quot; much in the same way as an LR parser
performs prefix merging. This is an improvement on the semantic-head-driven
generator that results in a much smaller search space. The amount of semantic
lookahead can be varied, and appropriate tradeoff points between table size and
resulting nondeterminism can be found automatically. This can be done by
removing all spurious nondeterminism for input sufficiently close to the
examples of a training corpus, and large portions of it for other input, while
preserving completeness.
</dc:description>
 <dc:description>Comment: 22 pages, uuencoded, gzipped PostScript</dc:description>
 <dc:date>1996-05-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605035</dc:identifier>
 <dc:identifier>R. Mitkov and N. Nicolov (eds.) &quot;Recent Advances in Natural
  Language Processing&quot;, vol. 136 of &quot;Current Issues in Linguistic Theory&quot;, John
  Benjamins, Amsterdam, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing Algorithms and Metrics</dc:title>
 <dc:creator>Goodman, Joshua</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Many different metrics exist for evaluating parsing results, including
Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several
others. However, most parsing algorithms, including the Viterbi algorithm,
attempt to optimize the same metric, namely the probability of getting the
correct labelled tree. By choosing a parsing algorithm appropriate for the
evaluation metric, better performance can be achieved. We present two new
algorithms: the ``Labelled Recall Algorithm,'' which maximizes the expected
Labelled Recall Rate, and the ``Bracketed Recall Algorithm,'' which maximizes
the Bracketed Recall Rate. Experimental results are given, showing that the two
new algorithms have improved performance over the Viterbi algorithm on many
criteria, especially the ones that they optimize.
</dc:description>
 <dc:date>1996-05-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605036</dc:identifier>
 <dc:identifier>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Trigram-based and Feature-based Methods for Context-Sensitive
  Spelling Correction</dc:title>
 <dc:creator>Golding, Andrew R.</dc:creator>
 <dc:creator>Schabes, Yves</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses the problem of correcting spelling errors that result in
valid, though unintended words (such as ``peace'' and ``piece'', or ``quiet''
and ``quite'') and also the problem of correcting particular word usage errors
(such as ``amount'' and ``number'', or ``among'' and ``between''). Such
corrections require contextual information and are not handled by conventional
spelling programs such as Unix `spell'. First, we introduce a method called
Trigrams that uses part-of-speech trigrams to encode the context. This method
uses a small number of parameters compared to previous methods based on word
trigrams. However, it is effectively unable to distinguish among words that
have the same part of speech. For this case, an alternative feature-based
method called Bayes performs better; but Bayes is less effective than Trigrams
when the distinction among words depends on syntactic constraints. A hybrid
method called Tribayes is then introduced that combines the best of the
previous two methods. The improvement in performance of Tribayes over its
components is verified experimentally. Tribayes is also compared with the
grammar checker in Microsoft Word, and is found to have substantially higher
performance.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1996-05-31</dc:date>
 <dc:date>1996-06-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605037</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9605038</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Normal-Form Parsing for Combinatory Categorial Grammar</dc:title>
 <dc:creator>Eisner, Jason</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Under categorial grammars that have powerful rules like composition, a simple
n-word sentence can have exponentially many parses. Generating all parses is
inefficient and obscures whatever true semantic ambiguities are in the input.
This paper addresses the problem for a fairly general form of Combinatory
Categorial Grammar, by means of an efficient, correct, and easy to implement
normal-form parsing technique. The parser is proved to find exactly one parse
in each semantic equivalence class of allowable parses; that is, spurious
ambiguity (as carefully defined) is shown to be both safely and completely
eliminated.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX packaged with three .sty files, also uses cgloss4e.sty</dc:description>
 <dc:date>1996-06-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9605038</dc:identifier>
 <dc:identifier>Proceedings of ACL '96 (34th Meeting of the Association for
  Computational Linguistics), Santa Cruz</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Bayesian hybrid method for context-sensitive spelling correction</dc:title>
 <dc:creator>Golding, Andrew R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Two classes of methods have been shown to be useful for resolving lexical
ambiguity. The first relies on the presence of particular words within some
distance of the ambiguous target word; the second uses the pattern of words and
part-of-speech tags around the target word. These methods have complementary
coverage: the former captures the lexical ``atmosphere'' (discourse topic,
tense, etc.), while the latter captures local syntax. Yarowsky has exploited
this complementarity by combining the two methods using decision lists. The
idea is to pool the evidence provided by the component methods, and to then
solve a target problem by applying the single strongest piece of evidence,
whatever type it happens to be. This paper takes Yarowsky's work as a starting
point, applying decision lists to the problem of context-sensitive spelling
correction. Decision lists are found, by and large, to outperform either
component method. However, it is found that further improvements can be
obtained by taking into account not just the single strongest piece of
evidence, but ALL the available evidence. A new hybrid method, based on
Bayesian classifiers, is presented for doing this, and its performance
improvements are demonstrated.
</dc:description>
 <dc:description>Comment: 15 pages</dc:description>
 <dc:date>1996-06-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Clustered Language Models with Context-Equivalent States</dc:title>
 <dc:creator>Ueberla, J. P.</dc:creator>
 <dc:creator>Gransden, I. R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, a hierarchical context definition is added to an existing
clustering algorithm in order to increase its robustness. The resulting
algorithm, which clusters contexts and events separately, is used to experiment
with different ways of defining the context a language model takes into
account. The contexts range from standard bigram and trigram contexts to part
of speech five-grams. Although none of the models can compete directly with a
backoff trigram, they give up to 9\% improvement in perplexity when
interpolated with a trigram. Moreover, the modified version of the algorithm
leads to a performance increase over the original version of up to 12\%.
</dc:description>
 <dc:description>Comment: 3 pages, latex</dc:description>
 <dc:date>1996-06-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Morphological Cues for Lexical Semantics</dc:title>
 <dc:creator>Light, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most natural language processing tasks require lexical semantic information.
Automated acquisition of this information would thus increase the robustness
and portability of NLP systems. This paper describes an acquisition method
which makes use of fixed correspondences between derivational affixes and
lexical semantic information. One advantage of this method, and of other
methods that rely only on surface characteristics of language, is that the
necessary input is currently available.
</dc:description>
 <dc:date>1996-06-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606003</dc:identifier>
 <dc:identifier>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Classification in Feature-based Default Inheritance Hierarchies</dc:title>
 <dc:creator>Light, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Increasingly, inheritance hierarchies are being used to reduce redundancy in
natural language processing lexicons. Systems that utilize inheritance
hierarchies need to be able to insert words under the optimal set of classes in
these hierarchies. In this paper, we formalize this problem for feature-based
default inheritance hierarchies. Since the problem turns out to be NP-complete,
we present an approximation algorithm for it. We show that this algorithm is
efficient and that it performs well with respect to a number of standard
problems for default inheritance. A prototype implementation has been tested on
lexical hierarchies and it has produced encouraging results. The work presented
here is also relevant to other types of default hierarchies.
</dc:description>
 <dc:date>1996-06-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606004</dc:identifier>
 <dc:identifier>Proceedings of KONVENS-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Part-of-Speech-Tagging using morphological information</dc:title>
 <dc:creator>Ludwig, Bernd</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents the results of an experiment to decide the question of
authenticity of the supposedly spurious Rhesus - a attic tragedy sometimes
credited to Euripides. The experiment involves use of statistics in order to
test whether significant deviations in the distribution of word categories
between Rhesus and the other works of Euripides can or cannot be found. To
count frequencies of word categories in the corpus, a part-of-speech-tagger for
Greek has been implemented. Some special techniques for reducing the problem of
sparse data are used resulting in an accuracy of ca. 96.6%.
</dc:description>
 <dc:description>Comment: Implementing a morphological part-of-speech-tagger with an
  application to the question of authenticity of the Euripidean Rhesus</dc:description>
 <dc:date>1996-06-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Coordination in Tree Adjoining Grammars: Formalization and
  Implementation</dc:title>
 <dc:creator>Sarkar, Anoop</dc:creator>
 <dc:creator>Joshi, Aravind</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we show that an account for coordination can be constructed
using the derivation structures in a lexicalized Tree Adjoining Grammar (LTAG).
We present a notion of derivation in LTAGs that preserves the notion of fixed
constituency in the LTAG lexicon while providing the flexibility needed for
coordination phenomena. We also discuss the construction of a practical parser
for LTAGs that can handle coordination including cases of non-constituent
coordination.
</dc:description>
 <dc:description>Comment: 6 pages, 16 Postscript figures, uses colap.sty. To appear in the
  proceedings of COLING 1996</dc:description>
 <dc:date>1996-06-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Word Sense Disambiguation using Conceptual Density</dc:title>
 <dc:creator>Agirre, Eneko</dc:creator>
 <dc:creator>Rigau, German</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a method for the resolution of lexical ambiguity of nouns
and its automatic evaluation over the Brown Corpus. The method relies on the
use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual
distance among concepts, captured by a Conceptual Density formula developed for
this purpose. This fully automatic method requires no hand coding of lexical
entries, hand tagging of text nor any kind of training process. The results of
the experiments have been automatically evaluated against SemCor, the
sense-tagged version of the Brown Corpus.
</dc:description>
 <dc:description>Comment: Postscript version. 8 pages. To appear in the proceedings of COLING
  1996</dc:description>
 <dc:date>1996-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Coordination as a Direct Process</dc:title>
 <dc:creator>Mela, Augusta</dc:creator>
 <dc:creator>Fouquere, Christophe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a treatment of coordination based on the concepts of functor,
argument and subcategorization. Its formalization comprises two parts which are
conceptually independent. On one hand, we have extended the feature structure
unification to disjunctive and set values in order to check the compatibility
and the satisfiability of subcategorization requirements by structured
complements. On the other hand, we have considered the conjunction {\em et
(and)} as the head of the coordinate structure, so that coordinate structures
stem simply from the subcategorization specifications of {\em et} and the
general schemata of a head saturation. Both parts have been encoded within HPSG
using the same resource that is the subcategorization and its principle which
we have just extended.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1996-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Modularizing Contexted Constraints</dc:title>
 <dc:creator>Griffith, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a method for compiling a constraint-based grammar into a
potentially more efficient form for processing. This method takes dependent
disjunctions within a constraint formula and factors them into non-interacting
groups whenever possible by determining their independence. When a group of
dependent disjunctions is split into smaller groups, an exponential amount of
redundant information is reduced. At runtime, this means that an exponential
amount of processing can be saved as well. Since the performance of an
algorithm for processing constraints with dependent disjunctions is highly
determined by its input, the transformation presented in this paper should
prove beneficial for all such algorithms.
</dc:description>
 <dc:description>Comment: 6 pages LaTeX (uses colap.sty)D; To appear in the proceedings of
  COLING 1996</dc:description>
 <dc:date>1996-06-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Information Structural Approach to Spoken Language Generation</dc:title>
 <dc:creator>Prevost, Scott</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an architecture for the generation of spoken monologues
with contextually appropriate intonation. A two-tiered information structure
representation is used in the high-level content planning and sentence planning
stages of generation to produce efficient, coherent speech that makes certain
discourse relationships, such as explicit contrasts, appropriately salient. The
system is able to produce appropriate intonational patterns that cannot be
generated by other systems which rely solely on word class and given/new
distinctions.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1996-06-10</dc:date>
 <dc:date>1996-07-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Empirical Study of Smoothing Techniques for Language Modeling</dc:title>
 <dc:creator>Chen, Stanley F.</dc:creator>
 <dc:creator>Goodman, Joshua T.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an extensive empirical comparison of several smoothing techniques
in the domain of language modeling, including those described by Jelinek and
Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the
first time how factors such as training data size, corpus (e.g., Brown versus
Wall Street Journal), and n-gram order (bigram versus trigram) affect the
relative performance of these methods, which we measure through the
cross-entropy of test data. In addition, we introduce two novel smoothing
techniques, one a variation of Jelinek-Mercer smoothing and one a very simple
linear interpolation technique, both of which outperform existing methods.
</dc:description>
 <dc:description>Comment: 9 pages, LaTeX, uses aclap.sty</dc:description>
 <dc:date>1996-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606011</dc:identifier>
 <dc:identifier>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL '96)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Inductive Unsupervised Semantic Tagger</dc:title>
 <dc:creator>Lua, K T</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We report our development of a simple but fast and efficient inductive
unsupervised semantic tagger for Chinese words. A POS hand-tagged corpus of
348,000 words is used. The corpus is being tagged in two steps. First, possible
semantic tags are selected from a semantic dictionary(Tong Yi Ci Ci Lin), the
POS and the conditional probability of semantic from POS, i.e., P(S|P). The
final semantic tag is then assigned by considering the semantic tags before and
after the current word and the semantic-word conditional probability P(S|W)
derived from the first step. Semantic bigram probabilities P(S|S) are used in
the second step. Final manual checking shows that this simple but efficient
algorithm has a hit rate of 91%. The tagger tags 142 words per second, using a
120 MHz Pentium running FOXPRO. It runs about 2.3 times faster than a Viterbi
tagger.
</dc:description>
 <dc:description>Comment: uuencoded postscript file. email: cmp-lg/9606012</dc:description>
 <dc:date>1996-06-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Relating Turing's Formula and Zipf's Law</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An asymptote is derived from Turing's local reestimation formula for
population frequencies, and a local reestimation formula is derived from Zipf's
law for the asymptotic behavior of population frequencies. The two are shown to
be qualitatively different asymptotically, but nevertheless to be instances of
a common class of reestimation-formula-asymptote pairs, in which they
constitute the upper and lower bounds of the convergence region of the
cumulative of the frequency function, as rank tends to infinity. The results
demonstrate that Turing's formula is qualitatively different from the various
extensions to Zipf's law, and suggest that it smooths the frequency estimates
towards a geometric distribution.
</dc:description>
 <dc:description>Comment: 9 pages, uuencoded, gzipped PostScript; some typos removed</dc:description>
 <dc:date>1996-06-11</dc:date>
 <dc:date>1996-06-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606013</dc:identifier>
 <dc:identifier>WVLC-4</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building Probabilistic Models for Natural Language</dc:title>
 <dc:creator>Chen, Stanley F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this thesis, we investigate three problems involving the probabilistic
modeling of language: smoothing n-gram models, statistical grammar induction,
and bilingual sentence alignment. These three problems employ models at three
different levels of language; they involve word-based, constituent-based, and
sentence-based models, respectively. We describe techniques for improving the
modeling of language at each of these levels, and surpass the performance of
existing algorithms for each problem. We approach the three problems using
three different frameworks. We relate each of these frameworks to the Bayesian
paradigm, and show why each framework used was appropriate for the given
problem. Finally, we show how our research addresses two central issues in
probabilistic modeling: the sparse data problem and the problem of inducing
hidden structure.
</dc:description>
 <dc:description>Comment: 162 pages, LaTeX, doctoral dissertation</dc:description>
 <dc:date>1996-06-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606015</identifier>
 <datestamp>2009-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stabilizing the Richardson Algorithm by Controlling Chaos</dc:title>
 <dc:creator>He, Song</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Condensed Matter</dc:subject>
 <dc:subject>Nonlinear Sciences - Chaotic Dynamics</dc:subject>
 <dc:subject>Nonlinear Sciences - Cellular Automata and Lattice Gases</dc:subject>
 <dc:description>  By viewing the operations of the Richardson purification algorithm as a
discrete time dynamical process, we propose a method to overcome the
instability of the algorithm by controlling chaos. We present theoretical
analysis and numerical results on the behavior and performance of the
stabilized algorithm.
</dc:description>
 <dc:description>Comment: Send email to song@bell-lab.com or song@physics.att.com for uuencoded
  tarred gzipped postscript files for the five figures</dc:description>
 <dc:date>1996-06-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606015</dc:identifier>
 <dc:identifier>doi:10.1063/1.168602</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Probabilistic Disambiguation Method Based on Psycholinguistic
  Principles</dc:title>
 <dc:creator>Li, Hang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address the problem of structural disambiguation in syntactic parsing. In
psycholinguistics, a number of principles of disambiguation have been proposed,
notably the Lexical Preference Rule (LPR), the Right Association Principle
(RAP), and the Attach Low and Parallel Principle (ALPP) (an extension of RAP).
We argue that in order to improve disambiguation results it is necessary to
implement these principles on the basis of a probabilistic methodology. We
define a `three-word probability' for implementing LPR, and a `length
probability' for implementing RAP and ALPP. Furthermore, we adopt the
`back-off' method to combine these two types of probabilities. Our experimental
results indicate our method to be effective, attaining an accuracy of 89.2%.
</dc:description>
 <dc:description>Comment: 16 pages, LaTex source, 7 eps figures, uses a4.sty and epsf.sty; in
  Proc. of the 4th Workshop on Very Large Corpora (WVLC-4), page 141-154, 1996</dc:description>
 <dc:date>1996-06-13</dc:date>
 <dc:date>1996-09-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>With raised eyebrows or the eyebrows raised ? A Neural Network Approach
  to Grammar Checking for Definiteness</dc:title>
 <dc:creator>Scheler, Gabriele</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we use a feature model of the semantics of plural determiners
to present an approach to grammar checking for definiteness. Using neural
network techniques, a semantics -- morphological category mapping was learned.
We then applied a textual encoding technique to the 125 occurences of the
relevant category in a 10 000 word narrative text and learned a surface --
semantics mapping. By applying the learned generation function to the newly
generated representations, we achieved a correct category assignment in many
cases (87 %). These results are considerably better than a direct surface
categorization approach (54 %), with a baseline (always guessing the dominant
category) of 60 %. It is discussed, how these results could be used in
multilingual NLP applications.
</dc:description>
 <dc:description>Comment: 14 pages</dc:description>
 <dc:date>1996-06-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compilation of Weighted Finite-State Transducers from Decision Trees</dc:title>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:creator>Riley, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We report on a method for compiling decision trees into weighted finite-state
transducers. The key assumptions are that the tree predictions specify how to
rewrite symbols from an input string, and the decision at each tree node is
stateable in terms of regular expressions on the input string. Each leaf node
can then be treated as a separate rule where the left and right contexts are
constructable from the decisions made traversing the tree from the root to the
leaf. These rules are compiled into transducers using the weighted rewrite-rule
rule-compilation algorithm described in (Mohri and Sproat, 1996).
</dc:description>
 <dc:date>1996-06-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606018</dc:identifier>
 <dc:identifier>34th Annual Meeting of the ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computational Complexity of Probabilistic Disambiguation by means of
  Tree-Grammars</dc:title>
 <dc:creator>Sima'an, Khalil</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper studies the computational complexity of disambiguation under
probabilistic tree-grammars and context-free grammars. It presents a proof that
the following problems are NP-hard: computing the Most Probable Parse (MPP)
from a sentence or from a word-graph, and computing the Most Probable Sentence
(MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph
also holds for Stochastic Context-Free Grammars. Consequently, the existence of
deterministic polynomial-time algorithms for solving these disambiguation
problems is a highly improbable event.
</dc:description>
 <dc:description>Comment: 6 pages. To appear in COLING 1996</dc:description>
 <dc:date>1996-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computing Optimal Descriptions for Optimality Theory Grammars with
  Context-Free Position Structures</dc:title>
 <dc:creator>Tesar, Bruce</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an algorithm for computing optimal structural
descriptions for Optimality Theory grammars with context-free position
structures. This algorithm extends Tesar's dynamic programming approach [Tesar
1994][Tesar 1995] to computing optimal structural descriptions from regular to
context-free structures. The generalization to context-free structures creates
several complications, all of which are overcome without compromising the core
dynamic programming approach. The resulting algorithm has a time complexity
cubic in the length of the input, and is applicable to grammars with universal
constraints that exhibit context-free locality.
</dc:description>
 <dc:description>Comment: 7 pages, uses aclap.sty. To appear in ACL 1996</dc:description>
 <dc:date>1996-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606020</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Iterative Algorithm to Build Chinese Language Models</dc:title>
 <dc:creator>Luo, Xiaoqiang</dc:creator>
 <dc:creator>Roukos, Salim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an iterative procedure to build a Chinese language model (LM). We
segment Chinese text into words based on a word-based Chinese language model.
However, the construction of a Chinese LM itself requires word boundaries. To
get out of the chicken-and-egg problem, we propose an iterative procedure that
alternates two operations: segmenting text into words and building an LM.
Starting with an initial segmented corpus and an LM based upon it, we use a
Viterbi-liek algorithm to segment another set of data. Then, we build an LM
based on the second set and use the resulting LM to segment again the first
corpus. The alternating procedure provides a self-organized way for the
segmenter to detect automatically unseen words and correct segmentation errors.
Our preliminary experiment shows that the alternating procedure not only
improves the accuracy of our segmentation, but discovers unseen words
surprisingly well. The resulting word-based LM has a perplexity of 188 for a
general Chinese corpus.
</dc:description>
 <dc:date>1996-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Two Questions about Data-Oriented Parsing</dc:title>
 <dc:creator>Bod, Rens</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I present ongoing work on the data-oriented parsing (DOP)
model. In previous work, DOP was tested on a cleaned-up set of analyzed
part-of-speech strings from the Penn Treebank, achieving excellent test
results. This left, however, two important questions unanswered: (1) how does
DOP perform if tested on unedited data, and (2) how can DOP be used for parsing
word strings that contain unknown words? This paper addresses these questions.
We show that parse results on unedited data are worse than on cleaned-up data,
although very competitive if compared to other models. As to the parsing of
word strings, we show that the hardness of the problem does not so much depend
on unknown words, but on previously unseen lexical categories of known words.
We give a novel method for parsing these words by estimating the probabilities
of unknown subtrees. The method is of general interest since it shows that good
performance can be obtained without the use of a part-of-speech tagger. To the
best of our knowledge, our method outperforms other statistical parsers tested
on Penn Treebank word strings.
</dc:description>
 <dc:description>Comment: 16 pages, Postscript; to appear in Proceedings Fourth Workshop on
  Very Large Corpora (WVLC-4)</dc:description>
 <dc:date>1996-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Robust System for Natural Spoken Dialogue</dc:title>
 <dc:creator>Allen, James F.</dc:creator>
 <dc:creator>Miller, Bradford W.</dc:creator>
 <dc:creator>Ringger, Eric K.</dc:creator>
 <dc:creator>Sikorski, Teresa</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes a system that leads us to believe in the feasibility of
constructing natural spoken dialogue systems in task-oriented domains. It
specifically addresses the issue of robust interpretation of speech in the
presence of recognition errors. Robustness is achieved by a combination of
statistical error post-correction, syntactically- and semantically-driven
robust parsing, and extensive use of the dialogue context. We present an
evaluation of the system using time-to-completion and the quality of the final
solution that suggests that most native speakers of English can use the system
successfully with virtually no training.
</dc:description>
 <dc:description>Comment: uuencoded, gzipped PostScript. Includes extra Appendix</dc:description>
 <dc:date>1996-06-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606023</dc:identifier>
 <dc:identifier>Proceedings of the 34th Annual Meeting of the ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Data-Oriented Approach to Semantic Interpretation</dc:title>
 <dc:creator>Bod, Rens</dc:creator>
 <dc:creator>Bonnema, Remko</dc:creator>
 <dc:creator>Scha, Remko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In Data-Oriented Parsing (DOP), an annotated language corpus is used as a
stochastic grammar. The most probable analysis of a new input sentence is
constructed by combining sub-analyses from the corpus in the most probable way.
This approach has been succesfully used for syntactic analysis, using corpora
with syntactic annotations such as the Penn Treebank. If a corpus with
semantically annotated sentences is used, the same approach can also generate
the most probable semantic interpretation of an input sentence. The present
paper explains this semantic interpretation method, and summarizes the results
of a preliminary experiment. Semantic annotations were added to the syntactic
annotations of most of the sentences of the ATIS corpus. A data-oriented
semantic interpretation algorithm was succesfully tested on this semantically
enriched corpus.
</dc:description>
 <dc:description>Comment: 10 pages, Postscript; to appear in Proceedings Workshop on
  Corpus-Oriented Semantic Analysis, ECAI-96, Budapest</dc:description>
 <dc:date>1996-06-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Two Sources of Control over the Generation of Software Instructions</dc:title>
 <dc:creator>Hartley, Anthony</dc:creator>
 <dc:creator>Paris, Cecile</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an analysis conducted on a corpus of software
instructions in French in order to establish whether task structure elements
(the procedural representation of the users' tasks) are alone sufficient to
control the grammatical resources of a text generator. We show that the
construct of genre provides a useful additional source of control enabling us
to resolve undetermined cases.
</dc:description>
 <dc:description>Comment: 8 pages, Latex file -- uses aclap.sty</dc:description>
 <dc:date>1996-06-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606025</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Compiler for Weighted Rewrite Rules</dc:title>
 <dc:creator>Mohri, Mehryar</dc:creator>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Context-dependent rewrite rules are used in many areas of natural language
and speech processing. Work in computational phonology has demonstrated that,
given certain conditions, such rewrite rules can be represented as finite-state
transducers (FSTs). We describe a new algorithm for compiling rewrite rules
into FSTs. We show the algorithm to be simpler and more efficient than existing
algorithms. Further, many of our applications demand the ability to compile
weighted rules into weighted FSTs, transducers generalized by providing
transitions with weights. We have extended the algorithm to allow for this.
</dc:description>
 <dc:date>1996-06-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606026</dc:identifier>
 <dc:identifier>34th Annual Meeting of the ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Linguistic Structure as Composition and Perturbation</dc:title>
 <dc:creator>de Marcken, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses the problem of learning language from unprocessed text
and speech signals, concentrating on the problem of learning a lexicon. In
particular, it argues for a representation of language in which linguistic
parameters like words are built by perturbing a composition of existing
parameters. The power of this representation is demonstrated by several
examples in text segmentation and compression, acquisition of a lexicon from
raw speech, and the acquisition of mappings between text and artificial
representations of meaning.
</dc:description>
 <dc:description>Comment: 7 pages</dc:description>
 <dc:date>1996-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606027</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Maximizing Top-down Constraints for Unification-based Systems</dc:title>
 <dc:creator>Tomuro, Noriko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A left-corner parsing algorithm with top-down filtering has been reported to
show very efficient performance for unification-based systems. However, due to
the nontermination of parsing with left-recursive grammars, top-down
constraints must be weakened. In this paper, a general method of maximizing
top-down constraints is proposed. The method provides a procedure to
dynamically compute *restrictor*, a minimum set of features involved in an
infinite loop for every propagation path; thus top-down constraints are
maximally propagated.
</dc:description>
 <dc:description>Comment: 1 postscript figure, uses psfig.sty and aclap.sty</dc:description>
 <dc:date>1996-06-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Directed Replacement</dc:title>
 <dc:creator>Karttunen, Lauri</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces to the finite-state calculus a family of directed
replace operators. In contrast to the simple replace expression, UPPER -&gt;
LOWER, defined in Karttunen (ACL-95), the new directed version, UPPER @-&gt;
LOWER, yields an unambiguous transducer if the lower language consists of a
single string. It transduces the input string from left to right, making only
the longest possible replacement at each point.
  A new type of replacement expression, UPPER @-&gt; PREFIX ... SUFFIX, yields a
transducer that inserts text around strings that are instances of UPPER. The
symbol ... denotes the matching part of the input which itself remains
unchanged. PREFIX and SUFFIX are regular expressions describing the insertions.
  Expressions of the type UPPER @-&gt; PREFIX ... SUFFIX may be used to compose a
deterministic parser for a ``local grammar'' in the sense of Gross (1989).
Other useful applications of directed replacement include tokenization and
filtering of text streams.
</dc:description>
 <dc:description>Comment: To appear in the Proceedings of ACL-96</dc:description>
 <dc:date>1996-06-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Minimizing Manual Annotation Cost In Supervised Training From Corpora</dc:title>
 <dc:creator>Engelson, Sean P.</dc:creator>
 <dc:creator>Dagan, Ido</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Corpus-based methods for natural language processing often use supervised
training, requiring expensive manual annotation of training corpora. This paper
investigates methods for reducing annotation cost by {\it sample selection}. In
this approach, during training the learning program examines many unlabeled
examples and selects for labeling (annotation) only those that are most
informative at each stage. This avoids redundantly annotating examples that
contribute little new information. This paper extends our previous work on {\it
committee-based sample selection} for probabilistic classifiers. We describe a
family of methods for committee-based sample selection, and report experimental
results for the task of stochastic part-of-speech tagging. We find that all
variants achieve a significant reduction in annotation cost, though their
computational efficiency differs. In particular, the simplest method, which has
no parameters to tune, gives excellent results. We also show that sample
selection yields a significant reduction in the size of the model used by the
tagger.
</dc:description>
 <dc:description>Comment: 8 pages, uses epsf.sty and aclap.sty, 6 postscript figures</dc:description>
 <dc:date>1996-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Research on Architectures for Integrated Speech/Language Systems in
  Verbmobil</dc:title>
 <dc:creator>G&#xf6;rz, G&#xfc;nther</dc:creator>
 <dc:creator>Kesseler, Marcus</dc:creator>
 <dc:creator>Spilker, J&#xf6;rg</dc:creator>
 <dc:creator>Weber, Hans</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The German joint research project Verbmobil (VM) aims at the development of a
speech to speech translation system. This paper reports on research done in our
group which belongs to Verbmobil's subproject on system architectures (TP15).
Our specific research areas are the construction of parsers for spontaneous
speech, investigations in the parallelization of parsing and to contribute to
the development of a flexible communication architecture with distributed
control.
</dc:description>
 <dc:description>Comment: 6 pages, 2 Postscript figures</dc:description>
 <dc:date>1996-06-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606031</dc:identifier>
 <dc:identifier>accepted for COLING 96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9606032</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An
  Exemplar-Based Approach</dc:title>
 <dc:creator>Ng, Hwee Tou</dc:creator>
 <dc:creator>Lee, Hian Beng</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we present a new approach for word sense disambiguation (WSD)
using an exemplar-based learning algorithm. This approach integrates a diverse
set of knowledge sources to disambiguate word sense, including part of speech
of neighboring words, morphological form, the unordered set of surrounding
words, local collocations, and verb-object syntactic relation. We tested our
WSD program, named {\sc Lexas}, on both a common data set used in previous
work, as well as on a large sense-tagged corpus that we separately constructed.
{\sc Lexas} achieves a higher accuracy on the common data set, and performs
better than the most frequent heuristic on the highly ambiguous words in the
large corpus tagged with the refined senses of {\sc WordNet}.
</dc:description>
 <dc:description>Comment: In Proceedings of ACL96, 8 pages</dc:description>
 <dc:date>1996-06-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9606032</dc:identifier>
 <dc:identifier>ACL-96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607001</identifier>
 <datestamp>2016-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>GramCheck: A Grammar and Style Checker</dc:title>
 <dc:creator>Bustamante, Flora Ram&#xed;rez</dc:creator>
 <dc:creator>Le&#xf3;n, Fernando S&#xe1;nchez</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a grammar and style checker demonstrator for Spanish and
Greek native writers developed within the project GramCheck. Besides a brief
grammar error typology for Spanish, a linguistically motivated approach to
detection and diagnosis is presented, based on the generalized use of PROLOG
extensions to highly typed unification-based grammars. The demonstrator,
currently including full coverage for agreement errors and certain
head-argument relation issues, also provides correction by means of an
analysis-transfer-synthesis cycle. Finally, future extensions to the current
system are discussed.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX format, uses colap.sty Published: To appear in
  Proceedings of COLING-96</dc:description>
 <dc:date>1996-07-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Inducing Constraint Grammars</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:creator>Tapanainen, Pasi</dc:creator>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Constraint Grammar rules are induced from corpora. A simple scheme based on
local information, i.e., on lexical biases and next-neighbour contexts,
extended through the use of barriers, reached 87.3 percent precision (1.12
tags/word) at 98.2 percent recall. The results compare favourably with other
methods that are used for similar tasks although they are by no means as good
as the results achieved using the original hand-written rules developed over
several years time.
</dc:description>
 <dc:description>Comment: 10 pages, uuencoded, gzipped PostScript</dc:description>
 <dc:date>1996-07-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607002</dc:identifier>
 <dc:identifier>ICGI-3</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Domain and Language Independent Feature Extraction for Statistical Text
  Categorization</dc:title>
 <dc:creator>Bayer, Thomas</dc:creator>
 <dc:creator>Renz, Ingrid</dc:creator>
 <dc:creator>Stein, Michael</dc:creator>
 <dc:creator>Kressel, Ulrich</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A generic system for text categorization is presented which uses a
representative text corpus to adapt the processing steps: feature extraction,
dimension reduction, and classification. Feature extraction automatically
learns features from the corpus by reducing actual word forms using statistical
information of the corpus and general linguistic knowledge. The dimension of
feature vector is then reduced by linear transformation keeping the essential
information. The classification principle is a minimum least square approach
based on polynomials. The described system can be readily adapted to new
domains or new languages. In application, the system is reliable, fast, and
processes completely automatically. It is shown that the text categorizer works
successfully both on text generated by document image analysis - DIA and on
ground truth data.
</dc:description>
 <dc:description>Comment: 12 pages, TeX file, 9 Postscript figures, uses epsf.sty</dc:description>
 <dc:date>1996-07-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607003</dc:identifier>
 <dc:identifier>proceedings of workshop on language engineering for document
  analysis and recognition - ed. by L. Evett and T. Rose, part of the AISB 1996
  Workshop Series, April 96, Sussex University, England, 21-32 (ISBN 0 905
  488628)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating Syntactic and Prosodic Information for the Efficient
  Detection of Empty Categories</dc:title>
 <dc:creator>Batliner, Anton</dc:creator>
 <dc:creator>Feldhaus, Anke</dc:creator>
 <dc:creator>Geissler, Stefan</dc:creator>
 <dc:creator>Kiessling, Andreas</dc:creator>
 <dc:creator>Kiss, Tibor</dc:creator>
 <dc:creator>Kompe, Ralf</dc:creator>
 <dc:creator>Noeth, Elmar</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a number of experiments that demonstrate the usefulness of
prosodic information for a processing module which parses spoken utterances
with a feature-based grammar employing empty categories. We show that by
requiring certain prosodic properties from those positions in the input where
the presence of an empty category has to be hypothesized, a derivation can be
accomplished more efficiently. The approach has been implemented in the machine
translation project VERBMOBIL and results in a significant reduction of the
work-load for the parser.
</dc:description>
 <dc:description>Comment: To appear in the Proceedings of Coling 1996, Copenhagen. 6 pages</dc:description>
 <dc:date>1996-07-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Head Automata and Bilingual Tiling: Translation with Minimal
  Representations</dc:title>
 <dc:creator>Alshawi, Hiyan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a language model consisting of a collection of costed
bidirectional finite state automata associated with the head words of phrases.
The model is suitable for incremental application of lexical associations in a
dynamic programming search for optimal dependency tree derivations. We also
present a model and algorithm for machine translation involving optimal
``tiling'' of a dependency tree with entries of a costed bilingual lexicon.
Experimental results are reported comparing methods for assigning cost
functions to these models. We conclude with a discussion of the adequacy of
annotated linguistic strings as representations for machine translation.
</dc:description>
 <dc:date>1996-07-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607005</dc:identifier>
 <dc:identifier>Proceedings of the 34th Annual Meeting of the Association for
  Computational Linguistics, 167-176, Santa Cruz, California, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Head Automata for Speech Translation</dc:title>
 <dc:creator>Alshawi, Hiyan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents statistical language and translation models based on
collections of small finite state machines we call ``head automata''. The
models are intended to capture the lexical sensitivity of N-gram models and
direct statistical translation models, while at the same time taking account of
the hierarchical phrasal structure of language. Two types of head automata are
defined: relational head automata suitable for translation by transfer of
dependency trees, and head transducers suitable for direct recursive lexical
translation.
</dc:description>
 <dc:date>1996-07-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607006</dc:identifier>
 <dc:identifier>Proceedings of ICSLP 96, the Fourth International Conference on
  Spoken Language Processing, Philadelphia, Pennsylvania, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parallel Replacement in Finite State Calculus</dc:title>
 <dc:creator>Kempe, Andre</dc:creator>
 <dc:creator>Karttunen, Lauri</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper extends the calculus of regular expressions with new types of
replacement expressions that enhance the expressiveness of the simple replace
operator defined in Karttunen (1995). Parallel replacement allows multiple
replacements to apply simultaneously to the same input without interfering with
each other. We also allow a replacement to be constrained by any number of
alternative contexts. With these enhancements, the general replacement
expressions are more versatile than two-level rules for the description of
complex morphological alternations.
</dc:description>
 <dc:description>Comment: 6 pages, dvi (+ 1x eps) tar gzip uuencode</dc:description>
 <dc:date>1996-07-05</dc:date>
 <dc:date>1996-08-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607007</dc:identifier>
 <dc:identifier>COLING-96, Copenhagen DK. August 5, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>From Submit to Submitted via Submission: On Lexical Rules in Large-Scale
  Lexicon Acquisition</dc:title>
 <dc:creator>Viegas, Evelyne</dc:creator>
 <dc:creator>Onyshkevych, Boyan</dc:creator>
 <dc:creator>Raskin, Victor</dc:creator>
 <dc:creator>Nirenburg, Sergei</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper deals with the discovery, representation, and use of lexical rules
(LRs) during large-scale semi-automatic computational lexicon acquisition. The
analysis is based on a set of LRs implemented and tested on the basis of
Spanish and English business- and finance-related corpora. We show that, though
the use of LRs is justified, they do not come cost-free. Semi-automatic output
checking is required, even with blocking and preemtion procedures built in.
Nevertheless, large-scope LRs are justified because they facilitate the
unavoidable process of large-scale semi-automatic lexical acquisition. We also
argue that the place of LRs in the computational process is a complex issue.
</dc:description>
 <dc:description>Comment: Compressed and encoded postscript file</dc:description>
 <dc:date>1996-07-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semantic-based Transfer</dc:title>
 <dc:creator>Dorna, Michael</dc:creator>
 <dc:creator>Emele, Martin C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This article presents a new semantic-based transfer approach developed and
applied within the Verbmobil Machine Translation project. We give an overview
of the declarative transfer formalism together with its procedural realization.
Our approach is discussed and compared with several other approaches from the
MT literature.
</dc:description>
 <dc:description>Comment: 6 pages (to appear in Proceedings of COLING '96)</dc:description>
 <dc:date>1996-07-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Implementation of a Semantic-based Transfer Approach</dc:title>
 <dc:creator>Dorna, Michael</dc:creator>
 <dc:creator>Emele, Martin C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This article gives an overview of a new semantic-based transfer approach
developed and applied within the Verbmobil Machine Translation project. We
present the declarative transfer formalism and discuss its implementation.
</dc:description>
 <dc:description>Comment: 5 pages (to appear in Proceedings of ECAI '96), uuencoded gzipped PS
  file</dc:description>
 <dc:date>1996-07-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Pattern-Based Context-Free Grammars for Machine Translation</dc:title>
 <dc:creator>Takeda, Koichi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes the use of ``pattern-based'' context-free grammars as a
basis for building machine translation (MT) systems, which are now being
adopted as personal tools by a broad range of users in the cyberspace society.
We discuss major requirements for such tools, including easy customization for
diverse domains, the efficiency of the translation algorithm, and scalability
(incremental improvement in translation quality through user interaction), and
describe how our approach meets these requirements.
</dc:description>
 <dc:description>Comment: 8 pages, latex + aclap.sty</dc:description>
 <dc:date>1996-07-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607011</dc:identifier>
 <dc:identifier>Proceedings of the 34th Meeting of the Association for
  Computational Linguistics (ACL'96)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>MBT: A Memory-Based Part of Speech Tagger-Generator</dc:title>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:creator>Zavrel, Jakub</dc:creator>
 <dc:creator>Berck, Peter</dc:creator>
 <dc:creator>Gillis, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce a memory-based approach to part of speech tagging. Memory-based
learning is a form of supervised learning based on similarity-based reasoning.
The part of speech tag of a word in a particular context is extrapolated from
the most similar cases held in memory. Supervised learning approaches are
useful when a tagged corpus is available as an example of the desired output of
the tagger. Based on such a corpus, the tagger-generator automatically builds a
tagger which is able to tag new text the same way, diminishing development time
for the construction of a tagger considerably. Memory-based tagging shares this
advantage with other statistical or machine learning approaches. Additional
advantages specific to a memory-based approach include (i) the relatively small
tagged corpus size sufficient for training, (ii) incremental learning, (iii)
explanation capabilities, (iv) flexible integration of information in case
representations, (v) its non-parametric nature, (vi) reasonably good results on
unknown words without morphological analysis, and (vii) fast learning and
tagging. In this paper we show that a large-scale application of the
memory-based approach is feasible: we obtain a tagging accuracy that is on a
par with that of known statistical approaches, and with attractive space and
time complexity properties when using {\em IGTree}, a tree-based formalism for
indexing and searching huge case bases.} The use of IGTree has as additional
advantage that optimal context size for disambiguation is dynamically computed.
</dc:description>
 <dc:description>Comment: 14 pages, 2 Postscript figures</dc:description>
 <dc:date>1996-07-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607012</dc:identifier>
 <dc:identifier>Proceedings WVLC, Copenhagen</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Unsupervised Discovery of Phonological Categories through Supervised
  Learning of Morphological Rules</dc:title>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:creator>Berck, Peter</dc:creator>
 <dc:creator>Gillis, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a case study in the application of {\em symbolic machine
learning} techniques for the discovery of linguistic rules and categories. A
supervised rule induction algorithm is used to learn to predict the correct
diminutive suffix given the phonological representation of Dutch nouns. The
system produces rules which are comparable to rules proposed by linguists.
Furthermore, in the process of learning this morphological task, the phonemes
used are grouped into phonologically relevant categories. We discuss the
relevance of our method for linguistics and language technology.
</dc:description>
 <dc:date>1996-07-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607013</dc:identifier>
 <dc:identifier>Proceedings COLING 1996, Copenhagen</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Corpus Study of Negative Imperatives in Natural Language Instructions</dc:title>
 <dc:creator>Linden, Keith Vander</dc:creator>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we define the notion of a preventative expression and discuss
a corpus study of such expressions in instructional text. We discuss our coding
schema, which takes into account both form and function features, and present
measures of inter-coder reliability for those features. We then discuss the
correlations that exist between the function and the form features.
</dc:description>
 <dc:description>Comment: 6 pages, uses colap.sty and acl.bst</dc:description>
 <dc:date>1996-07-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607014</dc:identifier>
 <dc:identifier>Proceedings of COLING 96 (Copenhagen)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Micro-Planning Rules for Preventative Expressions</dc:title>
 <dc:creator>Linden, Keith Vander</dc:creator>
 <dc:creator>Di Eugenio, ; Barbara</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Building text planning resources by hand is time-consuming and difficult.
Certainly, a number of planning architectures and their accompanying plan
libraries have been implemented, but while the architectures themselves may be
reused in a new domain, the library of plans typically cannot. One way to
address this problem is to use machine learning techniques to automate the
derivation of planning resources for new domains. In this paper, we apply this
technique to build micro-planning rules for preventative expressions in
instructional text.
</dc:description>
 <dc:description>Comment: 8 pages, 4 Postscript figures, uses colap.sty</dc:description>
 <dc:date>1996-07-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607015</dc:identifier>
 <dc:identifier>INLG96 -- Eight International Natural Language Generation Workshop</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Beyond Word N-Grams</dc:title>
 <dc:creator>Pereira, Fernando C. N.</dc:creator>
 <dc:creator>Singer, Yoram</dc:creator>
 <dc:creator>Tishby, Naftali</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe, analyze, and evaluate experimentally a new probabilistic model
for word-sequence prediction in natural language based on prediction suffix
trees (PSTs). By using efficient data structures, we extend the notion of PST
to unbounded vocabularies. We also show how to use a Bayesian approach based on
recursive priors over all possible PSTs to efficiently maintain tree mixtures.
These mixtures have provably and practically better performance than almost any
single model. We evaluate the model on several corpora. The low perplexity
achieved by relatively small PST mixture models suggests that they may be an
advantageous alternative, both theoretically and practically, to the widely
used n-gram models.
</dc:description>
 <dc:description>Comment: 15 pages, one PostScript figure, uses psfig.sty and fullname.sty.
  Revised version of a paper in the Proceedings of the Third Workshop on Very
  Large Corpora, MIT, 1995</dc:description>
 <dc:date>1996-07-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607017</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural Language Processing: Structure and Complexity</dc:title>
 <dc:creator>Zadrozny, Wlodek</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce a method for analyzing the complexity of natural language
processing tasks, and for predicting the difficulty new NLP tasks.
  Our complexity measures are derived from the Kolmogorov complexity of a class
of automata --- {\it meaning automata}, whose purpose is to extract relevant
pieces of information from sentences. Natural language semantics is defined
only relative to the set of questions an automaton can answer.
  The paper shows examples of complexity estimates for various NLP programs and
tasks, and some recipes for complexity management. It positions natural
language processing as a subdomain of software engineering, and lays down its
formal foundation.
</dc:description>
 <dc:description>Comment: 8 pp. Latex (documentstyle[ijcai89,named]). In: &quot;Proc. SEKE'96, 8th
  Int. Conf. on Software Engineering and Knowledge Engineering&quot;, Lake Tahoe,
  1996, pages 595-602</dc:description>
 <dc:date>1996-07-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>TSNLP - Test Suites for Natural Language Processing</dc:title>
 <dc:creator>Lehmann, Sabine</dc:creator>
 <dc:creator>Oepen, Stephan</dc:creator>
 <dc:creator>Regnier-Prost, Sylvie</dc:creator>
 <dc:creator>Netter, Klaus</dc:creator>
 <dc:creator>Lux, Veronika</dc:creator>
 <dc:creator>Klein, Judith</dc:creator>
 <dc:creator>Falkedal, Kirsten</dc:creator>
 <dc:creator>Fouvry, Frederik</dc:creator>
 <dc:creator>Estival, Dominique</dc:creator>
 <dc:creator>Dauphin, Eva</dc:creator>
 <dc:creator>Compagnion, Herve</dc:creator>
 <dc:creator>Baur, Judith</dc:creator>
 <dc:creator>Baur, Judith</dc:creator>
 <dc:creator>Balkan, Lorna</dc:creator>
 <dc:creator>Arnold, Doug</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The TSNLP project has investigated various aspects of the construction,
maintenance and application of systematic test suites as diagnostic and
evaluation tools for NLP applications. The paper summarizes the motivation and
main results of the project: besides the solid methodological foundation, TSNLP
has produced substantial multi-purpose and multi-user test suites for three
European languages together with a set of specialized tools that facilitate the
construction, extension, maintenance, retrieval, and customization of the test
data. As TSNLP results, including the data and technology, are made publicly
available, the project presents a valuable linguistic resourc e that has the
potential of providing a wide-spread pre-standard diagnostic and evaluation
tool for both developers and users of NLP applications.
</dc:description>
 <dc:description>Comment: 7 pages, uses colap.sty and oe.sty. tar gzip uuencode. To appear in
  Proceedings of COLING-96</dc:description>
 <dc:date>1996-07-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Mental State Adjectives: the Perspective of Generative Lexicon</dc:title>
 <dc:creator>Bouillon, Pierrette</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper focusses on mental state adjectives and offers a unified analysis
in the theory of Generative Lexicon (Pustejovsky, 1991, 1995). We show that,
instead of enumerating the various syntactic constructions they enter into,
with the different senses which arise, it is possible to give them a rich typed
semantic representation which will explain both their semantic and syntactic
polymorphism.
</dc:description>
 <dc:description>Comment: 6 pages, uses colap.sty. tar gzip uuencode. To appear in Proceedings
  of COLING-96</dc:description>
 <dc:date>1996-07-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607019</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Divide-and-Conquer Strategy for Parsing</dc:title>
 <dc:creator>Shiuan, Peh Li</dc:creator>
 <dc:creator>Ann, Christopher Ting Hian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we propose a novel strategy which is designed to enhance the
accuracy of the parser by simplifying complex sentences before parsing. This
approach involves the separate parsing of the constituent sub-sentences within
a complex sentence. To achieve that, the divide-and-conquer strategy first
disambiguates the roles of the link words in the sentence and segments the
sentence based on these roles. The separate parse trees of the segmented
sub-sentences and the noun phrases within them are then synthesized to form the
final parse. To evaluate the effects of this strategy on parsing, we compare
the original performance of a dependency parser with the performance when it is
enhanced with the divide-and-conquer strategy. When tested on 600 sentences of
the IPSM'95 data sets, the enhanced parser saw a considerable error reduction
of 21.2% in its accuracy.
</dc:description>
 <dc:description>Comment: 10 pages, 7 Postscript figures, uses fancyheadings.sty, psfig.sty,
  aclap.sty, uufiles package</dc:description>
 <dc:date>1996-07-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607020</dc:identifier>
 <dc:identifier>In Proceedings ACL/SIGPARSE, pp.57-66, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Morphological Analysis as Classification: an Inductive-Learning Approach</dc:title>
 <dc:creator>Bosch, Antal van den</dc:creator>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:creator>Weijters, Ton</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Morphological analysis is an important subtask in text-to-speech conversion,
hyphenation, and other language engineering tasks. The traditional approach to
performing morphological analysis is to combine a morpheme lexicon, sets of
(linguistic) rules, and heuristics to find a most probable analysis. In
contrast we present an inductive learning approach in which morphological
analysis is reformulated as a segmentation task. We report on a number of
experiments in which five inductive learning algorithms are applied to three
variations of the task of morphological analysis. Results show (i) that the
generalisation performance of the algorithms is good, and (ii) that the lazy
learning algorithm IB1-IG performs best on all three tasks. We conclude that
lazy learning of morphological analysis as a classification task is indeed a
viable approach; moreover, it has the strong advantages over the traditional
approach of avoiding the knowledge-acquisition bottleneck, being fast and
deterministic in learning and processing, and being language-independent.
</dc:description>
 <dc:description>Comment: 11 pages, 5 encapsulated postscript figures, uses non-standard NeMLaP
  proceedings style nemlap.sty; inputs ipamacs (international phonetic
  alphabet) and epsf macros</dc:description>
 <dc:date>1996-07-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607021</dc:identifier>
 <dc:identifier>Proceedings of NEMLAP-2</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Machine Learning Approach to the Classification of Dialogue Utterances</dc:title>
 <dc:creator>Andernach, Toine</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The purpose of this paper is to present a method for automatic classification
of dialogue utterances and the results of applying that method to a corpus.
Superficial features of a set of training utterances (which we will call cues)
are taken as the basis for finding relevant utterance classes and for
extracting rules for assigning these classes to new utterances. Each cue is
assumed to partially contribute to the communicative function of an utterance.
Instead of relying on subjective judgments for the tasks of finding classes and
rules, we opt for using machine learning techniques to guarantee objectivity.
</dc:description>
 <dc:description>Comment: 12 pages, using nemlap.sty, harvard.sty and agsm.bst, to appear in
  Proceedings of NeMLaP-2, Bilkent University, Ankara, Turkey</dc:description>
 <dc:date>1996-07-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Phonological modeling for continuous speech recognition in Korean</dc:title>
 <dc:creator>Lee, WonIl</dc:creator>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A new scheme to represent phonological changes during continuous speech
recognition is suggested. A phonological tag coupled with its morphological tag
is designed to represent the conditions of Korean phonological changes. A
pairwise language model of these morphological and phonological tags is
implemented in Korean speech recognition system. Performance of the model is
verified through the TDNN-based speech recognition experiments.
</dc:description>
 <dc:description>Comment: 5 pages, ACL96 sigphon workshop</dc:description>
 <dc:date>1996-07-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Applying Winnow to Context-Sensitive Spelling Correction</dc:title>
 <dc:creator>Golding, Andrew R.</dc:creator>
 <dc:creator>Roth, Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Multiplicative weight-updating algorithms such as Winnow have been studied
extensively in the COLT literature, but only recently have people started to
use them in applications. In this paper, we apply a Winnow-based algorithm to a
task in natural language: context-sensitive spelling correction. This is the
task of fixing spelling errors that happen to result in valid words, such as
substituting {\it to\/} for {\it too}, {\it casual\/} for {\it causal}, and so
on. Previous approaches to this problem have been statistics-based; we compare
Winnow to one of the more successful such approaches, which uses Bayesian
classifiers. We find that: (1)~When the standard (heavily-pruned) set of
features is used to describe problem instances, Winnow performs comparably to
the Bayesian method; (2)~When the full (unpruned) set of features is used,
Winnow is able to exploit the new features and convincingly outperform Bayes;
and (3)~When a test set is encountered that is dissimilar to the training set,
Winnow is better than Bayes at adapting to the unfamiliar test set, using a
strategy we will present for combining learning on the training set with
unsupervised learning on the (noisy) test set.
</dc:description>
 <dc:description>Comment: 9 pages</dc:description>
 <dc:date>1996-07-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607024</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>New Methods, Current Trends and Software Infrastructure for NLP</dc:title>
 <dc:creator>Cunningham, Hamish</dc:creator>
 <dc:creator>Wilks, Yorick</dc:creator>
 <dc:creator>Gaizauskas, Robert J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The increasing use of `new methods' in NLP, which the NeMLaP conference
series exemplifies, occurs in the context of a wider shift in the nature and
concerns of the discipline. This paper begins with a short review of this
context and significant trends in the field. The review motivates and leads to
a set of requirements for support software of general utility for NLP research
and development workers. A freely-available system designed to meet these
requirements is described (called GATE - a General Architecture for Text
Engineering). Information Extraction (IE), in the sense defined by the Message
Understanding Conferences (ARPA \cite{Arp95}), is an NLP application in which
many of the new methods have found a home (Hobbs \cite{Hob93}; Jacobs ed.
\cite{Jac92}). An IE system based on GATE is also available for research
purposes, and this is described. Lastly we review related work.
</dc:description>
 <dc:description>Comment: 12 pages, LaTeX, uses nemlap.sty (included)</dc:description>
 <dc:date>1996-07-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607025</dc:identifier>
 <dc:identifier>Proceedings of NEMLAP-2</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building Knowledge Bases for the Generation of Software Documentation</dc:title>
 <dc:creator>Paris, Cecile</dc:creator>
 <dc:creator>Linden, Keith Vander</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Automated text generation requires a underlying knowledge base from which to
generate, which is often difficult to produce. Software documentation is one
domain in which parts of this knowledge base may be derived automatically. In
this paper, we describe \drafter, an authoring support tool for generating
user-centred software documentation, and in particular, we describe how parts
of its required knowledge base can be obtained automatically.
</dc:description>
 <dc:description>Comment: 6 pages, from COLING-96</dc:description>
 <dc:date>1996-07-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607026</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Translation Rules From A Bilingual Corpus</dc:title>
 <dc:creator>Cicekli, Ilyas</dc:creator>
 <dc:creator>Guvenir, H. Altay</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes a mechanism for learning pattern correspondences between
two languages from a corpus of translated sentence pairs. The proposed
mechanism uses analogical reasoning between two translations. Given a pair of
translations, the similar parts of the sentences in the source language must
correspond the similar parts of the sentences in the target language.
Similarly, the different parts should correspond to the respective parts in the
translated sentences. The correspondences between the similarities, and also
differences are learned in the form of translation rules. The system is tested
on a small training dataset and produced promising results for further
investigation.
</dc:description>
 <dc:description>Comment: 8 pages, Latex, uses nemlap.sty</dc:description>
 <dc:date>1996-07-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607027</dc:identifier>
 <dc:identifier>Published in Proceedings of NEMLAP-2</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Grammar of Sense: Is word-sense tagging much more than
  part-of-speech tagging?</dc:title>
 <dc:creator>Wilks, Yorick</dc:creator>
 <dc:creator>Stevenson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This squib claims that Large-scale Automatic Sense Tagging of text (LAST) can
be done at a high-level of accuracy and with far less complexity and
computational effort than has been believed until now. Moreover, it can be done
for all open class words, and not just carefully selected opposed pairs as in
some recent work. We describe two experiments: one exploring the amount of
information relevant to sense disambiguation which is contained in the
part-of-speech field of entries in Longman Dictionary of Contemporary English
(LDOCE). Another, more practical, experiment attempts sense disambiguation of
all open class words in a text assigning LDOCE homographs as sense tags using
only part-of-speech information. We report that 92% of open class words can be
successfully tagged in this way. We plan to extend this work and to implement
an improved large-scale tagger, a description of which is included here.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX</dc:description>
 <dc:date>1996-07-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607028</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Design and Implementation of a Tactical Generator for Turkish, a Free
  Constituent Order Language</dc:title>
 <dc:creator>Hakkani, Dilek Zeynep</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This thesis describes a tactical generator for Turkish, a free constituent
order language, in which the order of the constituents may change according to
the information structure of the sentences to be generated. In the absence of
any information regarding the information structure of a sentence (i.e., topic,
focus, background, etc.), the constituents of the sentence obey a default
order, but the order is almost freely changeable, depending on the constraints
of the text flow or discourse. We have used a recursively structured finite
state machine for handling the changes in constituent order, implemented as a
right-linear grammar backbone. Our implementation environment is the GenKit
system, developed at Carnegie Mellon University--Center for Machine
Translation. Morphological realization has been implemented using an external
morphological analysis/generation component which performs concrete morpheme
selection and handles morphographemic processes.
</dc:description>
 <dc:description>Comment: M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. 146 pages (including
  title pages). Also available as:
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1996/BU-CEIS-9614.ps.z</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607030</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Multiple Sources of Information for Constraint-Based Morphological
  Disambiguation</dc:title>
 <dc:creator>Tur, Gokhan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This thesis presents a constraint-based morphological disambiguation approach
that is applicable to languages with complex morphology--specifically
agglutinative languages with productive inflectional and derivational
morphological phenomena. For morphologically complex languages like Turkish,
automatic morphological disambiguation involves selecting for each token
morphological parse(s), with the right set of inflectional and derivational
markers. Our system combines corpus independent hand-crafted constraint rules,
constraint rules that are learned via unsupervised learning from a training
corpus, and additional statistical information obtained from the corpus to be
morphologically disambiguated. The hand-crafted rules are linguistically
motivated and tuned to improve precision without sacrificing recall. In certain
respects, our approach has been motivated by Brill's recent work, but with the
observation that his transformational approach is not directly applicable to
languages like Turkish. Our approach also uses a novel approach to unknown word
processing by employing a secondary morphological processor which recovers any
relevant inflectional and derivational information from a lexical item whose
root is unknown. With this approach, well below 1% of the tokens remains as
unknown in the texts we have experimented with. Our results indicate that by
combining these hand-crafted, statistical and learned information sources, we
can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%,
and ambiguity of 1.02 to 1.03 parses per token.
</dc:description>
 <dc:description>Comment: M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. Also available as:
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1996/BU-CEIS-9615ps.z</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607030</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607031</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Compositional Semantics in Verbmobil</dc:title>
 <dc:creator>Bos, Johan</dc:creator>
 <dc:creator>Gamb&#xe4;ck, Bj&#xf6;rn</dc:creator>
 <dc:creator>Lieske, Christian</dc:creator>
 <dc:creator>Mori, Yoshiki</dc:creator>
 <dc:creator>Pinkal, Manfred</dc:creator>
 <dc:creator>Worm, Karsten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper discusses how compositional semantics is implemented in the
Verbmobil speech-to-speech translation system using LUD, a description language
for underspecified discourse representation structures. The description
language and its formal interpretation in DRT are described as well as its
implementation together with the architecture of the system's entire
syntactic-semantic processing module. We show that a linguistically sound
theory and formalism can be properly implemented in a system with (near)
real-time requirements.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, uses colap.sty</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607031</dc:identifier>
 <dc:identifier>Proceedings of COLING '96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607032</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Lexical Semantic Database for Verbmobil</dc:title>
 <dc:creator>Heinecke, Johannes</dc:creator>
 <dc:creator>Worm, Karsten L.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes the development and use of a lexical semantic database
for the Verbmobil speech-to-speech machine translation system. The motivation
is to provide a common information source for the distributed development of
the semantics, transfer and semantic evaluation modules and to store lexical
semantic information application-independently.
  The database is organized around a set of abstract semantic classes and has
been used to define the semantic contributions of the lemmata in the vocabulary
of the system, to automatically create semantic lexica and to check the
correctness of the semantic representations built up. The semantic classes are
modelled using an inheritance hierarchy. The database is implemented using the
lexicon formalism LeX4 developed during the project.
</dc:description>
 <dc:description>Comment: 13 Pages, 2 Postscript figures, uses epsf.sty, a4.sty, alltt.sty
  (included), chicago.sty; .tar.gz-file includes .dvi and .ps versions as well</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607032</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607033</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multiple Discourse Relations on the Sentential Level in Japanese</dc:title>
 <dc:creator>Mori, Yoshiki</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In the German government (BMBF) funded project Verbmobil, a semantic
formalism Language for Underspecified Discourse Representation Structures (LUD)
is used which describes several DRSs and allows for underspecification. Dealing
with Japanese poses challenging problems. In this paper, a treatment of
multiple discourse relation constructions on the sentential level is shown,
which are common in Japanese but cause a problem for the formalism,. The
problem is to distinguish discourse relations which take the widest scope
compared with other scope-taking elements on the one hand and to have them
underspecified among each other on the other hand. We also state a semantic
constraint on the resolution of multiple discourse relations which seems to
prevail over the syntactic c-command constraint.
</dc:description>
 <dc:description>Comment: 6 pages, Postscript</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607033</dc:identifier>
 <dc:identifier>Proceedings of COLING '96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607034</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using textual clues to improve metaphor processing</dc:title>
 <dc:creator>Ferrari, St&#xe9;phane</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we propose a textual clue approach to help metaphor detection,
in order to improve the semantic processing of this figure. The previous works
in the domain studied the semantic regularities only, overlooking an obvious
set of regularities. A corpus-based analysis shows the existence of surface
regularities related to metaphors. These clues can be characterized by
syntactic structures and lexical markers. We present an object oriented model
for representing the textual clues that were found. This representation is
designed to help the choice of a semantic processing, in terms of possible
non-literal meanings. A prototype implementing this model is currently under
development, within an incremental approach allowing step-by-step evaluations.
\footnote{This work takes part in a research project sponsored by the
AUPELF-UREF (Francophone Agency For Education and Research)}
</dc:description>
 <dc:description>Comment: 3 pages, single LaTeX file, uses aclap.sty</dc:description>
 <dc:date>1996-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607034</dc:identifier>
 <dc:identifier>Proceedings of the ACL'96, 34th Annual Meeting of the Association
  for Computational Linguistics, 351-353, Santa Cruz, California, 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607035</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Completeness of Compositional Translation for Context-Free Grammars</dc:title>
 <dc:creator>Huijsen, Willem-Olaf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A machine translation system is said to be *complete* if all expressions that
are correct according to the source-language grammar can be translated into the
target language. This paper addresses the completeness issue for compositional
machine translation in general, and for compositional machine translation of
context-free grammars in particular. Conditions that guarantee translation
completeness of context-free grammars are presented.
</dc:description>
 <dc:description>Comment: 14 pages, LaTeX 2e, to appear in Proceedings of CLIN'95. Also
  available as: http://wwwots.let.ruu.nl/Personal/Doc/clin-artikel-1995.ps</dc:description>
 <dc:date>1996-07-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607035</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607036</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Connected Text Recognition Using Layered HMMs and Token Passing</dc:title>
 <dc:creator>Ingels, Peter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a novel approach to lexical error recovery on textual input. An
advanced robust tokenizer has been implemented that can not only correct
spelling mistakes, but also recover from segmentation errors. Apart from the
orthographic considerations taken, the tokenizer also makes use of linguistic
expectations extracted from a training corpus. The idea is to arrange Hidden
Markov Models (HMM) in multiple layers where the HMMs in each layer are
responsible for different aspects of the processing of the input. We report on
experimental evaluations with alternative probabilistic language models to
guide the lexical error recovery process.
</dc:description>
 <dc:description>Comment: 12 pages, LaTeX format, 3 encapsulated Postscript figures, uses
  nemlap.sty</dc:description>
 <dc:date>1996-07-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607036</dc:identifier>
 <dc:identifier>Proceedings of NeMLaP-2</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9607037</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Construction of Clean Broad-Coverage Translation Lexicons</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Word-level translational equivalences can be extracted from parallel texts by
surprisingly simple statistical techniques. However, these techniques are
easily fooled by {\em indirect associations} --- pairs of unrelated words whose
statistical properties resemble those of mutual translations. Indirect
associations pollute the resulting translation lexicons, drastically reducing
their precision. This paper presents an iterative lexicon cleaning method. On
each iteration, most of the remaining incorrect lexicon entries are filtered
out, without significant degradation in recall. This lexicon cleaning technique
can produce translation lexicons with recall and precision both exceeding 90\%,
as well as dictionary-sized translation lexicons that are over 99\% correct.
</dc:description>
 <dc:description>Comment: PostScript file, 10 pages. To appear in Proceedings of AMTA-96</dc:description>
 <dc:date>1996-07-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9607037</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Storage of Natural Language Sentences in a Hopfield Network</dc:title>
 <dc:creator>Collier, Nigel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper look at how the Hopfield neural network can be used to store and
recall patterns constructed from natural language sentences. As a pattern
recognition and storage tool, the Hopfield neural network has received much
attention. This attention however has been mainly in the field of statistical
physics due to the model's simple abstraction of spin glass systems. A
discussion is made of the differences, shown as bias and correlation, between
natural language sentence patterns and the randomly generated ones used in
previous experiments. Results are given for numerical simulations which show
the auto-associative competence of the network when trained with natural
language patterns.
</dc:description>
 <dc:description>Comment: latex, 10 pages with 2 tex figures and a .bib file, uses nemlap.sty,
  to appear in Proceedings of NeMLaP-2</dc:description>
 <dc:date>1996-08-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Controlling Functional Uncertainty</dc:title>
 <dc:creator>Backofen, Rolf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  There have been two different methods for checking the satisfiability of
feature descriptions that use the functional uncertainty device,
namely~\cite{Kaplan:88CO} and \cite{Backofen:94JSC}. Although only the one in
\cite{Backofen:94JSC} solves the satisfiability problem completely, both
methods have their merits. But it may happen that in one single description,
there are parts where the first method is more appropriate, and other parts
where the second should be applied. In this paper, we present a common
framework that allows one to combine both methods. This is done by presenting a
set of rules for simplifying feature descriptions. The different methods are
described as different controls on this rule set, where a control specifies in
which order the different rules must be applied.
</dc:description>
 <dc:description>Comment: 5 pages (to appear in Proceedings of ECAI '96)</dc:description>
 <dc:date>1996-08-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stylistic Variation in an Information Retrieval Experiment</dc:title>
 <dc:creator>Karlgren, Jussi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Texts exhibit considerable stylistic variation. This paper reports an
experiment where a corpus of documents (N= 75 000) is analyzed using various
simple stylistic metrics. A subset (n = 1000) of the corpus has been previously
assessed to be relevant for answering given information retrieval queries. The
experiment shows that this subset differs significantly from the rest of the
corpus in terms of the stylistic metrics studied.
</dc:description>
 <dc:description>Comment: Proceedings of NEMLAP-2</dc:description>
 <dc:date>1996-08-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Patterns of Language - A Population Model for Language Structure</dc:title>
 <dc:creator>Freeman, Robert John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A key problem in the description of language structure is to explain its
contradictory properties of specificity and generality, the contrasting poles
of formulaic prescription and generative productivity. I argue that this is
possible if we accept analogy and similarity as the basic mechanisms of
structural definition. As a specific example I discuss how it would be possible
to use analogy to define a generative model of syntactic structure.
</dc:description>
 <dc:description>Comment: 6 pages, Postscript</dc:description>
 <dc:date>1996-08-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>CLEARS - An Education and Research Tool for Computational Semantics</dc:title>
 <dc:creator>Milward, David</dc:creator>
 <dc:creator>Konrad, Karsten</dc:creator>
 <dc:creator>Maier, Holger</dc:creator>
 <dc:creator>Pinkal, Manfred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The CLEARS (Computational Linguistics Education and Research for Semantics)
tool provides a graphical interface allowing interactive construction of
semantic representations in a variety of different formalisms, and using
several construction methods. CLEARS was developed as part of the FraCaS
project which was designed to encourage convergence between different semantic
formalisms, such as Montague-Grammar, DRT, and Situation Semantics. The CLEARS
system is freely available on the WWW from
http://coli.uni-sb.de/~clears/clears.html
</dc:description>
 <dc:description>Comment: 4 pages, includes 6 postscript figures, needs colap.sty and acl.bst</dc:description>
 <dc:date>1996-08-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Grapheme-to-Phoneme Conversion using Multiple Unbounded Overlapping
  Chunks</dc:title>
 <dc:creator>Yvon, Francois</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present in this paper an original extension of two data-driven algorithms
for the transcription of a sequence of graphemes into the corresponding
sequence of phonemes. In particular, our approach generalizes the algorithm
originally proposed by Dedina and Nusbaum (D&amp;N) (1991), which had originally
been promoted as a model of the human ability to pronounce unknown words by
analogy to familiar lexical items. We will show that DN's algorithm performs
comparatively poorly when evaluated on a realistic test set, and that our
extension allows us to improve substantially the performance of the
analogy-based model. We will also suggest that both algorithms can be
reformulated in a much more general framework, which allows us to anticipate
other useful extensions. However, considering the inability to define in these
models important notions like lexical neighborhood, we conclude that both
approaches fail to offer a proper model of the analogical processes involved in
reading aloud.
</dc:description>
 <dc:description>Comment: 11 pages, Postscript only, Proceedings of NeMLaP II</dc:description>
 <dc:date>1996-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Centering in Italian</dc:title>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper explores the correlation between centering and different forms of
pronominal reference in Italian, in particular zeros and overt pronouns in
subject position. Such correlations, that I had proposed in earlier work
(COLING 90), are verified through the analysis of a corpus of naturally
occurring texts. In the process, I extend my previous analysis in several ways,
for example by taking possessives and subordinates into account. I also provide
a more detailed analysis of the &quot;continue&quot; transition: more specifically, I
show that pronouns are used in a markedly different way in a &quot;continue&quot;
preceded by another &quot;continue&quot; or by a &quot;shift&quot;, and in a &quot;continue&quot; preceded by
a &quot;retain&quot;.
</dc:description>
 <dc:description>Comment: 18 pages, uses cgloss4e.sty and fullname.sty. To appear in &quot;Centering
  in Discourse&quot;, OUP, 1997</dc:description>
 <dc:date>1996-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The discourse functions of Italian subjects: a centering approach</dc:title>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper examines the discourse functions that different types of subjects
perform in Italian within the centering framework. I build on my previous work
(COLING90) that accounted for the alternation of null and strong pronouns in
subject position. I extend my previous analysis in several ways: for example, I
refine the notion of {\sc continue} and discuss the centering functions of full
NPs.
</dc:description>
 <dc:description>Comment: 7 pages, uses colap.sty. Revised version of COLING96 paper (fixes
  wrong chi-square values in published version)</dc:description>
 <dc:date>1996-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608008</dc:identifier>
 <dc:identifier>Proceedings COLING96, Copenhagen, August 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Centering theory and the Italian pronominal system</dc:title>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, I give an account of some phenomena of pronominalization in
Italian in terms of centering theory. After a general introduction to the
Italian pronominal system, I will review centering, and then show how the
original rules have to be extended or modified. Finally, I will show that
centering does not account for two phenomena: first, the functional role of an
utterance may override the predictions of centering; second, a null subject can
be used to refer to a whole discourse segment.
</dc:description>
 <dc:description>Comment: 6 pages, uses colap.sty</dc:description>
 <dc:date>1996-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608009</dc:identifier>
 <dc:identifier>Proceedings COLING90, Helsinki, August 1990</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fishing for Exactness</dc:title>
 <dc:creator>Pedersen, Ted</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Statistical methods for automatically identifying dependent word pairs (i.e.
dependent bigrams) in a corpus of natural language text have traditionally been
performed using asymptotic tests of significance. This paper suggests that
Fisher's exact test is a more appropriate test due to the skewed and sparse
data samples typical of this problem. Both theoretical and experimental
comparisons between Fisher's exact test and a variety of asymptotic tests (the
t-test, Pearson's chi-square test, and Likelihood-ratio chi-square test) are
presented. These comparisons show that Fisher's exact test is more reliable in
identifying dependent word pairs. The usefulness of Fisher's exact test extends
to other problems in statistical natural language processing as skewed and
sparse data appears to be the rule in natural language. The experiment
presented in this paper was performed using PROC FREQ of the SAS System.
</dc:description>
 <dc:description>Comment: 13 pages - postscript</dc:description>
 <dc:date>1996-08-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608010</dc:identifier>
 <dc:identifier>Proceedings of the South-Central SAS Users Group Conference
  (SCSUG-96), Austin, TX, Oct 27-29, 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Punctuation in Quoted Speech</dc:title>
 <dc:creator>Doran, Christine</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Quoted speech is often set off by punctuation marks, in particular quotation
marks. Thus, it might seem that the quotation marks would be extremely useful
in identifying these structures in texts. Unfortunately, the situation is not
quite so clear. In this work, I will argue that quotation marks are not
adequate for either identifying or constraining the syntax of quoted speech.
More useful information comes from the presence of a quoting verb, which is
either a verb of saying or a punctual verb, and the presence of other
punctuation marks, usually commas. Using a lexicalized grammar, we can license
most quoting clauses as text adjuncts. A distinction will be made not between
direct and indirect quoted speech, but rather between adjunct and non-adjunct
quoting clauses.
</dc:description>
 <dc:description>Comment: 11 pages, 11 ps figures, Proceedings of SIGPARSE 96 - Punctuation in
  Computational Linguistics</dc:description>
 <dc:date>1996-08-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multilingual Text Analysis for Text-to-Speech Synthesis</dc:title>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a model of text analysis for text-to-speech (TTS) synthesis based
on (weighted) finite-state transducers, which serves as the text-analysis
module of the multilingual Bell Labs TTS system. The transducers are
constructed using a lexical toolkit that allows declarative descriptions of
lexicons, morphological rules, numeral-expansion rules, and phonological rules,
inter alia. To date, the model has been applied to eight languages: Spanish,
Italian, Romanian, French, German, Russian, Mandarin and Japanese.
</dc:description>
 <dc:date>1996-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608012</dc:identifier>
 <dc:identifier>ECAI Workshop on Extended Finite-State Models of Language</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Word Grammar of Turkish with Morphophonemic Rules</dc:title>
 <dc:creator>Oztaner, S. Murat</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this thesis, morphological description of Turkish is encoded using the
two-level model. This description is made up of the phonological component that
contains the two-level morphophonemic rules, and the lexicon component which
lists the lexical items and encodes the morphotactic constraints. The word
grammar is expressed in tabular form. It includes the verbal and the nominal
paradigm. Vowel and consonant harmony, epenthesis, reduplication, etc. are
described in detail and coded in two-level notation. Loan-word phonology is
modelled separately.
  The implementation makes use of Lexc/Twolc from Xerox. Mechanisms to
integrate the morphological analyzer with the lexical and syntactic components
are discussed, and a simple graphical user interface is provided. Work is
underway to use this model in a classroom setting for teaching Turkish
morphology to non-native speakers.
</dc:description>
 <dc:description>Comment: 128 pages, postscript, MS Thesis in Dept of Computer Engineering</dc:description>
 <dc:date>1996-08-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Classifiers in Japanese-to-English Machine Translation</dc:title>
 <dc:creator>Bond, Francis</dc:creator>
 <dc:creator>Ogura, Kentaro</dc:creator>
 <dc:creator>Ikehara, Satoru</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes an analysis of classifiers into four major types: UNIT,
METRIC, GROUP and SPECIES, based on properties of both Japanese and English.
The analysis makes possible a uniform and straightforward treatment of noun
phrases headed by classifiers in Japanese-to-English machine translation, and
has been implemented in the MT system ALT-J/E. Although the analysis is based
on the characteristics of, and differences between, Japanese and English, it is
shown to be also applicable to the unrelated language Thai.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX, uses gb4e.sty, fullname.bst</dc:description>
 <dc:date>1996-08-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608014</dc:identifier>
 <dc:identifier>Proceedings of the 16th International Conference on Computational
  Linguistics (COLING'96), pp 125--130.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Morphological Productivity in the Lexicon</dc:title>
 <dc:creator>Sehitoglu, Onur</dc:creator>
 <dc:creator>Bozsahin, Cem</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we outline a lexical organization for Turkish that makes use of
lexical rules for inflections, derivations, and lexical category changes to
control the proliferation of lexical entries. Lexical rules handle changes in
grammatical roles, enforce type constraints, and control the mapping of
subcategorization frames in valency-changing operations. A lexical inheritance
hierarchy facilitates the enforcement of type constraints. Semantic
compositions in inflections and derivations are constrained by the properties
of the terms and predicates.
  The design has been tested as part of a HPSG grammar for Turkish. In terms of
performance, run-time execution of the rules seems to be a far better
alternative than pre-compilation. The latter causes exponential growth in the
lexicon due to intensive use of inflections and derivations in Turkish.
</dc:description>
 <dc:description>Comment: 10 pages LaTeX, {lingmacros,avm,psfig}.sty, 1 figure, 1 bibtex file</dc:description>
 <dc:date>1996-08-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608015</dc:identifier>
 <dc:identifier>Proc. of the ACL'96 SIGLEX Workshop, Santa Cruz, 105--114</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Sign-Based Phrase Structure Grammar for Turkish</dc:title>
 <dc:creator>Sehitoglu, Onur Tolga</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This study analyses Turkish syntax from an informational point of view. Sign
based linguistic representation and principles of HPSG (Head-driven Phrase
Structure Grammar) theory are adapted to Turkish. The basic informational
elements are nested and inherently sorted feature structures called signs.
  In the implementation, logic programming tool ALE (Attribute Logic Engine)
which is primarily designed for implementing HPSG grammars is used. A type and
structure hierarchy of Turkish language is designed. Syntactic phenomena such a
s subcategorization, relative clauses, constituent order variation, adjuncts,
nomina l predicates and complement-modifier relations in Turkish are analyzed.
A parser is designed and implemented in ALE.
</dc:description>
 <dc:description>Comment: MS. Thesis, Dept. of Computer Engineering, Middle East Technical
  University, Ankara January, 1996, 97 pages. 5 eps figures, uses
  avm,psfig,lingmacros,tree-dvips,ulem,QobiTree,alltt
  ulem.sty,QobiTree.sty,alltt.sty and eps files included in tar</dc:description>
 <dc:date>1996-08-26</dc:date>
 <dc:date>1996-08-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Alignment of English-Chinese Bilingual Texts of CNS News</dc:title>
 <dc:creator>Xu, Donghua</dc:creator>
 <dc:creator>Tan, Chew Lim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we address a method to align English-Chinese bilingual news
reports from China News Service, combining both lexical and satistical
approaches. Because of the sentential structure differences between English and
Chinese, matching at the sentence level as in many other works may result in
frequent matching of several sentences en masse. In view of this, the current
work also attempts to create shorter alignment pairs by permitting finer
matching between clauses from both texts if possible. The current method is
based on statiscal correlation between sentence or clause length of both texts
and at the same time uses obvious anchors such as numbers and place names
appearing frequently in the news reports as lexcial cues.
</dc:description>
 <dc:description>Comment: 9 pages, Postscript only. In the Proceedings of International
  Conference on Chinese Computing'96</dc:description>
 <dc:date>1996-08-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Algorithms for Speech Recognition and Language Processing</dc:title>
 <dc:creator>Mohri, Mehryar</dc:creator>
 <dc:creator>Riley, Michael</dc:creator>
 <dc:creator>Sproat, Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Speech processing requires very efficient methods and algorithms.
Finite-state transducers have been shown recently both to constitute a very
useful abstract model and to lead to highly efficient time and space algorithms
in this field. We present these methods and algorithms and illustrate them in
the case of speech recognition. In addition to classical techniques, we
describe many new algorithms such as minimization, global and local on-the-fly
determinization of weighted automata, and efficient composition of transducers.
These methods are currently used in large vocabulary speech recognition
systems. We then show how the same formalism and algorithms can be used in
text-to-speech applications and related areas of language processing such as
morphology, syntax, and local grammars, in a very efficient way. The tutorial
is self-contained and requires no specific computational or linguistic
knowledge other than classical results.
</dc:description>
 <dc:description>Comment: Postscript file tar-compressed and uuencoded, 189 pages</dc:description>
 <dc:date>1996-08-27</dc:date>
 <dc:date>1996-09-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using sentence connectors for evaluating MT output</dc:title>
 <dc:creator>Visser, Eric M.</dc:creator>
 <dc:creator>Fuji, Masaru</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper elaborates on the design of a machine translation evaluation
method that aims to determine to what degree the meaning of an original text is
preserved in translation, without looking into the grammatical correctness of
its constituent sentences. The basic idea is to have a human evaluator take the
sentences of the translated text and, for each of these sentences, determine
the semantic relationship that exists between it and the sentence immediately
preceding it. In order to minimise evaluator dependence, relations between
sentences are expressed in terms of the conjuncts that can connect them, rather
than through explicit categories. For an n-sentence text this results in a list
of n-1 sentence-to-sentence relationships, which we call the text's
connectivity profile. This can then be compared to the connectivity profile of
the original text, and the degree of correspondence between the two would be a
measure for the quality of the translation.
  A set of &quot;essential&quot; conjuncts was extracted for English and Japanese, and a
computer interface was designed to support the task of inserting the most
fitting conjuncts between sentence pairs. With these in place, several sets of
experiments were performed.
</dc:description>
 <dc:description>Comment: 4 pages, LaTeX, uses colap.sty</dc:description>
 <dc:date>1996-08-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608019</dc:identifier>
 <dc:identifier>Proceedings of COLING-96 (Poster Sessions, pgs. 1066-1069)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608020</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Phonetic Ambiguity : Approaches, Touchstones, Pitfalls and New
  Approaches</dc:title>
 <dc:creator>Juola, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Phonetic ambiguity and confusibility are bugbears for any form of bottom-up
or data-driven approach to language processing. The question of when an input
is ``close enough'' to a target word pervades the entire problem spaces of
speech recognition, synthesis, language acquisition, speech compression, and
language representation, but the variety of representations that have been
applied are demonstrably inadequate to at least some aspects of the problem.
This paper reviews this inadequacy by examining several touchstone models in
phonetic ambiguity and relating them to the problems they were designed to
solve. An good solution would be, among other things, efficient, accurate,
precise, and universally applicable to representation of words, ideally usable
as a ``phonetic distance'' metric for direct measurement of the ``distance''
between word or utterance pairs. None of the proposed models can provide a
complete solution to the problem; in general, there is no algorithmic theory of
phonetic distance. It is unclear whether this is a weakness of our
representational technology or a more fundamental difficulty with the problem
statement.
</dc:description>
 <dc:description>Comment: LaTex, 6 pages, self-contained</dc:description>
 <dc:date>1996-08-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608020</dc:identifier>
 <dc:identifier>CSNLP-96, Sept. 2-4, Dublin, Ireland</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9608021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Isolated-Word Confusion Metrics and the PGPfone Alphabet</dc:title>
 <dc:creator>Juola, Patrick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although the confusion of individual phonemes and features have been studied
and analyzed since (Miller and Nicely, 1955), there has been little work done
on extending this to a predictive theory of word-level confusions. The PGPfone
alphabet is a good touchstone problem for developing such word-level confusion
metrics. This paper presents some difficulties incurred, along with their
proposed solutions, in the extension of phonetic confusion results to a
theoretical whole-word phonetic distance metric. The proposed solutions have
been used, in conjunction with a set of selection filters, in a genetic
algorithm to automatically generate appropriate word lists for a radio
alphabet. This work illustrates some principles and pitfalls that should be
addressed in any numeric theory of isolated word perception.
</dc:description>
 <dc:description>Comment: 12 pages, includes minipage.tex and nemlapfig.eps, uses nemlap.sty</dc:description>
 <dc:date>1996-08-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9608021</dc:identifier>
 <dc:identifier>NeMLaP-96, Sept. 16-18, 1996, Ankara TURKEY</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Corrections and Higher-Order Unification</dc:title>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:creator>Kohlhase, Michael</dc:creator>
 <dc:creator>van Neusen, Noor</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose an analysis of corrections which models some of the requirements
corrections place on context. We then show that this analysis naturally extends
to the interaction of corrections with pronominal anaphora on the one hand, and
(in)definiteness on the other. The analysis builds on previous
unification--based approaches to NL semantics and relies on Higher--Order
Unification with Equivalences, a form of unification which takes into account
not only syntactic beta-eta-identity but also denotational equivalence.
</dc:description>
 <dc:description>Comment: 12 pages, LateX file, In Proccedings of the 3. Konferenz zur
  Verarbeitung natuerlicher Sprache (KONVENS), Bielefeld, 1996</dc:description>
 <dc:date>1996-09-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Inferring Acceptance and Rejection in Dialogue by Default Rules of
  Inference</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses the processes by which conversants in a dialogue can
infer whether their assertions and proposals have been accepted or rejected by
their conversational partners. It expands on previous work by showing that
logical consistency is a necessary indicator of acceptance, but that it is not
sufficient, and that logical inconsistency is sufficient as an indicator of
rejection, but it is not necessary. I show how conversants can use information
structure and prosody as well as logical reasoning in distinguishing between
acceptances and logically consistent rejections, and relate this work to
previous work on implicature and default reasoning by introducing three new
classes of rejection: {\sc implicature rejections}, {\sc epistemic rejections}
and {\sc deliberation rejections}. I show how these rejections are inferred as
a result of default inferences, which, by other analyses, would have been
blocked by the context. In order to account for these facts, I propose a model
of the common ground that allows these default inferences to go through, and
show how the model, originally proposed to account for the various forms of
acceptance, can also model all types of rejection.
</dc:description>
 <dc:description>Comment: 37 pages, uses fullpage, lingmacros, named</dc:description>
 <dc:date>1996-09-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609002</dc:identifier>
 <dc:identifier>Language and Speech, 39-2, 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Cue Phrase Classification Using Machine Learning</dc:title>
 <dc:creator>Litman, Diane J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure, e.g., for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models, but also for improving upon previous
results. When compared to manually derived classification models already in the
literature, the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition, the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally, the ease of retraining makes the learning approach more scalable and
flexible than manual methods.
</dc:description>
 <dc:description>Comment: 42 pages, uses jair.sty, theapa.bst, theapa.sty</dc:description>
 <dc:date>1996-09-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609003</dc:identifier>
 <dc:identifier>Journal of Artificial Intelligence Research 5 (1996) 53-94</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609004</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Principled Framework for Constructing Natural Language Interfaces To
  Temporal Databases</dc:title>
 <dc:creator>Androutsopoulos, Ion</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Most existing natural language interfaces to databases (NLIDBs) were designed
to be used with ``snapshot'' database systems, that provide very limited
facilities for manipulating time-dependent data. Consequently, most NLIDBs also
provide very limited support for the notion of time. The database community is
becoming increasingly interested in _temporal_ database systems. These are
intended to store and manipulate in a principled manner information not only
about the present, but also about the past and future.
  This thesis develops a principled framework for constructing English NLIDBs
for _temporal_ databases (NLITDBs), drawing on research in tense and aspect
theories, temporal logics, and temporal databases. I first explore temporal
linguistic phenomena that are likely to appear in English questions to NLITDBs.
Drawing on existing linguistic theories of time, I formulate an account for a
large number of these phenomena that is simple enough to be embodied in
practical NLITDBs. Exploiting ideas from temporal logics, I then define a
temporal meaning representation language, TOP, and I show how the HPSG grammar
theory can be modified to incorporate the tense and aspect account of this
thesis, and to map a wide range of English questions involving time to
appropriate TOP expressions. Finally, I present and prove the correctness of a
method to translate from TOP to TSQL2, TSQL2 being a temporal extension of the
SQL-92 database language. This way, I establish a sound route from English
questions involving time to a general-purpose temporal database language, that
can act as a principled framework for building NLITDBs. To demonstrate that
this framework is workable, I employ it to develop a prototype NLITDB,
implemented using ALE and Prolog.
</dc:description>
 <dc:description>Comment: PhD thesis; 405 pages; LaTeX2e, uses the packages/macros: amstex,
  xspace, avm, examples, dvips, varioref, makeidx, epic, eepic, ecltree;
  postscript figures included</dc:description>
 <dc:date>1996-09-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Centering in Japanese Discourse</dc:title>
 <dc:creator>Walker, Marilyn</dc:creator>
 <dc:creator>Iida, Masayo</dc:creator>
 <dc:creator>Cote, Sharon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we propose a computational treatment of the resolution of zero
pronouns in Japanese discourse, using an adaptation of the centering algorithm.
We are able to factor language-specific dependencies into one parameter of the
centering algorithm. Previous analyses have stipulated that a zero pronoun and
its cospecifier must share a grammatical function property such as {\sc
Subject} or {\sc NonSubject}. We show that this property-sharing stipulation is
unneeded. In addition we propose the notion of {\sc topic ambiguity} within the
centering framework, which predicts some ambiguities that occur in Japanese
discourse. This analysis has implications for the design of
language-independent discourse modules for Natural Language systems. The
centering algorithm has been implemented in an HPSG Natural Language system
with both English and Japanese grammars.
</dc:description>
 <dc:description>Comment: 7 pages, uses twocolumn</dc:description>
 <dc:date>1996-09-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609005</dc:identifier>
 <dc:identifier>COLING90: Proceedings 13th International Conference on
  Computational Linguistics, Helsinki</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Japanese Discourse and the Process of Centering</dc:title>
 <dc:creator>Walker, Marilyn</dc:creator>
 <dc:creator>Iida, Masayo</dc:creator>
 <dc:creator>Cote, Sharon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper has three aims: (1) to generalize a computational account of the
discourse process called {\sc centering}, (2) to apply this account to
discourse processing in Japanese so that it can be used in computational
systems for machine translation or language understanding, and (3) to provide
some insights on the effect of syntactic factors in Japanese on discourse
interpretation. We argue that while discourse interpretation is an inferential
process, syntactic cues constrain this process, and demonstrate this argument
with respect to the interpretation of {\sc zeros}, unexpressed arguments of the
verb, in Japanese. The syntactic cues in Japanese discourse that we investigate
are the morphological markers for grammatical {\sc topic}, the postposition
{\it wa}, as well as those for grammatical functions such as {\sc subject},
{\em ga}, {\sc object}, {\em o} and {\sc object2}, {\em ni}. In addition, we
investigate the role of speaker's {\sc empathy}, which is the viewpoint from
which an event is described. This is syntactically indicated through the use of
verbal compounding, i.e. the auxiliary use of verbs such as {\it kureta, kita}.
Our results are based on a survey of native speakers of their interpretation of
short discourses, consisting of minimal pairs, varied by one of the above
factors. We demonstrate that these syntactic cues do indeed affect the
interpretation of {\sc zeros}, but that having previously been the {\sc topic}
and being realized as a {\sc zero} also contributes to the salience of a
discourse entity. We propose a discourse rule of {\sc zero topic assignment},
and show that {\sc centering} provides constraints on when a {\sc zero} can be
interpreted as the {\sc zero topic}.
</dc:description>
 <dc:description>Comment: 38 pages, uses clstyle, lingmacros</dc:description>
 <dc:date>1996-09-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609006</dc:identifier>
 <dc:identifier>Computational Linguistics 20-2, 1994</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discourse Coherence and Shifting Centers in Japanese Texts</dc:title>
 <dc:creator>Iida, Masayo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In languages such as Japanese, the use of {\it zeros}, unexpressed arguments
of the verb, in utterances that shift the topic involves a risk that the
meaning intended by the speaker may not be transparent to the hearer. However,
this potentially undesirable conversational strategy often occurs in the course
of naturally-occurring discourse. In this chapter, I report on an empirical
study of 250 utterances with {\it zeros} in 20 Japanese newspaper articles.
Each utterance is analyzed in terms of centering transitions and the form in
which centers are realized by referring expressions. I also examine lexical
subcategorization information, and tense and aspect in order to test the
hypothesis that the speaker expects the hearer to use this information in
determining global discourse structure. I explain the occurrence of {\it zeros}
in {\sc retain} and {\sc rough-shift} centering transitions, by claiming that a
{\it zero} can only be used in these cases when the shift of centers is
supported by contextual information such as lexical semantics, tense and
aspect, and agreement features. I then propose an algorithm by which centering
can incorporate these observations to integrate centering with global discourse
structure, and thus enhance its ability for non-local pronoun resolution.
</dc:description>
 <dc:description>Comment: 20 pages, uses elsart12.sty, lingmacros.sty, named.sty</dc:description>
 <dc:date>1996-09-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609007</dc:identifier>
 <dc:identifier>Centering in Discourse, Oxford University Press; Eds. Walker,
  Joshi and Prince, In Press</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Designing Statistical Language Learners: Experiments on Noun Compounds</dc:title>
 <dc:creator>Lauer, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The goal of this thesis is to advance the exploration of the statistical
language learning design space. In pursuit of that goal, the thesis makes two
main theoretical contributions: (i) it identifies a new class of designs by
specifying an architecture for natural language analysis in which probabilities
are given to semantic forms rather than to more superficial linguistic
elements; and (ii) it explores the development of a mathematical theory to
predict the expected accuracy of statistical language learning systems in terms
of the volume of data used to train them.
  The theoretical work is illustrated by applying statistical language learning
designs to the analysis of noun compounds. Both syntactic and semantic analysis
of noun compounds are attempted using the proposed architecture. Empirical
comparisons demonstrate that the proposed syntactic model is significantly
better than those previously suggested, approaching the performance of human
judges on the same task, and that the proposed semantic model, the first
statistical approach to this problem, exhibits significantly better accuracy
than the baseline strategy. These results suggest that the new class of designs
identified is a promising one. The experiments also serve to highlight the need
for a widely applicable theory of data requirements.
</dc:description>
 <dc:description>Comment: PhD thesis (Macquarie University, Sydney; December 1995), LaTeX
  source, xii+214 pages</dc:description>
 <dc:date>1996-09-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Geometric Approach to Mapping Bitext Correspondence</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The first step in most corpus-based multilingual NLP work is to construct a
detailed map of the correspondence between a text and its translation. Several
automatic methods for this task have been proposed in recent years. Yet even
the best of these methods can err by several typeset pages. The Smooth
Injective Map Recognizer (SIMR) is a new bitext mapping algorithm. SIMR's
errors are smaller than those of the previous front-runner by more than a
factor of 4. Its robustness has enabled new commercial-quality applications.
The greedy nature of the algorithm makes it independent of memory resources.
Unlike other bitext mapping algorithms, SIMR allows crossing correspondences to
account for word order differences. Its output can be converted quickly and
easily into a sentence alignment. SIMR's output has been used to align over 200
megabytes of the Canadian Hansards for publication by the Linguistic Data
Consortium.
</dc:description>
 <dc:description>Comment: 15 pages, minor revisions on Sept. 30, 1996</dc:description>
 <dc:date>1996-09-28</dc:date>
 <dc:date>1996-09-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9609010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Detection of Omissions in Translations</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  ADOMIT is an algorithm for Automatic Detection of OMIssions in Translations.
The algorithm relies solely on geometric analysis of bitext maps and uses no
linguistic information. This property allows it to deal equally well with
omissions that do not correspond to linguistic units, such as might result from
word-processing mishaps. ADOMIT has proven itself by discovering many errors in
a hand-constructed gold standard for evaluating bitext mapping algorithms.
Quantitative evaluation on simulated omissions showed that, even with today's
poor bitext mapping technology, ADOMIT is a valuable quality control tool for
translators and translation bureaus.
</dc:description>
 <dc:description>Comment: 6 pages, minor revisions on Sept. 30, 1996</dc:description>
 <dc:date>1996-09-28</dc:date>
 <dc:date>1996-09-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9609010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Death and Lightness: Using a Demographic Model to Find Support Verbs</dc:title>
 <dc:creator>Dras, Mark</dc:creator>
 <dc:creator>Johnson, Mike</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Some verbs have a particular kind of binary ambiguity: they can carry their
normal, full meaning, or they can be merely acting as a prop for the nominal
object. It has been suggested that there is a detectable pattern in the
relationship between a verb acting as a prop (a \term{support verb}) and the
noun it supports.
  The task this paper undertakes is to develop a model which identifies the
support verb for a particular noun, and by extension, when nouns are
enumerated, a model which disambiguates a verb with respect to its support
status. The paper sets up a basic model as a standard for comparison; it then
proposes a more complex model, and gives some results to support the model's
validity, comparing it with other similar approaches.
</dc:description>
 <dc:description>Comment: LaTeX, 8 pages, uses aclap.sty</dc:description>
 <dc:date>1996-10-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610001</dc:identifier>
 <dc:identifier>CSNLP-96, Sept. 2-4, Dublin, Ireland</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Gathering Statistics to Aspectually Classify Sentences with a Genetic
  Algorithm</dc:title>
 <dc:creator>Siegel, Eric V.</dc:creator>
 <dc:creator>McKeown, Kathleen R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a method for large corpus analysis to semantically
classify an entire clause. In particular, we use cooccurrence statistics among
similar clauses to determine the aspectual class of an input clause. The
process examines linguistic features of clauses that are relevant to aspectual
classification. A genetic algorithm determines what combinations of linguistic
features to use for this task.
</dc:description>
 <dc:description>Comment: postscript, 9 pages, Proceedings of the Second International
  Conference on New Methods in Language Processing, Oflazer and Somers ed.</dc:description>
 <dc:date>1996-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stochastic Attribute-Value Grammars</dc:title>
 <dc:creator>Abney, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Probabilistic analogues of regular and context-free grammars are well-known
in computational linguistics, and currently the subject of intensive research.
To date, however, no satisfactory probabilistic analogue of attribute-value
grammars has been proposed: previous attempts have failed to define a correct
parameter-estimation algorithm.
  In the present paper, I define stochastic attribute-value grammars and give a
correct algorithm for estimating their parameters. The estimation algorithm is
adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model
parameters, it is necessary to compute the expectations of certain functions
under random fields. In the application discussed by Della Pietra, Della
Pietra, and Lafferty (representing English orthographic constraints), Gibbs
sampling can be used to estimate the needed expectations. The fact that
attribute-value grammars generate constrained languages makes Gibbs sampling
inapplicable, but I show how a variant of Gibbs sampling, the
Metropolis-Hastings algorithm, can be used instead.
</dc:description>
 <dc:description>Comment: 23 pages, 21 Postscript figures, uses rotate.sty</dc:description>
 <dc:date>1996-10-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Faster Structured-Tag Word-Classification Method</dc:title>
 <dc:creator>Zhang, Min</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Several methods have been proposed for processing a corpus to induce a tagset
for the sub-language represented by the corpus. This paper examines a
structured-tag word classification method introduced by McMahon (1994) and
discussed further by McMahon &amp; Smith (1995) in cmp-lg/9503011 . Two major
variations, (1) non-random initial assignment of words to classes and (2)
moving multiple words in parallel, together provide robust non-random results
with a speed increase of 200% to 450%, at the cost of slightly lower quality
than McMahon's method's average quality. Two further variations, (3) retaining
information from less- frequent words and (4) avoiding reclustering closed
classes, are proposed for further study.
  Note: The speed increases quoted above are relative to my implementation of
my understanding of McMahon's algorithm; this takes time measured in hours and
days on a home PC. A revised version of the McMahon &amp; Smith (1995) paper has
appeared (June 1996) in Computational Linguistics 22(2):217- 247; this refers
to a time of &quot;several weeks&quot; to cluster 569 words on a Sparc-IPC.
</dc:description>
 <dc:description>Comment: 10 pages, Microsoft Word 6.0, ps</dc:description>
 <dc:date>1996-10-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610004</dc:identifier>
 <dc:identifier>27-Aug-96 PRICAI-96 Workshop on Future Issues for Multi-lingual
  Text Processing, Cairns, Australia. ISBN 0 86857 730 8</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning string edit distance</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:creator>Yianilos, Peter N.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In many applications, it is necessary to determine the similarity of two
strings. A widely-used notion of string similarity is the edit distance: the
minimum number of insertions, deletions, and substitutions required to
transform one string into the other. In this report, we provide a stochastic
model for string edit distance. Our stochastic model allows us to learn a
string edit distance function from a corpus of examples. We illustrate the
utility of our approach by applying it to the difficult problem of learning the
pronunciation of words in conversational speech. In this application, we learn
a string edit distance with one fourth the error rate of the untrained
Levenshtein distance. Our approach is applicable to any string classification
problem that may be solved using a similarity function against a database of
labeled prototypes.
  Keywords: string edit distance, Levenshtein distance, stochastic
transduction, syntactic pattern recognition, prototype dictionary, spelling
correction, string correction, string similarity, string classification, speech
recognition, pronunciation modeling, Switchboard corpus.
</dc:description>
 <dc:description>Comment: http://www.cs.princeton.edu/~ristad/papers/pu-532-96.ps.gz</dc:description>
 <dc:date>1996-10-29</dc:date>
 <dc:date>1997-11-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9610006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Morphology-System and Part-of-Speech Tagger for German</dc:title>
 <dc:creator>Lezius, Wolfgang</dc:creator>
 <dc:creator>Rapp, Reinhard</dc:creator>
 <dc:creator>Wettler, Manfred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents an integrated tool for German morphology and statistical
part-of-speech tagging which aims at making some well established methods
widely available. The software is very user friendly, runs on any PC and can be
downloaded as a complete package (including lexicon and documentation) from the
World Wide Web. Compared with the performance of other tagging systems the
tagger produces similar results.
</dc:description>
 <dc:description>Comment: 10 pages</dc:description>
 <dc:date>1996-10-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9610006</dc:identifier>
 <dc:identifier>In:D.Gibbon,ed.,Natural Language Processing and Speech Technology.
  Results of the 3rd KONVENS Conference. Mouton de Gruyter, Berlin, 1996.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611001</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>OT SIMPLE - a construction-kit approach to Optimality Theory
  implementation</dc:title>
 <dc:creator>Walther, Markus</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper details a simple approach to the implementation of Optimality
Theory (OT, Prince and Smolensky 1993) on a computer, in part reusing standard
system software. In a nutshell, OT's GENerating source is implemented as a
BinProlog program interpreting a context-free specification of a GEN structural
grammar according to a user-supplied input form. The resulting set of textually
flattened candidate tree representations is passed to the CONstraint stage.
Constraints are implemented by finite-state transducers specified as `sed'
stream editor scripts that typically map ill-formed portions of the candidate
to violation marks. EVALuation of candidates reduces to simple sorting: the
violation-mark-annotated output leaving CON is fed into `sort', which orders
candidates on the basis of the violation vector column of each line, thereby
bringing the optimal candidate to the top. This approach gave rise to OT
SIMPLE, the first freely available software tool for the OT framework to
provide generic facilities for both GEN and CONstraint definition. Its
practical applicability is demonstrated by modelling the OT analysis of
apparent subtractive pluralization in Upper Hessian presented in Golston and
Wiese (1996).
</dc:description>
 <dc:description>Comment: 52 pages, uses examples.sty, chicago.sty. IPA phonetic font macro
  package ipa.sty and necessary WSU IPA fonts included. Source code of the
  software described available from
  http://www.phil-fak.uni-duesseldorf.de/~walther/otsimple.html</dc:description>
 <dc:date>1996-11-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Unsupervised Language Acquisition</dc:title>
 <dc:creator>de Marcken, Carl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This thesis presents a computational theory of unsupervised language
acquisition, precisely defining procedures for learning language from ordinary
spoken or written utterances, with no explicit help from a teacher. The theory
is based heavily on concepts borrowed from machine learning and statistical
estimation. In particular, learning takes place by fitting a stochastic,
generative model of language to the evidence. Much of the thesis is devoted to
explaining conditions that must hold for this general learning strategy to
arrive at linguistically desirable grammars. The thesis introduces a variety of
technical innovations, among them a common representation for evidence and
grammars, and a learning strategy that separates the ``content'' of linguistic
parameters from their representation. Algorithms based on it suffer from few of
the search problems that have plagued other computational approaches to
language acquisition.
  The theory has been tested on problems of learning vocabularies and grammars
from unsegmented text and continuous speech, and mappings between sound and
representations of meaning. It performs extremely well on various objective
criteria, acquiring knowledge that causes it to assign almost exactly the same
structure to utterances as humans do. This work has application to data
compression, language modeling, speech recognition, machine translation,
information retrieval, and other tasks that rely on either structural or
stochastic descriptions of language.
</dc:description>
 <dc:description>Comment: PhD thesis, 133 pages</dc:description>
 <dc:date>1996-11-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Data-Oriented Language Processing. An Overview</dc:title>
 <dc:creator>Bod, Rens</dc:creator>
 <dc:creator>Scha, Remko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  During the last few years, a new approach to language processing has started
to emerge, which has become known under various labels such as &quot;data-oriented
parsing&quot;, &quot;corpus-based interpretation&quot;, and &quot;tree-bank grammar&quot; (cf. van den
Berg et al. 1994; Bod 1992-96; Bod et al. 1996a/b; Bonnema 1996; Charniak
1996a/b; Goodman 1996; Kaplan 1996; Rajman 1995a/b; Scha 1990-92; Sekine &amp;
Grishman 1995; Sima'an et al. 1994; Sima'an 1995-96; Tugwell 1995). This
approach, which we will call &quot;data-oriented processing&quot; or &quot;DOP&quot;, embodies the
assumption that human language perception and production works with
representations of concrete past language experiences, rather than with
abstract linguistic rules. The models that instantiate this approach therefore
maintain large corpora of linguistic representations of previously occurring
utterances. When processing a new input utterance, analyses of this utterance
are constructed by combining fragments from the corpus; the
occurrence-frequencies of the fragments are used to estimate which analysis is
the most probable one.
  In this paper we give an in-depth discussion of a data-oriented processing
model which employs a corpus of labelled phrase-structure trees. Then we review
some other models that instantiate the DOP approach. Many of these models also
employ labelled phrase-structure trees, but use different criteria for
extracting fragments from the corpus or employ different disambiguation
strategies (Bod 1996b; Charniak 1996a/b; Goodman 1996; Rajman 1995a/b; Sekine &amp;
Grishman 1995; Sima'an 1995-96); other models use richer formalisms for their
corpus annotations (van den Berg et al. 1994; Bod et al., 1996a/b; Bonnema
1996; Kaplan 1996; Tugwell 1995).
</dc:description>
 <dc:description>Comment: 34 pages, Postscript</dc:description>
 <dc:date>1996-11-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Nonuniform Markov models</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:creator>Thomas, Robert G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A statistical language model assigns probability to strings of arbitrary
length. Unfortunately, it is not possible to gather reliable statistics on
strings of arbitrary length from a finite corpus. Therefore, a statistical
language model must decide that each symbol in a string depends on at most a
small, finite number of other symbols in the string. In this report we propose
a new way to model conditional independence in Markov models. The central
feature of our nonuniform Markov model is that it makes predictions of varying
lengths using contexts of varying lengths. Experiments on the Wall Street
Journal reveal that the nonuniform model performs slightly better than the
classic interpolated Markov model. This result is somewhat remarkable because
both models contain identical numbers of parameters whose values are estimated
in a similar manner. The only difference between the two models is how they
combine the statistics of longer and shorter strings.
  Keywords: nonuniform Markov model, interpolated Markov model, conditional
independence, statistical language model, discrete time series.
</dc:description>
 <dc:description>Comment: 17 pages</dc:description>
 <dc:date>1996-11-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611005</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating HMM-Based Speech Recognition With Direct Manipulation In A
  Multimodal Korean Natural Language Interface</dc:title>
 <dc:creator>Lee, Geunbae</dc:creator>
 <dc:creator>Lee, Jong-Hyeok</dc:creator>
 <dc:creator>Kim, Sangeok</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a HMM-based speech recognition engine and its integration
into direct manipulation interfaces for Korean document editor. Speech
recognition can reduce typical tedious and repetitive actions which are
inevitable in standard GUIs (graphic user interfaces). Our system consists of
general speech recognition engine called ABrain {Auditory Brain} and speech
commandable document editor called SHE {Simple Hearing Editor}. ABrain is a
phoneme-based speech recognition engine which shows up to 97% of discrete
command recognition rate. SHE is a EuroBridge widget-based document editor that
supports speech commands as well as direct manipulation interfaces.
</dc:description>
 <dc:description>Comment: 6 pages, ps file, presented at icmi96 (Bejing)</dc:description>
 <dc:date>1996-11-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9611006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Framework for Natural Language Interfaces to Temporal Databases</dc:title>
 <dc:creator>Androutsopoulos, I.</dc:creator>
 <dc:creator>Ritchie, G. D.</dc:creator>
 <dc:creator>Thanisch, P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Over the past thirty years, there has been considerable progress in the
design of natural language interfaces to databases. Most of this work has
concerned snapshot databases, in which there are only limited facilities for
manipulating time-varying information. The database community is becoming
increasingly interested in temporal databases, databases with special support
for time-dependent entries. We have developed a framework for constructing
natural language interfaces to temporal databases, drawing on research on
temporal phenomena within logic and linguistics. The central part of our
framework is a logic-like formal language, called TOP, which can capture the
semantics of a wide range of English sentences. We have implemented an
HPSG-based sentence analyser that converts a large set of English queries
involving time into TOP formulae, and have formulated a provably correct
procedure for translating TOP expressions into queries in the TSQL2 temporal
database language. In this way we have established a sound route from English
to a general-purpose temporal database language.
</dc:description>
 <dc:description>Comment: 9 pages, uses the included acsc.sty, and the included *.eps figures.
  To appear in the 20th Australasian Computer Science Conference, Sydney,
  February 1997</dc:description>
 <dc:date>1996-11-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9611006</dc:identifier>
 <dc:identifier>Proc. of the 20th Australasian Computer Science Conference,
  Sydney, 1997, pp. 307-315.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Comparative Experiments on Disambiguating Word Senses: An Illustration
  of the Role of Bias in Machine Learning</dc:title>
 <dc:creator>Mooney, Raymond J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.
</dc:description>
 <dc:description>Comment: 10 pages</dc:description>
 <dc:date>1996-12-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9612001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Specialized Language Models using Dialogue Predictions</dc:title>
 <dc:creator>Popovici, Cosmin</dc:creator>
 <dc:creator>Baggia, Paolo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper analyses language modeling in spoken dialogue systems for
accessing a database. The use of several language models obtained by exploiting
dialogue predictions gives better results than the use of a single model for
the whole dialogue interaction. For this reason several models have been
created, each one for a specific system question, such as the request or the
confirmation of a parameter.
  The use of dialogue-dependent language models increases the performance both
at the recognition and at the understanding level, especially on answers to
system requests. Moreover other methods to increase performance, like automatic
clustering of vocabulary words or the use of better acoustic models during
recognition, does not affect the improvements given by dialogue-dependent
language models.
  The system used in our experiments is Dialogos, the Italian spoken dialogue
system used for accessing railway timetable information over the telephone. The
experiments were carried out on a large corpus of dialogues collected using
Dialogos.
</dc:description>
 <dc:description>Comment: 4 pages, LaTeX, 2 eps figures, uses icassp.sty, a4.sty and psfig.tex;
  to appear in Proc. of ICASSP 1997, Munich, Germany</dc:description>
 <dc:date>1996-12-12</dc:date>
 <dc:date>1996-12-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9612002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Metrics for Evaluating Dialogue Strategies in a Spoken Language System</dc:title>
 <dc:creator>Danieli, Morena</dc:creator>
 <dc:creator>Gerbino, Elisabetta</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe a set of metrics for the evaluation of different
dialogue management strategies in an implemented real-time spoken language
system. The set of metrics we propose offers useful insights in evaluating how
particular choices in the dialogue management can affect the overall quality of
the man-machine dialogue. The evaluation makes use of established metrics: the
transaction success, the contextual appropriateness of system answers, the
calculation of normal and correction turns in a dialogue. We also define a new
metric, the implicit recovery, which allows to measure the ability of a
dialogue manager to deal with errors by different levels of analysis. We report
evaluation data from several experiments, and we compare two different
approaches to dialogue repair strategies using the set of metrics we argue for.
</dc:description>
 <dc:description>Comment: 6 pages, LaTex, uses aaai.sty; presented at the 1995 AAAI Spring
  Symposium Series (Symposium: Empirical Methods in Discourse Interpretation
  and Generation)</dc:description>
 <dc:date>1996-12-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9612003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Dialogos: a Robust System for Human-Machine Spoken Dialogue on the
  Telephone</dc:title>
 <dc:creator>Albesano, Dario</dc:creator>
 <dc:creator>Baggia, Paolo</dc:creator>
 <dc:creator>Danieli, Morena</dc:creator>
 <dc:creator>Gemello, Roberto</dc:creator>
 <dc:creator>Gerbino, Elisabetta</dc:creator>
 <dc:creator>Rullent, Claudio</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents Dialogos, a real-time system for human-machine spoken
dialogue on the telephone in task-oriented domains. The system has been tested
in a large trial with inexperienced users and it has proved robust enough to
allow spontaneous interactions both to users which get good recognition
performance and to the ones which get lower scores. The robust behavior of the
system has been achieved by combining the use of specific language models
during the recognition phase of analysis, the tolerance toward spontaneous
speech phenomena, the activity of a robust parser, and the use of
pragmatic-based dialogue knowledge. This integration of the different modules
allows to deal with partial or total breakdowns of the different levels of
analysis. We report the field trial data of the system and the evaluation
results of the overall system and of the submodules.
</dc:description>
 <dc:description>Comment: 4 pages, LaTeX, 1 eps figures, uses icassp91.sty, and psfig.tex; to
  appear in Proc. of ICASSP 1997, Munich, Germany</dc:description>
 <dc:date>1996-12-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9612004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9612005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Maximum Entropy Modeling Toolkit</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The Maximum Entropy Modeling Toolkit supports parameter estimation and
prediction for statistical language models in the maximum entropy framework.
The maximum entropy framework provides a constructive method for obtaining the
unique conditional distribution p*(y|x) that satisfies a set of linear
constraints and maximizes the conditional entropy H(p|f) with respect to the
empirical distribution f(x). The maximum entropy distribution p*(y|x) also has
a unique parametric representation in the class of exponential models, as
m(y|x) = r(y|x)/Z(x) where the numerator m(y|x) = prod_i alpha_i^g_i(x,y) is a
product of exponential weights, with alpha_i = exp(lambda_i), and the
denominator Z(x) = sum_y r(y|x) is required to satisfy the axioms of
probability.
  This manual explains how to build maximum entropy models for discrete domains
with the Maximum Entropy Modeling Toolkit (MEMT). First we summarize the steps
necessary to implement a language model using the toolkit. Next we discuss the
executables provided by the toolkit and explain the file formats required by
the toolkit. Finally, we review the maximum entropy framework and apply it to
the problem of statistical language modeling.
  Keywords: statistical language models, maximum entropy, exponential models,
improved iterative scaling, Markov models, triggers.
</dc:description>
 <dc:description>Comment: 32 pages, texinfo format</dc:description>
 <dc:date>1996-12-31</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9612005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Exploiting Context to Identify Lexical Atoms -- A Statistical View of
  Linguistic Context</dc:title>
 <dc:creator>Zhai, Chengxiang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Interpretation of natural language is inherently context-sensitive. Most
words in natural language are ambiguous and their meanings are heavily
dependent on the linguistic context in which they are used. The study of
lexical semantics can not be separated from the notion of context. This paper
takes a contextual approach to lexical semantics and studies the linguistic
context of lexical atoms, or &quot;sticky&quot; phrases such as &quot;hot dog&quot;. Since such
lexical atoms may occur frequently in unrestricted natural language text,
recognizing them is crucial for understanding naturally-occurring text. The
paper proposes several heuristic approaches to exploiting the linguistic
context to identify lexical atoms from arbitrary natural language text.
</dc:description>
 <dc:description>Comment: 10 pages, postscript file, to appear in Proceedings of International
  and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97)</dc:description>
 <dc:date>1997-01-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9701001</dc:identifier>
 <dc:identifier>Proceedings of the International and Interdisciplinary Conference
  on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brzil, Feb. 4-6,
  1997. 119-129.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Hybrid language processing in the Spoken Language Translator</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper presents an overview of the Spoken Language Translator (SLT)
system's hybrid language-processing architecture, focussing on the way in which
rule-based and statistical methods are combined to achieve robust and efficient
performance within a linguistically motivated framework. In general, we argue
that rules are desirable in order to encode domain-independent linguistic
constraints and achieve high-quality grammatical output, while corpus-derived
statistics are needed if systems are to be efficient and robust; further, that
hybrid architectures are superior from the point of view of portability to
architectures which only make use of one type of information. We address the
topics of ``multi-engine'' strategies for robust translation; robust bottom-up
parsing using pruning and grammar specialization; rational development of
linguistic rule-sets using balanced domain corpora; and efficient supervised
training by interactive disambiguation. All work described is fully implemented
in the current version of the SLT-2 system.
</dc:description>
 <dc:description>Comment: 4 pages, uses icassp97.sty; to appear in ICASSP-97; see
  http://www.cam.sri.com for related material</dc:description>
 <dc:date>1997-01-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9701002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Information-Sharing Subdialogues in Expert-User Consultation</dc:title>
 <dc:creator>Chu-Carroll, Jennifer</dc:creator>
 <dc:creator>Carberry, Sandra</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In expert-consultation dialogues, it is inevitable that an agent will at
times have insufficient information to determine whether to accept or reject a
proposal by the other agent. This results in the need for the agent to initiate
an information-sharing subdialogue to form a set of shared beliefs within which
the agents can effectively re-evaluate the proposal. This paper presents a
computational strategy for initiating such information-sharing subdialogues to
resolve the system's uncertainty regarding the acceptance of a user proposal.
Our model determines when information-sharing should be pursued, selects a
focus of information-sharing among multiple uncertain beliefs, chooses the most
effective information-sharing strategy, and utilizes the newly obtained
information to re-evaluate the user proposal. Furthermore, our model is capable
of handling embedded information-sharing subdialogues.
</dc:description>
 <dc:description>Comment: 9 pages, 1 figure; uses epsf.sty, times.sty, and named</dc:description>
 <dc:date>1997-01-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9701003</dc:identifier>
 <dc:identifier>IJCAI'95</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9701004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Implementation of the Head-Corner Parser</dc:title>
 <dc:creator>van Noord, Gertjan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an efficient and robust implementation of a
bi-directional, head-driven parser for constraint-based grammars. This parser
is developed for the OVIS system: a Dutch spoken dialogue system in which
information about public transport can be obtained by telephone.
  After a review of the motivation for head-driven parsing strategies, and
head-corner parsing in particular, a non-deterministic version of the
head-corner parser is presented. A memoization technique is applied to obtain a
fast parser. A goal-weakening technique is introduced which greatly improves
average case efficiency, both in terms of speed and space requirements.
  I argue in favor of such a memoization strategy with goal-weakening in
comparison with ordinary chart-parsers because such a strategy can be applied
selectively and therefore enormously reduces the space requirements of the
parser, while no practical loss in time-efficiency is observed. On the
contrary, experiments are described in which head-corner and left-corner
parsers implemented with selective memoization and goal weakening outperform
`standard' chart parsers. The experiments include the grammar of the OVIS
system and the Alvey NL Tools grammar.
  Head-corner parsing is a mix of bottom-up and top-down processing. Certain
approaches towards robust parsing require purely bottom-up processing.
Therefore, it seems that head-corner parsing is unsuitable for such robust
parsing techniques. However, it is shown how underspecification (which arises
very naturally in a logic programming environment) can be used in the
head-corner parser to allow such robust parsing techniques. A particular robust
parsing model is described which is implemented in OVIS.
</dc:description>
 <dc:description>Comment: 31 pages, uses cl.sty</dc:description>
 <dc:date>1997-01-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9701004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702001</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis
  Using Artificial Neural Networks</dc:title>
 <dc:creator>Wermter, Stefan</dc:creator>
 <dc:creator>Weber, Volker</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe a so-called screening approach for learning robust
processing of spontaneously spoken language. A screening approach is a flat
analysis which uses shallow sequences of category representations for analyzing
an utterance at various syntactic, semantic and dialog levels. Rather than
using a deeply structured symbolic analysis, we use a flat connectionist
analysis. This screening approach aims at supporting speech and language
processing by using (1) data-driven learning and (2) robustness of
connectionist networks. In order to test this approach, we have developed the
SCREEN system which is based on this new robust, learned and flat analysis.
  In this paper, we focus on a detailed description of SCREEN's architecture,
the flat syntactic and semantic analysis, the interaction with a speech
recognizer, and a detailed evaluation analysis of the robustness under the
influence of noisy or incomplete input. The main result of this paper is that
flat representations allow more robust processing of spontaneous spoken
language than deeply structured representations. In particular, we show how the
fault-tolerance and learning capability of connectionist networks can support a
flat analysis for providing more robust spoken-language processing within an
overall hybrid symbolic/connectionist framework.
</dc:description>
 <dc:description>Comment: 51 pages, Postscript. To be published in Journal of Artificial
  Intelligence Research 6(1), 1997</dc:description>
 <dc:date>1997-02-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Extraction of Subcategorization from Corpora</dc:title>
 <dc:creator>Briscoe, Ted</dc:creator>
 <dc:creator>Carroll, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a novel technique and implemented system for constructing a
subcategorization dictionary from textual corpora. Each dictionary entry
encodes the relative frequency of occurrence of a comprehensive set of
subcategorization classes for English. An initial experiment, on a sample of 14
verbs which exhibit multiple complementation patterns, demonstrates that the
technique achieves accuracy comparable to previous approaches, which are all
limited to a highly restricted set of subcategorization classes. We also
demonstrate that a subcategorization dictionary built with the system improves
the accuracy of a parser by an appreciable amount.
</dc:description>
 <dc:description>Comment: 8 pages; requires aclap.sty. To appear in ANLP-97</dc:description>
 <dc:date>1997-02-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702003</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Robust Text Processing Technique Applied to Lexical Error Recovery</dc:title>
 <dc:creator>Ingels, Peter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This thesis addresses automatic lexical error recovery and tokenization of
corrupt text input. We propose a technique that can automatically correct
misspellings, segmentation errors and real-word errors in a unified framework
that uses both a model of language production and a model of the typing
behavior, and which makes tokenization part of the recovery process.
  The typing process is modeled as a noisy channel where Hidden Markov Models
are used to model the channel characteristics. Weak statistical language models
are used to predict what sentences are likely to be transmitted through the
channel. These components are held together in the Token Passing framework
which provides the desired tight coupling between orthographic pattern matching
and linguistic expectation.
  The system, CTR (Connected Text Recognition), has been tested on two corpora
derived from two different applications, a natural language dialogue system and
a transcription typing scenario. Experiments show that CTR can automatically
correct a considerable portion of the errors in the test sets without
introducing too much noise. The segmentation error correction rate is virtually
faultless.
</dc:description>
 <dc:description>Comment: Licentiate Thesis, 101 pages</dc:description>
 <dc:date>1997-02-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Annotation Scheme for Free Word Order Languages</dc:title>
 <dc:creator>Skut, Wojciech</dc:creator>
 <dc:creator>Krenn, Brigitte</dc:creator>
 <dc:creator>Brants, Thorsten</dc:creator>
 <dc:creator>Uszkoreit, Hans</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe an annotation scheme and a tool developed for creating
linguistically annotated corpora for non-configurational languages. Since the
requirements for such a formalism differ from those posited for configurational
languages, several features have been added, influencing the architecture of
the scheme. The resulting scheme reflects a stratificational notion of
language, and makes only minimal assumptions about the interrelation of the
particular representational strata.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX; uses aclap.sty, epsf.sty, and gb4e.sty</dc:description>
 <dc:date>1997-02-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Software Infrastructure for Natural Language Processing</dc:title>
 <dc:creator>Cunningham, Hamish</dc:creator>
 <dc:creator>Humphreys, Kevin</dc:creator>
 <dc:creator>Gaizauskas, Robert</dc:creator>
 <dc:creator>Wilks, Yorick</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We classify and review current approaches to software infrastructure for
research, development and delivery of NLP systems. The task is motivated by a
discussion of current trends in the field of NLP and Language Engineering. We
describe a system called GATE (a General Architecture for Text Engineering)
that provides a software infrastructure on top of which heterogeneous NLP
processing modules may be evaluated and refined individually, or may be
combined into larger application systems. GATE aims to support both researchers
and developers working on component technologies (e.g. parsing, tagging,
morphological analysis) and those working on developing end-user applications
(e.g. information extraction, text summarisation, document generation, machine
translation, and second language learning). GATE promotes reuse of component
technology, permits specialisation and collaboration in large-scale projects,
and allows for the comparison and evaluation of alternative technologies. The
first release of GATE is now available - see
http://www.dcs.shef.ac.uk/research/groups/nlp/gate/
</dc:description>
 <dc:description>Comment: LaTeX, uses aclap.sty, 8 pages</dc:description>
 <dc:date>1997-02-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702005</dc:identifier>
 <dc:identifier>5th Conference on Applied Natural Language Processing, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Information Extraction - A User Guide</dc:title>
 <dc:creator>Cunningham, Hamish</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This technical memo describes Information Extraction from the point-of-view
of a potential user of the technology. No knowledge of language processing is
assumed. Information Extraction is a process which takes unseen texts as input
and produces fixed-format, unambiguous data as output. This data may be used
directly for display to users, or may be stored in a database or spreadsheet
for later analysis, or may be used for indexing purposes in Information
Retrieval applications. See also http://www.dcs.shef.ac.uk/~hamish
</dc:description>
 <dc:description>Comment: LaTeX2e with PostScript figures, 17 pages (figures replaced with
  smaller versions)</dc:description>
 <dc:date>1997-02-10</dc:date>
 <dc:date>1997-02-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural Language Dialogue Service for Appointment Scheduling Agents</dc:title>
 <dc:creator>Busemann, Stephan</dc:creator>
 <dc:creator>Declerck, Thierry</dc:creator>
 <dc:creator>Diagne, Abdel Kader</dc:creator>
 <dc:creator>Dini, Luca</dc:creator>
 <dc:creator>Klein, Judith</dc:creator>
 <dc:creator>Schmeier, Sven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Appointment scheduling is a problem faced daily by many individuals and
organizations. Cooperating agent systems have been developed to partially
automate this task. In order to extend the circle of participants as far as
possible we advocate the use of natural language transmitted by e-mail. We
describe COSMA, a fully implemented German language server for existing
appointment scheduling agent systems. COSMA can cope with multiple dialogues in
parallel, and accounts for differences in dialogue behaviour between human and
machine agents. NL coverage of the sublanguage is achieved through both
corpus-based grammar development and the use of message extraction techniques.
</dc:description>
 <dc:description>Comment: 8 or 9 pages, LaTeX; uses aclap.sty, epsf.tex</dc:description>
 <dc:date>1997-02-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702007</dc:identifier>
 <dc:identifier>Proc. 5th Conference on Applied Natural Language Processing, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Sequential Model Selection for Word Sense Disambiguation</dc:title>
 <dc:creator>Pedersen, Ted</dc:creator>
 <dc:creator>Bruce, Rebecca</dc:creator>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Statistical models of word-sense disambiguation are often based on a small
number of contextual features or on a model that is assumed to characterize the
interactions among a set of features. Model selection is presented as an
alternative to these approaches, where a sequential search of possible models
is conducted in order to find the model that best characterizes the
interactions among features. This paper expands existing model selection
methodology and presents the first comparative study of model selection search
strategies and evaluation criteria when applied to the problem of building
probabilistic classifiers for word-sense disambiguation.
</dc:description>
 <dc:description>Comment: 8 pages, Latex, uses aclap.sty</dc:description>
 <dc:date>1997-02-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702008</dc:identifier>
 <dc:identifier>Proceedings of the Fifth Conference on Applied Natural Language
  Processing, April 1997, Washington, DC</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast Statistical Parsing of Noun Phrases for Document Indexing</dc:title>
 <dc:creator>Zhai, Chengxiang</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Information Retrieval (IR) is an important application area of Natural
Language Processing (NLP) where one encounters the genuine challenge of
processing large quantities of unrestricted natural language text. While much
effort has been made to apply NLP techniques to IR, very few NLP techniques
have been evaluated on a document collection larger than several megabytes.
Many NLP techniques are simply not efficient enough, and not robust enough, to
handle a large amount of text. This paper proposes a new probabilistic model
for noun phrase parsing, and reports on the application of such a parsing
technique to enhance document indexing. The effectiveness of using syntactic
phrases provided by the parser to supplement single words for indexing is
evaluated with a 250 megabytes document collection. The experiment's results
show that supplementing single words with syntactic phrases for indexing
consistently and significantly improves retrieval performance.
</dc:description>
 <dc:description>Comment: 8 pages, LaTex; uses aclap.sty. To appear in Proceedings of the 5th
  Conference on Applied Natural Language Processing, Washington DC, 31 March -
  3 April, 1997.</dc:description>
 <dc:date>1997-02-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Selective Sampling of Effective Example Sentence Sets for Word Sense
  Disambiguation</dc:title>
 <dc:creator>Fujii, Atsushi</dc:creator>
 <dc:creator>Inui, Kentaro</dc:creator>
 <dc:creator>Tokunaga, Takenobu</dc:creator>
 <dc:creator>Tanaka, Hozumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper proposes an efficient example selection method for example-based
word sense disambiguation systems. To construct a practical size database, a
considerable overhead for manual sense disambiguation is required. Our method
is characterized by the reliance on the notion of the training utility: the
degree to which each example is informative for future example selection when
used for the training of the system. The system progressively collects examples
by selecting those with greatest utility. The paper reports the effectivity of
our method through experiments on about one thousand sentences. Compared to
experiments with random example selection, our method reduced the overhead
without the degeneration of the performance of the system.
</dc:description>
 <dc:description>Comment: 14 pages, uses epsbox.sty</dc:description>
 <dc:date>1997-02-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702010</dc:identifier>
 <dc:identifier>Proceedings of the Fourth Workshop on Very Large Corpora WVLC-4,
  pp. 56-69, 1996</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>How much has information technology contributed to linguistics?</dc:title>
 <dc:creator>Jones, Karen Sparck</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Information technology should have much to offer linguistics, not only
through the opportunities offered by large-scale data analysis and the stimulus
to develop formal computational models, but through the chance to use language
in systems for automatic natural language processing. The paper discusses these
possibilities in detail, and then examines the actual work that has been done.
It is evident that this has so far been primarily research within a new field,
computational linguistics, which is largely motivated by the demands, and
interest, of practical processing systems, and that information technology has
had rather little influence on linguistics at large. There are different
reasons for this, and not all good ones: information technology deserves more
attention from linguists.
</dc:description>
 <dc:description>Comment: Prepared for a British Academy Symposium on Information Technology
  and Scholarly Disciplines</dc:description>
 <dc:date>1997-02-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Design and Implementation of a Computational Lexicon for Turkish</dc:title>
 <dc:creator>Yorulmaz, Abdullah Kurtulus</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  All natural language processing systems (such as parsers, generators,
taggers) need to have access to a lexicon about the words in the language. This
thesis presents a lexicon architecture for natural language processing in
Turkish. Given a query form consisting of a surface form and other features
acting as restrictions, the lexicon produces feature structures containing
morphosyntactic, syntactic, and semantic information for all possible
interpretations of the surface form satisfying those restrictions. The lexicon
is based on contemporary approaches like feature-based representation,
inheritance, and unification. It makes use of two information sources: a
morphological processor and a lexical database containing all the open and
closed-class words of Turkish. The system has been implemented in SICStus
Prolog as a standalone module for use in natural language processing
applications.
</dc:description>
 <dc:description>Comment: M.Sc. Thesis submitted to the Department of Computer Engineering and
  Information Science, Bilkent University, Ankara, Turkey. 138 pages (including
  title pages). LaTeX. 8 figures. Uses avm.sty, lingmacros.sty, buthesis.sty,
  QobiTree.tex, psfig.tex. Also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1997/BU-CEIS-9701.ps.z</dc:description>
 <dc:date>1997-02-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Knowledge Acquisition for Content Selection</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:creator>Cawsey, Alison</dc:creator>
 <dc:creator>Osman, Liesl</dc:creator>
 <dc:creator>Roff, Yvonne</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  An important part of building a natural-language generation (NLG) system is
knowledge acquisition, that is deciding on the specific schemas, plans, grammar
rules, and so forth that should be used in the NLG system. We discuss some
experiments we have performed with KA for content-selection rules, in the
context of building an NLG system which generates health-related material.
These experiments suggest that it is useful to supplement corpus analysis with
KA techniques developed for building expert systems, such as structured group
discussions and think-aloud protocols. They also raise the point that KA issues
may influence architectural design issues, in particular the decision on
whether a planning approach is used for content selection. We suspect that in
some cases, KA may be easier if other constructive expert-system techniques
(such as production rules, or case-based reasoning) are used to determine the
content of a generated text.
</dc:description>
 <dc:description>Comment: To appear in the 1997 European NLG workshop. 10 pages, postscript</dc:description>
 <dc:date>1997-02-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Building a Generation Knowledge Source using Internet-Accessible
  Newswire</dc:title>
 <dc:creator>Radev, Dragomir R.</dc:creator>
 <dc:creator>McKeown, Kathleen R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we describe a method for automatic creation of a knowledge
source for text generation using information extraction over the Internet. We
present a prototype system called PROFILE which uses a client-server
architecture to extract noun-phrase descriptions of entities such as people,
places, and organizations. The system serves two purposes: as an information
extraction tool, it allows users to search for textual descriptions of
entities; as a utility to generate functional descriptions (FD), it is used in
a functional-unification based generation system. We present an evaluation of
the approach and its applications to natural language generation and
summarization.
</dc:description>
 <dc:description>Comment: 8 pages, uses epsf</dc:description>
 <dc:date>1997-02-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702014</dc:identifier>
 <dc:identifier>To appear in Proceedings of the 5th Conference on Applied Natural
  Processing, Washington DC, 31 March - 3 April, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Improvising Linguistic Style: Social and Affective Bases for Agent
  Personality</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:creator>Cahn, Janet E.</dc:creator>
 <dc:creator>Whittaker, Stephen J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces Linguistic Style Improvisation, a theory and set of
algorithms for improvisation of spoken utterances by artificial agents, with
applications to interactive story and dialogue systems. We argue that
linguistic style is a key aspect of character, and show how speech act
representations common in AI can provide abstract representations from which
computer characters can improvise. We show that the mechanisms proposed
introduce the possibility of socially oriented agents, meet the requirements
that lifelike characters be believable, and satisfy particular criteria for
improvisation proposed by Hayes-Roth.
</dc:description>
 <dc:description>Comment: 10 pages, uses aaai.sty, lingmacros.sty, psfig.sty</dc:description>
 <dc:date>1997-02-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702015</dc:identifier>
 <dc:identifier>Proceedings of the First International Conference on Autonomous
  Agents, Marina del Rey, California, USA. 1997. pp 96-105</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9702016</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Instructions for Temporal Annotation of Scheduling Dialogs</dc:title>
 <dc:creator>O'Hara, Tom</dc:creator>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:creator>Payne, Karen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Human annotation of natural language facilitates standardized evaluation of
natural language processing systems and supports automated feature extraction.
This document consists of instructions for annotating the temporal information
in scheduling dialogs, dialogs in which the participants schedule a meeting
with one another. Task-oriented dialogs, such as these are, would arise in many
useful applications, for instance, automated information providers and
automated phone operators. Explicit instructions support good inter-rater
reliability and serve as documentation for the classes being annotated.
</dc:description>
 <dc:description>Comment: 14 pages</dc:description>
 <dc:date>1997-02-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9702016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Domain Adaptation with Clustered Language Models</dc:title>
 <dc:creator>Ueberla, Joerg P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, a method of domain adaptation for clustered language models is
developed. It is based on a previously developed clustering algorithm, but with
a modified optimisation criterion. The results are shown to be slightly
superior to the previously published 'Fillup' method, which can be used to
adapt standard n-gram models. However, the improvement both methods give
compared to models built from scratch on the adaptation data is quite small
(less than 11% relative improvement in word error rate). This suggests that
both methods are still unsatisfactory from a practical point of view.
</dc:description>
 <dc:description>Comment: preprint - to appear in ICASSP 97</dc:description>
 <dc:date>1997-03-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9703001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703002</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Concept Clustering and Knowledge Integration from a Children's
  Dictionary</dc:title>
 <dc:creator>Barriere, Caroline</dc:creator>
 <dc:creator>Popowich, Fred</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Knowledge structures called Concept Clustering Knowledge Graphs (CCKGs) are
introduced along with a process for their construction from a machine readable
dictionary. CCKGs contain multiple concepts interrelated through multiple
semantic relations together forming a semantic cluster represented by a
conceptual graph. The knowledge acquisition is performed on a children's first
dictionary. A collection of conceptual clusters together can form the basis of
a lexical knowledge base, where each CCKG contains a limited number of highly
connected words giving useful information about a particular domain or
situation.
</dc:description>
 <dc:description>Comment: uses colap.sty</dc:description>
 <dc:date>1997-03-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9703002</dc:identifier>
 <dc:identifier>COLING'96</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Semantics-based Communication System for Dysphasic Subjects</dc:title>
 <dc:creator>Vaillant, Pascal</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Dysphasic subjects do not have complete linguistic abilities and only produce
a weakly structured, topicalized language. They are offered artificial symbolic
languages to help them communicate in a way more adapted to their linguistic
abilities. After a structural analysis of a corpus of utterances from children
with cerebral palsy, we define a semantic lexicon for such a symbolic language.
We use it as the basis of a semantic analysis process able to retrieve an
interpretation of the utterances. This semantic analyser is currently used in
an application designed to convert iconic languages into natural language; it
might find other uses in the field of language rehabilitation.
</dc:description>
 <dc:description>Comment: LaTeX 2.09, 12 pages, 2 Encapsulated Postscript figures, uses
  llncs.sty and epsf.sty. To appear in Proceedings of the 6th conference on
  Artificial Intelligence in Medicine Europe (AIME'97), march 1997, Grenoble
  (France)</dc:description>
 <dc:date>1997-03-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9703003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Insights into the Dialogue Processing of VERBMOBIL</dc:title>
 <dc:creator>Alexandersson, Jan</dc:creator>
 <dc:creator>Reithinger, Norbert</dc:creator>
 <dc:creator>Maier, Elisabeth</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present the dialogue module of the speech-to-speech translation system
VERBMOBIL. We follow the approach that the solution to dialogue processing in a
mediating scenario can not depend on a single constrained processing tool, but
on a combination of several simple, efficient, and robust components. We show
how our solution to dialogue processing works when applied to real data, and
give some examples where our module contributes to the correct translation from
German to English.
</dc:description>
 <dc:description>Comment: Latex, 8 pages, includes 4 postscript figures; appears in the
  proceedings of ANLP-97 Washington D.C</dc:description>
 <dc:date>1997-03-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9703004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9703005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semi-Automatic Acquisition of Domain-Specific Translation Lexicons</dc:title>
 <dc:creator>Resnik, Philip</dc:creator>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We investigate the utility of an algorithm for translation lexicon
acquisition (SABLE), used previously on a very large corpus to acquire general
translation lexicons, when that algorithm is applied to a much smaller corpus
to produce candidates for domain-specific translation lexicons.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1997-03-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9703005</dc:identifier>
 <dc:identifier>Proceedings of the 5th ANLP Conference, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluating Multilingual Gisting of Web Pages</dc:title>
 <dc:creator>Resnik, Philip</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a prototype system for multilingual gisting of Web pages, and
present an evaluation methodology based on the notion of gisting as decision
support. This evaluation paradigm is straightforward, rigorous, permits fair
comparison of alternative approaches, and should easily generalize to
evaluation in other situations where the user is faced with decision-making on
the basis of information in restricted or alternative form.
</dc:description>
 <dc:description>Comment: 7 pages, uses psfig and aaai styles</dc:description>
 <dc:date>1997-04-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Maximum Entropy Approach to Identifying Sentence Boundaries</dc:title>
 <dc:creator>Reynar, Jeffrey C.</dc:creator>
 <dc:creator>Ratnaparkhi, Adwait</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a trainable model for identifying sentence boundaries in raw text.
Given a corpus annotated with sentence boundaries, our model learns to classify
each occurrence of ., ?, and ! as either a valid or invalid sentence boundary.
The training procedure requires no hand-crafted rules, lexica, part-of-speech
tags, or domain-specific information. The model can therefore be trained easily
on any genre of English, and should be trainable on any other Roman-alphabet
language. Performance is comparable to or better than the performance of
similar systems, but we emphasize the simplicity of retraining for new domains.
</dc:description>
 <dc:description>Comment: 4 pages, uses aclap.sty and covingtn.sty</dc:description>
 <dc:date>1997-04-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704002</dc:identifier>
 <dc:identifier>Proceedings of the 5th ANLP Conference, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704003</identifier>
 <datestamp>2009-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Machine Transliteration</dc:title>
 <dc:creator>Knight, Kevin</dc:creator>
 <dc:creator>Graehl, Jonathan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  It is challenging to translate names and technical terms across languages
with different alphabets and sound inventories. These items are commonly
transliterated, i.e., replaced with approximate phonetic equivalents. For
example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.
Translating such items from Japanese back to English is even more challenging,
and of practical interest, as transliterated items make up the bulk of text
phrases not found in bilingual dictionaries. We describe and evaluate a method
for performing backwards transliterations by machine. This method uses a
generative model, incorporating several distinct stages in the transliteration
process.
</dc:description>
 <dc:description>Comment: 8 pages, postscript, to appear, ACL-97/EACL-97</dc:description>
 <dc:date>1997-04-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:creator>Litman, Diane J.</dc:creator>
 <dc:creator>Kamm, Candace A.</dc:creator>
 <dc:creator>Abella, Alicia</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a
general framework for evaluating spoken dialogue agents. The framework
decouples task requirements from an agent's dialogue behaviors, supports
comparisons among dialogue strategies, enables the calculation of performance
over subdialogues and whole dialogues, specifies the relative contribution of
various factors to performance, and makes it possible to compare agents
performing different tasks by normalizing for task complexity.
</dc:description>
 <dc:description>Comment: 10 pages, uses aclap, psfig, lingmacros, times</dc:description>
 <dc:date>1997-04-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704004</dc:identifier>
 <dc:identifier>Proceedings of the 35th Annual Meeting of the Association for
  Computational Linguistics</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tracking Initiative in Collaborative Dialogue Interactions</dc:title>
 <dc:creator>Chu-Carroll, Jennifer</dc:creator>
 <dc:creator>Brown, Michael K.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we argue for the need to distinguish between task and dialogue
initiatives, and present a model for tracking shifts in both types of
initiatives in dialogue interactions. Our model predicts the initiative holders
in the next dialogue turn based on the current initiative holders and the
effect that observed cues have on changing them. Our evaluation across various
corpora shows that the use of cues consistently improves the accuracy in the
system's prediction of task and dialogue initiative holders by 2-4 and 8-13
percentage points, respectively, thus illustrating the generality of our model.
</dc:description>
 <dc:description>Comment: 9 pages, uses psfig, and times</dc:description>
 <dc:date>1997-04-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704005</dc:identifier>
 <dc:identifier>ACL-97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Representing Constraints with Automata</dc:title>
 <dc:creator>Morawietz, Frank</dc:creator>
 <dc:creator>Cornell, Tom</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe an approach to constraint-based syntactic theories
in terms of finite tree automata. The solutions to constraints expressed in
weak monadic second order (MSO) logic are represented by tree automata
recognizing the assignments which make the formulas true. We show that this
allows an efficient representation of knowledge about the content of
constraints which can be used as a practical tool for grammatical theory
verification. We achieve this by using the intertranslatability of formulas of
MSO logic and tree automata and the embedding of MSO logic into a constraint
logic programming scheme. The usefulness of the approach is discussed with
examples from the realm of Principles-and-Parameters based parsing.
</dc:description>
 <dc:description>Comment: 8 pages; uses aclap.sty, harvard.sty and dcu.bst. Corrections to
  three examples</dc:description>
 <dc:date>1997-04-21</dc:date>
 <dc:date>1997-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Unsupervised Lexical Knowledge Methods for Word Sense
  Disambiguation</dc:title>
 <dc:creator>Rigau, German</dc:creator>
 <dc:creator>Atserias, Jordi</dc:creator>
 <dc:creator>Agirre, Eneko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a method to combine a set of unsupervised algorithms that
can accurately disambiguate word senses in a large, completely untagged corpus.
Although most of the techniques for word sense resolution have been presented
as stand-alone, it is our belief that full-fledged lexical ambiguity resolution
should combine several information sources and techniques. The set of
techniques have been applied in a combined way to disambiguate the genus terms
of two machine-readable dictionaries (MRD), enabling us to construct complete
taxonomies for Spanish and French. Tested accuracy is above 80% overall and 95%
for two-way ambiguous genus terms, showing that taxonomy building is not
limited to structured dictionaries such as LDOCE.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty</dc:description>
 <dc:date>1997-04-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704007</dc:identifier>
 <dc:identifier>Proceedings of ACL'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intonational Boundaries, Speech Repairs and Discourse Markers: Modeling
  Spoken Dialog</dc:title>
 <dc:creator>Heeman, Peter A.</dc:creator>
 <dc:creator>Allen, James F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  To understand a speaker's turn of a conversation, one needs to segment it
into intonational phrases, clean up any speech repairs that might have
occurred, and identify discourse markers. In this paper, we argue that these
problems must be resolved together, and that they must be resolved early in the
processing stream. We put forward a statistical language model that resolves
these problems, does POS tagging, and can be used as the language model of a
speech recognizer. We find that by accounting for the interactions between
these tasks that the performance on each task improves, as does POS tagging and
perplexity.
</dc:description>
 <dc:description>Comment: 8 pages, 3 postscript figures</dc:description>
 <dc:date>1997-04-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704008</dc:identifier>
 <dc:identifier>In proceedings of ACL/EACL'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Developing a hybrid NP parser</dc:title>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:creator>Padro, Lluis</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe the use of energy function optimization in very shallow syntactic
parsing. The approach can use linguistic rules and corpus-based statistics, so
the strengths of both linguistic and statistical approaches to NLP can be
combined in a single framework. The rules are contextual constraints for
resolving syntactic ambiguities expressed as alternative tags, and the
statistical language model consists of corpus-based n-grams of syntactic tags.
The success of the hybrid syntactic disambiguator is evaluated against a
held-out benchmark corpus. Also the contributions of the linguistic and
statistical language models to the hybrid model are estimated.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty, epsf.sty</dc:description>
 <dc:date>1997-04-23</dc:date>
 <dc:date>1997-04-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704009</dc:identifier>
 <dc:identifier>Proceedings of 5th ANLP, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Theoretical Status of Ontologies in Natural Language Processing</dc:title>
 <dc:creator>Bateman, John A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper discusses the use of `ontologies' in Natural Language Processing.
It classifies various kinds of ontologies that have been employed in NLP and
discusses various benefits and problems with those designs. Particular focus is
then placed on experiences gained in the use of the Upper Model, a
linguistically-motivated `ontology' originally designed for use with the Penman
text generation system. Some proposals for further NLP ontology design criteria
are then made.
</dc:description>
 <dc:description>Comment: 43pp, uses: twocolumn,named,a4wide,psfig, 7eps figs</dc:description>
 <dc:date>1997-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704011</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Morphological Disambiguation by Voting Constraints</dc:title>
 <dc:creator>Oflazer, Kemal</dc:creator>
 <dc:creator>Tur, Gokhan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a constraint-based morphological disambiguation system in which
individual constraints vote on matching morphological parses, and
disambiguation of all the tokens in a sentence is performed at the end by
selecting parses that receive the highest votes. This constraint application
paradigm makes the outcome of the disambiguation independent of the rule
sequence, and hence relieves the rule developer from worrying about potentially
conflicting rule sequencing. Our results for disambiguating Turkish indicate
that using about 500 constraint rules and some additional simple statistics, we
can attain a recall of 95-96% and a precision of 94-95% with about 1.01 parses
per token. Our system is implemented in Prolog and we are currently
investigating an efficient implementation based on finite state transducers.
</dc:description>
 <dc:description>Comment: 8 pages, Latex source. To appear in Proceedings of ACL/EACL'97
  Compressed postscript also available as
  ftp://ftp.cs.bilkent.edu.tr/pub/ko/acl97.ps.z</dc:description>
 <dc:date>1997-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Emphatic generation: employing the theory of semantic emphasis for text
  generation</dc:title>
 <dc:creator>Teich, Elke</dc:creator>
 <dc:creator>Firzlaff, Beate</dc:creator>
 <dc:creator>Bateman, John A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper deals with the problem of text generation and planning approaches
making only limited formally specifiable contact with accounts of grammar. We
propose an enhancement of a systemically-based generation architecture for
German (the KOMET system) by aspects of Kunze's theory of semantic emphasis.
Doing this, we gain more control over both concept selection in generation and
choice of fine-grained grammatical variation.
</dc:description>
 <dc:description>Comment: 11pp; uses: 11pt,twocolumn,named,a4wide,program,psfig; 1psfig</dc:description>
 <dc:date>1997-04-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Theory of Parallelism and the Case of VP Ellipsis</dc:title>
 <dc:creator>Hobbs, Jerry R.</dc:creator>
 <dc:creator>Kehler, Andrew</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We provide a general account of parallelism in discourse, and apply it to the
special case of resolving possible readings for instances of VP ellipsis. We
show how several problematic examples are accounted for in a natural and
straightforward fashion. The generality of the approach makes it directly
applicable to a variety of other types of ellipsis and reference.
</dc:description>
 <dc:description>Comment: LaTeX, 8 pages, requires aclap.sty</dc:description>
 <dc:date>1997-04-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704013</dc:identifier>
 <dc:identifier>Proceedings of ACL-97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9704014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Centering in-the-large: Computing referential discourse segments</dc:title>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Strube, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We specify an algorithm that builds up a hierarchy of referential discourse
segments from local centering data. The spatial extension and nesting of these
discourse segments constrain the reachability of potential antecedents of an
anaphoric expression beyond the local level of adjacent center pairs. Thus, the
centering model is scaled up to the level of the global referential structure
of discourse. An empirical evaluation of the algorithm is supplied.
</dc:description>
 <dc:description>Comment: LaTeX, 8 pages</dc:description>
 <dc:date>1997-04-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9704014</dc:identifier>
 <dc:identifier>Proceedings of ACL 97 / EACL 97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Co-evolution of Language and of the Language Acquisition Device</dc:title>
 <dc:creator>Briscoe, Ted</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A new account of parameter setting during grammatical acquisition is
presented in terms of Generalized Categorial Grammar embedded in a default
inheritance hierarchy, providing a natural partial ordering on the setting of
parameters. Experiments show that several experimentally effective learners can
be defined in this framework. Evolutionary simulations suggest that a learner
with default initial settings for parameters will emerge, provided that
learning is memory limited and the environment of linguistic adaptation
contains an appropriate language.
</dc:description>
 <dc:description>Comment: 10 pages, latex, 2 postscript figures, uses aclap.sty and
  graphics.sty, to appear ACL-EACL97</dc:description>
 <dc:date>1997-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Sloppy Identity</dc:title>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although sloppy interpretation is usually accounted for by theories of
ellipsis, it often arises in non-elliptical contexts. In this paper, a theory
of sloppy interpretation is provided which captures this fact. The underlying
idea is that sloppy interpretation results from a semantic constraint on
parallel structures and the theory is shown to predict sloppy readings for
deaccented and paycheck sentences as well as relational-, event-, and
one-anaphora. It is further shown to capture the interaction of sloppy/strict
ambiguity with quantification and binding.
</dc:description>
 <dc:description>Comment: 20 pages</dc:description>
 <dc:date>1997-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705002</dc:identifier>
 <dc:identifier>Logical Aspects of Computational Linguistics, Springer-Verlag.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Grammatical analysis in the OVIS spoken-dialogue system</dc:title>
 <dc:creator>Nederhof, Mark-Jan</dc:creator>
 <dc:creator>Bouma, Gosse</dc:creator>
 <dc:creator>Koeling, Rob</dc:creator>
 <dc:creator>van Noord, Gertjan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We argue that grammatical processing is a viable alternative to concept
spotting for processing spoken input in a practical dialogue system. We discuss
the structure of the grammar, the properties of the parser, and a method for
achieving robustness. We discuss test results suggesting that grammatical
processing allows fast and accurate processing of spoken input.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty</dc:description>
 <dc:date>1997-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705003</dc:identifier>
 <dc:identifier>ACL/EACL 1997 Workshop on Spoken Dialog Systems</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Computing Parallelism in Discourse</dc:title>
 <dc:creator>Gardent, Claire</dc:creator>
 <dc:creator>Kohlhase, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although much has been said about parallelism in discourse, a formal,
computational theory of parallelism structure is still outstanding. In this
paper, we present a theory which given two parallel utterances predicts which
are the parallel elements. The theory consists of a sorted, higher-order
abductive calculus and we show that it reconciles the insights of discourse
theories of parallelism with those of Higher-Order Unification approaches to
discourse semantics, thereby providing a natural framework in which to capture
the effect of parallelism on discourse semantics.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-05-01</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705004</dc:identifier>
 <dc:identifier>Proceedings of IJCAI'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Document Classification Using a Finite Mixture Model</dc:title>
 <dc:creator>Li, Hang</dc:creator>
 <dc:creator>Yamanishi, Kenji</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a new method of classifying documents into categories. The simple
method of conducting hypothesis testing over word-based distributions in
categories suffers from the data sparseness problem. In order to address this
difficulty, Guthrie et.al. have developed a method using distributions based on
hard clustering of words, i.e., in which a word is assigned to a single cluster
and words in the same cluster are treated uniformly. This method might,
however, degrade classification results, since the distributions it employs are
not always precise enough for representing the differences between categories.
We propose here the use of soft clustering of words, i.e., in which a word can
be assigned to several different clusters and each cluster is characterized by
a specific word probability distribution. We define for each document category
a finite mixture model, which is a linear combination of the probability
distributions of the clusters. We thereby treat the problem of classifying
documents as that of conducting statistical hypothesis testing over finite
mixture models. In order to accomplish this testing, we employ the EM algorithm
which helps efficiently estimate parameters in a finite mixture model.
Experimental results indicate that our method outperforms not only the method
using distributions based on hard clustering, but also the method using
word-based distributions and the method based on cosine-similarity.
</dc:description>
 <dc:description>Comment: latex file, uses aclap.sty and epsf.sty, 9 pages, to appear
  ACL/EACL-97</dc:description>
 <dc:date>1997-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Quantitative Constraint Logic Programming for Weighted Grammar
  Applications</dc:title>
 <dc:creator>Riezler, Stefan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Constraint logic grammars provide a powerful formalism for expressing complex
logical descriptions of natural language phenomena in exact terms. Describing
some of these phenomena may, however, require some form of graded distinctions
which are not provided by such grammars. Recent approaches to weighted
constraint logic grammars attempt to address this issue by adding numerical
calculation schemata to the deduction scheme of the underlying CLP framework.
Currently, these extralogical extensions are not related to the model-theoretic
counterpart of the operational semantics of CLP, i.e., they do not come with a
formal semantics at all. The aim of this paper is to present a clear formal
semantics for weighted constraint logic grammars, which abstracts away from
specific interpretations of weights, but nevertheless gives insights into the
parsing problem for such weighted grammars. Building on the formalization of
constraint logic grammars in the CLP scheme of Hoehfeld and Smolka 1988, this
formal semantics will be given by a quantitative version of CLP. Such a
quantitative CLP scheme can also be valuable for CLP tasks independent of
grammars.
</dc:description>
 <dc:description>Comment: 20 pages, uses llncs.sty</dc:description>
 <dc:date>1997-05-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705006</dc:identifier>
 <dc:identifier>Logical Aspects of Computational Linguistics (LACL'96), LNCS,
  Springer.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Recycling Lingware in a Multilingual MT System</dc:title>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Bretan, Ivan</dc:creator>
 <dc:creator>Eklund, Robert</dc:creator>
 <dc:creator>Wiren, Mats</dc:creator>
 <dc:creator>Hansen, Steffen Leo</dc:creator>
 <dc:creator>Kirchmeier-Andersen, Sabine</dc:creator>
 <dc:creator>Philp, Christina</dc:creator>
 <dc:creator>Sorensen, Finn</dc:creator>
 <dc:creator>Thomsen, Hanne Erdman</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe two methods relevant to multi-lingual machine translation
systems, which can be used to port linguistic data (grammars, lexicons and
transfer rules) between systems used for processing related languages. The
methods are fully implemented within the Spoken Language Translator system, and
were used to create versions of the system for two new language pairs using
only a month of expert effort.
</dc:description>
 <dc:description>Comment: 6 pages, needs aclap.sty. To appear in &quot;From Research to Commercial
  Applications&quot; workshop at ACL-97, see also http://www.cam.sri.com</dc:description>
 <dc:date>1997-05-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The TreeBanker: a Tool for Supervised Training of Parsed Corpora</dc:title>
 <dc:creator>Carter, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I describe the TreeBanker, a graphical tool for the supervised training
involved in domain customization of the disambiguation component of a speech-
or language-understanding system. The TreeBanker presents a user, who need not
be a system expert, with a range of properties that distinguish competing
analyses for an utterance and that are relatively easy to judge. This allows
training on a corpus to be completed in far less time, and with far less
expertise, than would be needed if analyses were inspected directly: it becomes
possible for a corpus of about 20,000 sentences of the complexity of those in
the ATIS corpus to be judged in around three weeks of work by a linguistically
aware non-expert.
</dc:description>
 <dc:description>Comment: 7 pages, needs aclap.sty. Replacement just corrects Figure 3</dc:description>
 <dc:date>1997-05-07</dc:date>
 <dc:date>1997-07-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705008</dc:identifier>
 <dc:identifier>&quot;Computational Environments ...&quot; (ENVGRAM) workshop at ACL-97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Charts, Interaction-Free Grammars, and the Compact Representation of
  Ambiguity</dc:title>
 <dc:creator>Dymetman, Marc</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Recently researchers working in the LFG framework have proposed algorithms
for taking advantage of the implicit context-free components of a unification
grammar [Maxwell 96]. This paper clarifies the mathematical foundations of
these techniques, provides a uniform framework in which they can be formally
studied and eliminates the need for special purpose runtime data-structures
recording ambiguity. The paper posits the identity: Ambiguous Feature
Structures = Grammars, which states that (finitely) ambiguous representations
are best seen as unification grammars of a certain type, here called
``interaction-free'' grammars, which generate in a backtrack-free way each of
the feature structures subsumed by the ambiguous representation. This work
extends a line of research [Billot and Lang 89, Lang 94] which stresses the
connection between charts and grammars: a chart can be seen as a specialization
of the reference grammar for a given input string. We show how this
specialization grammar can be transformed into an interaction-free form which
has the same practicality as a listing of the individual solutions, but is
produced in less time and space.
</dc:description>
 <dc:description>Comment: 15 pages (Latex, Postscript), to appear in Proceedings IJCAI-97</dc:description>
 <dc:date>1997-05-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Memory-Based Learning: Using Similarity for Smoothing</dc:title>
 <dc:creator>Zavrel, Jakub</dc:creator>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper analyses the relation between the use of similarity in
Memory-Based Learning and the notion of backed-off smoothing in statistical
language modeling. We show that the two approaches are closely related, and we
argue that feature weighting methods in the Memory-Based paradigm can offer the
advantage of automatically specifying a suitable domain-specific hierarchy
between most specific and most general conditioning information without the
need for a large number of parameters. We report two applications of this
approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art
performance in both domains, and allows the easy integration of diverse
information sources, such as rich lexical representations.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty, To appear in Proc. ACL/EACL 97</dc:description>
 <dc:date>1997-05-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Lexicon for Underspecified Semantic Tagging</dc:title>
 <dc:creator>Buitelaar, Paul</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper defends the notion that semantic tagging should be viewed as more
than disambiguation between senses. Instead, semantic tagging should be a first
step in the interpretation process by assigning each lexical item a
representation of all of its systematically related senses, from which further
semantic processing steps can derive discourse dependent interpretations. This
leads to a new type of semantic lexicon (CoreLex) that supports underspecified
semantic tagging through a design based on systematic polysemous classes and a
class-based acquisition of lexical knowledge for specific domains.
</dc:description>
 <dc:description>Comment: 8 pages, uses aclap.sty</dc:description>
 <dc:date>1997-05-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Comparative Study of the Application of Different Learning Techniques
  to Natural Language Interfaces</dc:title>
 <dc:creator>Winiwarter, Werner</dc:creator>
 <dc:creator>Kambayashi, Yahiko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we present first results from a comparative study. Its aim is
to test the feasibility of different inductive learning techniques to perform
the automatic acquisition of linguistic knowledge within a natural language
database interface. In our interface architecture the machine learning module
replaces an elaborate semantic analysis component. The learning module learns
the correct mapping of a user's input to the corresponding database command
based on a collection of past input data. We use an existing interface to a
production planning and control system as evaluation and compare the results
achieved by different instance-based and model-based learning algorithms.
</dc:description>
 <dc:description>Comment: 10 pages, to appear CoNLL97</dc:description>
 <dc:date>1997-05-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>FASTUS: A Cascaded Finite-State Transducer for Extracting Information
  from Natural-Language Text</dc:title>
 <dc:creator>Hobbs, Jerry R.</dc:creator>
 <dc:creator>Appelt, Douglas</dc:creator>
 <dc:creator>Bear, John</dc:creator>
 <dc:creator>Israel, David</dc:creator>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:creator>Stickel, Mark</dc:creator>
 <dc:creator>Tyson, Mabry</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  FASTUS is a system for extracting information from natural language text for
entry into a database and for other applications. It works essentially as a
cascaded, nondeterministic finite-state automaton. There are five stages in the
operation of FASTUS. In Stage 1, names and other fixed form expressions are
recognized. In Stage 2, basic noun groups, verb groups, and prepositions and
some other particles are recognized. In Stage 3, certain complex noun groups
and verb groups are constructed. Patterns for events of interest are identified
in Stage 4 and corresponding ``event structures'' are built. In Stage 5,
distinct event structures that describe the same event are identified and
merged, and these are used in generating database entries. This decomposition
of language processing enables the system to do exactly the right amount of
domain-independent syntax, so that domain-dependent semantic and pragmatic
processing can be applied to the right larger-scale structures. FASTUS is very
efficient and effective, and has been used successfully in a number of
applications.
</dc:description>
 <dc:description>Comment: 22 pages. In E. Roche and Y. Schabes, eds., Finite State Devices for
  Natural Language Processing, MIT Press, Cambridge, Massachusetts, in press</dc:description>
 <dc:date>1997-05-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705013</dc:identifier>
 <dc:identifier>In Roche E. and Y. Schabes, eds., Finite-State Language
  Processing, The MIT Press, Cambridge, MA, 1997, pages 383-406.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Incorporating POS Tagging into Language Modeling</dc:title>
 <dc:creator>Heeman, Peter A.</dc:creator>
 <dc:creator>Allen, James F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Language models for speech recognition tend to concentrate solely on
recognizing the words that were spoken. In this paper, we redefine the speech
recognition problem so that its goal is to find both the best sequence of words
and their syntactic role (part-of-speech) in the utterance. This is a necessary
first step towards tightening the interaction between speech recognition and
natural language understanding.
</dc:description>
 <dc:description>Comment: 5 pages, 2 postscript figures</dc:description>
 <dc:date>1997-05-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705014</dc:identifier>
 <dc:identifier>In proceedings of Eurospeech'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Translation Methodology in the Spoken Language Translator: An Evaluation</dc:title>
 <dc:creator>Carter, David</dc:creator>
 <dc:creator>Becket, Ralph</dc:creator>
 <dc:creator>Rayner, Manny</dc:creator>
 <dc:creator>Eklund, ; Robert</dc:creator>
 <dc:creator>MacDermid, Catriona</dc:creator>
 <dc:creator>Wiren, Mats</dc:creator>
 <dc:creator>Kirchmeier-Andersen, ; Sabine</dc:creator>
 <dc:creator>Philp, Christina</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe how the translation methodology adopted for the
Spoken Language Translator (SLT) addresses the characteristics of the speech
translation task in a context where it is essential to achieve easy
customization to new languages and new domains. We then discuss the issues that
arise in any attempt to evaluate a speech translator, and present the results
of such an evaluation carried out on SLT for several language pairs.
</dc:description>
 <dc:description>Comment: 10 pages, needs aclap.sty. To appear in Spoken Language Translation
  workshop at (E)ACL-97</dc:description>
 <dc:date>1997-05-27</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9705016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Sense Tagging: Semantic Tagging with a Lexicon</dc:title>
 <dc:creator>Wilks, Yorick</dc:creator>
 <dc:creator>Stevenson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Sense tagging, the automatic assignment of the appropriate sense from some
lexicon to each of the words in a text, is a specialised instance of the
general problem of semantic tagging by category or type. We discuss which
recent word sense disambiguation algorithms are appropriate for sense tagging.
It is our belief that sense tagging can be carried out effectively by combining
several simple, independent, methods and we include the design of such a
tagger. A prototype of this system has been implemented, correctly tagging 86%
of polysemous word tokens in a small test set, providing evidence that our
hypothesis is correct.
</dc:description>
 <dc:description>Comment: 6 pages, uses aclap LaTeX style file. Also in Proceedings of the
  SIGLEX Workshop &quot;Tagging Text with Lexical Semantics&quot;</dc:description>
 <dc:date>1997-05-29</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9705016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Assigning Grammatical Relations with a Back-off Model</dc:title>
 <dc:creator>de Lima, Erika F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a corpus-based method to assign grammatical
subject/object relations to ambiguous German constructs. It makes use of an
unsupervised learning procedure to collect training and test data, and the
back-off model to make assignment decisions.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of the Second Conference on Empirical
  Methods in Natural Language Processing, 7 pages, LaTeX</dc:description>
 <dc:date>1997-06-04</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706002</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Parse and Translation Decisions From Examples With Rich Context</dc:title>
 <dc:creator>Hermjakob, Ulf</dc:creator>
 <dc:creator>Mooney, Raymond J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a knowledge and context-based system for parsing and translating
natural language and evaluate it on sentences from the Wall Street Journal.
Applying machine learning techniques, the system uses parse action examples
acquired under supervision to generate a deterministic shift-reduce parser in
the form of a decision structure. It relies heavily on context, as encoded in
features which describe the morphological, syntactic, semantic and other
aspects of a given parse state.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, 3 postscript figures, uses aclap.sty</dc:description>
 <dc:date>1997-06-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706002</dc:identifier>
 <dc:identifier>Proceedings of ACL/EACL'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706003</identifier>
 <datestamp>2008-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Three New Probabilistic Models for Dependency Parsing: An Exploration</dc:title>
 <dc:creator>Eisner, Jason</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  After presenting a novel O(n^3) parsing algorithm for dependency grammar, we
develop three contrasting ways to stochasticize it. We propose (a) a lexical
affinity model where words struggle to modify each other, (b) a sense tagging
model where words fluctuate randomly in their selectional preferences, and (c)
a generative model where the speaker fleshes out each word's syntactic and
conceptual structure without regard to the implications for the hearer. We also
give preliminary empirical results from evaluating the three models' parsing
performance on annotated Wall Street Journal training text (derived from the
Penn Treebank). In these results, the generative (i.e., top-down) model
performs significantly better than the others, and does about equally well at
assigning part-of-speech tags.
</dc:description>
 <dc:description>Comment: 6 pages, LaTeX 2.09 packaged with 4 .eps files, also uses colap.sty
  and acl.bst</dc:description>
 <dc:date>1997-06-05</dc:date>
 <dc:date>1997-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706003</dc:identifier>
 <dc:identifier>Proceedings of the 16th International Conference on Computational
  Linguistics (COLING-96), Copenhagen, August 1996, pp. 340-345</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Empirical Comparison of Probability Models for Dependency Grammar</dc:title>
 <dc:creator>Eisner, Jason</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This technical report is an appendix to Eisner (1996): it gives superior
experimental results that were reported only in the talk version of that paper.
Eisner (1996) trained three probability models on a small set of about 4,000
conjunction-free, dependency-grammar parses derived from the Wall Street
Journal section of the Penn Treebank, and then evaluated the models on a
held-out test set, using a novel O(n^3) parsing algorithm.
  The present paper describes some details of the experiments and repeats them
with a larger training set of 25,000 sentences. As reported at the talk, the
more extensive training yields greatly improved performance. Nearly half the
sentences are parsed with no misattachments; two-thirds are parsed with at most
one misattachment.
  Of the models described in the original written paper, the best score is
still obtained with the generative (top-down) &quot;model C.&quot; However, slightly
better models are also explored, in particular, two variants on the
comprehension (bottom-up) &quot;model B.&quot; The better of these has an attachment
accuracy of 90%, and (unlike model C) tags words more accurately than the
comparable trigram tagger. Differences are statistically significant.
  If tags are roughly known in advance, search error is all but eliminated and
the new model attains an attachment accuracy of 93%. We find that the parser of
Collins (1996), when combined with a highly-trained tagger, also achieves 93%
when trained and tested on the same sentences. Similarities and differences are
discussed.
</dc:description>
 <dc:description>Comment: 18 pages, LaTeX packaged with 1 .eps and 1 .sty file, also uses
  fullpage.sty, eepic.sty, psfig.tex</dc:description>
 <dc:date>1997-06-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Comparing a Linguistic and a Stochastic Tagger</dc:title>
 <dc:creator>Samuelsson, Christer</dc:creator>
 <dc:creator>Voutilainen, Atro</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Concerning different approaches to automatic PoS tagging: EngCG-2, a
constraint-based morphological tagger, is compared in a double-blind test with
a state-of-the-art statistical tagger on a common disambiguation task using a
common tag set. The experiments show that for the same amount of remaining
ambiguity, the error rate of the statistical tagger is one order of magnitude
greater than that of the rule-based one. The two related issues of priming
effects compromising the results and disagreement between human annotators are
also addressed.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, 2 postscript figures. E-ACL'97</dc:description>
 <dc:date>1997-06-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Mistake-Driven Learning in Text Categorization</dc:title>
 <dc:creator>Dagan, Ido</dc:creator>
 <dc:creator>Karov, Yael</dc:creator>
 <dc:creator>Roth, Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Learning problems in the text processing domain often map the text to a space
whose dimensions are the measured features of the text, e.g., its words. Three
characteristic properties of this domain are (a) very high dimensionality, (b)
both the learned concepts and the instances reside very sparsely in the feature
space, and (c) a high variation in the number of active features in an
instance. In this work we study three mistake-driven learning algorithms for a
typical task of this nature -- text categorization. We argue that these
algorithms -- which categorize documents by learning a linear separator in the
feature space -- have a few properties that make them ideal for this domain. We
then show that a quantum leap in performance is achieved when we further modify
the algorithms to better address some of the specific characteristics of the
domain. In particular, we demonstrate (1) how variation in document length can
be tolerated by either normalizing feature weights or by using negative
weights, (2) the positive effect of applying a threshold range in training, (3)
alternatives in considering feature frequency, and (4) the benefits of
discarding features while training. Overall, we present an algorithm, a
variation of Littlestone's Winnow, which performs significantly better than any
other algorithm tested on this task using a similar feature set.
</dc:description>
 <dc:description>Comment: 9 pages, uses aclap.sty</dc:description>
 <dc:date>1997-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Aggregate and mixed-order Markov models for statistical language
  processing</dc:title>
 <dc:creator>Saul, Lawrence</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We consider the use of language models whose size and accuracy are
intermediate between different order n-gram models. Two types of models are
studied in particular. Aggregate Markov models are class-based bigram models in
which the mapping from words to classes is probabilistic. Mixed-order Markov
models combine bigram models whose predictions are conditioned on different
words. Both types of models are trained by Expectation-Maximization (EM)
algorithms for maximum likelihood estimation. We examine smoothing procedures
in which these models are interposed between different order n-grams. This is
found to significantly reduce the perplexity of unseen word combinations.
</dc:description>
 <dc:description>Comment: 9 pages, 4 PostScript figures, uses psfig.sty and aclap.sty; to
  appear in the proceedings of EMNLP-2</dc:description>
 <dc:date>1997-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Distinguishing Word Senses in Untagged Text</dc:title>
 <dc:creator>Pedersen, Ted</dc:creator>
 <dc:creator>Bruce, Rebecca</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes an experimental comparison of three unsupervised
learning algorithms that distinguish the sense of an ambiguous word in untagged
text. The methods described in this paper, McQuitty's similarity analysis,
Ward's minimum-variance method, and the EM algorithm, assign each instance of
an ambiguous word to a known sense definition based solely on the values of
automatically identifiable features in text. These methods and feature sets are
found to be more successful in disambiguating nouns rather than adjectives or
verbs. Overall, the most accurate of these procedures is McQuitty's similarity
analysis in combination with a high dimensional feature set.
</dc:description>
 <dc:description>Comment: 11 pages, latex, uses aclap.sty</dc:description>
 <dc:date>1997-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706008</dc:identifier>
 <dc:identifier>Appears in the Proceedings of the Second Conference on Empirical
  Methods in NLP (EMNLP-2), August 1-2, 1997, Providence, RI</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706009</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Library of Practical Abstractions, Release 1.2</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:creator>Yianilos, Peter N.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The library of practical abstractions (LIBPA) provides efficient
implementations of conceptually simple abstractions, in the C programming
language. We believe that the best library code is conceptually simple so that
it will be easily understood by the application programmer; parameterized by
type so that it enjoys wide applicability; and at least as efficient as a
straightforward special-purpose implementation. You will find that our software
satisfies the highest standards of software design, implementation, testing,
and benchmarking.
  The current LIBPA release is a source code distribution only. It consists of
modules for portable memory management, one dimensional arrays of arbitrary
types, compact symbol tables, hash tables for arbitrary types, a trie module
for length-delimited strings over arbitrary alphabets, single precision
floating point numbers with extended exponents, and logarithmic representations
of probability values using either fixed or floating point numbers.
  We have used LIBPA to implement a wide range of statistical models for both
continuous and discrete domains. The time and space efficiency of LIBPA has
allowed us to build larger statistical models than previously reported, and to
investigate more computationally-intensive techniques than previously possible.
We have found LIBPA to be indispensible in our own research, and hope that you
will find it useful in yours.
</dc:description>
 <dc:description>Comment: 19 pages, texinfo format</dc:description>
 <dc:date>1997-06-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Exemplar-Based Word Sense Disambiguation: Some Recent Improvements</dc:title>
 <dc:creator>Ng, Hwee Tou</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we report recent improvements to the exemplar-based learning
approach for word sense disambiguation that have achieved higher disambiguation
accuracy. By using a larger value of $k$, the number of nearest neighbors to
use for determining the class of a test example, and through 10-fold cross
validation to automatically determine the best $k$, we have obtained improved
disambiguation accuracy on a large sense-tagged corpus first used in
\cite{ng96}. The accuracy achieved by our improved exemplar-based classifier is
comparable to the accuracy on the same data set obtained by the Naive-Bayes
algorithm, which was reported in \cite{mooney96} to have the highest
disambiguation accuracy among seven state-of-the-art machine learning
algorithms.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706010</dc:identifier>
 <dc:identifier>In Proceedings of the Second Conference on Empirical Methods in
  Natural Language Processing (EMNLP-2), August 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Applying Reliability Metrics to Co-Reference Annotation</dc:title>
 <dc:creator>Passonneau, Rebecca J.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Studies of the contextual and linguistic factors that constrain discourse
phenomena such as reference are coming to depend increasingly on annotated
language corpora. In preparing the corpora, it is important to evaluate the
reliability of the annotation, but methods for doing so have not been readily
available. In this report, I present a method for computing reliability of
coreference annotation. First I review a method for applying the information
retrieval metrics of recall and precision to coreference annotation proposed by
Marc Vilain and his collaborators. I show how this method makes it possible to
construct contingency tables for computing Cohen's Kappa, a familiar
reliability metric. By comparing recall and precision to reliability on the
same data sets, I also show that recall and precision can be misleadingly high.
Because Kappa factors out chance agreement among coders, it is a preferable
measure for developing annotated corpora where no pre-existing target
annotation exists.
</dc:description>
 <dc:description>Comment: 10 pages, 2-column format; uuencoded, gzipped, tarfile</dc:description>
 <dc:date>1997-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Coreference in Information Extraction</dc:title>
 <dc:creator>Kehler, Andrew</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Certain applications require that the output of an information extraction
system be probabilistic, so that a downstream system can reliably fuse the
output with possibly contradictory information from other sources. In this
paper we consider the problem of assigning a probability distribution to
alternative sets of coreference relationships among entity descriptions. We
present the results of initial experiments with several approaches to
estimating such distributions in an application using SRI's FASTUS information
extraction system.
</dc:description>
 <dc:description>Comment: LaTeX, 11 pages, requires aclap.sty</dc:description>
 <dc:date>1997-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706012</dc:identifier>
 <dc:identifier>Proceedings of the Second Conference on Empirical Methods in NLP
  (EMNLP-2), August 1-2, 1997, Providence, RI</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Corpus-Based Approach for Building Semantic Lexicons</dc:title>
 <dc:creator>Riloff, Ellen</dc:creator>
 <dc:creator>Shepherd, Jessica</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Semantic knowledge can be a great asset to natural language processing
systems, but it is usually hand-coded for each application. Although some
semantic information is available in general-purpose knowledge bases such as
WordNet and Cyc, many applications require domain-specific lexicons that
represent words and categories for a particular topic. In this paper, we
present a corpus-based method that can be used to build semantic lexicons for
specific categories. The input to the system is a small set of seed words for a
category and a representative text corpus. The output is a ranked list of words
that are associated with the category. A user then reviews the top-ranked words
and decides which ones should be entered in the semantic lexicon. In
experiments with five categories, users typically found about 60 words per
category in 10-15 minutes to build a core semantic lexicon.
</dc:description>
 <dc:description>Comment: 8 pages - to appear in Proceedings of EMNLP-2</dc:description>
 <dc:date>1997-06-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706014</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Linear Observed Time Statistical Parser Based on Maximum Entropy
  Models</dc:title>
 <dc:creator>Ratnaparkhi, Adwait</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a statistical parser for natural language that obtains a
parsing accuracy---roughly 87% precision and 86% recall---which surpasses the
best previously published results on the Wall St. Journal domain. The parser
itself requires very little human intervention, since the information it uses
to make parsing decisions is specified in a concise and simple manner, and is
combined in a fully automatic way under the maximum entropy framework. The
observed running time of the parser on a test sentence is linear with respect
to the sentence length. Furthermore, the parser returns several scored parses
for a sentence, and this paper shows that a scheme to pick the best parse from
the 20 highest scoring parses could yield a dramatically higher accuracy of 93%
precision and recall.
</dc:description>
 <dc:description>Comment: 10 pages, LaTeX, uses aclap.sty, qtree.sty, to appear in EMNLP-2</dc:description>
 <dc:date>1997-06-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Determining Internal and External Indices for Chart Generation</dc:title>
 <dc:creator>Trujillo, Arturo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a compilation procedure which determines internal and
external indices for signs in a unification based grammar to be used in
improving the computational efficiency of lexicalist chart generation. The
procedure takes as input a grammar and a set of feature paths indicating the
position of semantic indices in a sign, and calculates the fixed-point of a set
of equations derived from the grammar. The result is a set of independent
constraints stating which indices in a sign can be bound to other signs within
a complete sentence. Based on these constraints, two tests are formulated which
reduce the search space during generation.
</dc:description>
 <dc:description>Comment: 8 pages, Latex; to appear in 7th International Conference on
  Theoretical and Methodological Issues in Machine Translation (TMI-97)</dc:description>
 <dc:date>1997-06-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Text Segmentation Using Exponential Models</dc:title>
 <dc:creator>Beeferman, Doug</dc:creator>
 <dc:creator>Berger, Adam</dc:creator>
 <dc:creator>Lafferty, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces a new statistical approach to partitioning text
automatically into coherent segments. Our approach enlists both short-range and
long-range language models to help it sniff out likely sites of topic changes
in text. To aid its search, the system consults a set of simple lexical hints
it has learned to associate with the presence of boundaries through inspection
of a large corpus of annotated data. We also propose a new probabilistically
motivated error metric for use by the natural language processing and
information retrieval communities, intended to supersede precision and recall
for appraising segmentation algorithms. Qualitative assessment of our algorithm
as well as evaluation using this new metric demonstrate the effectiveness of
our approach in two very different domains, Wall Street Journal articles and
the TDT Corpus, a collection of newswire articles and broadcast news
transcripts.
</dc:description>
 <dc:description>Comment: 12 pages, LaTeX source and postscript figures for EMNLP-2 paper</dc:description>
 <dc:date>1997-06-11</dc:date>
 <dc:date>1997-06-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Name Searching and Information Retrieval</dc:title>
 <dc:creator>Thompson, Paul</dc:creator>
 <dc:creator>Dozier, Christopher C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The main application of name searching has been name matching in a database
of names. This paper discusses a different application: improving information
retrieval through name recognition. It investigates name recognition accuracy,
and the effect on retrieval performance of indexing and searching personal
names differently from non-name terms in the context of ranked retrieval. The
main conclusions are: that name recognition in text can be effective; that
names occur frequently enough in a variety of domains, including those of legal
documents and news databases, to make recognition worthwhile; and that
retrieval performance can be improved using name searching.
</dc:description>
 <dc:date>1997-06-12</dc:date>
 <dc:date>1997-06-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Model of Lexical Attraction and Repulsion</dc:title>
 <dc:creator>Beeferman, Doug</dc:creator>
 <dc:creator>Berger, Adam</dc:creator>
 <dc:creator>Lafferty, John</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces new methods based on exponential families for modeling
the correlations between words in text and speech. While previous work assumed
the effects of word co-occurrence statistics to be constant over a window of
several hundred words, we show that their influence is nonstationary on a much
smaller time scale. Empirical data drawn from English and Japanese text, as
well as conversational speech, reveals that the ``attraction'' between words
decays exponentially, while stylistic and syntactic contraints create a
``repulsion'' between words that discourages close co-occurrence. We show that
these characteristics are well described by simple mixture models based on
two-stage exponential distributions which can be trained using the EM
algorithm. The resulting distance distributions can then be incorporated as
penalizing features in an exponential language model.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX source and postscript figures for ACL/EACL'97 paper</dc:description>
 <dc:date>1997-06-12</dc:date>
 <dc:date>1997-06-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluating Competing Agent Strategies for a Voice Email Agent</dc:title>
 <dc:creator>Walker, Marilyn</dc:creator>
 <dc:creator>Hindle, Donald</dc:creator>
 <dc:creator>Fromer, Jeanne</dc:creator>
 <dc:creator>Di Fabbrizio, Giuseppe</dc:creator>
 <dc:creator>Mestel, Craig</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper reports experimental results comparing a mixed-initiative to a
system-initiative dialog strategy in the context of a personal voice email
agent. To independently test the effects of dialog strategy and user expertise,
users interact with either the system-initiative or the mixed-initiative agent
to perform three successive tasks which are identical for both agents. We
report performance comparisons across agent strategies as well as over tasks.
This evaluation utilizes and tests the PARADISE evaluation framework, and
discusses the performance function derivable from the experimental data.
</dc:description>
 <dc:description>Comment: 6 pages latex, uses icassp91.sty, psfig</dc:description>
 <dc:date>1997-06-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706019</dc:identifier>
 <dc:identifier>Proceedings of the European Conference on Speech Communication and
  Technology, EUROSPEECH97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Empirical Approach to Temporal Reference Resolution</dc:title>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:creator>O'Hara, Tom</dc:creator>
 <dc:creator>McKeever, Kenneth</dc:creator>
 <dc:creator>Oehrstroem-Sandgren, Thorsten</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents the results of an empirical investigation of temporal
reference resolution in scheduling dialogs. The algorithm adopted is primarily
a linear-recency based approach that does not include a model of global focus.
A fully automatic system has been developed and evaluated on unseen test data
with good results. This paper presents the results of an intercoder reliability
study, a model of temporal reference resolution that supports linear recency
and has very good coverage, the results of the system evaluated on unseen test
data, and a detailed analysis of the dialogs assessing the viability of the
approach.
</dc:description>
 <dc:description>Comment: 13 pages, latex using aclap.sty</dc:description>
 <dc:date>1997-06-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706020</dc:identifier>
 <dc:identifier>Proceedings of the Second Conference On Empirical Methods in
  Natural Language Processing (EMNLP-2), August 1-2, 1997, Providence, RI</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706021</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Efficient Distribution of Labor in a Two Stage Robust Interpretation
  Process</dc:title>
 <dc:creator>Rose', Carolyn Penstien</dc:creator>
 <dc:creator>Lavie, Alon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although Minimum Distance Parsing (MDP) offers a theoretically attractive
solution to the problem of extragrammaticality, it is often computationally
infeasible in large scale practical applications. In this paper we present an
alternative approach where the labor is distributed between a more restrictive
partial parser and a repair module. Though two stage approaches have grown in
popularity in recent years because of their efficiency, they have done so at
the cost of requiring hand coded repair heuristics. In contrast, our two stage
approach does not require any hand coded knowledge sources dedicated to repair,
thus making it possible to achieve a similar run time advantage over MDP
without losing the quality of domain independence.
</dc:description>
 <dc:description>Comment: 9 pages, 1 Postscript figure, uses aclap.sty and psfig.tex, In
  Proceedings of EMNLP 1997</dc:description>
 <dc:date>1997-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706021</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706022</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Three Generative, Lexicalised Models for Statistical Parsing</dc:title>
 <dc:creator>Collins, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we first propose a new statistical parsing model, which is a
generative model of lexicalised context-free grammar. We then extend the model
to include a probabilistic treatment of both subcategorisation and wh-movement.
Results on Wall Street Journal text show that the parser performs at 88.1/87.5%
constituent precision/recall, an average improvement of 2.3% over (Collins 96).
</dc:description>
 <dc:description>Comment: 8 pages, to appear in Proceedings of ACL/EACL 97.</dc:description>
 <dc:date>1997-06-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706022</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706023</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Information Extraction Core System for Real World German Text
  Processing</dc:title>
 <dc:creator>Neumann, G.</dc:creator>
 <dc:creator>Backofen, R.</dc:creator>
 <dc:creator>Baur, J.</dc:creator>
 <dc:creator>Becker, M.</dc:creator>
 <dc:creator>Braun, C.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes SMES, an information extraction core system for real
world German text processing. The basic design criterion of the system is of
providing a set of basic powerful, robust, and efficient natural language
components and generic linguistic knowledge sources which can easily be
customized for processing different tasks in a flexible manner.
</dc:description>
 <dc:description>Comment: 9 pages; in Proc. of 5th ANLP, 1997</dc:description>
 <dc:date>1997-06-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706023</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706024</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Lexicalist Approach to the Translation of Colloquial Text</dc:title>
 <dc:creator>Popowich, Fred</dc:creator>
 <dc:creator>Turcato, Davide</dc:creator>
 <dc:creator>Laurens, Olivier</dc:creator>
 <dc:creator>McFetridge, Paul</dc:creator>
 <dc:creator>Nicholson, J. Devlan</dc:creator>
 <dc:creator>McGivern, Patrick</dc:creator>
 <dc:creator>Pena, Maricela Corzo</dc:creator>
 <dc:creator>Pidruchney, Lisa</dc:creator>
 <dc:creator>MacDonald, Scott</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Colloquial English (CE) as found in television programs or typical
conversations is different than text found in technical manuals, newspapers and
books. Phrases tend to be shorter and less sophisticated. In this paper, we
look at some of the theoretical and implementational issues involved in
translating CE. We present a fully automatic large-scale multilingual natural
language processing system for translation of CE input text, as found in the
commercially transmitted closed-caption television signal, into simple target
sentences. Our approach is based on the Whitelock's Shake and Bake machine
translation paradigm, which relies heavily on lexical resources. The system
currently translates from English to Spanish with the translation modules for
Brazilian Portuguese under development.
</dc:description>
 <dc:description>Comment: 11 pages, LaTeX, uses tmi.sty</dc:description>
 <dc:date>1997-06-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706024</dc:identifier>
 <dc:identifier>Proceedings of the 7th International Conference on Theoretical
  Issues in Machine Translation (TMI '97), Santa Fe, NM, 23-25 July 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706025</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Portable Algorithm for Mapping Bitext Correspondence</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The first step in most empirical work in multilingual NLP is to construct
maps of the correspondence between texts and their translations ({\bf bitext
maps}). The Smooth Injective Map Recognizer (SIMR) algorithm presented here is
a generic pattern recognition algorithm that is particularly well-suited to
mapping bitext correspondence. SIMR is faster and significantly more accurate
than other algorithms in the literature. The algorithm is robust enough to use
on noisy texts, such as those resulting from OCR input, and on translations
that are not very literal. SIMR encapsulates its language-specific heuristics,
so that it can be ported to any language pair with a minimal effort.
</dc:description>
 <dc:description>Comment: 8 pages</dc:description>
 <dc:date>1997-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706025</dc:identifier>
 <dc:identifier>Proceedings of the 35th Conference of the Association for
  Computational Linguistics (ACL'97), Madrid, Spain, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706026</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Word-to-Word Model of Translational Equivalence</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Many multilingual NLP applications need to translate words between different
languages, but cannot afford the computational expense of inducing or applying
a full translation model. For these applications, we have designed a fast
algorithm for estimating a partial translation model, which accounts for
translational equivalence only at the word level. The model's precision/recall
trade-off can be directly controlled via one threshold parameter. This feature
makes the model more suitable for applications that are not fully statistical.
The model's hidden parameters can be easily conditioned on information
extrinsic to the model, providing an easy way to integrate pre-existing
knowledge such as part-of-speech, dictionaries, word order, etc.. Our model can
link word tokens in parallel texts as well as other translation models in the
literature. Unlike other translation models, it can automatically produce
dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
</dc:description>
 <dc:description>Comment: 8 pages; this version has some typos corrected</dc:description>
 <dc:date>1997-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706026</dc:identifier>
 <dc:identifier>Proceedings of the 35th Conference of the Association for
  Computational Linguistics (ACL'97), Madrid, Spain, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706027</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Discovery of Non-Compositional Compounds in Parallel Data</dc:title>
 <dc:creator>Melamed, I. Dan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Automatic segmentation of text into minimal content-bearing units is an
unsolved problem even for languages like English. Spaces between words offer an
easy first approximation, but this approximation is not good enough for machine
translation (MT), where many word sequences are not translated word-for-word.
This paper presents an efficient automatic method for discovering sequences of
words that are translated as a unit. The method proceeds by comparing pairs of
statistical translation models induced from parallel texts in two languages. It
can discover hundreds of non-compositional compounds on each iteration, and
constructs longer compounds out of shorter ones. Objective evaluation on a
simple machine translation task has shown the method's potential to improve the
quality of MT output. The method makes few assumptions about the data, so it
can be applied to parallel data other than parallel texts, such as word
spellings and pronunciations.
</dc:description>
 <dc:description>Comment: 12 pages; uses natbib.sty, here.sty</dc:description>
 <dc:date>1997-06-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706027</dc:identifier>
 <dc:identifier>Proceedings of the 2nd Conference on Empirical Methods in Natural
  Language Processing (EMNLP'97), Providence, RI, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706028</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Efficient Construction of Underspecified Semantics under Massive
  Ambiguity</dc:title>
 <dc:creator>Doerre, Jochen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We investigate the problem of determining a compact underspecified semantical
representation for sentences that may be highly ambiguous. Due to combinatorial
explosion, the naive method of building semantics for the different syntactic
readings independently is prohibitive. We present a method that takes as input
a syntactic parse forest with associated constraint-based semantic construction
rules and directly builds a packed semantic structure. The algorithm is fully
implemented and runs in $O(n^4 log(n))$ in sentence length, if the grammar
meets some reasonable `normality' restrictions.
</dc:description>
 <dc:description>Comment: 9 pages, 7 postscript figures, LaTeX source (aclap, psfig styles)</dc:description>
 <dc:date>1997-06-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706028</dc:identifier>
 <dc:identifier>Proceedings of ACL/EACL'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9706029</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Parse and Translation Decisions From Examples With Rich Context</dc:title>
 <dc:creator>Hermjakob, Ulf</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a system for parsing and translating natural language that learns
from examples and uses some background knowledge.
  As our parsing model we choose a deterministic shift-reduce type parser that
integrates part-of-speech tagging and syntactic and semantic processing.
Applying machine learning techniques, the system uses parse action examples
acquired under supervision to generate a parser in the form of a decision
structure, a generalization of decision trees.
  To learn good parsing and translation decisions, our system relies heavily on
context, as encoded in currently 205 features describing the morphological,
syntactical and semantical aspects of a given parse state. Compared with recent
probabilistic systems that were trained on 40,000 sentences, our system relies
on more background knowledge and a deeper analysis, but radically fewer
examples, currently 256 sentences.
  We test our parser on lexically limited sentences from the Wall Street
Journal and achieve accuracy rates of 89.8% for labeled precision, 98.4% for
part of speech tagging and 56.3% of test sentences without any crossing
brackets. Machine translations of 32 Wall Street Journal sentences to German
have been evaluated by 10 bilingual volunteers and been graded as 2.4 on a 1.0
(best) to 6.0 (worst) scale for both grammatical correctness and meaning
preservation.
</dc:description>
 <dc:description>Comment: dissertation, 175 pages, LaTeX, 23 postscript figures, uses
  utthesis-compact.sty</dc:description>
 <dc:date>1997-06-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9706029</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Reluctant Paraphrase: Textual Restructuring under an Optimisation Model</dc:title>
 <dc:creator>Dras, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper develops a computational model of paraphrase under which text
modification is carried out reluctantly; that is, there are external
constraints, such as length or readability, on an otherwise ideal text, and
modifications to the text are necessary to ensure conformance to these
constraints. This problem is analogous to a mathematical optimisation problem:
the textual constraints can be described as a set of constraint equations, and
the requirement for minimal change to the text can be expressed as a function
to be minimised; so techniques from this domain can be used to solve the
problem.
  The work is done as part of a computational paraphrase system using the XTAG
system as a base. The paper will present a theoretical computational framework
for working within the Reluctant Paraphrase paradigm: three types of textual
constraints are specified, effects of paraphrase on text are described, and a
model incorporating mathematical optimisation techniques is outlined.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX source (pacling97, examples styles)</dc:description>
 <dc:date>1997-07-03</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Automatic Detection of Text Genre</dc:title>
 <dc:creator>Kessler, Brett</dc:creator>
 <dc:creator>Nunberg, Geoffrey</dc:creator>
 <dc:creator>Schuetze, Hinrich</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  As the text databases available to users become larger and more
heterogeneous, genre becomes increasingly important for computational
linguistics as a complement to topical and structural principles of
classification. We propose a theory of genres as bundles of facets, which
correlate with various surface cues, and argue that genre detection based on
surface cues is as successful as detection based on deeper structural
properties.
</dc:description>
 <dc:description>Comment: 7 pages</dc:description>
 <dc:date>1997-07-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707002</dc:identifier>
 <dc:identifier>Proceedings ACL/EACL 1997, Madrid, p. 32-38</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Flexible POS tagger Using an Automatically Acquired Language Model</dc:title>
 <dc:creator>Marquez, Lluis</dc:creator>
 <dc:creator>Padro, Lluis</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an algorithm that automatically learns context constraints using
statistical decision trees. We then use the acquired constraints in a flexible
POS tagger. The tagger is able to use information of any degree: n-grams,
automatically learned context constraints, linguistically motivated manually
written constraints, etc. The sources and kinds of constraints are
unrestricted, and the language model can be easily extended, improving the
results. The tagger has been tested and evaluated on the WSJ corpus.
</dc:description>
 <dc:description>Comment: 8 pages, aclap.sty, 2 eps figures. Appears in (E)ACL'97</dc:description>
 <dc:date>1997-07-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707003</dc:identifier>
 <dc:identifier>Proceedings of EACL/ACL 1997, Madrid, Spain</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Discourse Preferences in Dynamic Logic</dc:title>
 <dc:creator>Jaspars, Jan</dc:creator>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In order to enrich dynamic semantic theories with a `pragmatic' capacity, we
combine dynamic and nonmonotonic (preferential) logics in a modal logic
setting. We extend a fragment of Van Benthem and De Rijke's dynamic modal logic
with additional preferential operators in the underlying static logic, which
enables us to define defeasible (pragmatic) entailments over a given piece of
discourse. We will show how this setting can be used for a dynamic logical
analysis of preferential resolutions of ambiguous pronouns in discourse.
</dc:description>
 <dc:description>Comment: To appear in Van Glabbeek, R. et al. eds., a collection of papers
  from the Fourth CSLI Workshop in Logic, Language, and Computation, CSLI
  Publications. (This is a revised version of the paper titled &quot;Preferences in
  Dynamic Semantics&quot; in the Proceedings of the Tenth Amsterdam Colloquium,
  1995, pages 445-464.)</dc:description>
 <dc:date>1997-07-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Intrasentential Centering: A Case Study</dc:title>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  One of the necessary extensions to the centering model is a mechanism to
handle pronouns with intrasentential antecedents. Existing centering models
deal only with discourses consisting of simple sentences. It leaves unclear how
to delimit center-updating utterance units and how to process complex
utterances consisting of multiple clauses. In this paper, I will explore the
extent to which a straightforward extension of an existing intersentential
centering model contributes to this effect. I will motivate an approach that
breaks a complex sentence into a hierarchy of center-updating units and
proposes the preferred interpretation of a pronoun in its local context
arbitrarily deep in the given sentence structure. This approach will be
substantiated with examples from naturally occurring written discourses.
</dc:description>
 <dc:description>Comment: A chapter in Walker, M., A. Joshi, and E. Prince, eds., {\it
  Centering Theory in Discourse}, Oxford University Press, Oxford, in press</dc:description>
 <dc:date>1997-07-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Finite State Transducers Approximating Hidden Markov Models</dc:title>
 <dc:creator>Kempe, Andre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes the conversion of a Hidden Markov Model into a
sequential transducer that closely approximates the behavior of the stochastic
model. This transformation is especially advantageous for part-of-speech
tagging because the resulting transducer can be composed with other transducers
that encode correction rules for the most frequent tagging errors. The speed of
tagging is also improved. The described methods have been implemented and
successfully tested on six languages.
</dc:description>
 <dc:description>Comment: 8 pages, A4, LaTeX (+1x eps)</dc:description>
 <dc:date>1997-07-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707006</dc:identifier>
 <dc:identifier>ACL'97, pp.460-467, Madrid, Spain. July 10, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tailored Patient Information: Some Issues and Questions</dc:title>
 <dc:creator>Reiter, Ehud</dc:creator>
 <dc:creator>Osman, Liesl</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Tailored patient information (TPI) systems are computer programs which
produce personalised heath-information material for patients. TPI systems are
of growing interest to the natural-language generation (NLG) community; many
TPI systems have also been developed in the medical community, usually with
mail-merge technology. No matter what technology is used, experience shows that
it is not easy to field a TPI system, even if it is shown to be effective in
clinical trials. In this paper we discuss some of the difficulties in fielding
TPI systems. This is based on our experiences with 2 TPI systems, one for
generating asthma-information booklets and one for generating smoking-cessation
letters.
</dc:description>
 <dc:description>Comment: This is a paper about technology-transfer. It does not have much
  technical content</dc:description>
 <dc:date>1997-07-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707007</dc:identifier>
 <dc:identifier>Proceedings of the 1997 ACL Workshop on From Research to
  Commercial Applications: Making NLP Work in Practice</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stressed and Unstressed Pronouns: Complementary Preferences</dc:title>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I present a unified account of interpretation preferences of stressed and
unstressed pronouns in discourse. The central intuition is the Complementary
Preference Hypothesis that predicts the interpretation preference of a stressed
pronoun from that of an unstressed pronoun in the same discourse position. The
base preference must be computed in a total pragmatics module including
commonsense preferences. The focus constraint in Rooth's theory of semantic
focus is interpreted to be the salient subset of the domain in the local
attentional state in the discourse context independently motivated for other
purposes in Centering Theory.
</dc:description>
 <dc:description>Comment: Uses endnotes.sty and lingmacros.sty. To appear in Bosch, Peter and
  Rob van der Sandt, eds., Focus: Linguistic, Cognitive and Computational
  Perspectives. Cambridge University Press. (This is an extended and revised
  version of the paper with the same title in Bosch, Peter and Rob van der
  Sandt, eds., Focus and Natural Language Processing, Institute for Logic and
  Linguistics, IBM, Heidelberg, 1994, 475-484.)</dc:description>
 <dc:date>1997-07-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707008</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Recognizing Referential Links: An Information Extraction Perspective</dc:title>
 <dc:creator>Kameyama, Megumi</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present an efficient and robust reference resolution algorithm in an
end-to-end state-of-the-art information extraction system, which must work with
a considerably impoverished syntactic analysis of the input sentences.
Considering this disadvantage, the basic setup to collect, filter, then order
by salience does remarkably well with third-person pronouns, but needs more
semantic and discourse information to improve the treatments of other
expression types.
</dc:description>
 <dc:description>Comment: 8 pages. a paper in the proceedings of the ACL/EACL'97 workshop on
  anaphora resolution</dc:description>
 <dc:date>1997-07-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707009</dc:identifier>
 <dc:identifier>In Mitkov, R. and B. Boguraev, eds., Proceedings of ACL/EACL
  Workshop on Operational Factors in Practical, Robust Anaphora Resolution for
  Unrestricted Texts, Madrid, July 1997, pages 46-53.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Experiences with the GTU grammar development environment</dc:title>
 <dc:creator>Volk, Martin</dc:creator>
 <dc:creator>Richarz, Dirk</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we describe our experiences with a tool for the development and
testing of natural language grammars called GTU (German:
Grammatik-Testumgebumg; grammar test environment). GTU supports four grammar
formalisms under a window-oriented user interface. Additionally, it contains a
set of German test sentences covering various syntactic phenomena as well as
three types of German lexicons that can be attached to a grammar via an
integrated lexicon interface. What follows is a description of the experiences
we gained when we used GTU as a tutoring tool for students and as an
experimental tool for CL researchers. From these we will derive the features
necessary for a future grammar workbench.
</dc:description>
 <dc:description>Comment: 7 pages, uses aclap.sty</dc:description>
 <dc:date>1997-07-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707010</dc:identifier>
 <dc:identifier>Proceedings of Workshop on Computational Environments For Grammar
  Development And Linguistic Engineering at the ACL/EACL Joint Conference 1997,
  107-113</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A lexical database tool for quantitative phonological research</dc:title>
 <dc:creator>Bird, Steven</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A lexical database tool tailored for phonological research is described.
Database fields include transcriptions, glosses and hyperlinks to speech files.
Database queries are expressed using HTML forms, and these permit regular
expression search on any combination of fields. Regular expressions are passed
directly to a Perl CGI program, enabling the full flexibility of Perl extended
regular expressions. The regular expression notation is extended to better
support phonological searches, such as search for minimal pairs. Search results
are presented in the form of HTML or LaTeX tables, where each cell is either a
number (representing frequency) or a designated subset of the fields. Tables
have up to four dimensions, with an elegant system for specifying which
fragments of which fields should be used for the row/column labels. The tool
offers several advantages over traditional methods of analysis: (i) it supports
a quantitative method of doing phonological research; (ii) it gives universal
access to the same set of informants; (iii) it enables other researchers to
hear the original speech data without having to rely on published
transcriptions; (iv) it makes the full power of regular expression search
available, and search results are full multimedia documents; and (v) it enables
the early refutation of false hypotheses, shortening the
analysis-hypothesis-test loop. A life-size application to an African tone
language (Dschang) is used for exemplification throughout the paper. The
database contains 2200 records, each with approximately 15 fields. Running on a
PC laptop with a stand-alone web server, the `Dschang HyperLexicon' has already
been used extensively in phonological fieldwork and analysis in Cameroon.
</dc:description>
 <dc:description>Comment: 7 pages, uses ipamacs.sty</dc:description>
 <dc:date>1997-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707011</dc:identifier>
 <dc:identifier>Proceedings of the Third Meeting of the ACL Special Interest Group
  in Computational Phonology, pp. 33-39, Madrid, July 1997. ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Adjunction As Substitution: An Algebraic Formulation of Regular,
  Context-Free and Tree Adjoining Languages</dc:title>
 <dc:creator>Moennich, Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This note presents a method of interpreting the tree adjoining languages as
the natural third step in a hierarchy that starts with the regular and the
context-free languages. The central notion in this account is that of a
higher-order substitution. Whereas in traditional presentations of rule systems
for abstract language families the emphasis has been on a first-order
substitution process in which auxiliary variables are replaced by elements of
the carrier of the proper algebra - concatenations of terminal and auxiliary
category symbols in the string case - we lift this process to the level of
operations defined on the elements of the carrier of the algebra. Our own view
is that this change of emphasis provides the adequate platform for a better
understanding of the operation of adjunction. To put it in a nutshell:
Adjoining is not a first-order, but a second-order substitution operation.
</dc:description>
 <dc:description>Comment: Formal Grammar Conference, Aix-en-Provence, Aug. 97, 11pp., uses
  AMS-LaTeX, natbib</dc:description>
 <dc:date>1997-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On Cloning Context-Freeness</dc:title>
 <dc:creator>Moennich, Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  To Rogers (1994) we owe the insight that monadic second order predicate logic
with multiple successors (MSO) is well suited in many respects as a realistic
formal base for syntactic theorizing. However, the agreeable formal properties
of this logic come at a cost: MSO is equivalent with the class of regular tree
automata/grammars, and, thereby, with the class of context-free languages.
  This paper outlines one approach towards a solution of MSO's expressivity
problem. On the background of an algebraically refined Chomsky hierarchy, which
allows the definition of several classes of languages--in particular, a whole
hierarchy between CF and CS--via regular tree grammars over unambiguously
derivable alphabets of varying complexity plus their respective
yield-functions, it shows that not only some non-context-free string languages
can be captured by context-free means in this way, but that this approach can
be generalized to the corresponding structures. I.e., non-recognizable sets of
structures can--up to homomorphism--be coded context-freely. Since the class of
languages covered--Fischer's (1968} OI family of indexed languages--includes
all attested instances of non-context-freeness in natural language, there
exists an indirect, to be sure, but completely general way to formally describe
the natural languages using a weak framework like MSO.
</dc:description>
 <dc:description>Comment: 36pp., uses AMS-LaTeX, tree-dvips, natbib</dc:description>
 <dc:date>1997-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards a PURE Spoken Dialogue System for Information Access</dc:title>
 <dc:creator>Agarwal, Rajeev</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  With the rapid explosion of the World Wide Web, it is becoming increasingly
possible to easily acquire a wide variety of information such as flight
schedules, yellow pages, used car prices, current stock prices, entertainment
event schedules, account balances, etc. It would be very useful to have spoken
dialogue interfaces for such information access tasks. We identify portability,
usability, robustness, and extensibility as the four primary design objectives
for such systems. In other words, the objective is to develop a PURE (Portable,
Usable, Robust, Extensible) system. A two-layered dialogue architecture for
spoken dialogue systems is presented where the upper layer is
domain-independent and the lower layer is domain-specific. We are implementing
this architecture in a mixed-initiative system that accesses flight
arrival/departure information from the World Wide Web.
</dc:description>
 <dc:description>Comment: 8 pages, 2 encapsulated postscript figures, uses aclap.sty</dc:description>
 <dc:date>1997-07-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707014</dc:identifier>
 <dc:identifier>Proceedings of the ACL/EACL Workshop on &quot;Interactive Spoken Dialog
  Systems: Bringing Speech and NLP Together in Real Applications,&quot; Madrid,
  Spain, pp. 90-97, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707015</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging Grammatical Functions</dc:title>
 <dc:creator>Brants, Thorsten</dc:creator>
 <dc:creator>Skut, Wojciech</dc:creator>
 <dc:creator>Krenn, Brigitte</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses issues in automated treebank construction. We show how
standard part-of-speech tagging techniques extend to the more general problem
of structural annotation, especially for determining grammatical functions and
syntactic categories. Annotation is viewed as an interactive process where
manual and automatic processing alternate. Efficiency and accuracy results are
presented. We also discuss further automation steps.
</dc:description>
 <dc:description>Comment: 11 pages, LaTeX, uses aclap.sty, psfig.sty, and rotate.sty</dc:description>
 <dc:date>1997-07-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707015</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707016</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On aligning trees</dc:title>
 <dc:creator>Calder, Jo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The increasing availability of corpora annotated for linguistic structure
prompts the question: if we have the same texts, annotated for phrase structure
under two different schemes, to what extent do the annotations agree on
structuring within the text? We suggest the term tree alignment to indicate the
situation where two markup schemes choose to bracket off the same text
elements. We propose a general method for determining agreement between two
analyses. We then describe an efficient implementation, which is also modular
in that the core of the implementation can be reused regardless of the format
of markup used in the corpora. The output of the implementation on the Susanne
and Penn treebank corpora is discussed.
</dc:description>
 <dc:description>Comment: 6 pages, uses psfig.tex</dc:description>
 <dc:date>1997-07-25</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707016</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707017</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Stochastic phonological grammars and acceptability</dc:title>
 <dc:creator>Coleman, John</dc:creator>
 <dc:creator>Pierrehumbert, Janet</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In foundational works of generative phonology it is claimed that subjects can
reliably discriminate between possible but non-occurring words and words that
could not be English. In this paper we examine the use of a probabilistic
phonological parser for words to model experimentally-obtained judgements of
the acceptability of a set of nonsense words. We compared various methods of
scoring the goodness of the parse as a predictor of acceptability. We found
that the probability of the worst part is not the best score of acceptability,
indicating that classical generative phonology and Optimality Theory miss an
important fact, as these approaches do not recognise a mechanism by which the
frequency of well-formed parts may ameliorate the unacceptability of
low-frequency parts. We argue that probabilistic generative grammars are
demonstrably a more psychologically realistic model of phonological competence
than standard generative phonology or Optimality Theory.
</dc:description>
 <dc:description>Comment: compressed postscript, 8 pages, 1 figure</dc:description>
 <dc:date>1997-07-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707017</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707018</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multilingual phonological analysis and speech synthesis</dc:title>
 <dc:creator>Coleman, John</dc:creator>
 <dc:creator>Dirksen, Arthur</dc:creator>
 <dc:creator>Hussain, Sarmad</dc:creator>
 <dc:creator>Waals, Juliette</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We give an overview of multilingual speech synthesis using the IPOX system.
The first part discusses work in progress for various languages: Tashlhit
Berber, Urdu and Dutch. The second part discusses a multilingual phonological
grammar, which can be adapted to a particular language by setting parameters
and adding language-specific details.
</dc:description>
 <dc:description>Comment: tex file, 1 postscript figure. Paper is from Proceedings of the 2nd
  meeting of ACL SIGPHON, Santa Cruz, 1997</dc:description>
 <dc:date>1997-07-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707018</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707019</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Generating Coherent Messages in Real-time Decision Support: Exploiting
  Discourse Theory for Discourse Practice</dc:title>
 <dc:creator>Carberry, Sandra</dc:creator>
 <dc:creator>Harvey, Terrence</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a message planner, TraumaGEN, that draws on rhetorical
structure and discourse theory to address the problem of producing integrated
messages from individual critiques, each of which is designed to achieve its
own communicative goal. TraumaGEN takes into account the purpose of the
messages, the situation in which the messages will be received, and the social
role of the system.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-07-28</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707019</dc:identifier>
 <dc:identifier>Proceedings of the 19th Annual Conference of the Cognitive Science
  Society (1997)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9707020</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Czech Morphological Lexicon</dc:title>
 <dc:creator>Skoumalova, Hana</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, a treatment of Czech phonological rules in two-level
morphology approach is described. First the possible phonological alternations
in Czech are listed and then their treatment in a practical application of a
Czech morphological lexicon.
</dc:description>
 <dc:description>Comment: 7 pages, A4, aclap.sty</dc:description>
 <dc:date>1997-07-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9707020</dc:identifier>
 <dc:identifier>Proceedings of the Third Meeting of the ACL Special Interest Group
  in Computational Phonology, pp. 41-47, Madrid, July 1997. ACL</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Expectations in Incremental Discourse Processing</dc:title>
 <dc:creator>Cristea, Dan</dc:creator>
 <dc:creator>Webber, Bonnie Lynn</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The way in which discourse features express connections back to the previous
discourse has been described in the literature in terms of adjoining at the
right frontier of discourse structure. But this does not allow for discourse
features that express expectations about what is to come in the subsequent
discourse. After characterizing these expectations and their distribution in
text, we show how an approach that makes use of substitution as well as
adjoining on a suitably defined right frontier, can be used to both process
expectations and constrain discouse processing in general.
</dc:description>
 <dc:description>Comment: 9 pages, uses aclap.sty, psfig.tex</dc:description>
 <dc:date>1997-08-05</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708001</dc:identifier>
 <dc:identifier>Proceedings 35th Annual ACL, Madrid - June 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Natural Language Generation in Healthcare: Brief Review</dc:title>
 <dc:creator>Cawsey, Alison J.</dc:creator>
 <dc:creator>Webber, Bonnie L.</dc:creator>
 <dc:creator>Jones, Ray B.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Good communication is vital in healthcare, both among healthcare
professionals, and between healthcare professionals and their patients. And
well-written documents, describing and/or explaining the information in
structured databases may be easier to comprehend, more edifying and even more
convincing, than the structured data, even when presented in tabular or graphic
form. Documents may be automatically generated from structured data, using
techniques from the field of natural language generation. These techniques are
concerned with how the content, organisation and language used in a document
can be dynamically selected, depending on the audience and context. They have
been used to generate health education materials, explanations and critiques in
decision support systems, and medical reports and progress notes.
</dc:description>
 <dc:description>Comment: 15 pages, to appear in the Journal of the American Medical
  Informatics Association</dc:description>
 <dc:date>1997-08-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Structure and Ostension in the Interpretation of Discourse Deixis</dc:title>
 <dc:creator>Webber, Bonnie L.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper examines demonstrative pronouns used as deictics to refer to the
interpretation of one or more clauses. Although this usage is frowned upon in
style manuals (for example Strunk and White (1959) state that ``This. The
pronoun 'this', referring to the complete sense of a preceding sentence or
clause, cannot always carry the load and so may produce an imprecise
statement.''), it is nevertheless very common in written text. Handling this
usage poses a problem for Natural Language Understanding systems. The solution
I propose is based on distinguishing between what can be pointed to and what
can be referred to by virtue of pointing. I argue that a restricted set of
discourse segments yield what such demonstrative pronouns can point to and a
restricted set of what Nunberg (1979) has called referring functions yield what
they can refer to by virtue of that pointing.
</dc:description>
 <dc:description>Comment: 22 pages, uses psfig</dc:description>
 <dc:date>1997-08-07</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708003</dc:identifier>
 <dc:identifier>Language and Cognitive Processes 6(2), May 1991, pp. 107-135</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708004</identifier>
 <datestamp>2009-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Epistemic NP Modifiers</dc:title>
 <dc:creator>Abusch, Dorit</dc:creator>
 <dc:creator>Rooth, Mats</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The paper considers participles such as &quot;unknown&quot;, &quot;identified&quot; and
&quot;unspecified&quot;, which in sentences such as &quot;Solange is staying in an unknown
hotel&quot; have readings equivalent to an indirect question &quot;Solange is staying in
a hotel, and it is not known which hotel it is.&quot; We discuss phenomena including
disambiguation of quantifier scope and a restriction on the set of determiners
which allow the reading in question. Epistemic modifiers are analyzed in a DRT
framework with file (information state) discourse referents. The proposed
semantics uses a predication on files and discourse referents which is related
to recent developments in dynamic modal predicate calculus. It is argued that a
compositional DRT semantics must employ a semantic type of discourse referents,
as opposed to just a type of individuals. A connection is developed between the
scope effects of epistemic modifiers and the scope-disambiguating effect of &quot;a
certain&quot;.
</dc:description>
 <dc:description>Comment: Final pre-publication version, 27 pages, Postscript. Final version
  appears in the proceedings of SALT VII</dc:description>
 <dc:date>1997-08-06</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Centering, Anaphora Resolution, and Discourse Structure</dc:title>
 <dc:creator>Walker, Marilyn A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Centering was formulated as a model of the relationship between attentional
state, the form of referring expressions, and the coherence of an utterance
within a discourse segment (Grosz, Joshi and Weinstein, 1986; Grosz, Joshi and
Weinstein, 1995). In this chapter, I argue that the restriction of centering to
operating within a discourse segment should be abandoned in order to integrate
centering with a model of global discourse structure. The within-segment
restriction causes three problems. The first problem is that centers are often
continued over discourse segment boundaries with pronominal referring
expressions whose form is identical to those that occur within a discourse
segment. The second problem is that recent work has shown that listeners
perceive segment boundaries at various levels of granularity. If centering
models a universal processing phenomenon, it is implausible that each listener
is using a different centering algorithm.The third issue is that even for
utterances within a discourse segment, there are strong contrasts between
utterances whose adjacent utterance within a segment is hierarchically recent
and those whose adjacent utterance within a segment is linearly recent. This
chapter argues that these problems can be eliminated by replacing Grosz and
Sidner's stack model of attentional state with an alternate model, the cache
model. I show how the cache model is easily integrated with the centering
algorithm, and provide several types of data from naturally occurring
discourses that support the proposed integrated model. Future work should
provide additional support for these claims with an examination of a larger
corpus of naturally occurring discourses.
</dc:description>
 <dc:description>Comment: 35 pages, uses elsart12, lingmacros, named, psfig</dc:description>
 <dc:date>1997-08-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708005</dc:identifier>
 <dc:identifier>Centering In Discourse, eds. Marilyn A. Walker, Aravind K. Joshi
  and Ellen F. Prince, Oxford University Press, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Global Thresholding and Multiple Pass Parsing</dc:title>
 <dc:creator>Goodman, Joshua</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present a variation on classic beam thresholding techniques that is up to
an order of magnitude faster than the traditional method, at the same
performance level. We also present a new thresholding technique, global
thresholding, which, combined with the new beam thresholding, gives an
additional factor of two improvement, and a novel technique, multiple pass
parsing, that can be combined with the others to yield yet another 50%
improvement. We use a new search algorithm to simultaneously optimize the
thresholding parameters of the various algorithms.
</dc:description>
 <dc:description>Comment: Fixed latex errors; fixed minor errors in published version</dc:description>
 <dc:date>1997-08-13</dc:date>
 <dc:date>1997-08-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708006</dc:identifier>
 <dc:identifier>Proceedings of the Second Conference on Empirical Methods in
  Natural Language Processing, 11-25</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A complexity measure for diachronic Chinese phonology</dc:title>
 <dc:creator>Raman, Anand</dc:creator>
 <dc:creator>Newman, John</dc:creator>
 <dc:creator>Patrick, Jon</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses the problem of deriving distance measures between parent
and daughter languages with specific relevance to historical Chinese phonology.
The diachronic relationship between the languages is modelled as a
Probabilistic Finite State Automaton. The Minimum Message Length principle is
then employed to find the complexity of this structure. The idea is that this
measure is representative of the amount of dissimilarity between the two
languages.
</dc:description>
 <dc:description>Comment: uses psfig.sty, aclap.sty, fullname.bst, 9 pages, 2 ps figures</dc:description>
 <dc:date>1997-08-13</dc:date>
 <dc:date>1997-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication</dc:title>
 <dc:creator>Lee, Lillian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG
parsing. We prove a dual result: CFG parsers running in time $O(|G||w|^{3 -
\myeps})$ on a grammar $G$ and a string $w$ can be used to multiply $m \times
m$ Boolean matrices in time $O(m^{3 - \myeps/3})$. In the process we also
provide a formal definition of parsing motivated by an informal notion due to
Lang. Our result establishes one of the first limitations on general CFG
parsing: a fast, practical CFG parser would yield a fast, practical BMM
algorithm, which is not believed to exist.
</dc:description>
 <dc:description>Comment: 6 pages, uses aclap.sty and eepic.sty</dc:description>
 <dc:date>1997-08-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708008</dc:identifier>
 <dc:identifier>Proceedings of the 35th ACL/8th EACL, pp 9-15</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>DIA-MOLE: An Unsupervised Learning Approach to Adaptive Dialogue Models
  for Spoken Dialogue Systems</dc:title>
 <dc:creator>Moeller, Jens-Uwe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The DIAlogue MOdel Learning Environment supports an engineering-oriented
approach towards dialogue modelling for a spoken-language interface. Major
steps towards dialogue models is to know about the basic units that are used to
construct a dialogue model and possible sequences. In difference to many other
approaches a set of dialogue acts is not predefined by any theory or manually
during the engineering process, but is learned from data that are available in
an avised spoken dialogue system. The architecture is outlined and the approach
is applied to the domain of appointment scheduling. Even though based on a word
correctness of about 70% predictability of dialogue acts in DIA-MOLE turns out
to be comparable to human-assigned dialogue acts.
</dc:description>
 <dc:description>Comment: Postscript only</dc:description>
 <dc:date>1997-08-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708009</dc:identifier>
 <dc:identifier>Proc. EUROSPEECH 97, Rhodes, Greek, 2271-2274</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Similarity-Based Methods For Word Sense Disambiguation</dc:title>
 <dc:creator>Dagan, Ido</dc:creator>
 <dc:creator>Lee, Lillian</dc:creator>
 <dc:creator>Pereira, Fernando</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We compare four similarity-based estimation methods against back-off and
maximum-likelihood estimation methods on a pseudo-word sense disambiguation
task in which we controlled for both unigram and bigram frequency. The
similarity-based methods perform up to 40% better on this particular task. We
also conclude that events that occur only once in the training set have major
impact on similarity-based estimates.
</dc:description>
 <dc:description>Comment: 7 pages, uses psfig.tex and aclap.sty</dc:description>
 <dc:date>1997-08-18</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708010</dc:identifier>
 <dc:identifier>Proceedings of the 35th ACL/8th EACL, pp 56--63</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Similarity-Based Approaches to Natural Language Processing</dc:title>
 <dc:creator>Lee, Lillian</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This thesis presents two similarity-based approaches to sparse data problems.
The first approach is to build soft, hierarchical clusters: soft, because each
event belongs to each cluster with some probability; hierarchical, because
cluster centroids are iteratively split to model finer distinctions. Our second
approach is a nearest-neighbor approach: instead of calculating a centroid for
each class, as in the hierarchical clustering approach, we in essence build a
cluster around each word. We compare several such nearest-neighbor approaches
on a word sense disambiguation task and find that as a whole, their performance
is far superior to that of standard methods. In another set of experiments, we
show that using estimation techniques based on the nearest-neighbor model
enables us to achieve perplexity reductions of more than 20 percent over
standard techniques in the prediction of low-frequency events, and
statistically significant speech recognition error-rate reduction.
</dc:description>
 <dc:description>Comment: 71 pages (single-spaced)</dc:description>
 <dc:date>1997-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Encoding Frequency Information in Lexicalized Grammars</dc:title>
 <dc:creator>Carroll, John</dc:creator>
 <dc:creator>Weir, David</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We address the issue of how to associate frequency information with
lexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as a
representative framework. We consider systematically a number of alternative
probabilistic frameworks, evaluating their adequacy from both a theoretical and
empirical perspective using data from existing large treebanks. We also propose
three orthogonal approaches for backing off probability estimates to cope with
the large number of parameters involved.
</dc:description>
 <dc:description>Comment: 10 pages, uses fullname.sty</dc:description>
 <dc:date>1997-08-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708012</dc:identifier>
 <dc:identifier>5th International Workshop on Parsing Technologies (IWPT-97)</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9708013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>explanation-based learning of data oriented parsing</dc:title>
 <dc:creator>Sima'an, Khalil</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a new view of Explanation-Based Learning (EBL) of natural
language parsing. Rather than employing EBL for specializing parsers by
inferring new ones, this paper suggests employing EBL for learning how to
reduce ambiguity only partially.
  The present method consists of an EBL algorithm for learning partial-parsers,
and a parsing algorithm which combines partial-parsers with existing
``full-parsers&quot;. The learned partial-parsers, implementable as Cascades of
Finite State Transducers (CFSTs), recognize and combine constituents
efficiently, prohibiting spurious overgeneration. The parsing algorithm
combines a learned partial-parser with a given full-parser such that the role
of the full-parser is limited to combining the constituents, recognized by the
partial-parser, and to recognizing unrecognized portions of the input sentence.
Besides the reduction of the parse-space prior to disambiguation, the present
method provides a way for refining existing disambiguation models that learn
stochastic grammars from tree-banks.
  We exhibit encouraging empirical results using a pilot implementation:
parse-space is reduced substantially with minimal loss of coverage. The speedup
gain for disambiguation models is exemplified by experiments with the DOP
model.
</dc:description>
 <dc:description>Comment: Appeared in Proceedings of Computational Natural Language Learning
  (CoNLL97), ACL-EACL'97, Madrid, Spain</dc:description>
 <dc:date>1997-08-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9708013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709001</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The Complexity of Recognition of Linguistically Adequate Dependency
  Grammars</dc:title>
 <dc:creator>Neuhaus, Peter</dc:creator>
 <dc:creator>Broeker, Norbert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Results of computational complexity exist for a wide range of phrase
structure-based grammar formalisms, while there is an apparent lack of such
results for dependency-based formalisms. We here adapt a result on the
complexity of ID/LP-grammars to the dependency framework. Contrary to previous
studies on heavily restricted dependency grammars, we prove that recognition
(and thus, parsing) of linguistically adequate dependency grammars is
NP-complete.
</dc:description>
 <dc:description>Comment: 8 pages, requires LaTeX2e, epsfig, latexsym, amsmath</dc:description>
 <dc:date>1997-09-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709001</dc:identifier>
 <dc:identifier>Proc. ACL-EACL 1997, Madrid, Spain, pp.337-343</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709002</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Methods for Combining Linguistic Indicators to Classify Verbs</dc:title>
 <dc:creator>Siegel, Eric V.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Fourteen linguistically-motivated numerical indicators are evaluated for
their ability to categorize verbs as either states or events. The values for
each indicator are computed automatically across a corpus of text. To improve
classification performance, machine learning techniques are employed to combine
multiple indicators. Three machine learning methods are compared for this task:
decision tree induction, a genetic algorithm, and log-linear regression.
</dc:description>
 <dc:description>Comment: 7 pages, Latex, in the Proceedings of the Second Conference on
  Empirical Methods in Natural Language Processing, Providence, Rhode Island,
  1997</dc:description>
 <dc:date>1997-09-12</dc:date>
 <dc:date>1997-09-13</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709003</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Combining Multiple Methods for the Automatic Construction of
  Multilingual WordNets</dc:title>
 <dc:creator>Atserias, Jordi</dc:creator>
 <dc:creator>Climent, Salvador</dc:creator>
 <dc:creator>Farreres, Xavier</dc:creator>
 <dc:creator>Rigau, German</dc:creator>
 <dc:creator>Rodriguez, Horacio</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper explores the automatic construction of a multilingual Lexical
Knowledge Base from preexisting lexical resources. First, a set of automatic
and complementary techniques for linking Spanish words collected from
monolingual and bilingual MRDs to English WordNet synsets are described.
Second, we show how resulting data provided by each method is then combined to
produce a preliminary version of a Spanish WordNet with an accuracy over 85%.
The application of these combinations results on an increment of the extracted
connexions of a 40% without losing accuracy. Both coarse-grained (class level)
and fine-grained (synset assignment level) confidence ratios are used and
evaluated. Finally, the results for the whole process are presented.
</dc:description>
 <dc:description>Comment: 7 pages, 4 postscript figures</dc:description>
 <dc:date>1997-09-15</dc:date>
 <dc:date>1997-09-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709003</dc:identifier>
 <dc:identifier>RANLP'97 Bulgaria</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709004</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Integrating a Lexical Database and a Training Collection for Text
  Categorization</dc:title>
 <dc:creator>Hidalgo, Jose Maria Gomez</dc:creator>
 <dc:creator>Rodriguez, Manuel de Buenaga</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Automatic text categorization is a complex and useful task for many natural
language processing applications. Recent approaches to text categorization
focus more on algorithms than on resources involved in this operation. In
contrast to this trend, we present an approach based on the integration of
widely available resources as lexical databases and training collections to
overcome current limitations of the task. Our approach makes use of WordNet
synonymy information to increase evidence for bad trained categories. When
testing a direct categorization, a WordNet based one, a training algorithm, and
our integrated approach, the latter exhibits a better perfomance than any of
the others. Incidentally, WordNet based approach perfomance is comparable with
the training approach one.
</dc:description>
 <dc:description>Comment: 12 pages, 3 figures (2 tables)</dc:description>
 <dc:date>1997-09-15</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709004</dc:identifier>
 <dc:identifier>ACL/EACL Workshop on Automatic Extraction and Building of Lexical
  Semantic Resources for Natural Language Applications, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709005</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A generation algorithm for f-structure representations</dc:title>
 <dc:creator>Tuells, Toni</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows that previously reported generation algorithms run into
problems when dealing with f-structure representations. A generation algorithm
that is suitable for this type of representations is presented: the Semantic
Kernel Generation (SKG) algorithm. The SKG method has the same processing
strategy as the Semantic Head Driven generation (SHDG) algorithm and relies on
the assumption that it is possible to compute the Semantic Kernel (SK) and non
Semantic Kernel (Non-SK) information for each input structure.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-09-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709005</dc:identifier>
 <dc:identifier>In the Proceedings of RANLP'97 (pages 270-275), Tzigov Chark,
  Bulgaria, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709006</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semantic Processing of Out-Of-Vocabulary Words in a Spoken Dialogue
  System</dc:title>
 <dc:creator>Boros, Manuela</dc:creator>
 <dc:creator>Aretoulaki, Maria</dc:creator>
 <dc:creator>Gallwitz, Florian</dc:creator>
 <dc:creator>Noeth, Elmar</dc:creator>
 <dc:creator>Niemann, Heinrich</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  One of the most important causes of failure in spoken dialogue systems is
usually neglected: the problem of words that are not covered by the system's
vocabulary (out-of-vocabulary or OOV words). In this paper a methodology is
described for the detection, classification and processing of OOV words in an
automatic train timetable information system. The various extensions that had
to be effected on the different modules of the system are reported, resulting
in the design of appropriate dialogue strategies, as are encouraging evaluation
results on the new versions of the word recogniser and the linguistic
processor.
</dc:description>
 <dc:description>Comment: 4 pages, 2 eps figures, requires LaTeX2e, uses eurospeech.sty and
  epsfig</dc:description>
 <dc:date>1997-09-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709006</dc:identifier>
 <dc:identifier>Proceedings of EUROSPEECH'97, Vol.4, pp.1887-1890, Rhodes, Greece</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709007</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using WordNet to Complement Training Information in Text Categorization</dc:title>
 <dc:creator>Rodriguez, Manuel de Buenaga</dc:creator>
 <dc:creator>Hidalgo, Jose Maria Gomez</dc:creator>
 <dc:creator>Agudo, Belen Diaz</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Automatic Text Categorization (TC) is a complex and useful task for many
natural language applications, and is usually performed through the use of a
set of manually classified documents, a training collection. We suggest the
utilization of additional resources like lexical databases to increase the
amount of information that TC systems make use of, and thus, to improve their
performance. Our approach integrates WordNet information with two training
approaches through the Vector Space Model. The training approaches we test are
the Rocchio (relevance feedback) and the Widrow-Hoff (machine learning)
algorithms. Results obtained from evaluation show that the integration of
WordNet clearly outperforms training approaches, and that an integrated
technique can effectively address the classification of low frequency
categories.
</dc:description>
 <dc:description>Comment: 16 pages, 1 figure, 3 tables, previously with RANLP latext style</dc:description>
 <dc:date>1997-09-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709007</dc:identifier>
 <dc:identifier>Second International Conference on Recent Advances in Natural
  Language Processing, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709008</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy</dc:title>
 <dc:creator>Jiang, Jay J.</dc:creator>
 <dc:creator>Conrath, David W.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a new approach for measuring semantic similarity/distance
between words and concepts. It combines a lexical taxonomy structure with
corpus statistical information so that the semantic distance between nodes in
the semantic space constructed by the taxonomy can be better quantified with
the computational evidence derived from a distributional analysis of corpus
data. Specifically, the proposed measure is a combined approach that inherits
the edge-based approach of the edge counting scheme, which is then enhanced by
the node-based approach of the information content calculation. When tested on
a common data set of word pair similarity ratings, the proposed approach
outperforms other computational models. It gives the highest correlation value
(r = 0.828) with a benchmark based on human similarity judgements, whereas an
upper bound (r = 0.885) is observed when human subjects replicate the same
task.
</dc:description>
 <dc:description>Comment: 15 pages, Postscript only</dc:description>
 <dc:date>1997-09-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709008</dc:identifier>
 <dc:identifier>In the Proceedings of ROCLING X, Taiwan, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709009</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Evaluating Parsing Schemes with Entropy Indicators</dc:title>
 <dc:creator>Lyon, Caroline</dc:creator>
 <dc:creator>Brown, Stephen</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces an objective metric for evaluating a parsing scheme. It
is based on Shannon's original work with letter sequences, which can be
extended to part-of-speech tag sequences. It is shown that this regular
language is an inadequate model for natural language, but a representation is
used that models language slightly higher in the Chomsky hierarchy.
  We show how the entropy of parsed and unparsed sentences can be measured. If
the entropy of the parsed sentence is lower, this indicates that some of the
structure of the language has been captured.
  We apply this entropy indicator to support one particular parsing scheme that
effects a top down segmentation. This approach could be used to decompose the
parsing task into computationally more tractable subtasks. It also lends itself
to the extraction of predicate/argument structure.
</dc:description>
 <dc:description>Comment: 7 pages. LaTeX format, epsfig used, .sty file included</dc:description>
 <dc:date>1997-09-22</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709009</dc:identifier>
 <dc:identifier>5th Meeting on Mathematics of Language, MOL5, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709010</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Message-Passing Protocols for Real-World Parsing -- An Object-Oriented
  Model and its Preliminary Evaluation</dc:title>
 <dc:creator>Hahn, Udo</dc:creator>
 <dc:creator>Neuhaus, Peter</dc:creator>
 <dc:creator>Broeker, Norbert</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We argue for a performance-based design of natural language grammars and
their associated parsers in order to meet the constraints imposed by real-world
NLP. Our approach incorporates declarative and procedural knowledge about
language and language use within an object-oriented specification framework. We
discuss several message-passing protocols for parsing and provide reasons for
sacrificing completeness of the parse in favor of efficiency based on a
preliminary empirical evaluation.
</dc:description>
 <dc:description>Comment: 12 pages, uses epsfig.sty</dc:description>
 <dc:date>1997-09-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709010</dc:identifier>
 <dc:identifier>Proc. Int'l Workshop on Parsing Technologies, 1997, Boston/MA:
  MIT, pp 101-112</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709011</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Off-line Parsability and the Well-foundedness of Subsumption</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Typed feature structures are used extensively for the specification of
linguistic information in many formalisms. The subsumption relation orders TFSs
by their information content. We prove that subsumption of acyclic TFSs is
well-founded, whereas in the presence of cycles general TFS subsumption is not
well-founded. We show an application of this result for parsing, where the
well-foundedness of subsumption is used to guarantee termination for grammars
that are off-line parsable. We define a new version of off-line parsability
that is less strict than the existing one; thus termination is guaranteed for
parsing with a larger set of grammars.
</dc:description>
 <dc:description>Comment: 19 pages, 1 postscript figure, uses fullname.sty</dc:description>
 <dc:date>1997-09-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709012</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Using Single Layer Networks for Discrete, Sequential Data: An Example
  from Natural Language Processing</dc:title>
 <dc:creator>Lyon, Caroline</dc:creator>
 <dc:creator>Frank, Ray</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A natural language parser which has been successfully implemented is
described. This is a hybrid system, in which neural networks operate within a
rule based framework. It can be accessed via telnet for users to try on their
own text. (For details, contact the author.) Tested on technical manuals, the
parser finds the subject and head of the subject in over 90% of declarative
sentences.
  The neural processing components belong to the class of Generalized Single
Layer Networks (GSLN). In general, supervised, feed-forward networks need more
than one layer to process data. However, in some cases data can be
pre-processed with a non-linear transformation, and then presented in a
linearly separable form for subsequent processing by a single layer net. Such
networks offer advantages of functional transparency and operational speed.
  For our parser, the initial stage of processing maps linguistic data onto a
higher order representation, which can then be analysed by a single layer
network. This transformation is supported by information theoretic analysis.
</dc:description>
 <dc:description>Comment: 28 pages, 9 figures, Latex format, uses epsfig, .styfile included</dc:description>
 <dc:date>1997-09-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709012</dc:identifier>
 <dc:identifier>Neural Computing and Applications 5(4), 1997, 196-214</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709013</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>An Abstract Machine for Unification Grammars</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This work describes the design and implementation of an abstract machine,
Amalia, for the linguistic formalism ALE, which is based on typed feature
structures. This formalism is one of the most widely accepted in computational
linguistics and has been used for designing grammars in various linguistic
theories, most notably HPSG. Amalia is composed of data structures and a set of
instructions, augmented by a compiler from the grammatical formalism to the
abstract instructions, and a (portable) interpreter of the abstract
instructions. The effect of each instruction is defined using a low-level
language that can be executed on ordinary hardware.
  The advantages of the abstract machine approach are twofold. From a
theoretical point of view, the abstract machine gives a well-defined
operational semantics to the grammatical formalism. This ensures that grammars
specified using our system are endowed with well defined meaning. It enables,
for example, to formally verify the correctness of a compiler for HPSG, given
an independent definition. From a practical point of view, Amalia is the first
system that employs a direct compilation scheme for unification grammars that
are based on typed feature structures. The use of amalia results in a much
improved performance over existing systems.
  In order to test the machine on a realistic application, we have developed a
small-scale, HPSG-based grammar for a fragment of the Hebrew language, using
Amalia as the development platform. This is the first application of HPSG to a
Semitic language.
</dc:description>
 <dc:description>Comment: Doctoral Thesis, 96 pages, many postscript figures, uses pstricks,
  pst-node, psfig, fullname and a macros file</dc:description>
 <dc:date>1997-09-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709014</identifier>
 <datestamp>2008-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Amalia -- A Unified Platform for Parsing and Generation</dc:title>
 <dc:creator>Wintner, Shuly</dc:creator>
 <dc:creator>Gabrilovich, Evgeniy</dc:creator>
 <dc:creator>Francez, Nissim</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Contemporary linguistic theories (in particular, HPSG) are declarative in
nature: they specify constraints on permissible structures, not how such
structures are to be computed. Grammars designed under such theories are,
therefore, suitable for both parsing and generation. However, practical
implementations of such theories don't usually support bidirectional processing
of grammars. We present a grammar development system that includes a compiler
of grammars (for parsing and generation) to abstract machine instructions, and
an interpreter for the abstract machine language. The generation compiler
inverts input grammars (designed for parsing) to a form more suitable for
generation. The compiled grammars are then executed by the interpreter using
one control strategy, regardless of whether the grammar is the original or the
inverted version. We thus obtain a unified, efficient platform for developing
reversible grammars.
</dc:description>
 <dc:description>Comment: 8 pages postscript</dc:description>
 <dc:date>1997-09-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709014</dc:identifier>
 <dc:identifier>Proceedings of Recent Advances in Natural Language Programming
  (RANLP97), Tzigov Chark, Bulgaria, 11-13 September 1997, pp. 135-142</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9709015</identifier>
 <datestamp>2016-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Segmentation of Expository Texts by Hierarchical Agglomerative
  Clustering</dc:title>
 <dc:creator>Yaari, Yaakov</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We propose a method for segmentation of expository texts based on
hierarchical agglomerative clustering. The method uses paragraphs as the basic
segments for identifying hierarchical discourse structure in the text, applying
lexical similarity between them as the proximity test. Linear segmentation can
be induced from the identified structure through application of two simple
rules. However the hierarchy can be used also for intelligent exploration of
the text. The proposed segmentation algorithm is evaluated against an accepted
linear segmentation method and shows comparable results.
</dc:description>
 <dc:description>Comment: 7 pages, Latex2e, 4 postscript figures</dc:description>
 <dc:date>1997-09-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9709015</dc:identifier>
 <dc:identifier>RANLP'97, Bulgaria</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Use of Weighted Finite State Transducers in Part of Speech Tagging</dc:title>
 <dc:creator>Tzoukermann, Evelyne</dc:creator>
 <dc:creator>Radev, Dragomir R.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses issues in part of speech disambiguation using
finite-state transducers and presents two main contributions to the field. One
of them is the use of finite-state machines for part of speech tagging.
Linguistic and statistical information is represented in terms of weights on
transitions in weighted finite-state transducers. Another contribution is the
successful combination of techniques -- linguistic and statistical -- for word
disambiguation, compounded with the notion of word classes.
</dc:description>
 <dc:description>Comment: uses psfig, ipamacs</dc:description>
 <dc:date>1997-10-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Tagging French Without Lexical Probabilities -- Combining Linguistic
  Knowledge And Statistical Learning</dc:title>
 <dc:creator>Tzoukermann, Evelyne</dc:creator>
 <dc:creator>Radev, Dragomir R.</dc:creator>
 <dc:creator>Gale, William A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper explores morpho-syntactic ambiguities for French to develop a
strategy for part-of-speech disambiguation that a) reflects the complexity of
French as an inflected language, b) optimizes the estimation of probabilities,
c) allows the user flexibility in choosing a tagset. The problem in extracting
lexical probabilities from a limited training corpus is that the statistical
model may not necessarily represent the use of a particular word in a
particular context. In a highly morphologically inflected language, this
argument is particularly serious since a word can be tagged with a large number
of parts of speech. Due to the lack of sufficient training data, we argue
against estimating lexical probabilities to disambiguate parts of speech in
unrestricted texts. Instead, we use the strength of contextual probabilities
along with a feature we call ``genotype'', a set of tags associated with a
word. Using this knowledge, we have built a part-of-speech tagger that combines
linguistic and statistical approaches: contextual information is disambiguated
by linguistic rules and n-gram probabilities on parts of speech only are
estimated in order to disambiguate the remaining ambiguous tags.
</dc:description>
 <dc:description>Comment: uses ypsfig</dc:description>
 <dc:date>1997-10-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Disambiguating with Controlled Disjunctions</dc:title>
 <dc:creator>Blache, Philippe</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we propose a disambiguating technique called controlled
disjunctions. This extension of the so-called named disjunctions relies on the
relations existing between feature values (covariation, control, etc.). We show
that controlled disjunctions can implement different kind of ambiguities in a
consistent and homogeneous way. We describe the integration of controlled
disjunctions into a HPSG feature structure representation. Finally, we present
a direct implementation by means of delayed evaluation and we develop an
example within the functionnal programming paradigm.
</dc:description>
 <dc:description>Comment: requires LaTeX2e, uses tree-dvips, avm</dc:description>
 <dc:date>1997-10-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710003</dc:identifier>
 <dc:identifier>Proceedings of IWPT'97</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Parsing syllables: modeling OT computationally</dc:title>
 <dc:creator>Hammond, Michael</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, I propose to implement syllabification in OT as a parser. I
propose several innovations that result in a finite and small candidate set.
The candidate set problem is handled with several moves: i) MAX and DEP
violations are not hypothesized by the parser, ii) candidates are encoded
locally, and iii) EVAL is applied constraint by constraint.
  The parser I propose is implemented in Prolog. It has a number of desirable
consequences. First, it runs and thus provides an existence proof that
syllabification can be implemented in OT. There are a number of other desirable
consequences as well. First, constraints are implemented as finite-state
transducers. Second, the parser makes several interesting claims about the
phonological properties of so-called nonrecoverable insertions and deletions.
Third, the implementation suggests some particular reformulations of some of
the benchmark constraints in the OT arsenal, e.g. *COMPLEX, PARSE, ONSET, and
NOCODA.
</dc:description>
 <dc:description>Comment: 22pp, postscript</dc:description>
 <dc:date>1997-10-14</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Attaching Multiple Prepositional Phrases: Generalized Backed-off
  Estimation</dc:title>
 <dc:creator>Merlo, Paola</dc:creator>
 <dc:creator>Crocker, Matthew</dc:creator>
 <dc:creator>Berthouzoz, Cathy</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  There has recently been considerable interest in the use of lexically-based
statistical techniques to resolve prepositional phrase attachments. To our
knowledge, however, these investigations have only considered the problem of
attaching the first PP, i.e., in a [V NP PP] configuration. In this paper, we
consider one technique which has been successfully applied to this problem,
backed-off estimation, and demonstrate how it can be extended to deal with the
problem of multiple PP attachment. The multiple PP attachment introduces two
related problems: sparser data (since multiple PPs are naturally rarer), and
greater syntactic ambiguity (more attachment configurations which must be
distinguished). We present and algorithm which solves this problem through
re-use of the relatively rich data obtained from first PP training, in
resolving subsequent PP attachments.
</dc:description>
 <dc:description>Comment: 7 pages, appeared in the EMNLP-2 proceedings</dc:description>
 <dc:date>1997-10-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Learning Features that Predict Cue Usage</dc:title>
 <dc:creator>Di Eugenio, Barbara</dc:creator>
 <dc:creator>Moore, Johanna D.</dc:creator>
 <dc:creator>Paolucci, Massimo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Our goal is to identify the features that predict the occurrence and
placement of discourse cues in tutorial explanations in order to aid in the
automatic generation of explanations. Previous attempts to devise rules for
text generation were based on intuition or small numbers of constructed
examples. We apply a machine learning program, C4.5, to induce decision trees
for cue occurrence and placement from a corpus of data coded for a variety of
features previously thought to affect cue usage. Our experiments enable us to
identify the features with most predictive power, and show that machine
learning can be used to induce decision trees useful for text generation.
</dc:description>
 <dc:description>Comment: 10 pages, 2 Postscript figures, uses aclap.sty, psfig.tex</dc:description>
 <dc:date>1997-10-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710006</dc:identifier>
 <dc:identifier>Proceedings of ACL/EACL97, Madrid, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Corpus-Based Investigation of Definite Description Use</dc:title>
 <dc:creator>Poesio, Massimo</dc:creator>
 <dc:creator>Vieira, Renata</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We present the results of a study of definite descriptions use in written
texts aimed at assessing the feasibility of annotating corpora with information
about definite description interpretation. We ran two experiments, in which
subjects were asked to classify the uses of definite descriptions in a corpus
of 33 newspaper articles, containing a total of 1412 definite descriptions. We
measured the agreement among annotators about the classes assigned to definite
descriptions, as well as the agreement about the antecedent assigned to those
definites that the annotators classified as being related to an antecedent in
the text. The most interesting result of this study from a corpus annotation
perspective was the rather low agreement (K=0.63) that we obtained using
versions of Hawkins' and Prince's classification schemes; better results
(K=0.76) were obtained using the simplified scheme proposed by Fraurud that
includes only two classes, first-mention and subsequent-mention. The agreement
about antecedents was also not complete. These findings raise questions
concerning the strategy of evaluating systems for definite description
interpretation by comparing their results with a standardized annotation. From
a linguistic point of view, the most interesting observations were the great
number of discourse-new definites in our corpus (in one of our experiments,
about 50% of the definites in the collection were classified as discourse-new,
30% as anaphoric, and 18% as associative/bridging) and the presence of
definites which did not seem to require a complete disambiguation.
</dc:description>
 <dc:description>Comment: 47 pages, uses fullname.sty and palatino.sty</dc:description>
 <dc:date>1997-10-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710007</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9710008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Event Categorization</dc:title>
 <dc:creator>Wiebe, Janyce</dc:creator>
 <dc:creator>Bruce, Rebecca</dc:creator>
 <dc:creator>Duan, Lei</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes the automation of a new text categorization task. The
categories assigned in this task are more syntactically, semantically, and
contextually complex than those typically assigned by fully automatic systems
that process unseen test data. Our system for assigning these categories is a
probabilistic classifier, developed with a recent method for formulating a
probabilistic model from a predefined set of potential features. This paper
focuses on feature selection. It presents a number of fully automatic features.
It identifies and evaluates various approaches to organizing collocational
properties into features, and presents the results of experiments covarying
type of organization and type of property. We find that one organization is not
best for all kinds of properties, so this is an experimental parameter worth
investigating in NLP systems. In addition, the results suggest a way to take
advantage of properties that are low frequency but strongly indicative of a
class. The problems of recognizing and organizing the various kinds of
contextual information required to perform a linguistically complex
categorization task have rarely been systematically investigated in NLP.
</dc:description>
 <dc:date>1997-10-30</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9710008</dc:identifier>
 <dc:identifier>Recent Advances in Natural Language Processing (RANLP-97),
  European Commission, DG XIII, Tzigov Chark, Bulgaria, September 1997, pp.
  163--170.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Constraint Logic Programming</dc:title>
 <dc:creator>Riezler, Stefan</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper addresses two central problems for probabilistic processing
models: parameter estimation from incomplete data and efficient retrieval of
most probable analyses. These questions have been answered satisfactorily only
for probabilistic regular and context-free models. We address these problems
for a more expressive probabilistic constraint logic programming model. We
present a log-linear probability model for probabilistic constraint logic
programming. On top of this model we define an algorithm to estimate the
parameters and to select the properties of log-linear models from incomplete
data. This algorithm is an extension of the improved iterative scaling
algorithm of Della-Pietra, Della-Pietra, and Lafferty (1995). Our algorithm
applies to log-linear models in general and is accompanied with suitable
approximation methods when applied to large data spaces. Furthermore, we
present an approach for searching for most probable analyses of the
probabilistic constraint logic programming model. This method can be applied to
the ambiguity resolution problem in natural language processing applications.
</dc:description>
 <dc:description>Comment: 35 pages, uses sfbart.cls</dc:description>
 <dc:date>1997-11-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Approximating Context-Free Grammars with a Finite-State Calculus</dc:title>
 <dc:creator>Grimley-Evans, Edmund</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Although adequate models of human language for syntactic analysis and
semantic interpretation are of at least context-free complexity, for
applications such as speech processing in which speed is important finite-state
models are often preferred. These requirements may be reconciled by using the
more complex grammar to automatically derive a finite-state approximation which
can then be used as a filter to guide speech recognition or to reject many
hypotheses at an early stage of processing. A method is presented here for
calculating such finite-state approximations from context-free grammars. It is
essentially different from the algorithm introduced by Pereira and Wright
(1991; 1996), is faster in some cases, and has the advantage of being
open-ended and adaptable.
</dc:description>
 <dc:description>Comment: 8 pages, LaTeX, 2 PostScript figures, aclap.sty</dc:description>
 <dc:date>1997-11-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711002</dc:identifier>
 <dc:identifier>Proceedings of ACL-EACL 97, Madrid, pp 452-459, 1997.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Probabilistic Parsing Using Left Corner Language Models</dc:title>
 <dc:creator>Manning, Christopher D.</dc:creator>
 <dc:creator>Carpenter, Bob</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We introduce a novel parser based on a probabilistic version of a left-corner
parser. The left-corner strategy is attractive because rule probabilities can
be conditioned on both top-down goals and bottom-up derivations. We develop the
underlying theory and explain how a grammar can be induced from analyzed data.
We show that the left-corner approach provides an advantage over simple
top-down probabilistic context-free grammars in parsing the Wall Street Journal
using a grammar induced from the Penn Treebank. We also conclude that the Penn
Treebank provides a fairly weak testbed due to the flatness of its bracketings
and to the obvious overgeneration and undergeneration of its induced grammar.
</dc:description>
 <dc:description>Comment: 12 pages, uses iwpt97.sty</dc:description>
 <dc:date>1997-11-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711003</dc:identifier>
 <dc:identifier>Proceedings of the Fifth International Workshop on Parsing
  Technologies, MIT, Boston MA, 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Variation and Synthetic Speech</dc:title>
 <dc:creator>Miller, Corey</dc:creator>
 <dc:creator>Karaali, Orhan</dc:creator>
 <dc:creator>Massey, Noel</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe the approach to linguistic variation taken by the Motorola speech
synthesizer. A pan-dialectal pronunciation dictionary is described, which
serves as the training data for a neural network based letter-to-sound
converter. Subsequent to dictionary retrieval or letter-to-sound generation,
pronunciations are submitted a neural network based postlexical module. The
postlexical module has been trained on aligned dictionary pronunciations and
hand-labeled narrow phonetic transcriptions. This architecture permits the
learning of individual postlexical variation, and can be retrained for each
speaker whose voice is being modeled for synthesis. Learning variation in this
way can result in greater naturalness for the synthetic speech that is produced
by the system.
</dc:description>
 <dc:description>Comment: 18 pages, 2 figures</dc:description>
 <dc:date>1997-11-17</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Some apparently disjoint aims and requirements for grammar development
  environments: the case of natural language generation</dc:title>
 <dc:creator>Bateman, John A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Grammar development environments (GDE's) for analysis and for generation have
not yet come together. Despite the fact that analysis-oriented GDE's (such as
ALEP) may include some possibility of sentence generation, the development
techniques and kinds of resources suggested are apparently not those required
for practical, large-scale natural language generation work. Indeed, there is
no use of `standard' (i.e., analysis-oriented) GDE's in current
projects/applications targetting the generation of fluent, coherent texts. This
unsatisfactory situation requires some analysis and explanation, which this
paper attempts using as an example an extensive GDE for generation. The support
provided for distributed large-scale grammar development, multilinguality, and
resource maintenance are discussed and contrasted with analysis-oriented
approaches.
</dc:description>
 <dc:description>Comment: 9 pages, EPS figures, uses: aclap.sty, psfig.sty Paper presented at
  the ACL/EACL'97 Madrid Workshop on Computational Environments for Grammar
  Development and Linguistic Engineering</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711005</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Contextual Information and Specific Language Models for Spoken Language
  Understanding</dc:title>
 <dc:creator>Baggia, Paolo</dc:creator>
 <dc:creator>Danieli, Morena</dc:creator>
 <dc:creator>Gerbino, Elisabetta</dc:creator>
 <dc:creator>Moisa, Loreta M.</dc:creator>
 <dc:creator>Popovici, Cosmin</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper we explain how contextual expectations are generated and used
in the task-oriented spoken language understanding system Dialogos. The hard
task of recognizing spontaneous speech on the telephone may greatly benefit
from the use of specific language models during the recognition of callers'
utterances. By 'specific language models' we mean a set of language models that
are trained on contextually appropriated data, and that are used during
different states of the dialogue on the basis of the information sent to the
acoustic level by the dialogue management module. In this paper we describe how
the specific language models are obtained on the basis of contextual
information. The experimental result we report show that recognition and
understanding performance are improved thanks to the use of specific language
models.
</dc:description>
 <dc:description>Comment: 6 pages, Latex, uses aclap.sty</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711006</dc:identifier>
 <dc:identifier>Proceedings of SPECOM'97, Cluj-Napoca, Romania, pp. 51-56</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Language Modelling For Task-Oriented Domains</dc:title>
 <dc:creator>Popovici, Cosmin</dc:creator>
 <dc:creator>Baggia, Paolo</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper is focused on the language modelling for task-oriented domains and
presents an accurate analysis of the utterances acquired by the Dialogos spoken
dialogue system. Dialogos allows access to the Italian Railways timetable by
using the telephone over the public network. The language modelling aspects of
specificity and behaviour to rare events are studied. A technique for getting a
language model more robust, based on sentences generated by grammars, is
presented. Experimental results show the benefit of the proposed technique. The
increment of performance between language models created using grammars and
usual ones, is higher when the amount of training material is limited.
Therefore this technique can give an advantage especially for the development
of language models in a new domain.
</dc:description>
 <dc:description>Comment: 5 pages, LaTeX, 4 eps figures, uses icassp91.sty, and epsf.tex</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711007</dc:identifier>
 <dc:identifier>Proceedings of EUROSPEECH'97, Rhodes, Greece, vol. 3, pp.
  1459-1462</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>On the use of expectations for detecting and repairing human-machine
  miscommunication</dc:title>
 <dc:creator>Danieli, Morena</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper I describe how miscommunication problems are dealt with in the
spoken language system DIALOGOS. The dialogue module of the system exploits
dialogic expectations in a twofold way: to model what future user utterance
might be about (predictions), and to account how the user's next utterance may
be related to previous ones in the ongoing interaction (pragmatic-based
expectations). The analysis starts from the hypothesis that the occurrence of
miscommunication is concomitant with two pragmatic phenomena: the deviation of
the user from the expected behaviour and the generation of a conversational
implicature. A preliminary evaluation of a large amount of interactions between
subjects and DIALOGOS shows that the system performance is enhanced by the uses
of both predictions and pragmatic-based expectations.
</dc:description>
 <dc:description>Comment: 7 pages, LaTeX, uses aaai.sty</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711008</dc:identifier>
 <dc:identifier>Proceedings of AAAI-96 Workshop on Detecting, Preventing, and
  Repairing Human-Machine Miscommunications, Portland, OR, pp. 87-93</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Towards an Improved Performance Measure for Language Models</dc:title>
 <dc:creator>Ueberla, Joerg P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper a first attempt at deriving an improved performance measure for
language models, the probability ratio measure (PRM) is described. In a proof
of concept experiment, it is shown that PRM correlates better with recognition
accuracy and can lead to better recognition results when used as the
optimisation criterion of a clustering algorithm. Inspite of the approximations
and limitations of this preliminary work, the results are very encouraging and
should justify more work along the same lines.
</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711010</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Application-driven automatic subgrammar extraction</dc:title>
 <dc:creator>Henschel, Renate</dc:creator>
 <dc:creator>Bateman, John A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The space and run-time requirements of broad coverage grammars appear for
many applications unreasonably large in relation to the relative simplicity of
the task at hand. On the other hand, handcrafted development of
application-dependent grammars is in danger of duplicating work which is then
difficult to re-use in other contexts of application. To overcome this problem,
we present in this paper a procedure for the automatic extraction of
application-tuned consistent subgrammars from proved large-scale generation
grammars. The procedure has been implemented for large-scale systemic grammars
and builds on the formal equivalence between systemic grammars and typed
unification based grammars. Its evaluation for the generation of encyclopedia
entries is described, and directions of future development, applicability, and
extensions are discussed.
</dc:description>
 <dc:description>Comment: 8 pages, uses: aclap.sty, epic.sty, put-inserts Paper presented at
  the ACL/EACL'97 Madrid Workshop on Computational Environments for Grammar
  Development and Linguistic Engineering</dc:description>
 <dc:date>1997-11-19</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711011</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>The effect of alternative tree representations on tree bank grammars</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The performance of PCFGs estimated from tree banks is sensitive to the
particular way in which linguistic constructions are represented as trees in
the tree bank. This paper presents a theoretical analysis of the effect of
different tree representations for PP attachment on PCFG models, and introduces
a new methodology for empirically examining such effects using tree
transformations. It shows that one transformation, which copies the label of a
parent node onto the labels of its children, can improve the performance of a
PCFG model in terms of labelled precision and recall on held out data from 73%
(precision) and 69% (recall) to 80% and 79% respectively. It also points out
that if only maximum likelihood parses are of interest then many productions
can be ignored, since they are subsumed by combinations of other productions in
the grammar. In the Penn II tree bank grammar, almost 9% of productions are
subsumed in this way.
</dc:description>
 <dc:description>Comment: Adds missing bibliography (sorry!), 11 pages, to appear in
  Proceedings of NeMLAP 3, uses epic.sty and eepic.sty</dc:description>
 <dc:date>1997-11-20</dc:date>
 <dc:date>1997-11-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711011</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711012</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Proof Nets and the Complexity of Processing Center-Embedded
  Constructions</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper shows how proof nets can be used to formalize the notion of
``incomplete dependency'' used in psycholinguistic theories of the
unacceptability of center-embedded constructions. Such theories of human
language processing can usually be restated in terms of geometrical constraints
on proof nets. The paper ends with a discussion of the relationship between
these constraints and incremental semantic interpretation.
</dc:description>
 <dc:description>Comment: To appear in Proceedings of LACL 95; uses epic.sty, eepic.sty,
  rotate.sty</dc:description>
 <dc:date>1997-11-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711012</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711013</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Features as Resources in R-LFG</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper introduces a non-unification-based version of LFG called R-LFG
(Resource-based Lexical Functional Grammar), which combines elements from both
LFG and Linear Logic. The paper argues that a resource sensitive account
provides a simpler treatment of many linguistic uses of non-monotonic devices
in LFG, such as existential constraints and constraint equations.
</dc:description>
 <dc:description>Comment: 21 pages; appears in Proceedings of LFG-97; uses epic.sty, eepic.sty,
  ipa.sty</dc:description>
 <dc:date>1997-11-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711013</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9711014</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Type-driven semantic interpretation and feature dependencies in R-LFG</dc:title>
 <dc:creator>Johnson, Mark</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Once one has enriched LFG's formal machinery with the linear logic mechanisms
needed for semantic interpretation as proposed by Dalrymple et. al., it is
natural to ask whether these make any existing components of LFG redundant. As
Dalrymple and her colleagues note, LFG's f-structure completeness and coherence
constraints fall out as a by-product of the linear logic machinery they propose
for semantic interpretation, thus making those f-structure mechanisms
redundant. Given that linear logic machinery or something like it is
independently needed for semantic interpretation, it seems reasonable to
explore the extent to which it is capable of handling feature structure
constraints as well.
  R-LFG represents the extreme position that all linguistically required
feature structure dependencies can be captured by the resource-accounting
machinery of a linear or similiar logic independently needed for semantic
interpretation, making LFG's unification machinery redundant. The goal is to
show that LFG linguistic analyses can be expressed as clearly and perspicuously
using the smaller set of mechanisms of R-LFG as they can using the much larger
set of unification-based mechanisms in LFG: if this is the case then we will
have shown that positing these extra f-structure mechanisms is not
linguistically warranted.
</dc:description>
 <dc:description>Comment: 30 pages, to appear in the the ``Glue Language'' volume edited by
  Dalrymple, uses tree-dvips, ipa, epic, eepic, fullname</dc:description>
 <dc:date>1997-11-21</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9711014</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Applying Explanation-based Learning to Control and Speeding-up Natural
  Language Generation</dc:title>
 <dc:creator>Neumann, Guenter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a method for the automatic extraction of subgrammars to
control and speeding-up natural language generation NLG. The method is based on
explanation-based learning (EBL). The main advantage for the proposed new
method for NLG is that the complexity of the grammatical decision making
process during NLG can be vastly reduced, because the EBL method supports the
adaption of a NLG system to a particular use of a language.
</dc:description>
 <dc:description>Comment: 9 pages; in Proc. of ACL-EACL 1997, Madrid, Spain, pp. 214-221</dc:description>
 <dc:date>1997-12-08</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Machine Learning of User Profiles: Representational Issues</dc:title>
 <dc:creator>Bloedorn, Eric</dc:creator>
 <dc:creator>Mani, Inderjeet</dc:creator>
 <dc:creator>MacMillan, T. Richard</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:subject>Computer Science - Learning</dc:subject>
 <dc:description>  As more information becomes available electronically, tools for finding
information of interest to users becomes increasingly important. The goal of
the research described here is to build a system for generating comprehensible
user profiles that accurately capture user interest with minimum user
interaction. The research described here focuses on the importance of a
suitable generalization hierarchy and representation for learning profiles
which are predictively accurate and comprehensible. In our experiments we
evaluated both traditional features based on weighted term vectors as well as
subject features corresponding to categories which could be drawn from a
thesaurus. Our experiments, conducted in the context of a content-based
profiling system for on-line newspapers on the World Wide Web (the IDD News
Browser), demonstrate the importance of a generalization hierarchy and the
promise of combining natural language processing techniques with machine
learning (ML) to address an information retrieval (IR) problem.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-12-09</dc:date>
 <dc:date>1997-12-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Context as a Spurious Concept</dc:title>
 <dc:creator>Hirst, Graeme</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  I take issue with AI formalizations of context, primarily the formalization
by McCarthy and Buvac, that regard context as an undefined primitive whose
formalization can be the same in many different kinds of AI tasks. In
particular, any theory of context in natural language must take the special
nature of natural language into account and cannot regard context simply as an
undefined primitive. I show that there is no such thing as a coherent theory of
context simpliciter -- context pure and simple -- and that context in natural
language is not the same kind of thing as context in KR. In natural language,
context is constructed by the speaker and the interpreter, and both have
considerable discretion in so doing. Therefore, a formalization based on
pre-defined contexts and pre-defined `lifting axioms' cannot account for how
context is used in real-world language.
</dc:description>
 <dc:date>1997-12-09</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712003</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Multi-document Summarization by Graph Search and Matching</dc:title>
 <dc:creator>Mani, Inderjeet</dc:creator>
 <dc:creator>Bloedorn, Eric</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a new method for summarizing similarities and differences in a
pair of related documents using a graph representation for text. Concepts
denoted by words, phrases, and proper names in the document are represented
positionally as nodes in the graph along with edges corresponding to semantic
relations between items. Given a perspective in terms of which the pair of
documents is to be summarized, the algorithm first uses a spreading activation
technique to discover, in each document, nodes semantically related to the
topic. The activated graphs of each document are then matched to yield a graph
corresponding to similarities and differences between the pair, which is
rendered in natural language. An evaluation of these techniques has been
carried out.
</dc:description>
 <dc:description>Comment: 7 pages, 5 Postscript figures, uses aaai.sty</dc:description>
 <dc:date>1997-12-10</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712004</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Topic Graph Generation for Query Navigation: Use of Frequency Classes
  for Topic Extraction</dc:title>
 <dc:creator>Niwa, Yoshiki</dc:creator>
 <dc:creator>Nishioka, Shingo</dc:creator>
 <dc:creator>Iwayama, Makoto</dc:creator>
 <dc:creator>Takano, Akihiko</dc:creator>
 <dc:creator>Nitta, Yoshihiko</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  To make an interactive guidance mechanism for document retrieval systems, we
developed a user-interface which presents users the visualized map of topics at
each stage of retrieval process. Topic words are automatically extracted by
frequency analysis and the strength of the relationships between topic words is
measured by their co-occurrence. A major factor affecting a user's impression
of a given topic word graph is the balance between common topic words and
specific topic words. By using frequency classes for topic word extraction, we
made it possible to select well-balanced set of topic words, and to adjust the
balance of common and specific topic words.
</dc:description>
 <dc:description>Comment: 6 pages, 3 figures</dc:description>
 <dc:date>1997-12-12</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712005</dc:identifier>
 <dc:identifier>Proceedings of NLPRS'97, Natural Language Processing Pacific Rim
  Symposium '97, pages 95-100, Phuket, Thailand, Dec. 1997</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712006</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>&quot;I don't believe in word senses&quot;</dc:title>
 <dc:creator>Kilgarriff, Adam</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Word sense disambiguation assumes word senses. Within the lexicography and
linguistics literature, they are known to be very slippery entities. The paper
looks at problems with existing accounts of `word sense' and describes the
various kinds of ways in which a word's meaning can deviate from its core
meaning. An analysis is presented in which word senses are abstractions from
clusters of corpus citations, in accordance with current lexicographic
practice. The corpus citations, not the word senses, are the basic objects in
the ontology. The corpus citations will be clustered into senses according to
the purposes of whoever or whatever does the clustering. In the absence of such
purposes, word senses do not exist.
  Word sense disambiguation also needs a set of word senses to disambiguate
between. In most recent work, the set has been taken from a general-purpose
lexical resource, with the assumption that the lexical resource describes the
word senses of English/French/..., between which NLP applications will need to
disambiguate. The implication of the paper is, by contrast, that word senses
exist only relative to a task.
</dc:description>
 <dc:description>Comment: 25 pages</dc:description>
 <dc:date>1997-12-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712006</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712007</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Foreground and Background Lexicons and Word Sense Disambiguation for
  Information Extraction</dc:title>
 <dc:creator>Kilgarriff, Adam</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Lexicon acquisition from machine-readable dictionaries and corpora is
currently a dynamic field of research, yet it is often not clear how lexical
information so acquired can be used, or how it relates to structured meaning
representations. In this paper I look at this issue in relation to Information
Extraction (hereafter IE), and one subtask for which both lexical and general
knowledge are required, Word Sense Disambiguation (WSD). The analysis is based
on the widely-used, but little-discussed distinction between an IE system's
foreground lexicon, containing the domain's key terms which map onto the
database fields of the output formalism, and the background lexicon, containing
the remainder of the vocabulary. For the foreground lexicon, human lexicography
is required. For the background lexicon, automatic acquisition is appropriate.
  For the foreground lexicon, WSD will occur as a by-product of finding a
coherent semantic interpretation of the input. WSD techniques as discussed in
recent literature are suited only to the background lexicon. Once the
foreground/background distinction is developed, there is a match between what
is possible, given the state of the art in WSD, and what is required, for
high-quality IE.
</dc:description>
 <dc:description>Comment: 12 pages</dc:description>
 <dc:date>1997-12-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712007</dc:identifier>
 <dc:identifier>Proc. International Workshop on Lexically Driven Information
  Extraction. Frascati, Italy. July 1997. Pp 51--62.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712008</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>What is word sense disambiguation good for?</dc:title>
 <dc:creator>Kilgarriff, Adam</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Word sense disambiguation has developed as a sub-area of natural language
processing, as if, like parsing, it was a well-defined task which was a
pre-requisite to a wide range of language-understanding applications. First, I
review earlier work which shows that a set of senses for a word is only ever
defined relative to a particular human purpose, and that a view of word senses
as part of the linguistic furniture lacks theoretical underpinnings. Then, I
investigate whether and how word sense ambiguity is in fact a problem for
different varieties of NLP application.
</dc:description>
 <dc:description>Comment: 6 pages</dc:description>
 <dc:date>1997-12-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712008</dc:identifier>
 <dc:identifier>Proc. Natural Language Processing Pacific Rim Symposium. Phuket,
  Thailand. December 1997. Pp 209--214.</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712009</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Speech Repairs, Intonational Boundaries and Discourse Markers: Modeling
  Speakers' Utterances in Spoken Dialog</dc:title>
 <dc:creator>Heeman, Peter A.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this thesis, we present a statistical language model for resolving speech
repairs, intonational boundaries and discourse markers. Rather than finding the
best word interpretation for an acoustic signal, we redefine the speech
recognition problem to so that it also identifies the POS tags, discourse
markers, speech repairs and intonational phrase endings (a major cue in
determining utterance units). Adding these extra elements to the speech
recognition problem actually allows it to better predict the words involved,
since we are able to make use of the predictions of boundary tones, discourse
markers and speech repairs to better account for what word will occur next.
Furthermore, we can take advantage of acoustic information, such as silence
information, which tends to co-occur with speech repairs and intonational
phrase endings, that current language models can only regard as noise in the
acoustic signal. The output of this language model is a much fuller account of
the speaker's turn, with part-of-speech assigned to each word, intonation
phrase endings and discourse markers identified, and speech repairs detected
and corrected. In fact, the identification of the intonational phrase endings,
discourse markers, and resolution of the speech repairs allows the speech
recognizer to model the speaker's utterances, rather than simply the words
involved, and thus it can return a more meaningful analysis of the speaker's
turn for later processing.
</dc:description>
 <dc:description>Comment: 280 pages, doctoral dissertation (latex with postscript figures)</dc:description>
 <dc:date>1997-12-23</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712009</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9712010</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Orthographic Structuring of Human Speech and Texts: Linguistic
  Application of Recurrence Quantification Analysis</dc:title>
 <dc:creator>Orsucci, F.</dc:creator>
 <dc:creator>Walter, K.</dc:creator>
 <dc:creator>Giuliani, A.</dc:creator>
 <dc:creator>Webber, Jr., C. L.</dc:creator>
 <dc:creator>Zbilut, J. P.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  A methodology based upon recurrence quantification analysis is proposed for
the study of orthographic structure of written texts. Five different
orthographic data sets (20th century Italian poems, 20th century American
poems, contemporary Swedish poems with their corresponding Italian
translations, Italian speech samples, and American speech samples) were
subjected to recurrence quantification analysis, a procedure which has been
found to be diagnostically useful in the quantitative assessment of ordered
series in fields such as physics, molecular dynamics, physiology, and general
signal processing. Recurrence quantification was developed from recurrence
plots as applied to the analysis of nonlinear, complex systems in the physical
sciences, and is based on the computation of a distance matrix of the elements
of an ordered series (in this case the letters consituting selected speech and
poetic texts). From a strictly mathematical view, the results show the
possibility of demonstrating invariance between different language exemplars
despite the apparent low-level of coding (orthography). Comparison with the
actual texts confirms the ability of the method to reveal recurrent structures,
and their complexity. Using poems as a reference standard for judging speech
complexity, the technique exhibits language independence, order dependence and
freedom from pure statistical characteristics of studied sequences, as well as
consistency with easily identifiable texts. Such studies may provide
phenomenological markers of hidden structure as coded by the purely
orthographic level.
</dc:description>
 <dc:description>Comment: 8 pages, 7 figures, submitted to Intl. J. Chaos Theory Applications</dc:description>
 <dc:date>1997-12-24</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9712010</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Hierarchical Non-Emitting Markov Models</dc:title>
 <dc:creator>Ristad, Eric Sven</dc:creator>
 <dc:creator>Thomas, Robert G.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  We describe a simple variant of the interpolated Markov model with
non-emitting state transitions and prove that it is strictly more powerful than
any Markov model. More importantly, the non-emitting model outperforms the
classic interpolated model on the natural language texts under a wide range of
experimental conditions, with only a modest increase in computational
requirements. The non-emitting model is also much less prone to overfitting.
  Keywords: Markov model, interpolated Markov model, hidden Markov model,
mixture modeling, non-emitting state transitions, state-conditional
interpolation, statistical language model, discrete time series, Brown corpus,
Wall Street Journal.
</dc:description>
 <dc:description>Comment: http://www.cs.princeton.edu/~ristad/papers/pu-544-97.ps.gz</dc:description>
 <dc:date>1998-01-14</dc:date>
 <dc:date>1998-01-20</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9801001</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Identifying Discourse Markers in Spoken Dialog</dc:title>
 <dc:creator>Heeman, Peter A.</dc:creator>
 <dc:creator>Byron, Donna</dc:creator>
 <dc:creator>Allen, James F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In this paper, we present a method for identifying discourse marker usage in
spontaneous speech based on machine learning. Discourse markers are denoted by
special POS tags, and thus the process of POS tagging can be used to identify
discourse markers. By incorporating POS tagging into language modeling,
discourse markers can be identified during speech recognition, in which the
timeliness of the information can be used to help predict the following words.
We contrast this approach with an alternative machine learning approach
proposed by Litman (1996). This paper also argues that discourse markers can be
used to help the hearer predict the role that the upcoming utterance plays in
the dialog. Thus discourse markers should provide valuable evidence for
automatic dialog act prediction.
</dc:description>
 <dc:description>Comment: 8 pages, uses psfig</dc:description>
 <dc:date>1998-01-16</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9801002</dc:identifier>
 <dc:identifier>AAAI 1998 Spring Symposium on Applying Machine Learning to
  Discourse Processing</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801003</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Do not forget: Full memory in memory-based learning of word
  pronunciation</dc:title>
 <dc:creator>Bosch, Antal van den</dc:creator>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  Memory-based learning, keeping full memory of learning material, appears a
viable approach to learning NLP tasks, and is often superior in generalisation
accuracy to eager learning approaches that abstract from learning material.
Here we investigate three partial memory-based learning approaches which remove
from memory specific task instance types estimated to be exceptional. The three
approaches each implement one heuristic function for estimating exceptionality
of instance types: (i) typicality, (ii) class prediction strength, and (iii)
friendly-neighbourhood size. Experiments are performed with the memory-based
learning algorithm IB1-IG trained on English word pronunciation. We find that
removing instance types with low prediction strength (ii) is the only tested
method which does not seriously harm generalisation accuracy. We conclude that
keeping full memory of types rather than tokens, and excluding minority
ambiguities appear to be the only performance-preserving optimisations of
memory-based learning.
</dc:description>
 <dc:description>Comment: uses conll98, epsf, and ipamacs (WSU IPA)</dc:description>
 <dc:date>1998-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9801003</dc:identifier>
 <dc:identifier>Proceedings of NeMLaP3/CoNLL98, 195-204</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801004</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Modularity in inductively-learned word pronunciation systems</dc:title>
 <dc:creator>Bosch, Antal van den</dc:creator>
 <dc:creator>Weijters, Ton</dc:creator>
 <dc:creator>Daelemans, Walter</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  In leading morpho-phonological theories and state-of-the-art text-to-speech
systems it is assumed that word pronunciation cannot be learned or performed
without in-between analyses at several abstraction levels (e.g., morphological,
graphemic, phonemic, syllabic, and stress levels). We challenge this assumption
for the case of English word pronunciation. Using IGTree, an inductive-learning
decision-tree algorithms, we train and test three word-pronunciation systems in
which the number of abstraction levels (implemented as sequenced modules) is
reduced from five, via three, to one. The latter system, classifying letter
strings directly as mapping to phonemes with stress markers, yields
significantly better generalisation accuracies than the two multi-module
systems. Analyses of empirical results indicate that positive utility effects
of sequencing modules are outweighed by cascading errors passed on between
modules.
</dc:description>
 <dc:description>Comment: 10 pages, uses nemlap3.sty and epsf and ipamacs (WSU IPA) macros</dc:description>
 <dc:date>1998-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9801004</dc:identifier>
 <dc:identifier>Proceedings of NeMLaP3/CoNLL98, 185-194</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9801005</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A General, Sound and Efficient Natural Language Parsing Algorithm based
  on Syntactic Constraints Propagation</dc:title>
 <dc:creator>Quesada, Jose F.</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper presents a new context-free parsing algorithm based on a
bidirectional strictly horizontal strategy which incorporates strong top-down
predictions (derivations and adjacencies). From a functional point of view, the
parser is able to propagate syntactic constraints reducing parsing ambiguity.
  From a computational perspective, the algorithm includes different techniques
aimed at the improvement of the manipulation and representation of the
structures used.
</dc:description>
 <dc:description>Comment: 12 pages, 4 Postscript figures, uses epsfig</dc:description>
 <dc:date>1998-01-26</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9801005</dc:identifier>
 <dc:identifier>Proceedings CAEPIA'97, Malaga, Spain. pp. 775-786</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9802001</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into
  Finite State Transducers</dc:title>
 <dc:creator>Kempe, Andre</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  This paper describes the conversion of a Hidden Markov Model into a finite
state transducer that closely approximates the behavior of the stochastic
model. In some cases the transducer is equivalent to the HMM. This conversion
is especially advantageous for part-of-speech tagging because the resulting
transducer can be composed with other transducers that encode correction rules
for the most frequent tagging errors. The speed of tagging is also improved.
The described methods have been implemented and successfully tested.
</dc:description>
 <dc:description>Comment: 9 pages, A4, LaTeX (+4x eps) gzip tar gzip uuencode</dc:description>
 <dc:date>1998-02-02</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9802001</dc:identifier>
 <dc:identifier>NeMLaP3/CoNLL'98, pp.29-37, Sydney, Australia. January 15-17, 1998</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:cmp-lg/9802002</identifier>
 <datestamp>2007-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
 <dc:title>A Hybrid Environment for Syntax-Semantic Tagging</dc:title>
 <dc:creator>Padro, Lluis</dc:creator>
 <dc:subject>Computer Science - Computation and Language</dc:subject>
 <dc:description>  The thesis describes the application of the relaxation labelling algorithm to
NLP disambiguation. Language is modelled through context constraint inspired on
Constraint Grammars. The constraints enable the use of a real value statind
&quot;compatibility&quot;. The technique is applied to POS tagging, Shallow Parsing and
Word Sense Disambigation. Experiments and results are reported. The proposed
approach enables the use of multi-feature constraint models, the simultaneous
resolution of several NL disambiguation tasks, and the collaboration of
linguistic and statistical models.
</dc:description>
 <dc:description>Comment: PhD Thesis. 120 pages</dc:description>
 <dc:date>1998-02-11</dc:date>
 <dc:type>text</dc:type>
 <dc:identifier>http://arxiv.org/abs/cmp-lg/9802002</dc:identifier>
 </oai_dc:dc>
</metadata>
</record>
<resumptionToken cursor="146000" completeListSize="155308">2369777|147001</resumptionToken>
</ListRecords>
</OAI-PMH>
