magid	title	ground_truth	context
14317969	A formal characterization of the outcomes of rule-based argumentation systems	2104126268	There are two major categories of instantiations of Dung’s abstract argumentation framework [4].
14317969	A formal characterization of the outcomes of rule-based argumentation systems	2159569510	The notion of a derivation schema generalizes derivations as defined in [5,8] and others.
14317969	A formal characterization of the outcomes of rule-based argumentation systems	2159569510	The second category uses rule-based languages [3,5,7] which distinguish between facts, strict rules (they encode strict information), and defeasible rules (they describe general behavior with exceptional cases).
111088122	USING NATURAL LANGUAGE PROCESSING FOR AUTOMATIC EXTRACTION OF ONTOLOGY INSTANCES	2133109597	Their classes, relationships, constraints and axioms define a common vocabulary to share knowledge (Gruber, 1995).
111088122	USING NATURAL LANGUAGE PROCESSING FOR AUTOMATIC EXTRACTION OF ONTOLOGY INSTANCES	2151846280	Context feature approaches use a corpus to extract features from the context in which a semantic class tends to appear (Fleischman and Hovy, 2002).
111088122	USING NATURAL LANGUAGE PROCESSING FOR AUTOMATIC EXTRACTION OF ONTOLOGY INSTANCES	2094061585	The parsing task aims at building a parse tree of each sentence in the text and identifying syntactic dependences according to the Stanford dependences among terms (Marneffe and Manning, 2008), as shown in Figure 10.
132390448	INTENTION-BASED CORRECTIVE FEEDBACK GENERATION USING CONTEXT-AWARE MODEL	1582147469	Due to the difficulties of extracting and employing rich dialog context, most of them included just a few types of context such as previous dialog act (Poesio and Mikheev, 1998), or dialog state in finite-state model (Bohus and Rudnicky, 2003).
132390448	INTENTION-BASED CORRECTIVE FEEDBACK GENERATION USING CONTEXT-AWARE MODEL	1582147469	Due to the difficulties of extracting and employing rich dialog context, most of them included just a few types of context such as previous dialog act (Poesio and Mikheev, 1998), or dialog state in finite-state model (Bohus and Rudnicky, 2003). Recently, Ai et. al. (2007) investigated the effect of using rich dialog context and showed promising results.
132390448	INTENTION-BASED CORRECTIVE FEEDBACK GENERATION USING CONTEXT-AWARE MODEL	1534218608	As Foster (2007; 2005) and Lee (2009) generated a treebank of ungrammatical English, we also produced artificial grammar errors systemically.
1904102306	Declarations of significance: exploring the pragmatic nature of information models	2169571673	, 2004) and design theory (Gregor and Jones, 2007) perspective there are key advantages to specifying the propositional content of an informative act in a more formal manner.
1904102306	Declarations of significance: exploring the pragmatic nature of information models	2169571673	The current paper revisits the nature of information models and examines their relationship to a design theory (Gregor and Jones, 2007) we have referred to in previous work (Beynon-Davies, 2011).
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2144577430	[20], and, to a lesser extent, user-generated videos [72]. To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.g. object trajectories [45], tag localization [70] and semantic recognition [72]. Facial expressions has been summaries by considering smile/happy face expression.
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153597356	[58] and [65]. Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held. Notably, Liu et al. [47] construct a mid-level representation called expressionlet from spatio-temporal manifold. Cruz et al. [10] proposed a dynamic downsampling of facial expressio
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2141350700	. In addition, we use cosine similarity as the metric rather than the Euclidean distance since the semantic word vectors are intrinsically directional and cosine similarity is a better metric used in [19], [56]. The process is summarized in Algorithm 1. 3.4 Video Emotion Attribution Emotion attribution aims to identify the contribution of each frame to the video’s overall emotion. Emotion attribution
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	803423	) rewards key frames that are the most similar to other frames in the same video, which means that the selected set of key frames are representatives of the entire video. Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the ﬁrst term for emotion attribution) and information coverage (by the second term for eliminating redundancy and
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153579005	110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see Table 2 in [5]). These images are clustered into 2;000 clusters (i.e. D = 2000 in Eq (1)). As shown in [56], the large-scale text data can greatly beneﬁt the trained language model. We train the Skip-gram model (Eq 5) on a large-scale text corpora, which includes around 7 billion words from the UMBC WebBas
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2122429065	applications. Video recommendation services, such as those employed by YouTube and Netﬂix, can beneﬁt from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction. Better understanding of video emotions may enable the placement of advertisements that are consistent with the main video’s mood and help avoid soc
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2108745803	ch attempts to enable “single-instance” supervised learning algorithms to be directly applicable to multiinstance feature bags. This branch includes most of early works on MIL [1], [63] such as miSVM [2], MIBoosting [77], Citation-kNN [70], MI-Kernel [22], among others [24], [25]. These algorithms achieve satisfactory results in several applications [50], [75], but most of them can only handle small
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2134422453	d to ﬁne-tune the network. A few works [8], [76] also employed off-the-shelf CNN features. Recognizing emotional impact of videos. A Large number of early works studied emotion in movies (e.g., [32], [38], [69]). Wang and Cheong [69] used an SVM with diverse audio-visual features to classify 2040 scenes in 36 Hollywood movies into 7 emotions. Jou et al. [37] worked on animated GIF ﬁles. Irie et al. [3
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153579005	ddition, we use cosine similarity as the metric rather than the Euclidean distance since the semantic word vectors are intrinsically directional and cosine similarity is a better metric used in [19], [56]. The process is summarized in Algorithm 1. 3.4 Video Emotion Attribution Emotion attribution aims to identify the contribution of each frame to the video’s overall emotion. Emotion attribution can he
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2144577430	than two decades. A complete review is beyond the scope of this paper and we refer readers to [66]. There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72]. Video summarization has been explored for various types of content, including professional videos (e.g., movies or news reports) [55], [70], [72], surveillance videos [19], 4 Auxiliary I
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153579005	the distributional hypothesis [30], which states that a word’s meaning is captured by other words that co-occur with it. This representation has been demonstrated to exhibit generalization properties [56] and constructed vector space allows vector arithmetics Our own experiments corroborate the beneﬁts of such a model. For example, when we add the vectors representing “surprise” and “sadness”, we obta
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2148809503	the dth dimension to compute the feature vector ss i: s i;d= Xn i j=1  i;j;dcos(x i;j;˚˚ d): (5) Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34]. First, the traditional BoW encodes local descriptors, such as SIFT and STIP, which requires a dictionary orders of magnitude greater than our frame set. Thus directly using standard BoW [64] to our
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2048827870	e data instance in the original instance space. Popular algorithms include constructive clustering based ensemble (CCE) [81], multi-instance learning based on the Fisher Vector representation (Mi-FV) [74] and multi-instance learning via embedded instance selection [9]. Inspired by these works, we encode the video frame bags into singleinstance representations. It is worth noting our approach is differ
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2171221410	es, such as agreeing or feeling unsure. Bosch et al. [6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31]. Recognizing the emotional impact of still images on viewers. Machajdik and Hanbury [51] classiﬁed images into 8 affective categories: amusement, awe, contentment, excitement, anger, disgust, fear, a
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2007510844	etric for measuring the inter-rater agreement for qualitative items (i.e. annotations). The VideoStory-P14 dataset. The VideoStory-P14 dataset is derived from the recently proposed VideoStory dataset [27]. We use all the keywords of the Plutchik’s Wheel of Emotions [62] to query the VideoStory dataset in terms of its video captions. Emotion keywords are matched against all the words in the video’s cap
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2134860945	as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held. Notably, Liu et al. [47] construct a mid-level representation called expressionlet from spatio-temporal manifold. Cruz et al. [10] proposed a dynamic downsampling of facial expressions in video 3 at a rate proportional to th
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2119362355	fer readers to [68]. In broad strokes, we can classify work on video summarization into two major categories: approaches based on key frames [10], [16], [29], [46] and approaches based on video skims [17], [57], [71], [73]. Video summarization has been explored for various types of content, including professional videos like movies or news reports [57], [71], [73], egocentric videos [49], surveillance
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153265945	gnition [73]. Dhall and Roland [11] considered smile/happy facial expressions. User’s spontaneous reactions such as eye movement, blink and face expressions are measured by a Interest Meter system in [61] and used for video summarization. However, none of these approaches have considered video summarization based on more general video emotion content. Such video emotion is an important cue for ﬁnding
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2124773960	h the identiﬁcation of facial expressions and the associated emotion in static images and video, which has been a subject extensively studied. Two recent reviews of the topic can be found in [58] and [65]. Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have bee
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2123024445	ing to novel object classes [18], [43]. However, these approaches require semantic attributes to be manually deﬁned and annotated; the availability of annotation limits their scalability. Recent work [15], [66] explore zero-shot learning with representation of words as points in a multi-dimensional vector space that is constructed from large-scale text corpora. The intuition underlying this lexical re
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2003723718	ing set  w Te . Thus, given a test video V j in the testing set, its class label z~ jcan be estimated as z^~ j= argmax z2Z Te cos g(s j);  z : (10) Compared with the zero-shot learning algorithm in [21], we skip the intermediate level of latent attributes and directly apply the 1-step self-training in the semantic word vector space. In addition, we use cosine similarity as the metric rather than the
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2120645068	m sampling, which uniformly samples several clips from video. (2) K-means sampling, which simply clusters the clips and selects a clip closest to each cluster centroid. (3) Story-driven summarization [49]. This approach was developed to summarize very long egocentric videos. We slightly modify the implementation and make the length of the summary controllable for our task. (4) Real-time summarization
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2120645068	n video skims [17], [57], [71], [73]. Video summarization has been explored for various types of content, including professional videos like movies or news reports [57], [71], [73], egocentric videos [49], surveillance videos [16], [17], and, to a lesser extent, user-generated videos [26], [73]. A diverse set of features have been proposed, including low-level features such as visual saliency [51] and
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2124033848	novel object classes [18], [43]. However, these approaches require semantic attributes to be manually deﬁned and annotated; the availability of annotation limits their scalability. Recent work [15], [66] explore zero-shot learning with representation of words as points in a multi-dimensional vector space that is constructed from large-scale text corpora. The intuition underlying this lexical represen
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2105198535	nsampled around the apex of an expression. A few recent works focused on predicting emotions, affects, and emotion-related cognitive states outside the basic emotion categories. Kaliouby and Robinson [18] recognized emotion-related mental states, such as agreeing or feeling unsure. Bosch et al. [6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; othe
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	1762726187	onal content in video has many realworld applications. Video recommendation services can beneﬁt from matching user interests with the emotions of video content and prediction of interestingness [20], [21], [36], leading to improved user satisfaction. Better understanding of video emotions may enable advertising that is consistent with the main video’s mood and help avoid social inappropriateness such
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2150578721	ontent in video has many realworld applications. Video recommendation services can beneﬁt from matching user interests with the emotions of video content and prediction of interestingness [20], [21], [36], leading to improved user satisfaction. Better understanding of video emotions may enable advertising that is consistent with the main video’s mood and help avoid social inappropriateness such as pla
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2017411072	orks studied emotion in movies (e.g., [32], [38], [69]). Wang and Cheong [69] used an SVM with diverse audio-visual features to classify 2040 scenes in 36 Hollywood movies into 7 emotions. Jou et al. [37] worked on animated GIF ﬁles. Irie et al. [32] use Latent Dirichlet Allocation to extract audio-visual topics as mid-level features, which are combined with Hidden-Markov-like dynamic model. For a mor
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2048827870	rage pooling feature is computed as 1 n i P n i j=1 x i;j. The average pooling is the standard approach 8 of aggregating frame-level features into video-level descriptions as mentioned in [78]. Mi-FV [74]. MIL bags of training videos are mapped into a new bag-level Fisher Vector representation. Mi-FV is able handle large-scale MIL data efﬁciently. CCE [81]. The instances of all training bags are clust
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2163461766	rence is that sentiment attribution only considers positive or negative attitudes while we consider more diverse emotions for emotion attribution. Emotion attribution can help us ﬁnd video highlights [29], which are the interesting or important events happened in the video. Generally, the concepts of “interesting” and ”important” may be variable for different target video domains and applications, suc
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2119362355	rm the similarity is deﬁned directly in the feature space; (2) We empirically set = 1 to equally consider both emotion content and representativeness of the video. Comparing with previous work [16], [17], [29], [68], [73], Eq (12) considers the summary of both video highlights (by the ﬁrst term for emotion attribution) and information coverage (by the second term for eliminating redundancy and select
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2253891449	the rule of the third and depth of ﬁeld. Lu et al. [48] studied shape features along the dimensions of rounded-angular and simple-complex, and their effects in arousing viewers’ emotions. You et al. [79] designed a deep convolutional neural network (CNN) for visual sentiment analysis. After training on the entire training set, images on which the CNN performs poorly are stochastically removed. The re
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	1994002998	s experiments showed satisfactory results on emotion analysis task by using AlexNet architecture, we want to compare with different architectures to better understand deep features. VGG-16 and VGG-19 [7] and GoogLeNet-22 [67] achieved the state-of-the-art for image classiﬁcation on ImageNet challenge. Thus we conducted video emotion recognition using high layer features extracted from these architect
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2098166271	s include constructive clustering based ensemble (CCE) [81], multi-instance learning based on the Fisher Vector representation (Mi-FV) [74] and multi-instance learning via embedded instance selection [9]. Inspired by these works, we encode the video frame bags into singleinstance representations. It is worth noting our approach is different from existing MIL algorithms because (1) we perform the enco
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2097117768	satisfactory results on emotion analysis task by using AlexNet architecture, we want to compare with different architectures to better understand deep features. VGG-16 and VGG-19 [7] and GoogLeNet-22 [67] achieved the state-of-the-art for image classiﬁcation on ImageNet challenge. Thus we conducted video emotion recognition using high layer features extracted from these architectures as descriptors. T
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153579005	semantic vectors w (Eq (5)) is set to 500 to balance computational cost of training w from large-scale text corpora and the effectiveness of the syntactic and semantic regularities of representations [56]. Our AlexNet CNN model is trained by ourselves using 2;600 ImageNet classes with the Caffe toolkit [33], and we use the 4;096-dimensional activations of the 7th fully-connected layer after the Rectiﬁ
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2085940040	ss. In addition to color, texture, and statistics about faces and skin area present in the image, they also make use of composition features such as the rule of the third and depth of ﬁeld. Lu et al. [48] studied shape features along the dimensions of rounded-angular and simple-complex, and their effects in arousing viewers’ emotions. You et al. [79] designed a deep convolutional neural network (CNN)
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2116946038	has been studied for more than two decades. A complete review is beyond the scope of this paper and we refer readers to [66]. There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72]. Video summarization has been explored for various types of content, including professional videos (e.g., movies or news reports) [55], [70], [72], surveillance
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2078579128	supervised learning algorithms to be directly applicable to multi-instance feature bags. This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others. These algorithms achieved satisfactory accuracies in several applications, but most of them can only handle small or moderate-sized data. In other words, they are compu
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2062733121	ted mental states, such as agreeing or feeling unsure. Bosch et al. [6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31]. Recognizing the emotional impact of still images on viewers. Machajdik and Hanbury [51] classiﬁed images into 8 affective categories: amusement, awe, contentment, excitement, anger, d
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	1950136256	of testing videos. The ﬁnal bag labels are produced by majority vote of instance labels. This method is a variant of the Key Instance Detection (KID) [47] in multi-class multi-instance setting. AvgP [78]. We average the frame-level image features of one video as video-level feature descriptions for classiﬁcation. For the ith video, its average pooling feature is computed as 1 n i P n i j=1 x i;j. The
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2153579005	texts usually have similar meaning [30]. The distributed representations are embedded in a low-dimensional space RKin which emotion class labels can be related to each other. Following Mikilov et al. [56], we learn the distributed representation by predicting from each word its context words. Given a word w tand its surrounding context words (w t M; ;w t 1;w t+1; ;w t+M) within a window of size 2M, we
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2094077339	thms. The ﬁrst branch attempts to enable “single-instance” supervised learning algorithms to be directly applicable to multiinstance feature bags. This branch includes most of early works on MIL [1], [63] such as miSVM [2], MIBoosting [77], Citation-kNN [70], MI-Kernel [22], among others [24], [25]. These algorithms achieve satisfactory results in several applications [50], [75], but most of them can
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	1875160599	timent analysis. After training on the entire training set, images on which the CNN performs poorly are stochastically removed. The remaining images were used to ﬁne-tune the network. A few work [8], [74] also employed off-the-shelf CNN features. Recognizing emotional impact from videos. For a more comprehensive review, we refer reader to the latest survey [71]. A large number of early work studied em
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2134270519	tion words to the video domain via the semantic word vectors. Such a transferring step enables the zero-shot emotion recognition. We compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42]. For DAP, at test time each dimension of the word vectors of each test sample is predicted, from which the test class labels are inferred. DAP can be taken as directly using Eq (10) without the
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2006180404	trated on the right. [20], and, to a lesser extent, user-generated videos [72]. To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.g. object trajectories [45], tag localization [70] and semantic recognition [72]. Facial expressions has been summaries by considering smile/ha
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2529272619	us types of content, including professional videos like movies or news reports [57], [71], [73], egocentric videos [49], surveillance videos [16], [17], and, to a lesser extent, user-generated videos [26], [73]. A diverse set of features have been proposed, including low-level features such as visual saliency [51] and motion cues [57], [59], mid-level information such as object trajectories [46], tag
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2155893237	uce the computational cost. The dimension of the real-valued semantic vectors (Eq (7)) is set to 500. Our AlexNet CNN model is trained by ourselves using 2;600 ImageNet classes with the Caffe toolkit [32], and we use the 4;096-dimensional activations of the 7th fully-connected layer after the Rectiﬁed 8 Linear Units as features. The number of nearest neighbors in Eq (4) is empirically set to 10% of th
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	803423	ummarization has been studied for more than two decades. A complete review is beyond the scope of this paper and we refer readers to [66]. There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72]. Video summarization has been explored for various types of content, including professional videos (e.g., movies or news reports) [55], [70], [72],
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2131846894	umulate the effects of each frame on the dth dimension to compute the feature vector ss i: s i;d= Xn i j=1  i;j;dcos(x i;j;˚˚ d): (5) Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34]. First, the traditional BoW encodes local descriptors, such as SIFT and STIP, which requires a dictionary orders of magnitude greater than our frame set. Thus dir
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	1511030552	w-level features such as visual saliency [51] and motion cues [57], [59], mid-level information such as object trajectories [46], tag localization [71] and semantic recognition [73]. Dhall and Roland [11] considered smile/happy facial expressions. User’s spontaneous reactions such as eye movement, blink and face expressions are measured by a Interest Meter system in [61] and used for video summarizati
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2127429655	webcams. Histogram of Oriented Gradient (HOG) features were extracted based on 22 key points on the faces. Purchase intent is predicted based on the entire emotion trajectory over time. Kapoor et al. [38] used video, skin conductance, and pressure sensors on the chair and the mouse to predict frustration when a user interacted with an intelligent tutoring system. However, the success of this approach
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2116946038	xtract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.g. object trajectories [45], tag localization [70] and semantic recognition [72]. Facial expressions has been summaries by considering smile/happy face expression. However, none of these approaches have considered video summari
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2163461766	y frames that are the most similar to other frames in the same video, which means that the selected set of key frames are representatives of the entire video. Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the ﬁrst term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting in
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2163461766	zation has been studied for more than two decades. A complete review is beyond the scope of this paper and we refer readers to [66]. There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72]. Video summarization has been explored for various types of content, including professional videos (e.g., movies or news reports) [55], [70], [72], survei
2177696193	Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization	2003723718	zero-shot learning settings. For example, videos of joy usually have positive frames, whilst a sad video would have negative ones. To ameliorate such generalization problems, we take inspiration from [21] and apply Transductive 1-Step Self-Training (T1S) to adjust the word vector of new emotion classes. Speciﬁcally, for a class z?2Z Tethat is previously unseen, and the corresponding word vector z?, we
2180385518	Conceptual Vectors - A Complementary Tool to Lexical Networks	2145122019	[10] use a coocurrences network to extract typical relations like those presented in the previous section.
2180385518	Conceptual Vectors - A Complementary Tool to Lexical Networks	2138437366	They are employed in many tasks (lexical disambiguation [2]) or field applications (machine translation with multilingual networks like Papillon [3] or [4], information retrieval or text classification [5]).
2180385518	Conceptual Vectors - A Complementary Tool to Lexical Networks	1486093399	Thus, [ 8 ] add lexical signatures resulting from tagged corpora or Web.
2180385518	Conceptual Vectors - A Complementary Tool to Lexical Networks	2147152072	Vectors have long been used in information retrieval [11] and for meaning representation in the LSI model [12] from latent semantic analysis (LSA) studies in psycholinguistics.
2213717825	Sentence entailment in compositional distributional semantics	2109830295	anings of the words therein. The vectorial word and phrase/sentence representations have been applied to similarity-based language tasks such as disambiguation and semantic similarity (Schutze 1998;¨ Turney 2006). In order to apply the distributional representations to entailment tasks, distributional semanticists adhere to a distributional inclusion hypothesis: if word ventails word w, then the contexts of w
2213717825	Sentence entailment in compositional distributional semantics	16329575	antiated to concrete data and have been applied to word and phrase/sentence similarity-based tasks, outperforming the models where grammar was not taken into account (Grefenstette and Sadrzadeh 2011; Kartsaklis and Sadrzadeh 2013). In this paper we show how CCDS can be used to reason about entailment in a compositional fashion. In particular, we prove how the general compositional procedure of this model extends the entailment
2213717825	Sentence entailment in compositional distributional semantics	205765513	count their rank (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010). From the machine learning side, classiﬁers have been trained to learn the entailment relation at the word level (Baroni et al. 2012). All of these improvements are applicable to the above density matrix setting. Categorical Compositional Distributional Entailment Montague style semantics of natural language comes equipped with a n
2213717825	Sentence entailment in compositional distributional semantics	205765513	Dagan, Lee, and Pereira 1999; Weeds, Weir, and McCarthy 2004; Kotlerman et al. 2010), not much has been done when it comes to phrases/sentences. The work on entailment between quantiﬁed noun phrases (Baroni et al. 2012) is an exception, but it does not take into account composition. Compositionality is what is needed for a modular approach to the textual entailment challenge (Dagan, Glickman, and Magnini 2006), wher
2213717825	Sentence entailment in compositional distributional semantics	2120084270	e zero weights in !w. In other words, R KL( !v;! w) = 0 when vdoes not entail . The contrapositive of this provides a degree of entailment: !v ‘!w ) R KL( !v;!w) 6= 0 (4) The -skew divergence of Lee (Lee 1999) and a symmetric version of it based on JS(Dagan, Lee, and Pereira 1999) are variations on the above. Similarly, for density matrices one can use the degree of representativeness of two density matric
2213717825	Sentence entailment in compositional distributional semantics	1484288670	Fragments of them have been instantiated to concrete data and have been applied to word and phrase/sentence similarity-based tasks, outperforming the models where grammar was not taken into account (Grefenstette and Sadrzadeh 2011; Kartsaklis and Sadrzadeh 2013). In this paper we show how CCDS can be used to reason about entailment in a compositional fashion. In particular, we prove how the general compositional procedure of t
2213717825	Sentence entailment in compositional distributional semantics	1484288670	l matrix multiplication becomes !nT Tv, for !n the transpose of the vector of the noun. For details of these computations, we refer the reader to our previous work (Coecke, Sadrzadeh, and Clark 2010; Grefenstette and Sadrzadeh 2011; Kartsaklis, Sadrzadeh, and Pulman 2012), where these have been worked out for a variety of different examples. Vectors of nouns !n are created using the usual distributional method. For producing th
2213717825	Sentence entailment in compositional distributional semantics	1986069434	mpositional; that is, we derive a phrase/sentence-level entailment from the entailments between the words thereof. What makes this possible is the concept of ‘upward monotonicity’ from Natural Logic (MacCartney and Manning 2007). Roughly put, this expresses that phrases/sentences of an upward monotone vocabulary entail each other. Proposition. For all i;1 i nand v i;w i upwardly monotone words, we have v i ‘ w i ) 1 2 n 1 2
2213717825	Sentence entailment in compositional distributional semantics	1887388671	nce level entailment. Density matrices have been previously used in compositional distributional semantics to represent parsing information (Blacoe, Kasheﬁ, and Lapata 2013) and ambiguity of meaning (Piedeleu et al. 2015). We conclude by providing a small scale experiment on data obtained from British National Corpus (BNC) applied to a toy short-sentence entailment task. This involves implementing a concrete way of bu
2213717825	Sentence entailment in compositional distributional semantics	1986069434	ngam 2013). In the current paper we prove that in CCDS this notion soundly extends from word vectors to sentence vectors and provides a notion of sentence entailment similar to that of Natural Logic (MacCartney and Manning 2007). In the presence of correlations between contexts, the notion of KL-divergence naturally lifts from vectors to density matrices via von Neumann entropy. The results of this paper build on the develop
2213717825	Sentence entailment in compositional distributional semantics	1887388671	here v^ and ^n are the density matrices of the verb and the noun, respectively, and the tensor product in CPM(FHilb R). This simpliﬁes to the following formula: Tr N(^v(^n 1 S)) (7) For details, see Piedeleu et al. (2015). The density matrix of a noun-verb phrase is computed similarly by swapping the corresponding identity 1 Sand epsilon maps  Nin . The density matrix for a word w, regardless of its grammatical type,
2275329194	Communicating with sentences: A multi-word naming game model	2086382064	ith an additional rule: only when all hearers reached consensus with the same speaker, they reach local consensus. The NG in groups (NGG) is a multiple-speaker and multiple-hearer model, developed in [11], which allows every agent in a selected group from the 7 .CL ] 5 2 population to play the dual role as a speaker and also as a hearer. All the aforementioned models have demonstrated that the converg
2275329194	Communicating with sentences: A multi-word naming game model	2112090702	in their memories. 3 Simulation Results 3.1 Simulation setup Large-scale numerical simulations are carried out on three typical network topologies, namely, random-graph (RG) [5,16], small-world (SW) [7,8,17] and scale-free (SF) [5,6] networks. The performances of emergence, propagation and consensus of sentences and their patterns are examined. Different types of NGs are simulated with comparisons. The a
2275329194	Communicating with sentences: A multi-word naming game model	2086382064	be the object by the same name. This conventional NG will be called the single-word naming game (SWNG) below. Previous studies on the conventional NG focus on two aspects: one is about agent dynamics [2,5, 6, 7, 8, 9, 10, 11] and the other is about information dynamics [12, 13, 14]. The former concerns about the topological relationships of agents, the roles of speaker and hearer, and the communication model among the age
2275329194	Communicating with sentences: A multi-word naming game model	2148260416	their patterns are examined. Different types of NGs are simulated with comparisons. The agents store nothing in their memories initially, and the memory size of each agent is large enough or infinite [12,18]. Totally 5 conventional patterns in English language are used (shown in Figure 3) to form various sentences. A total of 12 settings of the communication network are investigated, each with 500 nodes
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2164973920	1 Jul 2016 1.1 Comparison with related work Automatic learning of word senses (Word Sense Induction) usually is done via variants of the exemplar approach of Yarowsky (1995) (see also (Schutze, 1998; Reisinger and Mooney, 2010; Di Marco and Navigli, 2013)), which identies multiple word senses by clustering neighboring words. Firth’s hypothesis justies this approach, since the senses are dierent if and only if their cont
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2091812280	highly reminiscent of past results from hierarchical topic models (Griths and Tenenbaum, 2004). Indeed, the model (1) used to compute the word embeddings is related to a log-linear topic model from (Mnih and Hinton, 2007). However, the discourses here are computed via sparse coding on word embeddings, which is very distinct from topic modeling. The atoms are also reminiscent of coherent \word clusters&quot; detected i
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2159426623	lve a custom similarity score based upon WordNet that is hard to interpret and may not be available for other languages. Here a new simple test |{inspired by word-intrusion tests for topic coherence (Chang et al., 2009)|is proposed, which has the advantages of being easy to understand, and can also be administered to humans. The testbed uses 200 polysemous words and their 704 senses according to WordNet. Each \sense
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2164973920	monolithic approach of representing a word by a single word vector, with its inner information extracted only via inner product, is felt to fail in capturing this ner structure (Griths et al., 2007; Reisinger and Mooney, 2010). The current paper goes beyond this monolithic view, by describing how multiple senses of a word actually reside within the word embedding in linear superposition, and can be recovered by simple spar
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2164019165	ntexts involve dierent word distributions1. Extensions of this idea can be used to represent polysemous words using more complicated representations than a single vector (e.g., (Murphy et al., 2012; Huang et al., 2012)). Word sense disambiguation (WSD) is a more general (and more dicult) problem of identifying the sense of a word used in each occurrence during a document, and will not be considered in the current
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2140610559	scourse, a new notion introduced in Section 3. The idea of applying sparse coding to word embeddings has been tried before as a way of getting representations that are more useful in other NLP tasks (Faruqui et al., 2015), but not in connection with polysemy. 2 Linear structure of word senses Consider a polysemous word, say tie, which can refer to an article of clothing, or a drawn match, or a physical act. Let’s take
2295508206	Linear Algebraic Structure of Word Senses, with Applications to Polysemy	2123442489	to the subspace spanned by the word embeddings of its words. For a given polysemous word we take its ve atoms as well as atoms for its in ectional forms (e.g., past tense, plural etc., generated by (Manning et al., 2014)). This yields between 10 to 20 atoms for the word, whereupon each sentence is scored with respect to each atom by an appropriate cosine similarity between the atom and the semantic representation of
2398722869	EXPRESSIVE REASONING ABOUT CULTURAL HERITAGE KNOWLEDGE USING WEB ONTOLOGIES	2122368812	Horrocks and Sattler (2005)  have introduced a decision procedure for the SHOIQ Description Logic; this algorithm is claimed to exhibit controllable efficiency and is currently under implementation in two high-end inference engines.
2398722869	EXPRESSIVE REASONING ABOUT CULTURAL HERITAGE KNOWLEDGE USING WEB ONTOLOGIES	92935989	In order to transform the ontology to OWL syntax, we initially utilized the RACER system ( Haarslev & Moller 2003,  Haarslev & Moller 2004).
2398722869	EXPRESSIVE REASONING ABOUT CULTURAL HERITAGE KNOWLEDGE USING WEB ONTOLOGIES	1990890881	OWL Lite and OWL DL are in fact very expressive description logics, using RDF syntax (Horrocks et al. 2003).
2401619332	EXPLOITATION OF ONTOLOGY LANGUAGES FOR BOTH PERSISTENCE AND REASONING PURPOSES - Mapping PLIB, OWL and Flight Ontology Models	1542417898	Some languages like those founded on Description Logic(DL)( Baader et al., 2003 ) do not impose strong typing.
2401619332	EXPLOITATION OF ONTOLOGY LANGUAGES FOR BOTH PERSISTENCE AND REASONING PURPOSES - Mapping PLIB, OWL and Flight Ontology Models	156844249	A PLIB ontology model is ( Pierra, 2004 ) : (1) conceptual i.e., each entity and each property are completely defined.Formally, (for a more complete description, see( Pierra, 2004 )) a PLIB ontology O is defined as the 4-tuple O:  , where:
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2040466507,2072644219,2134731454,2171343266	, 2009) that extends PLSA (Hofmann, 2001) to streaming document collections and that is used in this study, other approaches (Mei and Zhai, 2005; Blei and Lafferty, 2006; Wang and McCallum, 2006) model dynamic document collections, too.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2110642088	Some allow for words to become obsolete and irrelevant while others emerge (AlSumait et al., 2008; Chou and Chen, 2008).
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2043396212	That ambiguity of tags is likely to be promoted by the very fact that tagging is a social activity: if a ”leader” user (Goyal et al., 2008) favors a specific tag for a document, other users may decide to use the same tag for documents even when those documents diverge in content.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2134731454	Document prototypes are derived from topics learned by probabilistic latent semantic analysis (PLSA) (Hofmann, 2001).
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2134731454	Estimating mixture weights for new documents Mixture weights of new documents new ~ D t are estimated by folding-in (Hofmann, 2001) these documents into ζ t .
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2167760907	For example, (Moxley et al., 2009) derive semantics of tags assigned to Flickr pictures by analyzing geographical coordinates of the depicted locations.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2072644219	To report topics learned by dynamic topic modeling, (Blei and Lafferty, 2006) list the most likely words for topics at several time points.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2113855231,2159426623	Research (Boyd-Graber et al., 2009; Mei et al., 2007) to enhance presentation of topics for human inspection suggests to present words that not necessarily have to be the most likely words but the most descriptive words for a topic.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2123297508	To study the stream of documents ~ Dt we define a sliding window covering l successive documents (Guha et al., 2003) that comprise a partial document collection under tag t.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2134731454	Therefore, we define a stream of documents and learn topics for successive parts of the stream using an extension of probabilistic latent semantic analysis (Hofmann, 2001) described in Section 4.
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2134731454	Topic modeling often assumes topics to be represented by multinomial distributions over words of the vocabulary (Hofmann, 2001; Blei et al., 2003).
2407538448	VISUALLY SUMMARIZING THE EVOLUTION OF DOCUMENTS UNDER A SOCIAL TAG	2134731454	In topic modeling literature (Hofmann, 2001; Blei et al., 2003), topics, which are not predefined but learned from documents and represented as discrete probability distributions over the vocabulary, are often presented by listing most likely words.
2407960512	TEMPORAL ENTITIES - Types, Tokens and Qualifications	118671747	We believe this to be a more accurate comparison than Vila’s likening of situation terms to time terms in the method of temporal arguments (MTA) (Haugh, 1987).
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2131744502	anslation. 1 Introduction There has been signiﬁcant progress on English sentiment analysis in recent years using models based on neural networks (Socher et al., 2013; ˙Irsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015). Most of these, however, rely on a massive amount of labeled training data or ﬁne-grained annotations such as the Stanford Sentiment Treebank (Socher et al., 20
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2118090838	of BWE. For the reason why all systems show inferior results with BilBOWA, we conjecture that BilBOWA may have slightly reduced quality since it does not require word alignments during training as by Zou et al. (2013). By only training on sentence-aligned corpus, BilBOWA requires less resource and is much faster to train, potentially at the expense of quality. Model Random BilBOWA Pre-trained DAN 21.66% 28.75% 29.
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1836465849	d Qhave two. All hidden layers contains 900 hidden units. This choice is more or less ad-hoc, and the performance could potentially be improved with more careful model selection. Batch Normalization (Ioffe and Szegedy, 2015) is used in each hidden layer in Pand Q. F does not use BN. Fand Pare optimized together by Adam (Kingma and Ba, 2015) with a learning rate of 0:05 for Chinese and 01 for Arabic experiments. Qis train
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2096873754	d satisfactory results for our task. TCA (Pan et al., 2011) did not work since it required quadratic space in terms of the number of samples (650k). SDA (Glorot et al., 2011) and the subsequent mSDA (Chen et al., 2012) are proven very effective for crossdomain sentiment classiﬁcation on Amazon reviews. However, as shown in Table 1 (Row 3), mSDA did not perform competitively. We speculate that this is because many d
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2115403315	AN with domain adaptation baselines, since it can be viewed as a generalization of the cross-lingual task. Nonetheless, domain adaptation methods did not yield satisfactory results for our task. TCA (Pan et al., 2011) did not work since it required quadratic space in terms of the number of samples (650k). SDA (Glorot et al., 2011) and the subsequent mSDA (Chen et al., 2012) are proven very effective for crossdomai
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2096873754	Domain Adaptation tries to learn effective classiﬁers for which the training and test samples are from different underlying distributions (Blitzer et al., 2007; Pan et al., 2011; Glorot et al., 2011; Chen et al., 2012; Liu et al., 2015). This can be thought of as a generalization of cross-lingual text classiﬁcation. However, one main difference is that, when applied to text classiﬁcation tasks such as sentiment an
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1828724394	e only use the text from the annotated data (without labels) during training. English-Arabic Bilingual Word Embeddings. For Arabic, since no pre-trained BWE is available, we train a 300d BilBOWA BWE (Gouws et al., 2015) on the United Nations corpus (Ziemski et al., 2016). 3.2 Cross-Lingual Sentiment Classiﬁcation Our main results are shown in Table 1, which shows very similar trends for Chinese and AraSetting Approa
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2071332064	eds to a few thousand instances (Tan and Zhang, 2008; Lee and Renganathan, 2011). Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008, 2009; Lu et al., 2011; Mohammad et al., 2016a), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) that are expensive to obtain. Some of these papers (Zh
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2115403315	es to improve the performance on both. Domain Adaptation tries to learn effective classiﬁers for which the training and test samples are from different underlying distributions (Blitzer et al., 2007; Pan et al., 2011; Glorot et al., 2011; Chen et al., 2012; Liu et al., 2015). This can be thought of as a generalization of cross-lingual text classiﬁcation. However, one main difference is that, when applied to text
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2123442489	fectiveness of our model, we experiment on Chinese and Arabic sentiment classiﬁcation, using English as SOURCE for both. For all data used in experiments, tokenization is done using Stanford CoreNLP (Manning et al., 2014). 3.1 Data Labeled English Data. We use a balanced dataset of 700kYelp reviews from Zhang et al. (2015) with their sentiment ratings as labels (scale 1-5). We also adopt their train-validation split:
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2118090838	Foperates on both SOURCE and TARGET sentences, it is favorable if the word representations for both languages align approximately in a shared space. Thus, we employ bilingual word embeddings (BWEs) (Zou et al., 2013; Gouws et al., 2015) to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of l
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1882958252	framework is unsupervised in the sense that it requires only unlabeled text in the target language. In particular, we propose ADAN, an end-to-end adversarial neural network (Goodfellow et al., 2014; Ganin and Lempitsky, 2015). It uses labeled data to train a sentiment classiﬁer for the source language, and simultaneously transfers the learned sentiment analysis knowledge to the target language. Our trained system then dir
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1577366066	gh-quality labeled data in many non-English languages (Mihalcea et al., 2007; Banea et al., 2008, 2010). For Chinese and Arabic in particular, there are several representative works (Wan, 2008, 2009; He et al., 2010; Lu et al., 2011; Mohammad et al., 2016a). Our work is comparable to these papers in objective but very different in method. The work by Wan uses machine translation to directly convert English train
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	648143168	in image generation has used architectures similar to ours, by pitting a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015). More relevant to this work, adversarial architectures have produced the stateof-the-art in unsupervised domain adaptation for image object recognition: Ganin and Lempitsky (2015) train with many lab
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1882958252	jovsky et al. (2017) and observed in our experiments, minimizing the Wasserstein distance is much more stable w.r.t. hyperparameter selection, saving the hassle of carefully varying during training (Ganin and Lempitsky, 2015). In addition, traditional adversarial training methods need to laboriously coordinate the alternating training of the two competing components (Goodfellow et al., 2014) by setting a hyperparameter k,
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2071332064	by the lack of high-quality labeled data in many non-English languages (Mihalcea et al., 2007; Banea et al., 2008, 2010). For Chinese and Arabic in particular, there are several representative works (Wan, 2008, 2009; He et al., 2010; Lu et al., 2011; Mohammad et al., 2016a). Our work is comparable to these papers in objective but very different in method. The work by Wan uses machine translation to directl
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2295710275	n baselines (Row 4-5) that (1) translate the TARGET text into English and then (2) use the better of the train-on-source-only models for sentiment classiﬁcation. Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language. For Chinese, where
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2096873754	n Table 1, which shows very similar trends for Chinese and AraSetting Approach Accuracy Chinese Arabic Train-on-source-only Logistic Regression 30.58% 45.83% DAN 29.11% 48.00% Domain Adaptation mSDA (Chen et al., 2012) 31.44% 48.33% Machine Translation Logistic Regression + MT 34.01% 51.67% DAN + MT 39.66% 52.50% Ours ADAN (CN: 50d, AR:300d) 42.95%y 55.33% yp&lt;0:001 under a McNemar test. Table 1: ADAN performance
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1882958252	nd unlabeled TARGET text. The idea of incorporating an adversary in neural networks has achieved great success in computer vision for image generation (Goodfellow et al., 2014) and domain adaptation (Ganin and Lempitsky, 2015). However, to our best knowledge, ours is the ﬁrst to develop an adversarial network for language adaptation, i.e. cross-lingual NLP tasks. In addition, inspired by Arjovsky et al. (2017), we modify t
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2251939518	ne approach that relies on state-of-the-art Machine Translation. 1 Introduction There has been signiﬁcant progress on English sentiment analysis in recent years using models based on neural networks (Socher et al., 2013; ˙Irsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015). Most of these, however, rely on a massive amount of labeled training data or ﬁne-grained annotations such as th
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2158139315	predicts a scalar score indicating whether xis from SOURCE or TARGET. An input document is modeled as a sequence of words x = w 1;:::;w n, where each word w is represented by its word embedding v w (Turian et al., 2010). Because the same feature extractor Foperates on both SOURCE and TARGET sentences, it is favorable if the word representations for both languages align approximately in a shared space. Thus, we emplo
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2118090838	RGET data used in training ADAN, we use another 150kunlabeled Chinese hotel reviews. English-Chinese Bilingual Word Embeddings. For Chinese, we used the pre-trained bilingual word embeddings (BWE) by Zou et al. (2013). Their work provides 50-dimensional embeddings for 100kEnglish words and another set of 100k Chinese words. For more experiments and discussions on BWE, see Section 3.3.3. Labeled Arabic Data. We use
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2403788517	ries to learn effective classiﬁers for which the training and test samples are from different underlying distributions (Blitzer et al., 2007; Pan et al., 2011; Glorot et al., 2011; Chen et al., 2012; Liu et al., 2015). This can be thought of as a generalization of cross-lingual text classiﬁcation. However, one main difference is that, when applied to text classiﬁcation tasks such as sentiment analysis, most work a
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1828724394	rmance of ADAN is further boosted. Therefore, it seems the quality of BWE plays an important role in cross-lingual classiﬁcation. To investigate the impact of BWE, we also trained a 100d BilBOWA BWE (Gouws et al., 2015) using the UN parallel corpus for Chinese. All systems achieve slightly lower performance compared to the pre-trained BWE, yet ADAN still outperforms other baseline methods (Table 2), demonstrating th
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2096707493	sis for lowresource languages focuses on inducing sentiment lexicons (Mohammad et al., 2016b) or training linear classiﬁers on small domain-speciﬁc datasets with hundreds to a few thousand instances (Tan and Zhang, 2008; Lee and Renganathan, 2011). Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008, 2009; Lu et al., 2011; Mohammad et al., 2
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1828724394	SOURCE and TARGET sentences, it is favorable if the word representations for both languages align approximately in a shared space. Thus, we employ bilingual word embeddings (BWEs) (Zou et al., 2013; Gouws et al., 2015) to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language. In some prio
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1522301498	tentially be improved with more careful model selection. Batch Normalization (Ioffe and Szegedy, 2015) is used in each hidden layer in Pand Q. F does not use BN. Fand Pare optimized together by Adam (Kingma and Ba, 2015) with a learning rate of 0:05 for Chinese and 01 for Arabic experiments. Qis trained with Adam with learning rate of 0:00005. The weights of Qare clipped to [ 0:01; 01]. ADAN is implemented in Torch7
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1882958252	ter Stability In this section, we show that the training of ADAN is stable over a large set of hyperparameters, and provide improved performance compared to traditional adversarial training method by Ganin and Lempitsky (2015). We implemented a variant of ADAN similar to the adversarial domain adaptation network ! &quot;#$%&amp;# &quot;#$%&amp;# &apos;(&apos;)*+,-./0-*1#22342-3,5*(,2-#563 &apos;(&apos;) Figure 4: A grid se
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	1882958252	we train Fto make these two distributions as close as possible to learn language-invariant features for better cross-lingual generalization. Departing from previous research in adversarial training (Ganin and Lempitsky, 2015), in this work we minimize the Wasserstein distance, following Arjovsky et al. (2017). As argued by Arjovsky et al. (2017), existing approaches to training adversarial networks are equivalent to minim
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2104246439	troduction There has been signiﬁcant progress on English sentiment analysis in recent years using models based on neural networks (Socher et al., 2013; ˙Irsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015). Most of these, however, rely on a massive amount of labeled training data or ﬁne-grained annotations such as the Stanford Sentiment Treebank (Socher et al., 2013), which provide
2413332972	Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification	2142262074	use early stopping to select the best model on the validation set. 4 Related Work Cross-lingual Sentiment Analysis is motivated by the lack of high-quality labeled data in many non-English languages (Mihalcea et al., 2007; Banea et al., 2008, 2010). For Chinese and Arabic in particular, there are several representative works (Wan, 2008, 2009; He et al., 2010; Lu et al., 2011; Mohammad et al., 2016a). Our work is compa
2418050709	Learning Stylometric Representations for Authorship Analysis	9292421	than character n-grams and syntactic n-grams when all the possible n-grams are used as features [Ding et al. 2015]. Moreover, it has been shown to be effective in identifying the gender of tweeters [Burger et al. 2011]. However, the study presented by Rao and Yarowsky [2010] shows that the socio-linguistic features outperform the lexical n-gram approach (n21;2) when characterizing gender and age, but when characte
2418050709	Learning Stylometric Representations for Authorship Analysis	1014449310	the documents into a simple logistic regression model to predict the characteristics of a text’s author. 4.1. The Twitter characterization dataset We choose the ICWSM 2012 labeled Twitter dataset [Al Zamal et al. 2012] in our experiment. This dataset consists of three categories of labels, and it is publicly available. Due to the limitation of Twitter’s policy, the actual content of tweets were not included with t
2418050709	Learning Stylometric Representations for Authorship Analysis	9292421	e used for AA studies. They have been shown to be effective for authorship veriﬁcation [Halvani and Steinebach 2014; Potha and Stamatatos 2014], attribution [Nasir et al. 2014], and characterization [Burger et al. 2011]. Sapkota et al. [2014] show that character n-gram features are robust and perform well even in the condition where the training data and testing data are on different topics. In contrast, our previo
2418050709	Learning Stylometric Representations for Authorship Analysis	2158698691	eir ~v ! into a single one for each !. To measure the performance of the proposed approaches and the baselines on this problem, we use the Area Under Receiver Operating Characteristic curve (AUROC) [Fawcett 2006]. It is a well-known evaluation measure for binary classiﬁers where both positive labels and negative labels are equally important. Since changing the threshold of the similarity value results in dif
2418050709	Learning Stylometric Representations for Authorship Analysis	2002018624	and Ganascia 2014]. AA techniques are also used to quantify the performance of literary translators, since the best translators will not have their own writing style reﬂected in the translated works [Almishari et al. 2014]. Moreover, AA techniques have been used to understand the personality of students [Almishari et al. 2014], ﬁrst languages [Almishari et al. 2014; Torney et al. 2012], and self-reported names [Liu an
2418050709	Learning Stylometric Representations for Authorship Analysis	1014449310	given author. We attempt to include only the tweets that are authentically authored by the labeled Twitter user. The labels in this dataset are generated semi-automatically and manually inspected [Al Zamal et al. 2012]. This dataset consists of three categories of labels for Twitter users: age, gender, and political orientation. The cleaned dataset is summarized in Table IV. There are 1170 Twitter users in total.
2418050709	Learning Stylometric Representations for Authorship Analysis	1014449310	irthday to me”. — Political orientation. This dataset provides political Twitter users with a label: either Democrat or Republican. Twitter users are collected from the wefollow Twitter directory [Al Zamal et al. 2012]. Figure 7 shows the empirical distribution, kernel density and histogram on the tweets’ length in terms of lexical token size. In general, tweets are very short text snippets. 90 percent of tweets h
2418050709	Learning Stylometric Representations for Authorship Analysis	2119804197	social blogs [Yang and Chow 2014; Stolerman et al. 2014]. Stylometric techniques have been used as evidence in the form of expert knowledge in the courts of the UK, the US, and Australia [Juola 2006; Brennan et al. 2012]. In a well-known case in the UK, linguistic experts showed Authors’ addresses: Benjamin C. M. Fung (corresponding author), School of Information Studies, McGill University, Montreal, QC, Canada H3A
2418050709	Learning Stylometric Representations for Authorship Analysis	1014449310	other SVM-based model trained on the features that adds additional social-network features (e.g., average of the neighborhood’s feature vectors). We notice that the baseline measures adopted from [Al Zamal et al. 2012] have more advantages to our proposed approaches and our baseline approaches. First, they use a SVM model that typically outperforms a simple logistic regression model when the same data is given. Se
2418050709	Learning Stylometric Representations for Authorship Analysis	1014449310	t to the Area Under Receiver Operating Characteristic curve (AUROC). By using a simple logistic regression classiﬁer over the learned representations, we achieve the best result on the ICWSM 2012 [Al Zamal et al. 2012] authorship characterization dataset without any social network related structural information. The characteristics include age range, gender, and political orientation. The rest of this paper is org
2418050709	Learning Stylometric Representations for Authorship Analysis	68298479	other types of businesses. Moreover, most of social network analyses are based on some attributes of the user inside the network, and AA techniques can provide more hidden labels for these analyses [Cohen and Ruths 2013]. — Literary science and education. Authorship analysis also has a signiﬁcant impact in the ﬁelds related to literary science. Many AA techniques have been developed to infer the disputed authorship
2418050709	Learning Stylometric Representations for Authorship Analysis	2119804197	ues can be dated back to the 19th century. Many customized approaches focusing on different sub-problems and scenarios have been proposed [Stamatatos 2009]. It has been a successful line of research [Brennan et al. 2012]. Research problems in authorship analysis can be broadly categorized into three types: authorship identiﬁcation (i.e., identify the most plausible author given a set of candidates [Iqbal et al. 2013
2469382944	Saying is not modelling	1542417898	Description logic [19] is a good example of logic appropriated to knowledge representation.
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2395899413	, 1994) AX/ABX discrimination (Carlin, Thomas, Jansen, & Hermansky, 2011; Schatz et al., 2013), cosine similarity (Landauer & Dumais, 1997)
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2055408826	, 1994; Sakas &amp; Fodor, 2012). An illustration of such a system for learning phonemes and words from raw speech uses a very speciﬁc generative architecture to guide the learning process (Lee &amp; Glass, 2012; Lee, O’Donnell, &amp; Glass, 2015, see Figure 3). The second idea is that of soft constraints coming from a large interconnected system. Instead of trying to learn each subcomponent of language in i
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2055408826	and baselines (see www.zerospeech.com) attracted considerable interest in the community of speech technology (Versteegh, Anguera, Jansen, &amp; Dupoux, 2016). Such so-called zero-resource algorithms (Glass, 2012; Jansen, Dupoux, et al., 2013) are not only interesting models of infant early phonetic and lexical acquisition, they can also provide technical solutions for the 18 E. DUPOUX Figure 5. The learning
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2055408826	ch shorter (30 ms), highly context-sensitive acoustic events. To ﬁnd phoneme-sized units would seem to require a dierent algorithm with strong priors on the temporal structure of phonemes (Lee &amp; Glass, 2012). This example reveals that contrary to the hypothesis in Maye et al. (2002), ﬁnding phonetic units is not only a problem of constructing categories (clustering), it is also a problem of segmenting co
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2010315297	Current supercomputers can simulate at a synapse level only a fraction of a brain and several orders of magnitude slower than real time (Kunkel et al., 2014).
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2010188467	put to discover candidate words (Ten Bosch &amp; Cranen, 2007, see also Park &amp; Glass, 2008; Muscariello, Gravier, &amp; Bimbot, 2009, etc.), or to learn word-meaning associations (see a review in Räsänen, 2012, and a comprehensive model in Räsänen &amp; Rasilo, 2015), although the speech was collected in the laboratory, not in real life situations. In sum, developmental AI represents the clearest attempt s
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2134453531	ey, 2000), or completely close o the corpora to anybody outside the institution that has recorded the data (as in the Riken corpus, Mazuka, Igarashi, &amp; Nishikawa, 2006, or the Speechome corpus D. Roy, 2009). The ﬁrst strategy sacriﬁces privacy and is impossible to scale up to dense recordings. The second strategy puts such an obstacle to the scientiﬁc use of the corpora that it almost defeats the purpos
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2063597751	istic signals. Dysﬂuencies or speech errors at many levels (Fromkin, 1984), as well as individuallevel sources of variability, added to structural ambiguity at all linguistic levels (e.g., homophony: Ke, 2006) may make the learning problem orders of magnitude more dicult than in simpliﬁed situations. Yet, real inputs may also bring about potential beneﬁts in the shape of side information. As an example, s
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2604132379	later, the entire speech processing pipeline has been replaced by neural networks trained endto-end, with performance claimed to achieve human parity on a dictation task (Xiong et al., 2016, but see Saon et al., 2017). In the following, we very brieﬂy review how such systems are constructed before turning on whether they could be used to inform infants language acquisition studies. 5.1 The new AI spring One import
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	1933349210	Musolino, 2002) visual question answering (Antol et al., 2015)
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2134453531	ord virtually unlimited amounts of good quality audio and video data in children’s environments. Perhaps the most ambitious data collection eort so far has been done within the Speechome project (D. Roy, 2009), where video and audio equipment was installed in each room of an apartment, recording 3 years’ worth of data around one infant. Wearable recorders (see for instance the LENA system, Xu et al., 2008)
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2110485445	problem of segmenting a continuous stream of phonemes into word-like units. One idea is to use distributional properties that distinguish within word and between word phoneme sequences (Harris, 1954; Elman, 1990; Christiansen, Conway, &amp; Curtin, 2005). A second idea is to simultaneously build a lexicon and segment sentences into words (Olivier, 1968; de Marcken, 1996; Goldwater, 2007). These ideas are now
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	2110485445	sed of rather classical elements popularized in the late 80’s the (the multi-layer perceptron, backpropagration training, convolutional networks, recurrent networks: Rumelhart &amp; McClelland, 1986; Elman, 1990). What has changed, though, is the scale of the networks and the volume of data on which they are trained, enabled by tremendous progress in computer hardware and in mathematical optimization techniqu
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	1480583224	sed in Section 5 (speech recognition: Amodei et al., 2016; object recognition: Girshick, Donahue, Darrell, &amp; Malik, 2016; action recognition: Rahmani, Mian, &amp; Shah, 2016; emotion recognition: Kahou et al., 2015) will enable the semi-automatic annotations of large amounts of data. As for ethical issues, the main challenge is to ﬁnd a point of equilibrium between the requirement of sharability and open scienti
2483390977	Cognitive Science in the era of Artificial Intelligence: A roadmap for reverse-engineering the infant language-learner	1524333225	t, this is changing quickly as open source speech databases are being constructed (for instance, the Librispeech dataset9, Panayotov, Chen, Povey, &amp; Khudanpur, 2015, and the Kaldi speech tools10, Povey et al., 2011). 5 Feasibility and Challenges To address the feasibility of the reverse engineering approach as applied to early language acquisition, we ﬁrst limit ourselves to the following simplifying framework:
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2160660844	, since a small portion of slang words from UD have also been deﬁned by existing sentiment lexicons, such as SentiWordNet (1), LIWC (13), MPQA (12), and the sentiment lexicon compiled by earlier work (3), we leveraged these existing deﬁnitions to label the corresponding words. For example, we labeled the words obtained from UD that also appear in existing sentiment lexicons, such as “hilarious” and “
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	50799546	used to analyze slang words. The vocabulary can help identify slang words in online reviews (17). Chen et al. further identify sentiment expressions by using UD as a resource of slang word vocabulary (18). Our work is different from theirs, since we further leverage different resources to estimate the sentiment strength for slang words from UD, instead of merely using the vocabulary. Table 1 illustrat
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2251939518	ation in sentences (7). Moreover, building domain-speciﬁc lexicons (8) and leveraging crowdsourced annotations (9) have been studied in previous work. Other methods directly model sentence sentiments (10,11). Our work is also related to generating lexical databases. The most commonly used English lexical dictionary is WordNet (4), where words, meanings, relationships are well compiled in the structured d
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	1743243001,2137958601	ble 3: Accuracy of different models. Method Twitter SMS DeeplyMoving 56.17% 66.28% SentiStrength 65.08% 73.15% SentiStrength SSD 84.84% 86.55% As a common practice in labeling tweets at a large scale (20,21), we use a predetermined set of emoticons11 to tag the sentiment strength of tweets. The SMS dataset is sampled from SemEval 2013 dataset12, which is publicly available. The dataset contains sentences
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2081580037	not capable of measuring slang sentiment words. For example, the commonly used lexicons including SentiWordNet (1), Micro-WNOp (2), and the lexicon used in (3)2, all adopted the vocabulary of WordNet (4), where slang words and phrases are not present. In this work, we introduce the ﬁrst sentiment dictionary of slang words. Building an extensive sentiment dictionary is challenging since (1) Most slang
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2137642042	dictionary for analyzing sentiment in short and informal user-generated content. Urban Dictionary has been used to analyze slang words. The vocabulary can help identify slang words in online reviews (17). Chen et al. further identify sentiment expressions by using UD as a resource of slang word vocabulary (18). Our work is different from theirs, since we further leverage different resources to estima
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2160660844	ed by classifying the sentiment strength of WordNet synsets. Commonly used sentiment dictionaries also include Harvard Inquirer5, Micro-WNOp (2), MPQA (12), LIWC (13), VADER (14), the lexicon used in (3)6, etc. Other methods focus on structures and compositions of sentences (10,11). Existing efforts on identifying and measuring sentiment slang words mainly focus on speciﬁc corpus, by leveraging the c
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2251939518	mental Settings Sentiment analysis methods can be generally categorized into sentence-structure-based methods and lexicon-based methods. Sentence-structure-based methods focus on composition of words (11) and lexicon-based methods focus on ﬁnding key subjective expressions (22), such as sentiment words/phrases and negations, to determine the polarity. Two corresponding algorithms below are chosen for
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2081580037,2160660844	been proposed over the past few years to generate lexicons. For example, sentiment strengths can be propagated with synonymous relationships between words. The relationship can be synsets in WordNet (3,4), syntactic patterns in corpora (6), and contextual information in sentences (7). Moreover, building domain-speciﬁc lexicons (8) and leveraging crowdsourced annotations (9) have been studied in previo
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2159457224	s to generate lexicons. For example, sentiment strengths can be propagated with synonymous relationships between words. The relationship can be synsets in WordNet (3,4), syntactic patterns in corpora (6), and contextual information in sentences (7). Moreover, building domain-speciﬁc lexicons (8) and leveraging crowdsourced annotations (9) have been studied in previous work. Other methods directly mod
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2251939518	sentiment dictionaries also include Harvard Inquirer5, Micro-WNOp (2), MPQA (12), LIWC (13), VADER (14), the lexicon used in (3)6, etc. Other methods focus on structures and compositions of sentences (10,11). Existing efforts on identifying and measuring sentiment slang words mainly focus on speciﬁc corpus, by leveraging the context of slang words. For example, “LOL” is found to appear frequently around
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2160660844	of all slang words on UD fall in it. Sentimentpropagation: A list of related words may be available for slang words in UD. Such synonymous relationships have been studied to infer sentiment polarity (1,3), since words with similar meanings are likely to share the same sentiment polarity. In particular, we use the slang words with sentiment strength as the seed set, and then annotate the connected unla
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2040467972	can be synsets in WordNet (3,4), syntactic patterns in corpora (6), and contextual information in sentences (7). Moreover, building domain-speciﬁc lexicons (8) and leveraging crowdsourced annotations (9) have been studied in previous work. Other methods directly model sentence sentiments (10,11). Our work is also related to generating lexical databases. The most commonly used English lexical dictiona
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2086277751	t strengths can be propagated with synonymous relationships between words. The relationship can be synsets in WordNet (3,4), syntactic patterns in corpora (6), and contextual information in sentences (7). Moreover, building domain-speciﬁc lexicons (8) and leveraging crowdsourced annotations (9) have been studied in previous work. Other methods directly model sentence sentiments (10,11). Our work is a
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2081580037	Table 1: Comparison of related lexical resources in terms of vocabulary and sentiment polarity. Formal Words Slang Words Sentiment Existing Lexicons (1–3,12–14) Extensive Incomplete Available WordNet (4) Extensive Unavailable Unavailable UrbanDictionary Incomplete Extensive Unavailable SlangSD Incomplete Extensive Available 3 SlangSD: A Sentiment Dictionary of Slang Words In this section, we introduc
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	40549020	timent slang words mainly focus on speciﬁc corpus, by leveraging the context of slang words. For example, “LOL” is found to appear frequently around “funny”, so “LOL” probably means causing amusement (15). Tang et al. obtain a seed set of slang words from UD to help generate representation for words in Twitter (16). These methods are limited by the completeness of the corpus, which usually generate do
2517796890	SlangSD: building, expanding and using a sentiment dictionary of slang words for short-text sentiment classification	2160660844	yzing formal content, existing lexicons are not capable of measuring slang sentiment words. For example, the commonly used lexicons including SentiWordNet (1), Micro-WNOp (2), and the lexicon used in (3)2, all adopted the vocabulary of WordNet (4), where slang words and phrases are not present. In this work, we introduce the ﬁrst sentiment dictionary of slang words. Building an extensive sentiment di
2526012259	A Hackathon for Classical Tibetan	2006832571	and late, translated and autochthonous texts. We used a perceptron classifier with stochastic gradient descent. For our experiments on translations versus autochthonous, we used features similar to [Volansky et al., 2015], mainly: mean syllable length; mean sentence length; frequency of verbal prefixes and function words; frequency of foreign (Sanskrit) words; and type-to-token ratio. For authorship detection, we fir
2526012259	A Hackathon for Classical Tibetan	2250739653	LSTMs have been used in the past for word segmentation of Chinese text [Chen et al., 2015].
2539360289	Stylometric analysis of Early Modern period English plays	1505524995	ods rely mainly on the frequency of usage of function words. Numerous other stylistic features have since been used in authorship attribution studies, including the study of vocabulary richness [18], [19] and the use of part of speech taggers [20]. Our method for attributing texts, developed in [21], also measures function word usage to distinguish author styles. Rather than only considering word freq
2539360289	Stylometric analysis of Early Modern period English plays	2096194908	order to gain further insight into its underlying elements, such as authorship or genre. Along with common uses in digital forensics (De Vel et al., 2001; Stamatatos, 2009) and plagiarism detection (Meuschke and Gipp, 2013), stylometry has also become the primary method for evaluating authorship disputes in historical texts, such as the Federalist papers (Mosteller and Wallace, 1964; Holmes and Forsyth, 1995) and the Mo
2539360289	Stylometric analysis of Early Modern period English plays	2126631960	the quantitative analysis of a text’s linguistic features in order to gain further insight into its underlying elements, such as authorship or genre. Along with common uses in digital forensics [1], [2] and plagiarism detection [3], stylometry has also become the primary method for evaluating authorship disputes in historical texts, such as the Federalist papers [4], [5] and Mormon scripture [6], in
2539360289	Stylometric analysis of Early Modern period English plays	2055699824	wing one particular word with another. We can then quantify similarity between WANs by using a measure of relative entropy. Markov chains have previously been used in (Khmelev and Tweedie, 2001) and (Sanderson and Guenter, 2006) for the purposes of authorship attribution, though neither consider the use of function words. Results in (Segarra et al., 2015) show an increase in attribution accuracy compared to frequency-based m
2549988729	Probabilistic Models for Semantic Representation	1987248663,2101075635	While Concept – concept relations could be modeled using the prototype theory that plays a central role in linguistics, as part of the mapping from phonological structure to semantics [12], most interesting for us, the Concept – action relations can be revealed using the theory of emergent semantics pointed out by Santini and Grosky [13, 14].
2549988729	Probabilistic Models for Semantic Representation	1984251878	The description of both Word – Word and Word – Concept relations, related to the light part of semantics, is based on an extension of the computational model depicted above and discussed in [1] and [11].
2549988729	Probabilistic Models for Semantic Representation	1546802190	On the other hand, it has been shown how (see, for instance [19]) uncertainty exists in almost every aspects of ontology engineering, and probabilistic directed Graphical Models (GMs) such as Bayesian Nets (BN) can provide a suitable tool for coping with uncertainty.
2549988729	Probabilistic Models for Semantic Representation	1984251878	In the light of this discussion we argue that, as pointed out by Steyvers and his colleagues [1], the semantic knowledge can be thought of as knowledge about relations among several types of elements, including words, concepts, and percepts.
2549988729	Probabilistic Models for Semantic Representation	1984251878	The main idea here is the introduction of a method for automatic construction of ontology based on the extension of the probabilistic topic model introduced in [1] and [23].
2549988729	Probabilistic Models for Semantic Representation	1484719069	Most of them are manual, however among the semiautomatic and automatic methods we can distinguish these based on Machine Learning techniques from these based on pure Artificial Intelligence theory [18].
2549988729	Probabilistic Models for Semantic Representation	1984251878	Semantic knowledge can be thought of as knowledge about relations among several types of elements: words, concepts, percepts and actions [1].
2549988729	Probabilistic Models for Semantic Representation	1984251878	Specifically, the description of both Word – word and Word – concept relations, namely light semantics, is based on an extension of the computational model, namely the topic model, introduced by Steyvers in [1] and [11].
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2163107094	: http://spark.apache.org/mllib/ 20 See details in the source code, class ru.ispras.atr.utils.Cacher. 12 Nikita Astrakhantsev 4Evaluation 4.1 Experiments design We evaluate ATR4S on 7 datasets: GENIA [23], FAO [32], Krapivin [26], Patents [21], ACL RD-TEC [45], ACL RD-TEC 2.0 [39], EuroParl [24] with Eurovoc thesaurus21. See table 1 for summary statistics. Table 1 Datasets summary statistics Dataset D
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	1546842539	5]. Despite this importance, the ATR task is still far from being solved: researches continue to propose new methods for ATR, which usually show average precision below 80% even on top 500-1000 terms [17,46,47] and thus are hardly used in practice. Moreover, there is still no fair and reliable comparison of already developed methods. Most works compare 1-2 newly proposed methods with several old baselines o
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	1546842539	6]. Note that linear combination does not require scores of other term candidates to be computed in advance, so it is simpler and faster18, but misses potentially useful information. Voting algorithm [46] considers values of all term candidates and it was shown [46] to outperform single methods and weighted average (i.e. linear combination): V (t) = Xn i=1 1 r(f i(t)) , (13) where r(f i(t)) is a rank
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2049107599	cescontexts Methods from this group follows the distributional hypothesis [20] and try to distinguish terms from non-terms by considering their contexts. We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modiﬁcation of the former and was shown to work better [6], ATR4S includes only it. DomainCoherence works in 3 steps. First, it extracts 200 best term c
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2030314525	d from other words and collocations by comparing occurrences statistics of considereddomain-speciﬁc collection with statistics ofsome reference corpus (usually, from general domain). DomainPertinence [33] is the simplest implementation of this idea: DomainPertinence(t) = TF target(t) TF reference(t) , (7) where TF target(t) is a frequency of term candidate t in target (domain-speciﬁc) collection; TF r
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2049107599	ds constituting term candidate and ignoring other information. Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3]. ATF simply normalizesterm frequency by number of documents containing this term candidate. TF-IDF is a classical information retrieval measure showing high values for
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	179719743	en 0 and 1), thus reducing the task to ranking by one method. One of the most popular method is a linear combination of features with somepredeﬁned(usually,equal) coeﬃcients.Examplesinclude PostRankDC[6] and GlossEx [36]. Note that linear combination does not require scores of other term candidates to be computed in advance, so it is simpler and faster18, but misses potentially useful information. Vo
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	179719743	etter quality [17]; also it shows that other available tools lack the best methods, i.e. actual state-of-the-art methods, namely PU-ATR [3], KeyConceptRelatedness [3], NovelTopicModel [27], and Basic [6]. It is obvious that ATR4S does not include all methods capable to outperform already implemented ones on some settings, but we believe that these implementations can be used as a basis for developmen
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2164946598	Expected Terms Source of terms GENIA Biomedicine 2000 494 35104 Manual markup FAO Agriculture 779 26672 1554 Authors’ keywords Krapivin Computer Science 2304 21189 8766 Authors’ keywords and Protodog [16] glossary Patents Engineering 12 120 1595 Manual markup ACL Computational Linguistics 10085 41202 21543 Manual annotation of top 82000 candidates ACL 2.0 Computational Linguistics 300 33 3095 Manual m
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	1593045043	can extract more speciﬁc terms and vice versa. Note that ATR4S does not include methods based on word association measures like z-test[12],t-test [9],χ2-test, Loglikelihood[13],Mutual Information(MI)[10], LexicalCohesion[36], Term Cohesion[25], because they wererepeatedly shown to obtain not better results than simple frequency [44,35,47]. 3.3.2Methodsbasedonoccurrencescontexts Methods from this grou
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	179719743	this group follows the distributional hypothesis [20] and try to distinguish terms from non-terms by considering their contexts. We are aware of only 2 such methods: NC-Value [18] and DomainCoherence [6]; since the latter is a modiﬁcation of the former and was shown to work better [6], ATR4S includes only it. DomainCoherence works in 3 steps. First, it extracts 200 best term candidates by using Basic
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	179719743	ing term candidate and ignoring other information. Besides Term frequency (TF) itself, this group contains Average term frequency (ATF) [47], TF-IDF [15], Residual IDF (RIDF) [47], CValue [18], Basic [6], and ComboBasic [3]. ATF simply normalizesterm frequency by number of documents containing this term candidate. TF-IDF is a classical information retrieval measure showing high values for term candid
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	1546842539	n; it is written in Scala with parallel collections wherever appropriate, so it utilizes all CPU cores. Experimental comparison conﬁrms observations that (1) no single method is best for all datasets [46] and (2) multiple features should be combined for better quality [17]; also it shows that other available tools lack the best methods, i.e. actual state-of-the-art methods, namely PU-ATR [3], KeyConce
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2245896265	ndidates that occur frequently, but not as parts of other term candidates. This method was supposed to work with multi-word term candidates only; ATR4S includes modiﬁcation proposed by Ventura et al. [43] that supports one-word term candidates as well: C-Value(t) = ( log2(|t| +0.1)· TF(t), if {s : t ⊂ s} = ∅; log2(|t| +0.1)·  TF(t)− P s TF(s) |{s:t⊂s}|  , else. (3) where |t| is a length of term cand
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	1546842539	odular and adaptable. However, it lacks a lot of actual state-of-the-art methods, namely those based on occurrencescontexts, topic models, Wikipedia, and non-trivial ranking algorithms such as Voting [46] and PU-ATR [3]. It also depends on Apache Solr7, which may simplify its integration to the application that already uses Solr, but may as well complicate its usage as a library. 3Architecture ATR4S f
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2123442489	to sentences, tokenizes obtained sentences, and ﬁnds part of speech tags and lemmas for obtained tokens. In order to perform these tasks, ATR4S incorporated 3 external NLP libraries: Stanford CoreNLP [30], Emory nlp4j8 and Apache OpenNLP9. We use the ﬁrst one in all experiments10. 3.2 Term candidates collection ATR4S extracts consecutive word n-grams of speciﬁed orders (by default, from 1 to 4) as ter
2553301550	ATR4S: toolkit with state-of-the-art automatic terms recognition methods in Scala	2127776561	test[12],t-test [9],χ2-test, Loglikelihood[13],Mutual Information(MI)[10], LexicalCohesion[36], Term Cohesion[25], because they wererepeatedly shown to obtain not better results than simple frequency [44,35,47]. 3.3.2Methodsbasedonoccurrencescontexts Methods from this group follows the distributional hypothesis [20] and try to distinguish terms from non-terms by considering their contexts. We are aware of o
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1813623662	In [2], we propose to represent semantic relations in terms of intrinsic and algebraic properties.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1572520478,1813623662	In [4] we do an analysis of “task-neutral” LR and point out the need to provide relations with intrinsic semantics in order to prevent the taxonomic flaws of these resources, and in [2, 3] we propose to divide the semantics of relations into algebraic and intrinsic properties and to apply the principles of SE to the development of OBLR.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1813623662	with the contents of the semantic link around which the backbone taxonomy is constructed? Third, although the algebraic properties of relations can be well understood in our proposal [2], the intrinsic properties are left unspecified.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1590255531	Once this is done, it uses a set of principles [16] to create and structure a backbone taxonomy where the differences and similarities between concepts are expressed in natural language.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1998567684	As explained in [21], RET is an effort to provide an exhaustive as possible classification (under the form of a taxonomy) of binary semantic relations (here, the reader must notice the resemblance between RET and the relationship classification of OCD), on the basis of the nature of the relation between a parent or domain concept and a child or range concept.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1590255531,2056420273	Moreover, although the ontology structuring problems represent a serious obstacle in the development and integration of ontological resources, there are only two proposals that actually deal with it: The OntoClean [15] and [16] methodologies.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1998567684	Moreover, providing that we have a suitable algebra, there is the possibility of doing plausible inference [21] by using the semantic primitives of relations to infer new relation instances between sets of entities.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1813623662,1998567684	Moreover, although the underspecification of semantic links has been pointed out for main OBLR [4, 11], few initiatives [2, 21] exist that aim at providing not only a richer semantics to these links but also to propose specific operations that can be performed on them.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1549515149,1572520478,1813623662	As part of the SINAMED and ISIS projects [1], we have pointed out in [2, 3] the need to: a) provide the relations used in ontologies with definite semantics and b) to develop ontology-based linguistic resources (OBLR) following a software engineering approach.
2554277174	Semantic Relation Modeling and Representation for Problem-Solving Ontology-based Linguistic Resources: Issues and Proposals	1998567684	In spite of this, [21] argues that the scope of these primitives could be restricted to a knowledge domain of interest or to a context within a knowledge domain, in order to avoid the complications that arise when aiming for a universal set of primitives that could describe every relation in every domain.
2560181496	A Grammatical View of Language Evolution	1497124204	See also [10, 11, 13] for comparison with other attempts to adaptable or extensible grammar formalisms.
2560181496	A Grammatical View of Language Evolution	1497124204	In [13], the author shows how such grammars may capture standard non-context-free languages used in the literature (e.
2560181496	A Grammatical View of Language Evolution	1492825967	[32] for the emergence of the “-ed” suffix Closely related to the problem of language change is the formal representation of language death [1, 38, 37], bilingualism [36, 8, 49] and creolization / dialectalization [42, 43].
2560181496	A Grammatical View of Language Evolution	1586357756	Also, [30] has developed a grammar induction method that produces Stochastic Context Free Grammars.
2560181496	A Grammatical View of Language Evolution	1555990983,2161288717	Currently, the research in diachronic change is based on the understanding of language as a complex adaptive system [55] and an evolutionary system [16, 5].
2560181496	A Grammatical View of Language Evolution	1509180139	In fact, our approach is more closely related to abductive reasoning in logic programmming (see [18] for an overview), as it proceeds by adding one rule or as few new rules as possible in order to accommodate to one sample at a time, rather that revising everything.
2560181496	A Grammatical View of Language Evolution	2057292349,2170837769	g, [20, 15, 19, 26], also known as grammatical inference, automatic language acquisition or automata induction, is a specialized subfield of machine learning that deals with the learning of formal languages from a set of data.
2560181496	A Grammatical View of Language Evolution	2062686286	Inductive logic programming [39, 31, 41, 40] is also a subfield of machine learning particularly useful in bioinformatics and natural language processing, which also represent powerful methods for grammar induction, e.
2560181496	A Grammatical View of Language Evolution	2097988708,2150961342	Iterated learning [25, 52] is a model for language evolution through language acquisition that was introduced by Kirby in 2001.
2560181496	A Grammatical View of Language Evolution	2150961342	Language evolution through language acquisition has several key contributions in the works of Briscoe [6], Kirby [25], Komarova, Niyogi & Nowak [28] and Komarova & Nowak [29].
2560181496	A Grammatical View of Language Evolution	1976274322	The origins and emergence of language was first tackled from a computational and formal perspective by the pioneering work of Steels [53, 54], and summarized by Knight, Studdert-Kennedy & Hurford [27].
2560181496	A Grammatical View of Language Evolution	2041726556	We may refer to [12] that describes a quite analogous system for abductive reasoning in which a constraint-logic implementation attempts to guess new Prolog rules that makes a given top-level goal provable; such techniques might also apply to the present application.
2560181496	A Grammatical View of Language Evolution	1497124204	A surprisingly simple implementation of adaptable grammars in Prolog was shown in [13], which will be extended below with additional auxiliaries for rule generation; this implementation makes adaptable grammars also a powerful tool for experimentation.
2560346187	Video Captioning with Multi-Faceted Attention	2142900973	,839 video/description pairs. Each description on average contains about 8 words. We use 1,200 videos for training, 100 videos for validation and 670 videos for testing, as provided by previous work (Guadarrama et al., 2013). MSR-VTT: We also evaluate on the MSR Videoto-Text (MSR-VTT) dataset (Xu et al., 2016), a new large-scale video benchmark for video captioning. MSR-VTT provides 10,000 web video clips. Each video is
2560346187	Video Captioning with Multi-Faceted Attention	1573040851	(T) and using only motion features (M). We compare our methods with six state-of-the-art methods: LSTM-YT (Venugopalan et al., 2015b), S2VT (Venugopalan et al., 2015a), TA (Yao et al., 2015), LSTM-E (Pan et al., 2016), HRNE-A (Pan et al., 2015), and h-RNN (Yu et al., 2015). Table 1 pro0.429 0.411 0.392 0.367 0.356 0.345 0.323 0.316 0.313 0.304 0.304 0.300 0.320 0.340 0.360 0.380 0.400 0.420 0 20 40 60 80 100 Noisy
2560346187	Video Captioning with Multi-Faceted Attention	1586939924	al., 2015a; Donahue et al., 2015; Venugopalan et al., 2015a; Venugopalan et al., 2016) of frame-level features, or on a dynamic linear combination of context vectors obtained via temporal attention (Yao et al., 2015). Recently, hierarchical recurrent neural encoders (HRNE) with attention mechanism have been proposed to encode video (Pan et al., 2015). A recent paper (Yu et al., 2015) additionally exploits several
2560346187	Video Captioning with Multi-Faceted Attention	1753482797	anslation. Some of the ﬁrst widely noted successes of deep sequence-to-sequence learning models were for the task of machine translation (Cho et al., 2014b; Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Li et al., 2015; Lin et al., 2015). In several respects, this is actually a similar task to video caption generation, just with a rather different input modality. What they share in common is that b
2560346187	Video Captioning with Multi-Faceted Attention	2136036867,2139501017	d p t using the model described above. Thus, we can generate captions starting from the special symbol hBOSiwith Beam Search (Yu et al., 2015). Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr LSTM-YT (Venugopalan et al., 2015b) - - - 0.333 0.291 - S2VT (Venugopalan et al., 2015a) - - - - 0.298 - TA (Yao et al., 2015) 0.800 0.647 0.526 0.419 0.296 0.517 TA 0.811 0.655 0.541 0.422 0.304 0.524 LSTM-E (Pan et al., 2016) 0.788
2560346187	Video Captioning with Multi-Faceted Attention	2154652894	d on temporal ResNet-152 features. 4.3 Evaluation Metrics We rely on four standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work. We use the Microsoft COCO evaluation server (Chen et a
2560346187	Video Captioning with Multi-Faceted Attention	2110485445	detail, including our novel multi-faceted attention mechanism to consider temporal, motion, and semantic attribute perspectives. 3.1 Long Short Term Memory Networks A Recurrent Neural Network (RNN) (Elman, 1990) is a neural network adding extra feedback connections to feed-forward networks, so as to be able to work with sequences. The network is updated not only based on the input but also based on the previ
2560346187	Video Captioning with Multi-Faceted Attention	2122180654	egarded as a greatly simpliﬁed case of video captioning, with videos consisting of just a single frame. Recurrent architectures are often used here as well (Karpathy et al., 2014; Kiros et al., 2014; Chen and Zitnick, 2015; Mao et al., 2015; Vinyals et al., 2015). Spatial attention mechanisms allow for focusing on different areas of an image (Xu et al., 2015b). Recently, image captioning incorporating semantic concepts
2560346187	Video Captioning with Multi-Faceted Attention	1811254738	el relies on several attention layers to selectively focus on important parts of temporal, motion, and semantic features. The output words are generated via a softmax reading from a multimodal layer (Mao et al., 2015), which integrates information from the different attention layers. An additional multimodal layer integrates information before the input reaches the sentence generator to enable better hidden repres
2560346187	Video Captioning with Multi-Faceted Attention	1586939924	en proposed to capture motion information in short videos, while LSTMs (Hochreiter and Schmidhuber, 1997) can be used to generate natural language, and a variety of different visual attention models (Yao et al., 2015; Pan et al., 2015; Yu et al., 2015) have been deployed, attempting to capture the relationship between caption words and the video content. These methods, however, only make use of visual information
2560346187	Video Captioning with Multi-Faceted Attention	1573040851	enugopalan et al., 2015b) - - - 0.333 0.291 - S2VT (Venugopalan et al., 2015a) - - - - 0.298 - TA (Yao et al., 2015) 0.800 0.647 0.526 0.419 0.296 0.517 TA 0.811 0.655 0.541 0.422 0.304 0.524 LSTM-E (Pan et al., 2016) 0.788 0.660 0.554 0.453 0.310 - HRNE-A (Pan et al., 2015) 0.792 0.663 0.551 0.438 0.331 - h-RNN (Yu et al., 2015) 0.815 0.704 0.604 0.499 0.326 0.658 h-RNN 0.824 0.711 0.610 0.504 0.329 0.675 T (Ours
2560346187	Video Captioning with Multi-Faceted Attention	1895577753	eo captioning, with videos consisting of just a single frame. Recurrent architectures are often used here as well (Karpathy et al., 2014; Kiros et al., 2014; Chen and Zitnick, 2015; Mao et al., 2015; Vinyals et al., 2015). Spatial attention mechanisms allow for focusing on different areas of an image (Xu et al., 2015b). Recently, image captioning incorporating semantic concepts have achieved inspiring results. A seman
2560346187	Video Captioning with Multi-Faceted Attention	2123301721	ers (Pan et al., 2015) to predict the subject and verb separately based on temporal ResNet-152 features. 4.3 Evaluation Metrics We rely on four standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work.
2560346187	Video Captioning with Multi-Faceted Attention	2133564696	es for video captioning are inspired by neural machine translation ones, including soft attention mechanisms to focus on different parts of the input when generating the target sentence word by word (Bahdanau et al., 2015). Image Captioning. Image captioning can be regarded as a greatly simpliﬁed case of video captioning, with videos consisting of just a single frame. Recurrent architectures are often used here as well
2560346187	Video Captioning with Multi-Faceted Attention	2164290393	esults of models combining visual and semantic attention on MSVD. 4 Experimental Results 4.1 Datasets MSVD: We evaluate our video captioning models on the Microsoft Research Video Description Corpus (Chen and Dolan, 2011). MSVD consists of 1,970 video clips typically depicting a single activity, downloaded from YouTube. Each video clip is annotated with multiple human generated descriptions in several languages. We on
2560346187	Video Captioning with Multi-Faceted Attention	2064675550	f attention for video captioning. Figure 2 illustrates the architecture of our model. The core of our model is a sentence generator based on generator is a simple Long Short Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997). Instead of a traditional sentence generator, which directly receives a previous word and selects the next word, our model relies on several attention layers to selectively focus on important parts o
2560346187	Video Captioning with Multi-Faceted Attention	2136036867,2139501017	f different sorts of visual cues, we also report the results of using only temporal features (T) and using only motion features (M). We compare our methods with six state-of-the-art methods: LSTM-YT (Venugopalan et al., 2015b), S2VT (Venugopalan et al., 2015a), TA (Yao et al., 2015), LSTM-E (Pan et al., 2016), HRNE-A (Pan et al., 2015), and h-RNN (Yu et al., 2015). Table 1 pro0.429 0.411 0.392 0.367 0.356 0.345 0.323 0.3
2560346187	Video Captioning with Multi-Faceted Attention	1586939924	ing only temporal features (T) and using only motion features (M). We compare our methods with six state-of-the-art methods: LSTM-YT (Venugopalan et al., 2015b), S2VT (Venugopalan et al., 2015a), TA (Yao et al., 2015), LSTM-E (Pan et al., 2016), HRNE-A (Pan et al., 2015), and h-RNN (Yu et al., 2015). Table 1 pro0.429 0.411 0.392 0.367 0.356 0.345 0.323 0.316 0.313 0.304 0.304 0.300 0.320 0.340 0.360 0.380 0.400 0.
2560346187	Video Captioning with Multi-Faceted Attention	2115613106	ly noted successes of deep sequence-to-sequence learning models were for the task of machine translation (Cho et al., 2014b; Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Li et al., 2015; Lin et al., 2015). In several respects, this is actually a similar task to video caption generation, just with a rather different input modality. What they share in common is that both require bridg
2560346187	Video Captioning with Multi-Faceted Attention	2250539671	M is tanh and the activation functions of both multimodal layers are linear. The dropout rates of both the input and output multimodal layers are set to 0:5. We use pretrained 300- dimensional GloVe (Pennington et al., 2014) vectors as our word embedding matrix. We rely on the RMSPROP algorithm (Tieleman and Hinton, 2012) to update parameters for better convergence, with the learning rate 10 4. The beam size during sente
2560346187	Video Captioning with Multi-Faceted Attention	1586939924	ol hBOSiwith Beam Search (Yu et al., 2015). Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr LSTM-YT (Venugopalan et al., 2015b) - - - 0.333 0.291 - S2VT (Venugopalan et al., 2015a) - - - - 0.298 - TA (Yao et al., 2015) 0.800 0.647 0.526 0.419 0.296 0.517 TA 0.811 0.655 0.541 0.422 0.304 0.524 LSTM-E (Pan et al., 2016) 0.788 0.660 0.554 0.453 0.310 - HRNE-A (Pan et al., 2015) 0.792 0.663 0.551 0.438 0.331 - h-RNN (Y
2560346187	Video Captioning with Multi-Faceted Attention	1947481528	oning, many works utilize a recurrent neural architecture to generate video descriptions, conditioned on either an average-pooling (Venugopalan et al., 2015b) or recurrent encoding (Xu et al., 2015a; Donahue et al., 2015; Venugopalan et al., 2015a; Venugopalan et al., 2016) of frame-level features, or on a dynamic linear combination of context vectors obtained via temporal attention (Yao et al., 2015). Recently, hier
2560346187	Video Captioning with Multi-Faceted Attention	1956340063	ower BLEU scores. Several studies, including on caption generation, have concluded that BLEU is not a sufﬁciently reliable metric in terms of replicating human judgment scores (Kulkarni et al., 2013; Vedantam et al., 2015). Figure 4 shows several example captions generated by our approach for MSVD videos. To further investigate the inﬂuence of noise, we randomly select genuine high-quality subject and verb attributes a
2560346187	Video Captioning with Multi-Faceted Attention	2064675550	parts of the output caption may pertain to different parts of the video. In previous work, 3D ConvNets (Du et al., 2015) have been proposed to capture motion information in short videos, while LSTMs (Hochreiter and Schmidhuber, 1997) can be used to generate natural language, and a variety of different visual attention models (Yao et al., 2015; Pan et al., 2015; Yu et al., 2015) have been deployed, attempting to capture the relati
2560346187	Video Captioning with Multi-Faceted Attention	1811254738	pliﬁed case of video captioning, with videos consisting of just a single frame. Recurrent architectures are often used here as well (Karpathy et al., 2014; Kiros et al., 2014; Chen and Zitnick, 2015; Mao et al., 2015; Vinyals et al., 2015). Spatial attention mechanisms allow for focusing on different areas of an image (Xu et al., 2015b). Recently, image captioning incorporating semantic concepts have achieved ins
2560346187	Video Captioning with Multi-Faceted Attention	1956340063	ptimize the hyperparameters using random search to maximize METEOR on the validation set, following previous studies that found that METEOR is more consistent with human judgments than BLEU or ROUGE (Vedantam et al., 2015). After the parameters are learned, during the testing phase, we also have temporal and motion features extracted from the video as well as semantic attributes, which were either already given or are
2560346187	Video Captioning with Multi-Faceted Attention	2101105183	rarchical recurrent neural encoders (Pan et al., 2015) to predict the subject and verb separately based on temporal ResNet-152 features. 4.3 Evaluation Metrics We rely on four standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare
2560346187	Video Captioning with Multi-Faceted Attention	2130942839	Related Work Machine Translation. Some of the ﬁrst widely noted successes of deep sequence-to-sequence learning models were for the task of machine translation (Cho et al., 2014b; Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Li et al., 2015; Lin et al., 2015). In several respects, this is actually a similar task to video caption generation, just with a rather different input modality. Wha
2560346187	Video Captioning with Multi-Faceted Attention	2251849926	s of deep sequence-to-sequence learning models were for the task of machine translation (Cho et al., 2014b; Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013; Li et al., 2015; Lin et al., 2015). In several respects, this is actually a similar task to video caption generation, just with a rather different input modality. What they share in common is that both require bridging different repre
2560346187	Video Captioning with Multi-Faceted Attention	1956340063	the subject and verb separately based on temporal ResNet-152 features. 4.3 Evaluation Metrics We rely on four standard metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) to evaluate our methods. These are commonly used in image and video captioning tasks, and allow us to compare our results against previous work. We use the Microsoft COCO eval
2560346187	Video Captioning with Multi-Faceted Attention	2136036867,2139501017	t for temporal and motion aspects. Video captioning. For video captioning, many works utilize a recurrent neural architecture to generate video descriptions, conditioned on either an average-pooling (Venugopalan et al., 2015b) or recurrent encoding (Xu et al., 2015a; Donahue et al., 2015; Venugopalan et al., 2015a; Venugopalan et al., 2016) of frame-level features, or on a dynamic linear combination of context vectors ob
2560346187	Video Captioning with Multi-Faceted Attention	2097606805	titles, or other semantic information about the videos. Nevertheless, we can reproduce a setting with semantic attributes by extracting attributes from captions. First, we invoke the Stanford Parser (Klein and Manning, 2003) to parse captions and choose the nsubj edges to ﬁnd the subject-verb pairs for each caption. We then select the most frequent subject and verb across captions of each video as the high-quality semant
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	36556894	3.4 with the actual scores. On testing with FCE-TEST (containing 97 texts), the model had a correlation of 0.64 and a MAE of 3.6. The same train-test setup was used in Yannakoudakiset al. (2011) and Yannakoudakis and Briscoe (2012). In the ﬁrst study, they reported a highest correlati on of 0.74 between the predicted and actual scores. In a second study with coherence features, they reported a highest correlation of 0.75 betwee
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2158240052	active area of research for about four decades now and several assessment systems such as Intelligent Essay Grader (Landauer et al., 2003), Project Essay Grade (Page, 2003), E-Rater (Burstein, 2003; Attali and Burstein, 2006) and IntelliMetric (Elliot, 2003) are being used in real-world applications as a companion to human graders. AES systems are typically developed using human scorer judgments as the gold standard train
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1511133876	anguage processing based approaches. The features employed to measure written language cover aspects such as: grammatical and orthographic correctness, language quality, lexical diversity and ﬂuency (Dikli, 2006). The deﬁnitions of individual aspects are typically based on hum an expert judgments. Some of the commonly 4 Automated assessment of non-native learner essays: Investigating the role of linguistic fe
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2165608467	availability of more learner corpora and language processing software for the language. However, the past half-decade saw the emergence of AES research in non-English (primarily European) languages. Ostling et al. (2013) developed an AES approach for detecting Swedish language proﬁciency using a corpus of high-school level exams conducted nationwide in Sweden. Hancke and Meurers (2013) described a proﬁciency classiﬁc
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2124725212	cy. One reason could be that the AES systems are proprietary software. Recent years saw the release of a few public datasets that can be used for developing AES models (e.g., Randall and Groom, 2009; Yannakoudakis et al., 2011; Kaggle, 2012; Blanchard et al., 2013). Yannakoudakis et al. (2011) released a corpus of learner essays written for Cambridge First Certiﬁcate in English (FCE) exam, and condu cted several experiment
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2124725212	ely on trigram frequency features, compared to the rich feature set described in this paper. FCE Corpus Our second corpus is the First Certiﬁcate of English (FCE)co rpus that was publicly released by Yannakoudakis et al. (2011). They released a corpus of First Certiﬁcate of Englis h exam transcripts, which is a subset of the larger Cambridge Learner Corpus (CLC). The corpus contains texts produced by takers of English as Se
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2133280805	es, errors, and others. All the underlying linguistic representations (tags, parses, coreference information etc.,) for the feature calculations described below are obtained from the Stanford Parser (Socher et al., 2013) and the Stanford CoreNLP package (Manning et al., 2014).2 Word Level Features This group of ﬁve features consists of the measures of lexica l diversity typically used in ﬁrst and second language acqu
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2140676672	f each of the four senses, per sentence. This group will be referred to as DISC-CONN in this paper. To our knowledge, these features too have not been used for automatic proﬁciency assessment before. Barzilay and Lapata (2008) introduced a model of text coherence based on the concept of an entity grid, where the various mentions of an entity in a text are labeled with their syntactic roles (subject, object, neither, and ab
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1971020201	features, and there is a syntactic feature from SLA (num. T-units) among the mostpredictive positive weight features, which is consistent with the results from analysis of L2 writing in SLA research Lu (2010). Number of nonAutomated assessment of non-native learner essays: Investigating the role of linguistic features 17 Table 9 TOEFL11SUBSET Feature Weights high positive weight high negative weight weigh
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2250290162	ful direction to Automated assessment of non-native learner essays: Investigating the role of linguistic features 23 take for handling this aspect will be to consider task independent features (e.g., Zesch et al., 2015) for modeling learner language. Prompt has only been considered as a categorical feature in this paper, whereas it clearly needs to be better modeled in the context of recent research that showed how
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2124725212	generalizability i.e., what features are useful to develop predictive models across multiple data sources. In terms of research on publicly available corpora, the current work can compare closely to Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012), who worked on the First Certiﬁcate of English corpus, which is one of the corpora used in this paper. In contrast with the pairwise-ranking approach used in thei
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2133990480	h resulted in poor precision and recall for the other two proﬁciency categories. Hence, a sample of 1069 texts per category was selected, using the SpreadSubSample method implemented in WEKA toolkit (Hall et al., 2009) to train balanced classiﬁers described in later sect ions of this paper.1 We will refer to this corpus as TOEFL11SUBSET for the rest of this paper. Table 1 shows average number of tokens and sentence
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1971020201	m the syntactic parse trees of sentences in learner essays. 14 of the features were used in the past to measure syntactic complexity in second language writing and its relation to writing proﬁciency (Lu, 2010, 2011). The se features are implemented based on the descriptions in Lu (2010) and using Tregex tree pattern matching tool (Levy and Andrew, 2006) with syntactic parse trees, for extracting speciﬁc p
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2250290162	mated assessment of non-native learner essays: Investigating the role of linguistic features 5 large publicly available corpora. There has been some recent research modeling task independence in AES (Zesch et al., 2015; Phandi et al., 2015), and they used another publicly available dataset of scored essays (Kaggle, 2012). The research described in this paper differs from this strand of research primarily in two asp
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	645837107,2405740272	he native language of the learner (L1) are considered as additional (categorical) features for both datasets. Linguistic properties of writing prompts were also shown to inﬂuence the student writing (Crossley et al., 2013a). Since there is prompt information for both datasets, this was included in the feature set. However, it has to be noted that prompt was used only as a categorical feature without including any feat
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1971020201	nglish learner texts. Apart from predictive modeling based approaches, there are also studies that focused on identifying distinctive linguistic features between proﬁ ciency levels (e.g., Tono, 2000; Lu, 2010, 2012; Vyatkina, 2012) and L1 backgrounds (Lu and Ai, 2015). Most of the research in AES has been related to English writing owing to its widespread use and the availability of more learner corpora a
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2250372169	non-native learner essays: Investigating the role of linguistic features 5 large publicly available corpora. There has been some recent research modeling task independence in AES (Zesch et al., 2015; Phandi et al., 2015), and they used another publicly available dataset of scored essays (Kaggle, 2012). The research described in this paper differs from this strand of research primarily in two aspects: Firstly, they fo
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2145611971	oprietary systems, recent availability of publicly accessible learner corpora facilitated comparable and replicable research on second language (L2) proﬁciency assessment (Yannakoudakis et al., 2011; Nedungadi and Raj, 2014). One non-test taking application of automatic analysis of writing is in providing real-time feedback to the writers on their language use, highlighting their mistakes and suggesting ways for them to
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2124725212	ost intuitive features to use in assessing the proﬁciency of a writer. Some of the existing research on this topic used large language models with word and POS unigrams to model learner errors (e.g., Yannakoudakis et al., 2011)). In this paper, an open source rule based spelling and grammar check tool called LanguageTool5 was used to calculate spelling and grammar features. The following four error features were extracted f
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2133990480	is possible to create a discretized version of FCE too, that will not consider the fact that both datasets are ordinal in nature. Hence, that conversion is not performed in this paper. WEKA toolkit (Hall et al., 2009) was used to train and test our prediction models. For TOEFLSUBSET, the classiﬁcation models were evaluated in terms of classi ﬁcation accuracy in a 10 fold cross validation setting. For FCE, the perf
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	645837107,2405740272	rossley et al. (2014) studied the relationship between linguistic properties and TOEFL scores using several linguistic indices based on Coh-Metrix (Graesser et al., 2012) and Writing Assessment Tool (Crossley et al., 2013b). They used a corpus of 480 essays scored on a scale of 1–5 for this study. In another study, Crossley et al. (2015) studied the role of various linguistic features in AES using a corpus of 997 pers
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	36556894	rpus of learner essays written for Cambridge First Certiﬁcate in English (FCE) exam, and condu cted several experiments to do predictive modeling of human scores on these essays. This was extended in Yannakoudakis and Briscoe (2012) who focused exclusively on how discourse coherence can be modeled for learner essays. More recently, Crossley et al. (2014) studied the relationship between linguistic properties and TOEFL scores usi
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1967082761	used in such scenarios for analysis of student writing. A related area of research, but not speciﬁc to language use, is automatic content assessment, typically for scoring short answers to questions (Burrows et al., 2015). Considering that many people enrolling in Massive Open Online Courses (MOOCs) are nonnative English speakers, such automated methods could also be useful in providing feedback for them in scenarios
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2124725212	search on AES has been on proprietary systems, recent availability of publicly accessible learner corpora facilitated comparable and replicable research on second language (L2) proﬁciency assessment (Yannakoudakis et al., 2011; Nedungadi and Raj, 2014). One non-test taking application of automatic analysis of writing is in providing real-time feedback to the writers on their language use, highlighting their mistakes and su
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	36556894	no signiﬁcant difference between the correlations reported in Yannakoudakis et al. (2011) and what was reported in this paper (0.64) with any of the tests. However, the result from the second study (Yannakoudakis and Briscoe, 2012) is signiﬁcantly better (p &lt;0.05) than the current model, which can perhaps be attributed to the presence of additional coherence features in their model (e.g., those based on incremental semantic
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2133944470	text, number of unique entities per text and average number of words per entity as additional features, as used in some of the earlier research in analyzing text coherence for readability assessment (Feng et al., 2010). This group of features is referred to as DISC-ENTITIES in this paper. Co-reference chains refer to the chains of references to thesame entity in a piece of text, as we 3http://cohmetrix.com/ 4http:/
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	36556894	tures are useful to develop predictive models across multiple data sources. In terms of research on publicly available corpora, the current work can compare closely to Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012), who worked on the First Certiﬁcate of English corpus, which is one of the corpora used in this paper. In contrast with the pairwise-ranking approach used in their work, our model uses regression. Wh
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	36556894	ubject, object, neither, and absent). These roles are then used to build a grid of entity transitions which capture how an entity changes across different sentences in a text. Burstein et al. (2010); Yannakoudakis and Briscoe (2012) used entity grid based features for automated writing assessment before and they were among the more predictive feature groups in Yannakoudakis and Briscoe (2012). The current paper uses 16 features
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	1971020201	umConjunctions POS_verbVar2 POS_numInterjections POS_numLexicalWords POS_numDeterminers POS_numPrepositions POS_numVerbs POS_numWhPronouns Table 3 Features based on syntactic parses SLA features from Lu (2010) Other Syntactic Features SYN_avgSentenceLength SYN_avgParseTreeHeightPerSen SYN_MeanLengthofClauses SYN_numSentences SYN_MeanLengthofTunits SYN_numConstitutentsPerSen SYN_ComplexNominalsPerClause SYN
2560739164	Automated assessment of non-native learner essays: Investigating the role of linguistic features	2170496269	urrence of various types of connectives in text based on word lists (e.g., causal, temporal, contrastive, etc.,). However, not all connective words are used always as discourse connectives in a text. Pitler and Nenkova (2009) described an approach to disambiguate the discourse versus non-discourse usage of connectives based on the syntactic parse trees. Along with this, they also provide a sense classiﬁcation for the disc
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2107587102	eneration (Hastie and Belz, 2014), dealing with different aspects of evaluation methodology. However, most of this work focuses on simple tasks, such as referring expressions generation. For example, Belz and Kow (2011) investigated the impact of continuous and discrete scales for generated weather descriptions, as well as and simple image descriptions that typically consist of a few words (e.g., ”the small blue fan
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2250428499	Evaluation Given a generated verse, we ask annotators to determine the ﬂuency and coherence of the lyrics. Even though our evaluation is for systems that produce entire verses, we follow the work of Wu (2014) and annotate ﬂuency, as well as coherence, at the line level. To assess ﬂuency, we ask to what extent a given line can be considered a valid English utterance. Since a language model may produce high
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	1245470533	lar strategy was used by Gervas (2000), where the author´ had annotators evaluate generated verses with regard to syntactic correctness and overall aesthetic value, providing scores in the range 1-5. Wu et al. (2013) had annotators determine the effectiveness of various systems based on ﬂuency as well as rhyming. Some heuristic-based automated approaches have also been used. For example, Oliveira et al. (2014) us
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2250428499	measures of ﬂuency and coherence, despite sharing a similar goal, have a relatively low correlation of 0.4. Such low correlations emphasize our contribution, since other works (Barbieri et al., 2012; Wu, 2014; Malmi et al., 2015) do not provide a comprehensive evaluation methodology, and evaluate just one or two particular aspects. For example, Wu (2014) evaluated only ﬂuency and rhyming, and Barbieri et
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2250428499	semantic, as opposed to syntactic, in nature, causing it to be more subjective. Note that while the agreement is relatively low, it is expected, given the subjective nature of the task. For example, Wu (2014) report similar agreement values for the ﬂuency annotation they perform. Fluency evaluation Figure 1: Percentage of lines annotated as strongly ﬂuent, weakly ﬂuent, and not ﬂuent. The numbers above th
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2251762574	tanding of the task’s goals and future research directions. 2 Related Work In the past few years there has been a signiﬁcant amount of work dedicated to the evaluation of natural language generation (Hastie and Belz, 2014), dealing with different aspects of evaluation methodology. However, most of this work focuses on simple tasks, such as referring expressions generation. For example, Belz and Kow (2011) investigated
2565321876	Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting	2250428499	he use of internal6 or polysyllabic rhymes7. Rhyme density is able to capture this in a single metric, since the tool we use is able to detect these various forms of rhymes. Moreover, as was shown in Wu (2014), the inter-annotator agreement (IAA) for manual rhyme detection is low (the highest IAA was only 0.283 using a two-scale annotation scheme), which is expected due to the subjective nature of the task
2566807511	Towards a System Architecture for Integrating Cross-modal Context in Syntactic Disambiguation	1582900015	With reported attachment accuracies of 92% for German [6] and 94.9% [ 7 ] to 95.77% [8] for English, statistical approaches to PP attachment based on machine learning are the most successful and most frequently used ones in large-coverage parsers today.
2566807511	Towards a System Architecture for Integrating Cross-modal Context in Syntactic Disambiguation	2015080665	Typical sources of additional knowledge include cross-modal sensory information [1], discourse history, context [2] and common sense or world knowledge [3].
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2294970769	1; Turney and Pantel, 2010) have been shown to be beneﬁcial in the unsupervised induction of morphology (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001). Embeddings were shown to act similarly (Soricut and Och, 2015). Our method differs from this line of research in two key ways. (i) We present a probabilistic model of the process of synthesizing the word’s meaning from the meaning of its morphemes. Prior work wa
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2102443632	we can accurately restore character-level changes. We encode this portion of the model as a weighted ﬁnite-state machine for ease of computation. This factor generalizes probabilistic edit distance (Ristad and Yianilos, 1998) by looking at additional input and output context; see Cotterell et al. (2014) for details. As mentioned above and in contrast to Cotterell et al. (2014), we bound the insertion limit in the edit dis
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1662133657	ake M=10,000 importance samples and select the highest weighted sample. 5 Related Work The idea that vector semantics is useful for morphological segmentation is not new. Count vectors (Salton, 1971; Turney and Pantel, 2010) have been shown to be beneﬁcial in the unsupervised induction of morphology (Schone and Jurafsky, 2000; Schone and Jurafsky, 2001). Embeddings were shown to act similarly (Soricut and Och, 2015). Our
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1899794420	aracter stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retroﬁtting is a novel approach. Given a vector vfor a word form w, we seek
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2103152638	constructed is coherent. These are the cases where a joint model is expected to be beneﬁcial for both segmentation and interpretation. 3 A Joint Model From an NLP perspective, canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016b) is the task that seeks to algorithmically decompose a word into its canonical sequence of morphemes. It is a version of morphological segmentation that requires the learner
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2104518905	dels. For example, Botha and Blunsom (2014) train a log-bilinear language model that models the composition of morphological structure. Likewise, Luong et al. (2013) train a recursive neural network (Goller and Kuchler, 1996) over a heuristi-¨ cally derived tree structure to learn morphological composition over continuous vectors. Our work is different in that we learn a joint model of segmentation and composition. Moreov
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2047603832	at has different goals concerns morphological generation. Two recent papers that address this problem using deep learning are (Faruqui et al., 2016a; Faruqui et al., 2016b). In an older line of work, Yarowsky and Wicentowski (2000) and Wicentowski (2002) exploit log frequency ratios of inﬂectionally related forms to tease apart that, e.g., the past tense of sing is not singed, but instead sang. Related work by Dreyer and Eisner
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2130942839	eﬁned the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016). Importantly, this model has no knowledge of morphology—it can only rely on representations it extracts from the characters. This gives us a clear ablation
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2153579005	h a before-and-after window of size 2. They then apply pointwise mutual information (PMI) weighting and dimensionality reduction by non-negative matrix factorization. In contrast, we employ WORD2VEC (Mikolov et al., 2013a), a model that is also interpretable as the factorization of a PMI matrix (Levy and Goldberg, 2014b). We consider three WORD2VEC models: two bag-of-word (BOW) models with before-and-after windows of
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2103170111	icentowski (2000) and Wicentowski (2002) exploit log frequency ratios of inﬂectionally related forms to tease apart that, e.g., the past tense of sing is not singed, but instead sang. Related work by Dreyer and Eisner (2011) uses a Dirichlet process to model a corpus as a “mixture of a paradigm”, allowing for the semi-supervised incorporation of distributional semantics into a structured model of inﬂectional paradigm com
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2287914047	ion features that ﬁre on pairs of sequential labels. See Cotterell et al. (2015) for details. Recent work has also showed that a neural parameterization can remove the need for manual feature design (Kong et al., 2016). 3.3 Composition Factor The composition factor takes the form of an unnormalized multivariate Gaussian density: exp 1 2˙2 jjvC (s;l)jj 2 2 , where the mean is computed by the (potentially non-linear)
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2251752871	model as a weighted ﬁnite-state machine for ease of computation. This factor generalizes probabilistic edit distance (Ristad and Yianilos, 1998) by looking at additional input and output context; see Cotterell et al. (2014) for details. As mentioned above and in contrast to Cotterell et al. (2014), we bound the insertion limit in the edit distance model.8 Computing the score between two strings uand wrequires a dynamic
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2153579005	he modeling of phrase compositionality in embedding models, where it can be better to not further decompose noncompositional multiword units like named entities and idiomatic expressions; see, e.g., (Mikolov et al., 2013b; Wang et al., 2014; Yin and Schutze, 2015;¨ Yaghoobzadeh and Schutze, 2015; Hashimoto and¨ Tsuruoka, 2016).6 In this paper, we refer to the semantic aspect of the model either as semantic synthesis
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1957775505	nce a system may return a sequence of morphemes whose concatenation is not the same length as the concatenation of the gold morphemes. This rules out metrics for surface segmentation like border F 1 (Kurimo et al., 2010), which require the strings to be of the same length. We now deﬁne the metrics. (i) Segmentation accuracy measures whether every single canonical morpheme in the returned sequence is correct. It is in
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1938755728	nto a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retroﬁtting is a novel approach. Given a vector vfor a word form w, we seek a function to minimize the following objective 1 2 jjv h Njj2 2; (10) where
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2158028897	ompositionality in embedding models, where it can be better to not further decompose noncompositional multiword units like named entities and idiomatic expressions; see, e.g., (Mikolov et al., 2013b; Wang et al., 2014; Yin and Schutze, 2015;¨ Yaghoobzadeh and Schutze, 2015; Hashimoto and¨ Tsuruoka, 2016).6 In this paper, we refer to the semantic aspect of the model either as semantic synthesis or as coherence. The
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1505680913	onal semantics into a structured model of inﬂectional paradigm completion. Our work is also related to recent attempts to integrate morphological knowledge into general embedding models. For example, Botha and Blunsom (2014) train a log-bilinear language model that models the composition of morphological structure. Likewise, Luong et al. (2013) train a recursive neural network (Goller and Kuchler, 1996) over a heuristi-¨
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2064675550	it operations, e.g., substitute ifor y, in varying context granularities. See Cotterell et al. (2016b) for details. Recent work has also explored weighting of WFST arcs with scores computed by LSTMs (Hochreiter and Schmidhuber, 1997), obviating the need for human selection of feature templates (Rastogi et al., 2016). 3.2 Segmentation Factor The second factor is the segmentation factor: exp f(s;l;u)&gt; . The goal of this factor
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1860935423	opular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016). To the best of our knowledge, character-level retroﬁtting is a novel approach. Given a vector vfor a word form w, we seek a function to minimize the followin
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2296590378	reak a word up into its constituent morphemes. The output segmentation has been shown to aid a diverse set of applications, such as automatic speech recognition (Aﬁfy et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and C¸etinoglu, 2015). In contrast˘ to much of this prior work, we focus on supervised segmentation, i.e., we provide the model wit
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1839584883	Schone and Jurafsky (2000) and Soricut and Och (2015), being fully unsupervised, do not distinguish between inﬂection and derivation and Schone and Jurafsky (2001) focus on inﬂection. More recently, Narasimhan et al. (2015) look at the unsupervised induction of “morphological chains” with semantic vectors as a crucial feature. Their goal is to jointly ﬁgure out an ordering of word formation and a morphological segmentat
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2053306448	se assume a gold morphological analysis and primarily focus on the effect of derivation on distributional semantics. The second body of work, e.g., the unsupervised morphological segmenter MORFESSOR (Creutz and Lagus, 2007), does not deal with semantics and makes no distinction between inﬂectional and derivational morphology.3 Even though the boundary between inﬂectional and derivational morphology is a continuum rather
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2137607259	re the simplest such models. Part of the motivation for considering a richer class of models lies in our removal of the twomorpheme assumption. Indeed, it is unclear that the wadd and fulladd models (Mitchell and Lapata, 2008) are useful models in the general case of multimorphemic words—the weights are tied by position, i.e., the ﬁrst morpheme’s vector (be it a preﬁx or stem) is always multiplied by the same matrix. Compa
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2251752871	and the stem “post” are different. We can view this factor as an unnormalized ﬁrst8As our transduction model is an unnormalized factor in a CRF, we do not require the local normalization discussed in Cotterell et al. (2014)—a weight on an edge may be any nonnegative real number since we will renormalize later. The underlying model, however, remains the same. model composition function stem c = P N i=1 1 l i =stemm l i s
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1513168562	at the toughest part of the canonical segmentation task is reversing the orthographic changes. icalization as an orthographic analogue to phonology. On this interpretation, the ﬁnite-state systems of Kaplan and Kay (1994), which computationally applies SPE-style phonological rules (Chomsky and Halle, 1968), may be run backwards to get canonical underlying forms. 6 Experiments and Results We conduct experiments on Engl
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2064675550	uld be improved in future work as a reviewer points out, e.g., by setting such v(s i) to an average of a suitable chosen set of known word vectors. 10We do not explore more complex RNNs, e.g., LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014a) as words in our data have 7 morphemes. These architectures make the learning of long distance dependencies easier, but are no more powerful than an Elman RNN, at least i
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2185720331	or a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016). Importantly, this model has no knowledge of morphology—it can only rely on representations it extracts from the characters. This gives us a clear ablation on the beneﬁt of addin
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2064675550	vious hidden state and A and B are matrices. While we have deﬁned the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016). Importantly, this model has no knowledge of morphology—it can only rely on representations it extracts fro
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	1839584883	work. Productivity and Semantic Coherence. We highlight two related issues in derivation that motivated the development of our model: productivity and semantic coherence. Roughly, a productive afﬁx 3Narasimhan et al. (2015) also make no distinction between inﬂectional and derivational morphology, but their model is an exception in that it includes vector similarity as a semantic feature. See x5 for discussion. is one th
2570821393	Joint Semantic Synthesis and Morphological Analysis of the Derived Word	2110485445	xpressive composition model, a recurrent neural network (RNN). Let Nbe the number of segments. Then C (s;l) = h Nwhere h iis a hidden vector, deﬁned by the recursion:8 h i = tanh Xh i 1 + Um l i s i (Elman, 1990). Again, we optimize the morpheme embeddings ml i s i only when l i 6= stem along with the other parameters of the RNN, i.e., the matrices U and X. 4 Inference and Learning Exact inference is intracta
2584220694	RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems	2154652894	ation Metrics Automatic evaluation is crucial to the research of language generation tasks such as dialog systems (Li et al.,2015), machine translation (Papineni et al.,2002), and text summarization (Lin, 2004). The Workshop on Machine Translation (WMT) organizes shared tasks for evaluation metrics (Stanojevic et al.´ ,2015;Bojar et al.,2016), attracting a large number of researchers and greatly promoting t
2584220694	RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems	2140054881	Meanwhile,Liu et al.(2016) conduct extensive empirical experiments and show the weak correlation of existing metrics (e.g., BLEU, R and METEOR) with human judgements for dialog systems. Based on BLEU,Galley et al. (2015) propose BLEU, which considers several reference replies. However, multiple references are hard to obtain in practice. Recent advances in generative dialog systems have raised the problem of universal
2584220694	RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems	2101105183	r of researchers and greatly promoting the development of translation models. Most existing metrics evaluate generated sentences by word overlapping against a groundtruth sentence. For example, BLEU (Papineni et al., 2002) computes geometric mean of the precision for n-gram (n = 1; ;4); NIST (Doddington,2002) replaces geometric mean with arithmatic mean. Summarization tasks prefer recalloriented metrics like R OUGE(Lin
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2108598243	76]) making up 0:8% of the NES library, for 4 orders of magnitude smaller than a standard language corpus. Standard image corpora are even larger than the Gigaword corpus, such as the ImageNet corpus [77] at 155GB. All told, work in this area is always going to be constrained by the amount of data, even if more work is put into creating larger, better corpora. While most machine learning is focused on
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2290393232	an-produced level, this method can generate maps that both adhere to some important aspects of level design while deviating from others in novel ways. 2) LSTMs for Mario Levels Summerville and Mateas [19] used Long ShortTerm Memory Recurrent Neural Networks (LSTM RNNs) [46] to generate levels learned from a tile representation of Super Mario Bros. and Super Mario Bros. 2 (JP) [47] levels. LSTMs are a
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2290393232	aper C/R Data Content Methods Task Hoover, et. al. [36]: Cat. Seq. Mario Levels Feed Forward NN, Autonomous generation, Functional Scaffolding for Mario NEAT, FSMC Domain transfer Summerville, Mateas [19]: Cat. Seq. Mario Levels Recurrent Neural Autonomous generation LSTMs for Mario Networks (RNNs) Jain, et. al. [20]: Cat. Seq. Mario Levels Neural Network Autonomous generation Autoencoders for Mario A
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2025768430	ct every coin and question-mark block had more coins and question-marks in their generated levels). 3) Autoencoders for Mario Levels In [20] Jain, Isaksen, Holmgård and Togelius show how autoencoders [50] may be trained to reproduce levels from the original Super Mario Bros. game. The autoencoders are trained on series of vertical level windows and compress the typical 6 Bias Ground (t) Ground (t-1) G
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1725676355	eractive Fiction Summerville et. al. [91]: Cat., Graph Zelda Dungeons Bayes Nets, Autonomous generation, Bayes Nets and PCA for Zelda Real Graphical Models Co-creative generation Shaker, Abou-Zleikha [73]: Real Seq. Mario Levels Non-Negative Matrix Design Pattern Modeling, Matrix Factorization for Level Gen. Factorization (NNMF) Autonomous generation the paper to play a similar role as the SearchBased
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1725676355	Figure10. 2) Matrix Factorization for Level Generation Many generators (both machine and human) create in a particular identiﬁable style with a limited expressive range [72]. Shaker and Abou-Zleikha [73] use Non-Negative Matrix Factorization (NNMF) to learn patterns from 5 unique non-ML-based unique generators of Super Mario Bros. levels (Notch, Parameterized, Grammatical Evolution, Launchpad, and Ho
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2108806781	gner and an algorithm work together to create content. This approach has previously been explored with other methods such as constraint satisfaction algorithms and evolutionary algorithms [15], [16], [17]. 3 Again, because the designer can train the machinelearning algorithm by providing examples in the target domain, the designer is “speaking the same language” the algorithm requires for input and ou
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2228991311	a human designer and an algorithm work together to create content. This approach has previously been explored with other methods such as constraint satisfaction algorithms and evolutionary algorithms [15], [16], [17]. 3 Again, because the designer can train the machinelearning algorithm by providing examples in the target domain, the designer is “speaking the same language” the algorithm requires for
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2290393232	this involves iterating over the data from left-to-right then top-to-bottom [20], however depending on the technique, this can obfuscate spatial relationships. This loss can be mitigated, for example [19] utilizes a “snaking” path while [36] represents vertical and horizontal relations by splitting two-dimensional maps into one-dimensional sequences of columns. Super Mario Bros. stores its levels in a
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2168115594	Matrix Design Pattern Modeling, Matrix Factorization for Level Gen. Factorization (NNMF) Autonomous generation the paper to play a similar role as the SearchBased Procedural Content Generation paper [2], which pointed out existing research as well as work that was yet to be done. Much research that was proposed in that paper was subsequently carried out by various authors. In TableI, we present a su
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2395774634	n designer and an algorithm work together to create content. This approach has previously been explored with other methods such as constraint satisfaction algorithms and evolutionary algorithms [15], [16], [17]. 3 Again, because the designer can train the machinelearning algorithm by providing examples in the target domain, the designer is “speaking the same language” the algorithm requires for input
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1899406468	nation of a wave function collapse approach. A. Artiﬁcial Neural Networks Recently neural networks have been used in the context of generating images [40], [41] and learning to play games [42], [43], [44] with great success. Unsurprisingly, neural networks are also being explored in PCG for level and map generation as well as cooperative trading card generation. 1) Function Scaffolding for Mario Level
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2168115594	ns of PCG are what could be called “constructive” methods, using grammars or noise-based algorithms to create content in a pipeline without evaluation. Many techniques use either search-based methods [2] (for example using evolutionary algorithms) or solverbased methods [3] to generate content in settings that maximize objectives and/or preserve constraints. What these methods have in common is that
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2064675550	ome important aspects of level design while deviating from others in novel ways. 2) LSTMs for Mario Levels Summerville and Mateas [19] used Long ShortTerm Memory Recurrent Neural Networks (LSTM RNNs) [46] to generate levels learned from a tile representation of Super Mario Bros. and Super Mario Bros. 2 (JP) [47] levels. LSTMs are a variant of RNNs that represent the current state-of-the-art for sequen
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2167112043	at was originally developed to compose music. The original FSMC representation posits 1) music can be represented as a function of time and 2) musical voices in a given piece are functionally related [45]. Through a method for evolving artiﬁcial neural networks (ANNs) called NeuroEvolution of Augmenting Topologies (NEAT) [39], additional musical voices are then evolved to be played simultaneously with
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2290393232	PCGML algorithms can identify areas that are not playable (e.g. if an unplayable level or impossible rule set has been speciﬁed) and also offer suggestions for how to ﬁx them. Summerville and Mateas [19] use a special tile that represents where an AI would choose to move the player in their training set, so that the algorithm only generates playable content; the system inherently has learned the diff
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2092773680	process. In total Guzdial and Riedl made use of nine gameplay videos for their work with Super Mario Bros., roughly 4 hours of gameplay in total. Guzdial and Riedl’s model structure was adapted from [65], a graph structure meant to encode styles of shapes and their probabilistic relationships. The shapes in this case refer to collections of identical sprites tiled over space in different conﬁguration
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2152660354	rs or noise-based algorithms to create content in a pipeline without evaluation. Many techniques use either search-based methods [2] (for example using evolutionary algorithms) or solverbased methods [3] to generate content in settings that maximize objectives and/or preserve constraints. What these methods have in common is that the algorithms, parameters, constraints, and objectives that create the
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1636244751	speech [7]. But many other machine learning methods can also be utilized in a generative role, including n-grams, Markov arXiv:1702.00539v1 [cs.AI] 2 Feb 2017 2 models, autoencoders, and others [8], [9], [10]. The basic idea is to train a model on instances sampled from some distribution, and then use this model to produce new samples. This paper is about the nascent idea and practice of generating
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2115733720	ta, even if more work is put into creating larger, better corpora. While most machine learning is focused on using large corpora, there is work in the ﬁeld focused on One-Shot Learning/Generalization [78]. Humans have the ability to generalize very quickly after being shown a single (or very small set, e.g., n&lt;5) object to other objects of the same type, and OneShot Learning is similarly concerned
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1541966139	te new room content as seen in Figure10. 2) Matrix Factorization for Level Generation Many generators (both machine and human) create in a particular identiﬁable style with a limited expressive range [72]. Shaker and Abou-Zleikha [73] use Non-Negative Matrix Factorization (NNMF) to learn patterns from 5 unique non-ML-based unique generators of Super Mario Bros. levels (Notch, Parameterized, Grammatica
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1541966139	but not the training data necessary for machine learning approaches. In addition to competitions, there has been some work in developing metric-based benchmarks for comparing techniques. Horn et al. [72] proposed a benchmark for Super Mario Bros. level generators in a large comparative study of multiple generators, which provided several evaluation metrics that are commonly used now, but only for lev
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2290393232	unlikely to have similar semantics between Super Mario Bros. [28] and Metroid [29], despite the fact that both are platformers). As such, data sources are often limited to within a game series [30], [19] and more commonly within individual games [31], [32], [33]. This in turn means that data scarcity is likely to plague PCGML for the foreseeable future, as there are a small, ﬁnite number of ofﬁcial m
2586544230	Procedural Content Generation via Machine Learning (PCGML)	2099471712	r a variety of tasks in machine learning, including the generation of content. For example, generative adversarial networks have been applied to generating artifacts such as images, music, and speech [7]. But many other machine learning methods can also be utilized in a generative role, including n-grams, Markov arXiv:1702.00539v1 [cs.AI] 2 Feb 2017 2 models, autoencoders, and others [8], [9], [10].
2586544230	Procedural Content Generation via Machine Learning (PCGML)	1484769873	vels needed to be converted to strings in order for n-grams to be applicable. This was done through dividing the levels into vertical “slices,” where most slices recur many times throughout the level [57]. This representational trick is dependent on there being a large amount of redundancy in the level design, something that is true in many games. Models were trained using various levels of n, and it
2586847566	A Knowledge-Grounded Neural Conversation Model	2311783643	-sequence (S EQ2S model (Hochreiter and Schmidhuber,1997;Sutskever et al.,2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al.,2015;Vinyals and Le,2015;Li et al., 2016a). Both encoder and decoder are recurrent neural network (RNN) models: an RNN that encodes a variable-length input string into a ﬁxed-length vector representation and an RNN that decodes the vector r
2586847566	A Knowledge-Grounded Neural Conversation Model	2311783643	ion system (MTASK-RF). Entities marked with [...] have been anonymized to avoid (potentially negative) publicity. models (Sordoni et al.,2015;Serban et al.,2016; Shang et al.,2015;Vinyals and Le,2015;Li et al., 2016a,b). The introduction of contextual models by (Sordoni et al.,2015) is an important advance; we build on this by incorporating context from outside the conversation. This work distinguishes itself fr
2586847566	A Knowledge-Grounded Neural Conversation Model	2311783643	logP(SjR) of the source given the response. The third feature is added to deal with the issue of generating commonplace and generic responses such as “I don’t know”, which is discussed in details in (Li et al., 2016a). Our models often do not need the third feature to be effective, but—since our baseline needs it to avoid commonplace responses—we include this feature in all systems. This yields the following rer
2586847566	A Knowledge-Grounded Neural Conversation Model	2101105183	n all systems. This yields the following reranking score: logP(RjS;F)+logP(SjR)+ jRj and are free parameters, which we tune on our development N-best lists using MERT (Och,2003) by optimizing BLEU (Papineni et al., 2002a). To estimate P(SjR) we train a Sequenceto-sequence model by swapping messages and responses. In this model we do not use any facts. 4.3 Evaluation Metrics Following (Sordoni et al.,2015;Wen et al.,
2586847566	A Knowledge-Grounded Neural Conversation Model	10957333	ork The present work extends the data-driven paradigm of conversation generation by injecting knowledge from textual data into models derived from conversational data. This paradigm was introduced by Ritter et al. (2011) who ﬁrst proposed using statistical Machine Translation models to generate conversational responses from social media data. It has been was further advanced by the introduction of neural network 7(“s
2586847566	A Knowledge-Grounded Neural Conversation Model	1518951372	respond seamlessly and appropriately, and the task of conversational response generation has recently become an active area of research in natural language processing. Recent work (Ritter et al.,2011;Sordoni et al., 2015;Shang et al.,2015;Vinyals and Le,2015) has shown that it is possible to train conversational models in an end-to-end and completely * This work was conducted at Microsoft. User input: Going to Kusaka
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2251803266	2008) that contains 44 concepts in 6 categories; and the Battig test set (Baroni &amp; Lenci, 2010) that contains 83 words in 10 categories. Here we follow the setting and the proposed algorithm in (Baroni et al., 2014; Schnabel et al., 2015) – we cluster words (via their representations) using the classical k-Means algorithm (with ﬁxed k). Again, the processed vectors perform consistently better on all three datas
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	1854884267	3000 pairs of words are rated by crowdsourced participants; the MTurk dataset (Radinsky et al., 2011) where the 287 pairs of words are rated in terms of relatedness; the SimLex-999 (SimLex) dataset (Hill et al., 2016) where the score measures “genuine&quot; similarity; and lastly the SimVerb-3500 (SimVerb) dataset (Gerz et al., 2016), a newly released large dataset focusing on similarity of verbs. In our experimen
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2252211741	ains 44 concepts in 6 categories; and the Battig test set (Baroni and Lenci,2010) that contains 83 words in 10 categories. Here we follow the setting and the proposed algorithm in (Baroni et al.,2014;Schnabel et al., 2015) – we cluster words (via their representations) using the classical k-Means algorithm (with ﬁxed k). Again, the processed vectors perform consistently better on all three datasets (with average improv
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2158139315	is well captured by the similarity of the corresponding vector representations. A variety of approaches have been proposed in recent years to learn the word representations: Collobert et al. (2011); Turian et al. (2010) learn the representations via semi-supervised learning by jointly training the language model and downstream applications; Bengio et al. (2003); Mikolov et al. (2010); Huang et al. (2012) do so by ﬁt
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2518186251	we see corresponding consistent improvements due to postprocessing in Appendix C. 2.2 POSTPROCESSING AS A “ROUNDING” TOWARDS ISOTROPY The idea of isotropy comes from the partition function deﬁned in (Arora et al., 2016), Z(c) = X w2V exp c&gt;v(w) ; where Z(c) should approximately be a constant with any unit vector c(c.f. Lemma 2.1 in (Arora et al., 2016)). Hence, we mathematically deﬁne a measure of isotropy as fol
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2067438047	d to (a gold set of) human judgements. For this experiment, we use seven standard datasets: the ﬁrst published RG65 dataset (Rubenstein and Goodenough,1965); the widely used WordSim-353 (WS) dataset (Finkelstein et al., 2001) which contains 353 pairs of commonly used verbs and nouns; the rare-words (RW) dataset (Luong et al.,2013) composed of rarely used words; the MEN dataset (Bruni et al.,2014) where the 3000 pairs of w
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2518186251	ew point (Browne, 1979; Hotelling, 1936) and recently in the NLP context (Stratos et al., 2015), a corresponding effort for the PMI-based methods has only recently been conducted in an inspired work (Arora et al., 2016). Arora et al. (2016) propose a generative model (named RAND-WALK) of sentences, where every word is parameterized by a d-dimensional vector. With a key postulate that the word vectors are angularly u
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2251803266	The key underlying principle behind word representations is that similar words should have similar representations. Following the tradition of evaluating word representations (Schnabel et al., 2015; Baroni et al., 2014), we perform three canonical lexical-level tasks: (a) word similarity; (b) concept categorization; (c) word analogy; and one sentence-level task: (d) semantic textual similarity. The processed represe
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2518186251	l network language model; Mikolov et al. (2013); Mnih &amp; Hinton (2007) by log-linear models; and Dhillon et al. (2012); Pennington et al. (2014); Levy &amp; Goldberg (2014); Stratos et al. (2015); Arora et al. (2016) by producing a lowdimensional representation of the cooccurrence statistics. Despite the wide disparity of algorithms to induce word representations, the performance of several of the recent methods
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2026487812	re-words (RW) dataset (Luong et al., 2013) composed of rarely used words; the MEN dataset (Bruni et al., 2014) where the 3000 pairs of words are rated by crowdsourced participants; the MTurk dataset (Radinsky et al., 2011) where the 287 pairs of words are rated in terms of relatedness; the SimLex-999 (SimLex) dataset (Hill et al., 2016) where the score measures “genuine&quot; similarity; and lastly the SimVerb-3500 (Si
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2252211741	representations v0(w). The key underlying principle behind word representations is that similar words should have similar representations. Following the tradition of evaluating word representations (Schnabel et al., 2015;Baroni et al.,2014), we perform three canonical lexical-level tasks: (a) word similarity; (b) concept categorization; (c) word analogy; and one sentence-level task: (d) semantic textual similarity. T
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2295800168	technical results of (Arora et al.,2016), and a discussion where we interpret the postprocessing operation as a “rounding” or “projection” towards isotropy (with better self-normalization properties (Andreas and Klein, 2015)) is in AppendixD. 6 Conclusion We present a simple postprocessing operation that renders word representations even stronger. Due to their popularity, we have used the published representations of WOR
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2250790822	ual similarity datasets 7 Published as a conference paper at ICLR 2018 from the 2012-2015 SemEval STS tasks (Agirre et al., 2012; 2013; 2014; 2015), and the 2012 SemEval Semantic Related task (SICK) (Marelli et al., 2014). Representing sentences by the average of their constituent word representations is surprisingly effective in encoding the semantic information of sentences (Wieting et al., 2015; Adi et al., 2016) a
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2518186251	word representations: four publicly available word representations (WORD2VEC1 (Mikolov et al., 2013) trained using Google News, GLOVE2 (Pennington et al., 2014) trained using Common Crawl, RAND-WALK (Arora et al., 2016) trained using Wikipedia and TSCCA3 trained using English Gigaword) and two self-trained word representations using CBOW and Skip-gram (Mikolov et al., 2013) on the 2010 Wikipedia corpus from (Al-Rfou
2587690726	All-but-the-Top: Simple and Effective Postprocessing for Word Representations	2518186251	at word vectors learnt through PMI-based approaches are not of zero-mean and are not isotropic (c.f. Section 2) contradicts with this postulate. The isotropy conditions are relaxed in Section 2.2 of (Arora et al., 2016), but the match with the spectral properties observed in Figure 1 is not immediate. This contradiction is explicitly resloved by relaxing the constraints on the word vectors to directly ﬁt the observe
2588213204	Learning concept embeddings for dataless classification via efficient bag-of-concepts densification	2099868020	al., 2013;Shalaby and Zadrozny,2015),dataless classiﬁcation (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al., 2015), search and relevancy ranking (Egozi et al., 2011), event detection and coreference resolution (Peng et al., 2016). Similar to the traditional bag-of-words representation, the BOC vector is a high dimensional sparse vector whose dimensionality is the
2588213204	Learning concept embeddings for dataless classification via efficient bag-of-concepts densification	2009169514	edness as an intrinsic evaluation. Entity relatedness has been recently used to model entity coherence in many named entity disambiguation systems. 4.1.1 Dataset We use a benchmark dataset created by Ceccarelli et al. (2013) from the CoNLL 2003 data. As in previous studies (Yamada et al., 2016; Method nDCG@1 nDCG@5 nDCG@10 MAP WLM (Huang et al., 2015) 0.54 0.52 0.55 0.48 Yamada et al. (2016) 0.59 0.56 0.59 0.52 3C 0.53 0
2591200570	On the Complexity of CCG Parsing	2114282531	. As we shall see in this article, the availability of rule restrictions also affects the computational complexity of VW-CCG. Our point of departure is the result by Vijay-Shanker and Weir (1993) and Kuhlmann and Satta (2014), who present a polynomial-time parsing algorithm for VWCCG. The asymptotic complexity of both algorithms is in O(n6), where n is the length of the input sentence. This matches the asymptotic complexi
2591200570	On the Complexity of CCG Parsing	2112428003	2007), there has also been a surge of interest in this formalism within statistical natural language processing, and a wide range of applications including data-driven parsing (Clark and Curran 2007; Zhang and Clark 2011), broad-coverage semantic parsing (Lewis and Steedman 2014;Lee, Lewis, and Zettlemoyer2016),andmachinetranslation(Lewis and Steedman 2013). ∗ Department of Computer and Information Science, Linköping
2591200570	On the Complexity of CCG Parsing	2159636675	CG is still able to use a very compact representation of the lexicon, and at the same time to handle non-local dependencies in a simple and effective way. After the release of CCG-annotated datasets (Hockenmaier and Steedman 2007), there has also been a surge of interest in this formalism within statistical natural language processing, and a wide range of applications including data-driven parsing (Clark and Curran 2007; Zhang
2591200570	On the Complexity of CCG Parsing	2114282531	is in contrast with the scenario for TAG, where the runtime of standard parsing algorithms is (roughly) quadratic with respect to grammar size (Schabes 1990), and it raises the question, left open by Kuhlmann and Satta (2014), of whether the exponential factor is just a property of the existing parsing algorithms, or whether VW-CCG parsing is inherently more complex than TAG parsing when grammar size is taken into account
2591200570	On the Complexity of CCG Parsing	2154127298	the degree of composition rules for all grammars in the class VW-CCG. This would break all of the constructions in this article. The second possible scenario is one that has been brieﬂy mentioned by Weir and Joshi (1988, Section 5.2). We could deﬁne a formalism alternative to VW-CCG, in which an individual grammar is allowed to use composition rules of unbounded degree. This would mean that the box notation introduc
2591200570	On the Complexity of CCG Parsing	2114282531	G|1+2cG ·|w|6 =2log|G|+2|G|log|G| ·|w|6 , which is an exponential function in the size of the input. Extension of the Algorithm of Kuhlmann and Satta to VW-CCG. As already mentioned, the algorithm by Kuhlmann and Satta (2014) works for a grammar G with no empty categories. More speciﬁcally, the algorithm starts by adding to the parsing table items of the form [X,i,i +1]for each category X that is assigned by G’s lexicon t
2591200570	On the Complexity of CCG Parsing	2114282531	ma 4. 4.3 Membershipin EXPTIME It remains to prove the following: Lemma 5 The universal recognition for unrestricted VW-CCG is in EXPTIME. To show this, we extend an existing recognition algorithm by Kuhlmann and Satta (2014) that takes as input a VW-CCG G with no empty categories and a string w, and decides whether w ∈ L(G). Complexityof the Algorithmof Kuhlmann and Satta.The algorithm of Kuhlmann and Satta (2014) is bas
2591200570	On the Complexity of CCG Parsing	2114282531	be the maximum degree of a composition rule in G, let a be the maximum arity of an argument in L, and let ℓbe the maximum number of arguments in the categories in G’s lexicon. We set cG =max{d +a,ℓ}. Kuhlmann and Satta (2014) report for their algorithm a running time in O(|A|·|L|2cG ·|w|6). Toseethattheaboveupperboundisanexponentialfunctioninthesizeoftheinput, observe that the quantities |A| and |L| are both bounded by |G
2591200570	On the Complexity of CCG Parsing	2250748818	ormalism within statistical natural language processing, and a wide range of applications including data-driven parsing (Clark and Curran 2007; Zhang and Clark 2011), broad-coverage semantic parsing (Lewis and Steedman 2014;Lee, Lewis, and Zettlemoyer2016),andmachinetranslation(Lewis and Steedman 2013). ∗ Department of Computer and Information Science, Linköping University, 58183 Linköping, Sweden. E-mail: marco.kuhlman
2593104319	Automatic prediction of discourse connectives	2040960947	g connectives between sentences. Already in 2002,Marcu and Echihabi(2002) proposed to map certain unambiguous connectives to discourse relations in order to collect a large discourse relation dataset.Sporleder and Lascarides (2008), however, showed that models trained using such automatically collected training data do not generalize well. Nevertheless, by classifying freely-omissible connectives and only using those,Rutherford
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	2402402867	backpropagate their dynamics through time to calculate the total return gradient. Figure 2 shows the dependency chain between two timesteps. A similar approach was employed by (Foerster et al., 2016; Sukhbaatar et al., 2016) to compute gradients for communication actions, although the latter still employed modelfree methods for physical action computation. The physical state dynamics, including discontinuous contact even
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	2130942839	a more fundamental requirement for compositional syntax formation. 2. Related Work Recent years have seen substantial progress in practical natural language applications such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), sentiment analysis (Socher et al., 2013), document summarization (Durrett et al., 2016), and domain-speciﬁc dialogue (Dhingra et al., 2016). Much of this success is a result
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	2402402867	he recent work that is most similar to ours is the application of reinforcement learning approaches towards the purposes of learning a communication protocol, as exempliﬁed by (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2016a). Emergence of Grounded Compositional Language in Multi-Agent Populations 3. Problem Formulation The setting we are considering is a cooperative partially observable Markov g
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	27281439	rticular interest in these ﬁelds has been the question of how syntax and compositional structure in language emerged, and why it is largely unique to human languages (Kirby, 1999; Nowak et al., 2000; Steels, 2005). Models such as Rational Speech Acts (Frank &amp; Goodman, 2012) and Iterated Learning (Kirby et al., 2014) have been popular in cognitive science and evolutionary linguistics, but such approaches te
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	1523934301	tics and cognitive science. Of particular interest in these ﬁelds has been the question of how syntax and compositional structure in language emerged, and why it is largely unique to human languages (Kirby, 1999; Nowak et al., 2000; Steels, 2005). Models such as Rational Speech Acts (Frank &amp; Goodman, 2012) and Iterated Learning (Kirby et al., 2014) have been popular in cognitive science and evolutionary
2602275733	Emergence of Grounded Compositional Language in Multi-Agent Populations	2410540304	of two-player reference games (Golland et al., 2010; Vogel et al., 2014; Andreas &amp; Klein, 2016) focusing on the task of identifying object references through a learned language. (Winograd, 1973; Wang et al., 2016) ground language in a physical environment and focusing on language interaction with humans for completion of tasks in the physical environment. In such a pragmatic setting, language use for communica
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2014902591	.,2004), classiﬁcation of movie review sentiment (MR) (Pang &amp; Lee,2005), product reviews (CR) (Hu &amp; Liu,2004), subjectivity classiﬁcation (SUBJ)(Pang &amp; Lee,2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classiﬁcation (TREC) (Voorhees, 2002). To classify, we use the code provided by (Kiros et al.,2015) in the same manner as in (Hill et al., 2016a). For the MSRP dataset, containing p
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2251047310	14) as well as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al.,2015) trained on the Paraphrase Database (Ganitkevitch et al.,2013). In a very different line of work, C-PHRASE (Pham et al., 2015) relies on additional information from the syntactic parse tree of each sentence, which is incorporated into the C-BOW training objective. (Huang &amp; Anandkumar,2016) show that single layer CNNs can
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2250539671	f0;1gjVjis a binary vector encoding S(bag of words encoding). Fixed-length context windows S running over the corpus are used in word embedding methods as in C-BOW (Mikolov et al.,2013b;a) and GloVe (Pennington et al., 2014). Here we have k = jVjand each cost function f S: Rk!R only depends on a single row of its input, describing the observed target word for the given ﬁxed-length context S. In contrast, for sentence emb
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2123442489	nt datasets have been used to train our models: the Toronto book corpus3, Wikipedia sentences and tweets. The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library (Manning et al., 2014), while for tweets we used the NLTK tweets tokenizer (Bird et al.,2009). For training, we select a sentence randomly from the dataset and then proceed to select all the 3http://www.cs.toronto.edu/˜mbw
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2493916176	probability q n(w) := p f w  P w i2V f w i , where f w is the normalized frequency of win the corpus. To select the possible target unigrams (positives), we use subsampling as in (Joulin et al.,2017;Bojanowski et al., 2017), each word wbeing discarded with probability 1 q p(w) where q p(w) := min p 1; t=f w+ t=f w . Where t is the subsampling hyper-parameter. Subsampling prevents very frequent words of having too much i
2605035112	UNSUPERVISED LEARNING OF SENTENCE EMBEDDINGS USING COMPOSITIONAL N-GRAM FEATURES	2250790822	Unsupervised Similarity Evaluation. We perform unsupervised evaluation of the the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 (Agirre et al.,2014) and SICK 2014 (Marelli et al., 2014) datasets. These similarity scores are compared to the gold-standard human judgements using Pearson’s r(Pearson,1895) and Spearman’s ˆ(Spearman,1904) correlation scores. The SICK dataset consists of a
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	105848778	.2(b) Backward hierarchical (blue) and timing (green) propagation of graph-LSTM. cussions. Previous studies found that the time when the comment/post was published has a big impact on its popularity (Lakkaraju et al., 2013). In addition, the number of immediate responses can be predictive of the popularity, but some comments with a high number of replies can be either controversial or have a highly negative score. Langu
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2508030421	/downvote), then the difference in positive/negative votes provides a more informative score for popularity prediction. This deﬁnition of popularity, which has also been called community endorsement (Fang et al., 2016), is the task of interest in our work on tree-structured modeling of disarXiv:1704.02080v1 [cs.CL] 7 Apr 2017 (a) Forward hierarchical and timing structure (b) Backward hierarchical and timing structu
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2097343308	arity in social media platforms has been the subject of several studies. Popularity as deﬁned in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as communi
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2340452757	ciﬁc factors is useful in understanding the components of a successful post, but it does not reﬂect a realistic scenario. Studies that do not include such constraints have looked at Twitter retweets (Bi and Cho, 2016) and Reddit karma (He et al., 2016; Fang et al., 2016). The work in (He et al., 2016) uses reinforcement learning to identify popular threads to track given the past comment history, so it is learning
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2170857652	ency of these events and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the ﬂow of information in order to better model the context in which a comment is su
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2090464520	ent (Fang et al., 2016). Popularity prediction is a difﬁ- cult task where many factors can play a role, which is why most prior studies control for speciﬁc factors, including topic (Tan et al., 2014; Weninger et al., 2013), timing (Tan et al., 2014; Jaech et al., 2015), and/or comment content (Lakkaraju et al., 2013). Controlling for speciﬁc factors is useful in understanding the components of a successful post, but it
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2508030421	formation about the graph context and timing of the comment. We analyze the results to show that the graph LSTM provides a useful summary representation of the language context of the comment. As in (Fang et al., 2016), but unlike other work (He et al., 2016), our model makes use of the full discussion thread in predicting popularity. While knowledge of the full discussion is only useful for post-hoc analysis of pa
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	836524228	g et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al., 2016). Popularity prediction is a difﬁ- cult task where many factors can play a role, which is why most prior studies control for speciﬁc f
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2170857652	ity prediction is a difﬁ- cult task where many factors can play a role, which is why most prior studies control for speciﬁc factors, including topic (Tan et al., 2014; Weninger et al., 2013), timing (Tan et al., 2014; Jaech et al., 2015), and/or comment content (Lakkaraju et al., 2013). Controlling for speciﬁc factors is useful in understanding the components of a successful post, but it does not reﬂect a realist
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	1996263819	k The problem of predicting popularity in social media platforms has been the subject of several studies. Popularity as deﬁned in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2054885337	l studies. Popularity as deﬁned in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al., 2016). Popularity prediction
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2289899728	Some options include summarizing over the children, adding a separate forget gate for each child (Tai et al., 2015), recurrent propagation among siblings (Zhang et al., 2016), or use of stack LSTMs (Dyer et al., 2015). Our work differs from these studies in two respects: the tree structure here characterizes a discussion rather than a single sentence; Ex karma Comment 1 0 7 7 0 Republicans are fundamentally dishon
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2104246439	rom using text features is larger for our model, which represents language context. Tree LSTMs are a modiﬁcation of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	105848778	shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al., 2016). Popularity prediction is a difﬁ- cult task where many factors can play a role, which is why most prior studies c
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2508030421	sion, specifically: i) subtract the mean feature value in the thread, and ii) divide by the square root of the rank of the feature value in the thread. These features are a superset of those used in (Fang et al., 2016). The subvector including all these features is denoted xs t. The comment text features, denoted xc t, are generated using a simple average bag-of-words representation learned during the training: xc=
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2170857652	subject of several studies. Popularity as deﬁned in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al., 2016). Pop
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2508030421	Task and evaluation metrics Reddit karma has a Zipﬁan distribution, highly skewed toward the low-karma comments. Since the rare high karma comments are of greatest interest in popularity prediction, (Fang et al., 2016) proposes a task of predicting quantized karma (using a nonlinear head-tail break rule for binning) with evaluation 2reddit.com using a macro average of the F1 scores for predicting whether a comment
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	836524228	ts and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the ﬂow of information in order to better model the context in which a comment is submitted, including bo
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2164859115	ts language context. Tree LSTMs are a modiﬁcation of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget gate for each child (Tai et al., 2015), recurrent propagatio
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	1879966306	tures is larger for our model, which represents language context. Tree LSTMs are a modiﬁcation of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget gate for each chi
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2340452757	ty as deﬁned in terms of volume of response has been explored for shares on Facebook (Cheng et al., 2014) and Twitter (Bandari et al., 2012) and Twitter retweets (Tan et al., 2014; Zhao et al., 2015; Bi and Cho, 2016). Studies on Reddit predict karma as popularity (Lakkaraju et al., 2013; Jaech et al., 2015; He et al., 2016) or as community endorsement (Fang et al., 2016). Popularity prediction is a difﬁ- cult tas
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	105848778	ult due to the low frequency of these events and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the ﬂow of information in order to better model the context in whi
2605659599	Conversation Modeling on Reddit Using a Graph-Structured LSTM	2508030421	utions leading up to a node, and in the other, we characterize the response to that comment. Motivated by prior ﬁndings that both response structure and timing are important in predicting popularity (Fang et al., 2016), the LSTM units include both hierachical and temporal components to the update, which distinguishes this work from prior treestructured LSTM models. We assess the utility of the model in experiments
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2149837184	16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR(Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR 1These datasets are currently available upon request from the authors. Golde Silver f Parser e Parser f FULL-CYCLE Parser e
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2148708890	ain the MT models. The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR(Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR 1These datasets are currently available upon request
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	1523189118	ational divergence and discuss how it affects parsing, following the classiﬁcation used in previous work (Dorr et al., 2002; Dorr, 1994), which identiﬁes classes of divergences for several languages. Sulem et al. (2015) also follow the same categorization for French. Figure 4 shows six sentences displaying these divergences. The aim of this analysis is to assess howthe parsers deal withthe different kind of translat
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2123442489	ble at http://www.github.com/mdtux89/amr-eager-multilingual.2 It requires tokenization, POS tagging, NER tagging and dependency parsing, which for English, German and Chinese are provided by CoreNLP (Manning et al., 2014). We use Freeling (Carreras et al., 2004) for Spanish, as CoreNLP does not provide dependency parsing for this language. Italian is not supported in CoreNLP: we use Tint (Aprosio and Moretti, 2016), a
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2250777616	d for English for the target language as well, withouttranslating anyword. Tothebestofourknowledge, theonly previous workthat attempts toautomatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules. A comparison with this work
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2515003191	evelopment sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR(Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR 1These datasets are currently available upon request from the authors. Golde Silver f Parser e Parser f FULL-CYCLE Parser e Ref Eval SILVER Ref Eval GOLD Re
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	1523189118	guages, they only make use of literal translation. Previous work has also focused on assessing the stability across languages of semantic frameworks such as AMR (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al., 2015) and Propbank (Van der Plas et al., 2010). Cross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, usually English. The annot
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2152691628	ng (Evang and Bos, 2016). Another common thread of crosslingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al.,2011; McDonald et al., 2011; Søgaard, 2011). 7 Conclusions Weintroduced the problem of parsing AMRstructures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR.
2606900862	CROSS-LINGUAL ABSTRACT MEANING REPRESENTATION PARSING	2016630033	ojection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inﬂectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), rolelabeling(Pado´ and Lapata,2009;Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of crossl
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2310102669	, 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited successes (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to problems in NLU have involved models that are trained on data that closely resembles the target evaluation data, both in task and s
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2221711388	., 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; arXiv:1704.05426v3 [cs.CL] 5 Sep 2017 Met my ﬁrst girlfriend that way. FACE-TO-FACE contradiction C C N C I didn’
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1840435438	’s label 85.8% 85.2% Gold label = author’s label 91.2% 92.6% Gold label 6=author’s label 6.8% 5.6% No gold label (no 3 labels match) 2.0% 1.8% Table 2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI. ing issues, and a link to the frequently asked questions (FAQ) page. We provide one FAQ page tailored to each prompt. FAQs are modeled on their SNLI counterparts and include additional
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1840435438	additional round of annotation on our test and development examples to ensure that their labels are accurate. The validation phase follows the procedure used in SICK (Marelli et al., 2014b) and SNLI (Bowman et al., 2015); workers are presented with pairs of sentences and asked to supply a single label (entailment, contradiction, neutral) for the pair. Each pair is relabeled by four workers, yielding a total of ﬁve la
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1522301498	best performance for each genre is obtained by the model trained on that genre. BiLSTM models, we tune Dropout on the SNLI development set and ﬁnd that a drop rate of 0.1 works well. We use the Adam (Kingma and Ba, 2015) optimizer with the default parameters. We train models on SNLI, on MultiNLI, and on a mixture of both corpora. In the mixed setting, we use the full MultiNLI training set but downsample SNLI by rando
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2045254372	ce (NLI) is uniquely well-positioned to serve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences—like one of those in Figure 1— and asked to judge the relationship between their meanings by picking
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2185175083	collected in largely the same way as MultiNLI, no SNLI examples are included in the distributed MultiNLI corpus. SNLI consists only of sentences derived from image captions from the Flickr30k corpus (Young et al., 2014), and thus can be treated as a large additional CAPTIONS genre. Hypothesis Collection To collect a sentence pair using this method, we present a crowdworker with a sentence from a source text and ask
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2310102669	enting each sentence and compute label predictions based on the two resulting vectors. To do this, they concatenate the two representations, their difference, and their elementwise product (following Mou et al., 2016b), and pass the result to a single tanh layer followed by a three-way softmax classiﬁer. The ﬁrst such model is a simple continuous bag of words (CBOW) model in which each sentence is represented as
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1840435438	er-annotator agreement, since it is not clear that it is possible to deﬁne an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert). Bowman et al. (2015) attempt to avoid this problem by using an annotation prompt that is highly dependent on the concreteness of image descrip1For the CBOW models trained on individual genres, we use a dropout rate of 0.
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2513651200	l of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; arXiv:1704.05426v3 [cs.CL] 5 Sep 2017 Met my ﬁrst girlfriend that way. FACE-TO-FACE contradiction C C N C I didn’t meet my ﬁrst girlfriend until la
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1840435438	of natural language to allow the model to extract reasonable representations of sentence meaning. As the only large, human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a standard benchmark for sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2250539671	ng. We use the base ESIM without ensembling with a TreeLSTM (as is done in one published variant of the model). All three models are initialized with 300D reference GloVe vectors (840B token version; Pennington et al., 2014). Out-of-vocabulary (OOV) words are initialized randomly, and all word embeddings are ﬁne-tuned during training. The models use 300D hidden states, as in most prior work on SNLI. We use Dropout (Sriva
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1840435438	NLI, different annotator decisions about the coreference between entities and events across the pair can lead to very different assignments of labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015). Drawing an example from Bowman et al. (2015), the pair “a boat sank in the Paciﬁc Ocean” and “a boat sank in the Atlantic Ocean” can be labeled either contradiction or neutral depending on (among ot
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2155541015	ossible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited successes (see, for example, Mou et al., 2016a). Nearly all successful applications
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2310102669	sentence understanding and spurring work on core representation learning techniques for NLU such as attention (Wang and Jiang, 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; arXiv:1704.05426v3 [cs.CL] 5 Sep 2017 Met my ﬁrst girlfriend that way. FACE-TO-FACE contradiction C C N C I didn’t meet my ﬁrst girlfriend until later. He turned and saw Jon sleeping in his half-te
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	1849277567	techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited successes (see, for example, Mou et al., 2016a). Nearly all
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2087451659	ve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (RTE; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences—like one of those in Figure 1— and asked to judge the relationship between their meanings by picking a label from a small set: typically entailment, neu
2607892599	A BROAD-COVERAGE CHALLENGE CORPUS FOR SENTENCE UNDERSTANDING THROUGH INFERENCE	2097606805	ve. sentence2: The hypothesis sentence for the pair, composed by a crowd worker as a companion sentence for sentence1. sentencef1,2gparse: Each sentence as parsed by the Stanford PCFG Parser (3.5.2 Klein and Manning, 2003). sentencef1,2gbinary parse: The above parses in unlabeled binary-branching format. promptID: A unique identiﬁer for each premise sentence. Note that a single premise will typically appear in three
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	102708294	2007; . 2013). DBPedia provides a fully-structured knowledge representation for the semi-structured content of Wikipedia, which is further linked to existing knowledge bases such as YAGO and OpenCyc (Auer et al. 2007; Lehmann et al. 2015). More recently, Gupta et al. (2016) induce a uniﬁed taxonomy of entities and categories from English WCN using a novel set of high-precision heuristics that classify WCN edges i
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	87289778	2013), thus leading to a consistent body of research in this direction. The ﬁrst line of work on taxonomy induction from Wikipedia mainly focuses on the English language. This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al. 2016). The second line of work aims to exp
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	60048903	body of research in this direction. The ﬁrst line of work on taxonomy induction from Wikipedia mainly focuses on the English language. This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al. 2016). The second line of work aims to exploit the multilingual nature of
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	87289778	focus on the English language. WikiTaxonomy, one of the ﬁrst attempts to taxonomize Wikipedia, labels English WCN edges as isa or not-is-a using a cascade of heuristics based on handcrafted features (Ponzetto and Strube 2008). WikiNet extends WikiTaxonomy by expanding not-is-a relations into more ﬁne-grained relations such as meronymy (i.e., part-of) and geo-location (i.e., located-in). YAGO induces a taxonomy by linking
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	102708294	mainly focuses on the English language. This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al. 2016). The second line of work aims to exploit the multilingual nature of Wikipedia. MENTA (de Melo and Weikum 2010), one of the largest multilingual lexical knowled
2607999239	280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification	2122865749	xonomy induction from Wikipedia mainly focuses on the English language. This includes WikiTaxonomy (Ponzetto and Strube 2008), WikiNet (Nastase et al. 2010), YAGO (Suchanek, Kasneci, and Weikum 2007; Hoffart et al. 2013), DBPedia (Auer et al. 2007), and Heads Taxonomy (Gupta et al. 2016). The second line of work aims to exploit the multilingual nature of Wikipedia. MENTA (de Melo and Weikum 2010), one of the largest
2608320015	PUNNY CAPTIONS: WITTY WORDPLAY IN IMAGE DESCRIPTIONS	2269406410	5) learn to rank cartoon captions based on their funniness. Unlike the typically boring context (images) in our task, memes and cartoons involve a context (images) that are already funny or atypical4.Chandrasekaran et al. (2016) modify an abstract scene to make it more funny. Their task is restricted to altering the input (visual) modality, while our task is to generate witty natural language remarks for a novel image. 3 App
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2097726431	. Sentiment analysis traditionally utilizes bag-of-words approaches, which merely count the frequency of words (and tuples thereof) to obtain a mathematical representation of documents in matrix form [1, 2]. As such, these approaches are not capable of taking into consideration semantic relationships between sections and sentences of a document. Let us, for instance, consider the following two examples,
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2097726431	ds into the representation of text [11]. Here research commonly utilizes n-grams of size two (bi-grams) and three (tri-grams), which are often enriched with additional features such as part-of-speech [1]. This approach allows for implicitly formalizing dependencies between adjacent words. Specic use cases are the detection of negation scopes, e.g. \not bad&quot;, or compound nouns, e.g. \computer sc
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2097726431	of all hierarchy types in an RST tree as ˝ RST. 3.2. Polarity features We follow common procedures in sentiment analysis and utilize a pre-dened dictionary that labels terms as positive or negative [1, 2]. This approach has multiple advantages, as it is domain-independent and works reliably even with few training observations. In addition, one can easily exchange the underlying dictionary for one that
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2097726431	leaf nfrom the tree and appends two newly created child nodes land rwhich, subsequently present the leaves, while the nbecomes an inner node. We compute ˙ l and r by multiplying n by random weights !2[0;1] and (1 !), i.e. ˙ l= ! n; (35) ˙ r= (1 !) ˙ n: (36) These update rules thus try to keep the overall information unchanged, but distribute the values from ninto two separate children given a certain r
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2064675550	persists for a few iterations [18]. A viable remedy is provided by the long shortterm memory (LSTM) network. These enhance recurrent neural networks by capturing long dependencies among input signals [28]. We thus utilize LSTMs as part of our baselines later, as well as inside all tree-structured networks. Previous research has proposed a Tree-LSTM that can deal with representation learning, for insta
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2097726431	sentiment analysis facilitates the extraction of subjective information from user-generated content, or narrative materials in general, by quantifying the positivity or negativity of natural language [1, 2]. Among the many applications of sentiment analysis are tracking customer opinions [3], evaluating survey responses [4], mining user reviews [5, 6, 7], trading upon nancial news [8, 9] and predicting
2608621888	Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse trees	2168681504	tzwald et al., 2018). Among the many applications of sentiment analysis are tracking customer opinions (Tanaka, 2010; Araque et al., 2017; Bohanec et al., 2017), mining user reviews (Ye et al., 2009; Mostafa, 2013; Kontopoulos et al., 2013), trading upon nancial news (Khadjeh Nassirtoussi et al., 2015; Kraus &amp; Feuerriegel, 2017; Weng et al., 2018), detect social events (Yoo et al., 2018) and predicting sal
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2150824314	to gold-standard human-constructed abstractive summaries on DUC data (Section 3). (a) Speciﬁcally, we show that when the documents themselves from the DUC 2001-2002 datasets are compared using ROUGE [19] to abstractive summaries, the average Rouge-1 (unigram) recall is around 90%. On ROUGE evaluations, no extractive summarizer can do better than just returning the document itself (in practice it will
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2036371118,2152992673	ht units the same. 3. Further, if “thought units” are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31]. This also means that the optimization problem of our model is NP-hard at least and NP-complete when W D(t) is a constant function and I(S;T) is boolean-valued. Theorem 1. The optimization problem of
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2150824314	le 2 shows that, for the DUC 20022 dataset, when the document themselves are considered as summaries and evaluated against a set of 100-word human abstractive summaries, the average Rouge-1 (unigram) [19] score is approximately 91 %. Tables 1 through 4 and Figures 1 and 2 use the following abbreviations: (i) R-n means ROUGE metric using n-gram matching, and (ii) lowercase sdenotes the use of stopword
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2250968833	mmarization. For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions. In [28], the authors present results for single-document summarization on a sub
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2036371118,2152992673	n the papers cited in this paragraph. Abstractive summarization. Abstractive summarization systems include [5,12,6,20,30,7]. Frameworks. Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36]. Metrics and Evaluation. Of course, ROUGE is not the only metric for evaluating summaries. Human evaluators were used at NIST for scor
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2152992673	n recursion, but this algorithm quickly runs out of time/space because of repeated computations. In addition to this optimal algorithm, DocSumm also implements a version of the algorithm presented in [23]. McDonald frames the problem of document summarization as the maximization of a scoring function that is based on relevance and redundancy. In essence, selected sentences are scored higher for releva
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	1501617060	omatic summarizers could not outperform a baseline summary consisting of the ﬁrst 100 words of a news article. Those that did outperform the baseline could not do so in a statistically signiﬁcant way [27]. Summarization can be extractive or abstractive [21]: in extractive summarization sentences are chosen from the article(s) given as input, whereas in abstractive summarization sentences may be genera
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2001135856	research until 2015. Here, we give a sampling of the literature and focus more on recent research and/or evaluation work. Single-document extractive summarization. For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical str
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2036371118	rithm is approximate, because the inclusion/exclusion of the algorithm inﬂuences the score of other sentences. A couple of greedy algorithms and a dynamic programming algorithm of DocSumm appeared in [31], the rest are new. 4.2 Results Our results include experiments on running time comparisons of DocSumm’s algorithms. In addition we compare the performance measures of DocSumm on DUC 2001 and DUC 2002
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2150824314	s such as linguistic quality, etc. There is also the Pyramid approach [29] and BE [32], for example. Our choice of ROUGE is based on its popularity, ease of use, and correlation with human assessment [19]. Our choice of ROUGE conﬁgurations includes the one that was found to be best according to the paper [14]. 3 Limits on Extractive Summarization In all instances the ROUGE evaluations include the best
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2251607282	s not the only metric for evaluating summaries. Human evaluators were used at NIST for scoring summaries on seven different metrics such as linguistic quality, etc. There is also the Pyramid approach [29] and BE [32], for example. Our choice of ROUGE is based on its popularity, ease of use, and correlation with human assessment [19]. Our choice of ROUGE conﬁgurations includes the one that was found to
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	1843891098,1939882552,2065354331,2164755781	s a special case, but no experimental results are presented for this important special case in the papers cited in this paragraph. Abstractive summarization. Abstractive summarization systems include [5,12,6,20,30,7]. Frameworks. Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36]. Metrics and Evaluation. Of course, ROUGE is no
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2102269292	on of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35]. Supervised and semi-supervised learning based extractive summarization was studied in [34]. Of course, single-document summarization can be considered as a special case, but no experimental results are presented for this important special case in the papers cited in this paragraph. Abstrac
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	1696101414,2150869743,2152992673	ti-document extractive summarization. For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35]. Supervised and semi-supervised learning based extractive summarization was studied in [34]. Of course, single-document summarization can be considered as a special case, but no experimental results
2609080309	Extractive Summarization: Limits, Compression, Generalized Model and Heuristics	2251803607	tractive summarization systems include [5,12,6,20,30,7]. Frameworks. Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36]. Metrics and Evaluation. Of course, ROUGE is not the only metric for evaluating summaries. Human evaluators were used at NIST for scoring summaries on seven different metrics such as linguistic quali
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1604644367	on (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction. Various efforts have been put to combat the longcriticized wrong labeling problem. Riedel et al. (2010), Hoffmann et al. (2011), and Surdeanu etal.(2012)haveattemptedamulti-instance learning (Dietterich et al., 1997) framework to soften the assumption of distant supervision, but their models are still
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2250539671	bjective is to minimize Θ = 1 |E| X i,j:˜p(r j|t i)&gt;0 (logp(r j|t i) −log ˜p(r j|t i)) 2, (4) where Eis the edge set of the relation graph. It is modeled as a regression problem, similar to GloVe (Pennington et al., 2014). Baseline. We also deﬁne a baseline approach where the unnormalized co-occurrence counts are directly used. The objective is to maximize: Θ′ = 1 P i,jn ij X i,j:n ij&gt;0 n ijlogp(r j|t i). (5) Itals
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1604644367	ds. 6.1 Experimental Setup Dataset. Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al., 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al., 2008). Articles from year 2005-2006 are used as training, and articles from 2007 are used as testing. S
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2251135946	et a better evaluation of the candidate relational fact. One straightforward aggregation is max pooling, i.e., only using the largest score max s∈CG(z|s), similar to the at-least-one strategy used by Zeng et al. (2015). But it will lose the useful signals from those neglected sentences (Lin et al., 2016). Because of the wrong labeling problem, mean pooling is problematic as well. The wrongly labeled contextual sent
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2251135946	on extraction models. We evaluate with four recent relation extraction models whose source code is publicly available3. We use the optimized parameters provided by the authors. •CNN+ONE and PCNN+ONE (Zeng et al., 2015): A convolutional neural network (CNN)isused to embed contextual sentences for relation classiﬁcation. Multi-instance learning with at-least-one (ONE) assumption is used to combat the wrong labeling p
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1750263989	f many relation extraction models. Because of their exact feature matching, early kernel based models (Bunescu and Mooney, 2005) can hardly exploit ﬁne-grained word similarities. More recent studies (Xu et al., 2015a,b, 2016; Liu et al., 2016) have explored embedding textual relations via neural networks. However, they have all focused on the supervised setting, where the embedding model is trained on a set of s
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2107598941	hs without typed dependency relations, while a convolutional neuralnetwork isused in(Xu et al.,2015a). However, they are all based on the supervised setting with a limited scale. Distant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction. Various efforts have been put to combat the longcriticized wrong labeling problem. Riedel et al. (2010), H
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2251135946	l. (2011), and Surdeanu etal.(2012)haveattemptedamulti-instance learning (Dietterich et al., 1997) framework to soften the assumption of distant supervision, but their models are still feature-based. Zeng et al. (2015) combine multi-instance learning with neural networks, with the assumption that at least one of the contextual sentences of an entity pair is expressing the target relation, but this will lose useful
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2138627627	mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate features (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently neural network models have been introduced to embed words, relations, and ∗ Equally contributed. sentences in continuous feature space, and have shown a remarkable succ
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2053238041	n can be improvedfrom 83.9% to 89.3%. 1 Introduction Relation extraction requires deep understanding of the relation between entities. Early studies mainly use hand-crafted features (Kambhatla, 2004; Zhou et al., 2005), and later kernel methods are introduced to automatically generate features (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently neural network m
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2138627627	n. Textual relations are one of the most discriminative textual signals that lay the foundation of many relation extraction models. Because of their exact feature matching, early kernel based models (Bunescu and Mooney, 2005) can hardly exploit ﬁne-grained word similarities. More recent studies (Xu et al., 2015a,b, 2016; Liu et al., 2016) have explored embedding textual relations via neural networks. However, they have al
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1750263989	oduced to embed words, relations, and ∗ Equally contributed. sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016). In this work, we study the problem of embedding textual relations, deﬁned as the shortest dependency path1 between two entities in the dependency graph of a se
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1750263989	previous methods and have shown a remarkable success (e.g.,(Socher et al.,2012;Zeng et al.,2014)). Among those, the most related are the ones embedding shortest dependency paths with neural networks (Xu et al., 2015a,b, 2016; Liu et al., 2016). For example, Xu et al. (2015b) use a recurrent neural network (RNN) with LSTM units toembed shortest dependency paths without typed dependency relations, while a convolut
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2251135946	rds, relations, and ∗ Equally contributed. sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016). In this work, we study the problem of embedding textual relations, deﬁned as the shortest dependency path1 between two entities in the dependency graph of a sentence, to improve r
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2251135946	at it could signiﬁcantly improve the performance of several recent relation extraction methods. 6.1 Experimental Setup Dataset. Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al., 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al., 2008). A
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2250861254	t i)(e,e ′), is the number of occurrences of the corresponding relational fact (e,t i,e′) in 2In the experiments entity linking is assumed given, and dependency parsing is done using Stanford Parser (Chen and Manning, 2014) with universal dependencies. the corpus. For example, if the support of t i is S(t i) = {(e1,e′ 1),(e1,e′1 ),(e2,e′2 ),...}, entity pair (e1,e′ 1) has a multiplicity of 2 because the relational fact(
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1541280084	te the negative impact of noise by modeling and learning noise transition patterns from data. Liu et al. (2017) propose to infer the true label of a context sentence using a truth discovery approach (Li et al., 2016). Wu et al. (2017) incorporate adversarial training, i.e., injecting random perturbations in training, to improve the robustness of relation extraction. Using PCNN+ATT(Lin et al., 2016) as base model,
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2138627627	tegrated in a max entropy model. With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). The most related work to ours is by Bunescu and Mooney (2005), where the authors point out and demonstrate the importance of shortest dependency paths for relation extraction. M
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2174833404	tics to combat the wrong labeling problem of distant supervision. In universal schema (Riedel et al., 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al.,2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns. A matrix entry is 1 if the relational fact is observed in training, and 0 otherw
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1604644367	tics are thus more robust to the noise introduced by the wrong labeling problem. We augment existing relation extractions using thelearned textual relationembeddings. Onapopular dataset introduced by Riedel et al. (2010), we show that a number of recent relation extraction models, which are based on local statistics, can be signiﬁcantly improved using our textual relation embeddings. Most remarkably, a new best perfo
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2107598941	ting: The training data typically consists of several thousands of annotated sentences and around 10 target relations (Liu et al., 2016). Incontrast, weembedtextual relations withdistant supervision (Mintz et al., 2009), which provides much larger-scale training data without the need of manual annotation. However, the assertion of distant supervision, “any sentence containing a pair of entities that participate in a
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2130942839	tributions. We use a separate GRU cell followed by softmax to map a textual relation embedding to a distribution over KB relations; the full model thusresembles the sequence-to-sequence architecture (Sutskever et al., 2014). Given a textual relation t i and its embedding h m,the predicted conditional probability of a KB relation r j is thus: p(r j|t i) = softmax(GRU(φ(&lt;GO&gt;),h m)) j, (3) where() j denotes the j-th
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	2250539671	une the embedding model in each local window (e.g., 10 consecutive words). In contrast, in global statistics based methods, exempliﬁed by latent semantic analysis (Deerwester et al., 1990) and GloVe (Pennington et al., 2014), we process the entire corpus to collect global statistics like word-word co-occurrence counts, normalize the raw statistics, and train an embedding model directly on the normalized global statistics
2609844752	GLOBAL RELATION EMBEDDING FOR RELATION EXTRACTION	1522301498	on the validation set. For the embedding model, the mini-batch size is set to 128, and the state size of the GRU cells is 300. For the merging model, the mini-batch size is set to 1024. We use Adam (Kingma and Ba, 2014) with parameters suggested by the authors for optimization. Word embeddings are initialized with the 300-dimensional word2vec (Mikolov et al., 2013) vectors pre-trained on the Google News corpus4. Ear
2612675303	A Deep Reinforced Model for Abstractive Summarization	1544827683	ataset with an extra loss term to increase temporal coverage of the encoder attention function. 5 Datasets 5.1 CNN/Daily Mail We evaluate our model on a modiﬁed version of the CNN/Daily Mail dataset (Hermann et al., 2015), following the same pre-processing steps described inNallapati et al.(2016). We refer the reader to that paper for a detailed description. The ﬁnal dataset contains 286,817 training examples, 1http:/
2612675303	A Deep Reinforced Model for Abstractive Summarization	2153579005	to encode an input sentence into a ﬁxed vector, and create a new output sequence from that vector using another RNN. To apply this sequence-to-sequence approach to natural language, word embeddings (Mikolov et al., 2013;Pennington et al.,2014) are used to convert language tokens to vectors that can be used as inputs for these networks. Attention mechanisms (Bahdanau et al.,2014) make these models more performant and
2612675303	A Deep Reinforced Model for Abstractive Summarization	2115808177	n though this dataset has been used to train extractive summarization systems (Hong and Nenkova,2014;Li et al.,2016) or closely-related models for predicting the importance of a phrase in an article (Yang and Nenkova, 2014;Nye and Nenkova,2015;Hong et al.,2015), we are the ﬁrst group to run an end-to-end abstractive summarization model on the article-abstract pairs of this dataset. While CNN/Daily Mail summaries have a
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2250539671	out 2M (question, rewrite, answer) triples. We remove queries where all rewrites yield identical rewards, which removes about half of the training data. We use pre-trained 100-dimensional embeddings (Pennington et al., 2014) for the tokens. Our CNN-based selection model encodes the three strings into 100-dimensional vectors using a 1D CNN with kernel width 3 and output dimension 100 over the embedded tokens, followed by
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2427527485	e gold answer. We present results on the full validation and test sets (referred to as n-gram in (Dunn et al., 2017)). Overall, SearchQA appears to be harder than other recent QA tasks such as SQuAD (Rajpurkar et al., 2016), for both machines and humans. BiDAF’s performance drops by 40 F1 points on SearchQA compared to SQuAD. However, BiDAF is still competitive on SeachQA, improving over the Attention Sum Reader network
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2079168273	er query, where the document is the context from which the answer is selected, as a measure of how informative a term is.4 As another measure of query performance, we also compute Query Clarity (QC) (Cronen-Townsend et al., 2002).5 Figure 2 summarizes statistics of the questions and rewrites. We ﬁrst consider the top hypothesis generated by the pre-trained NMT reformulation system, before reinforcement learning (Base-NMT). Th
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2625113742	et, AQA-QR signiﬁcantly outperforms all others in the QA task. Training the agent starting from the Base-NMT+Quora model yielded comparable results as starting from Base-NMT. 6.3 DISCUSSION Recently, Lewis et al. (2017) trained chatbots that negotiate via language utterances in order to complete a task. They report that the agent’s language diverges from human language if there is no incentive for ﬂuency in the rewa
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2131726681	ewrite is What month and year goes back to the morning and year?. To improve quality, we resume training on a smaller monolingual dataset, extracted from the Paralex database of question paraphrases (Fader et al., 2013).2 Unfortunately, this data contains many noisy pairs. We ﬁlter many of these pairs out by keeping only those where the Jaccard coefﬁcient between the sets of source and target terms is above 0.5. Fur
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2410983263	nding across many problems. For example, Narasimhan et al. (2015) use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al. (2016) use RL for dialogue generation. Policy gradient methods have been investigated recently for MT and other sequence-to-sequence problems. They alleviate limitations inherent to the word-level optimizat
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2144600658	tions each, of which we select a random one as the source and the other four as references. We use beam search, to compute the top hypothesis and report uncased, moses-tokenized BLEU using multeval9 (Clark et al., 2011). Please note, that the MSCOCO data is only used for evaluation purposes. Examples of all systems can be found in Appendix C. The Base-NMT model performs at 11.4 BLEU (see Table 1 for the QA eval numb
2617128460	Ask the Right Questions: Active Question Reformulation with Reinforcement Learning	2546950329	during training (Ranzato et al., 2015). We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. (2017), who train a semantic parser to query a knowledge base, and Seo et al. (2017b) who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reas
2617420226	Abstract Argumentation / Persuasion / Dynamics	2104126268	here a3 attacks a1, in which case B is not persuaded into holding a2. For judging the success of a persuasion act based on other arguments, the theoretical framework of abstract argumentation by Dung [17] is viable, as it facilitates acceptability of a set of arguments when attacks among them are given. We incorporate the persuasion acts into argumentation frameworks, and study how they inﬂuence abstr
2617420226	Abstract Argumentation / Persuasion / Dynamics	2104126268	dmissibility so that such a query as ‘Is a6 going to be an admissible argument in whatever order persuasive acts may take place?’ is possible. 2 TECHNICAL BACKGROUNDS In Dung’s abstract argumentation [17], an argumentation framework is a tuple „A;R”where A is (usually) a ﬁnite set of arguments (and in the rest any A with or without a subscript is assumed to be a set of arguments) while R is a binary r
2617420226	Abstract Argumentation / Persuasion / Dynamics	1496764493,1558324760	e [2, 8, 10, 18–20, 22–25]. While we do not assume any dialogue here, our admissibilities should provide means of describing many types of argumentation queries. Studies on temporal arguments include [4, 5, 21]. These actually consider arguments that may be time-dependent. APA frameworks keep arguments abstract, but observe temporal progress through actual execution of persuasive acts. Our use of temporal l
2617420226	Abstract Argumentation / Persuasion / Dynamics	2142438850	Our present plan is to take into account nuances of persuasive acts like pseudo-logic, scapegoating, threat, and half-truths in addition to the future work we mentioned earlier. For dynamic aspects, [6, 11, 12, 14, 15, 26] calculate a modiﬁed argumentation framework (post-state) from an original argumentation framework (pre-state) given some input. In spirit, they are similar to the AGM belief revision [1]. Coordinatio
2617420226	Abstract Argumentation / Persuasion / Dynamics	229834379	tion of dynamics and statics, on the other hand, is still under-investigated. A kind was studied in [3] for coalition proﬁtability and formability semantics. More closely, propositional dynamic logic [16] was used to represent abstract argumentation so that in the language addition and removal of arguments and attacks may be expressed. In an extended work, we will detail comparisons with their work. A
2617501642	Learning Structured Text Representations	2123442489	.7% depth 3 12.8% 22.4% depth 4 12.5% 23.4% depth 5 12.0% 14.4% depth 6 10.3% 4.5% Same Edges 38.7% Table 5: Descriptive statistics for dependency trees produced by our model and the Stanford parser (Manning et al., 2014) on the SNLI test set. rithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores. We report various statistics on the characteristics of the induced trees
2617501642	Learning Structured Text Representations	1840435438	2. We compared our model (and variants thereof) against several related systems. Results (in terms of 3-class accuracy) are shown in Table 1. Most preModels Acc  Classiﬁer with handcrafted features (Bowman et al., 2015) 78.2 — 300D LSTM encoders (Bowman et al., 2015) 80.6 3.0M 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 83.2 3.7M 100D LSTM with inter-attention (Rockt¨aschel et al., 2016)
2617501642	Learning Structured Text Representations	2115242108	8) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST
2617501642	Learning Structured Text Representations	2250539671	bution over the labels. The hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the learning rate was set to 0.05. The hidden size of the two
2617501642	Learning Structured Text Representations	2409591106	on its context. We then exploit the structure of Twhich we induce based on an attention mechanism detailed below to obtain more precise representations. Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSTM output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deﬁciencies, we decompose
2617501642	Learning Structured Text Representations	2140830123	for document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be ﬂexible and the dependency edges may cross. Secondly, the inside-outside algorithm involves a dynamic programming process which is
2617501642	Learning Structured Text Representations	2133564696	those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of
2617501642	Learning Structured Text Representations	2586050494	he document. Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees.
2617501642	Learning Structured Text Representations	2133564696	e normalized attention score between tokens u i and u j. each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences. Figure 2 provides a schematic view of the intrasentential attention mechanism. Given a sentence represented as a sequence of nword
2617501642	Learning Structured Text Representations	2586597293	e of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser beneﬁts text categorization in learning representations that focus on salient content. Linguistically motivated representatio
2617501642	Learning Structured Text Representations	2064675550	el Let T = [u 1;u 2; ;u n] denote a sentence containing a sequence of words, each represented by a vector u, which can be pre-trained on a large corpus. Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have u1 u2 u3 u4 d1 d2 d3 d4 e1 e2 e3 e4 Calculate Structured A te nio Update Sem antic Vectors r1 r2 r3 r4 Figure 3: Sentence representation model: u t is the input vector for the t-th word, e tand
2617501642	Learning Structured Text Representations	1600133633	epresentations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et
2617501642	Learning Structured Text Representations	2133330855	es of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson
2617501642	Learning Structured Text Representations	2221711388	an et al., 2015) 80.6 3.0M 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 83.2 3.7M 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 83.5 252K 200D Matching LSTMs (Wang and Jiang, 2016) 86.1 1.9M 450D LSTMN with deep attention fusion (Cheng et al., 2016) 86.3 3.4M Decomposable Attention over word embeddings (Parikh et al., 2016) 86.8 582K Enhanced BiLSTM Inference Model (Chen et al.
2617501642	Learning Structured Text Representations	2549835527	ifferent from linguistics ones, without ever exposing the model to linguistic annotations or an external parser. Directions for future work are many and varied. Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism. We also plan to explore how document-level trees can be usefully employed in summar
2617501642	Learning Structured Text Representations	1598801360	which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Rec
2617501642	Learning Structured Text Representations	2115242108	k in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporati
2617501642	Learning Structured Text Representations	2044599851	l., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document stru
2617501642	Learning Structured Text Representations	2586597293	l neural network (Tang et al., 2015a) 59.7 — — — — Convolutional gated RNN (Tang et al., 2015a) 63.7 42.5 — — — LSTM gated RNN (Tang et al., 2015a) 65.1 45.3 — — — RST-based recursive neural network (Ji and Smith, 2017) — — — 75.7 — 75D Hierarchical attention networks (Yang et al., 2016) 68.2 49.4 80.8 74.0 273K 75D No Attention 66.7 47.5 80.5 73.7 330K 100D Simple Attention 67.7 48.2 81.4 75.3 860K 100D Structured
2617501642	Learning Structured Text Representations	1840435438	l on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9
2617501642	Learning Structured Text Representations	835791623	labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired
2617501642	Learning Structured Text Representations	2142972908	lassiﬁcation; we followed the preprocessing introduced in Tang et al. (2015a) and report experiments on their training, development, and testing partitions (80/10/10). IMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies. Each review is associated with user ratings ranging from 1 to 10. Czech reviews were obtained from Brychcın and Habernal (2013). The dataset contains rev
2617501642	Learning Structured Text Representations	2586597293	and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Structur
2617501642	Learning Structured Text Representations	1986500605	our model that prevents its application to individual sentences. Advantageously, it can induce non-projective structures which are required for representing languages with free or ﬂexible word order (McDonald and Satta, 2007). Our contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efﬁ- cient training procedure which allows to comput
2617501642	Learning Structured Text Representations	2586050494	ng Our model can be trained in an end-to-end fashion since all operations required for computing structured attention and using it to update the semantic vectors are differentiable. In contrast to in Kim et al. (2017), training can be done efﬁciently. The major complexity of our model lies in the computation of the gradients of the the inverse matrix. Let Adenote a matrix depending on a real parameter x; assuming
2617501642	Learning Structured Text Representations	2153579005	nly retained words appearing more than ﬁve times in building the vocabulary and replaced words with lesser frequencies with a special UNK token. Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset. In our experiments, we set the word embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel LSTMs to 100 (the
2617501642	Learning Structured Text Representations	2586050494	process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to speciﬁc tasks without an external parser. The inside-outside model of Kim et al. (2017) and our model both have a O(n3) worst case complexity. However, major operations in our approach can be parallelized efﬁciently on GPU computing hardware. Although our primary focus is on document mo
2617501642	Learning Structured Text Representations	2586050494	resentations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Speciﬁcally, we embed a differentiable non-projective parsing algorithm into a neural model a
2617501642	Learning Structured Text Representations	2251803607	rm of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s 1;s 2; ;s n], for each sentence s i, the input is a sequence of word embeddings [u i1;u i2; ;u im], where mis the number of tokens in
2617501642	Learning Structured Text Representations	2123442489	rt dependency parser trained on the English Penn Treebank. Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014). As can be seen, the induced dependency structures are simpler compared to those obtained from the Stanford parser. The trees are generally less deep (their height is 5.78 compared to 8.99 for the St
2617501642	Learning Structured Text Representations	2586050494	on is shallow, limited to word-word dependencies. Since attention is computed as a simple probability distribution, it cannot capture more elaborate structural dependencies such as trees (or graphs). Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism. Speciﬁcally, they normalize f ij with a projective dependency
2617501642	Learning Structured Text Representations	2166957049	son, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bh
2617501642	Learning Structured Text Representations	2140676672	sumed several guises in the literature, such as trees in the style of Rhetorical Structure Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008) has led to the development of off-the
2617501642	Learning Structured Text Representations	2133564696	u t is the input vector for the t-th word, e t and d t are semantic and structure vectors, respectively. been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition (Graves et al., 2013), and image caption generation (Xu et al., 2015). In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or
2617501642	Learning Structured Text Representations	2143612262	tand d are semantic and structure vectors, respectively. been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition (Graves et al., 2013), and image caption generation (Xu et al., 2015). In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capt
2617501642	Learning Structured Text Representations	2586597293	tention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence. On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser. Table 4 presents three variants4 of our model, one with structured attention on the sentence leve
2617501642	Learning Structured Text Representations	1600133633	tion Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016a; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document repr
2617501642	Learning Structured Text Representations	2158211888	ument level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s 1;s 2; ;s n], for each sentence s i, the input is a sequence of word e
2617501642	Learning Structured Text Representations	1514535095	vely. been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition (Graves et al., 2013), and image caption generation (Xu et al., 2015). In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an “inﬁnite” window around
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2141411743	[12] DRNN add. Aurora-2 [77] LSTM-RNN add. Buckeye [79] LSTM-RNN con. TV control [78] LSTM-RNN add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92] DNN add. Aurora-4 IRM (MA, SA; Mel) [94] LSTM-DRNN add.&amp;con. 2nd CHiME IRM (PSA; Mel) [96] LSTM-RNN add.&amp;con. 2nd CHiME IRM (SA; mag) [98] DRNN add. TIMIT, etc. IBM, IRM (MA; mag/power) [91]
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2031647436	. (15)), the terms of yn and xn are in the complex domain,makingthe networklearn to shrinkthe mask estimates when the noise is high [96]. Additionally, a multi-task learning framework was proposed in [97], [98] to jointly learn multiple sources (i.e., speech and noise) and the mask simultaneously. The assumption behind this idea is that the relationship between noise and its caused speech distortion c
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1973681148	. 2nd CHiME log power [69] DBM add. TIMIT [71] DBM add. Aurora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2nd CHiME [12] DRNN add. Aurora-2 [77] LSTM-RNN add. Buckeye [79] LSTM-RNN con. TV control [78] LSTM-RNN add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2046869671	. Buckeye [79] LSTM-RNN con. TV control [78] LSTM-RNN add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92] DNN add. Aurora-4 IRM (MA, SA; Mel) [94] LSTM-DRNN add.&amp;con. 2nd CHiME IRM (PSA; Mel) [96] LSTM-RNN add.&amp;con. 2nd CHiME IRM (SA; mag) [98] DRNN add. TIMIT, etc. IBM, IRM (MA; mag/power) [91] DNN add. TIMIT IBM+IRM(MA,SA; mag) [93] D
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1790748249	; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92] DNN add. Aurora-4 IRM (MA, SA; Mel) [94] LSTM-DRNN add.&amp;con. 2nd CHiME IRM (PSA; Mel) [96] LSTM-RNN add.&amp;con. 2nd CHiME IRM (SA; mag) [98] DRNN add. TIMIT, etc. IBM, IRM (MA; mag/power) [91] DNN add. TIMIT IBM+IRM(MA,SA; mag) [93] DNN add. SiSEC2015 PSM (SA; logMel) [88] DRNN add.&amp;con. 2nd CHiME cIRM (MA; complex) [89] DNN add.&amp;
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1872348600	-RNNbased AM was discriminatively trained to generate frame-wise phone predictions. The double-stream emission probability is computed as p(y n|s )=pG(y |s )λpL(y |s )1−λ, (16) where the variable λ ∈ [0,1] denotes the stream weight. Thanks to a LSTM-RNN-based AM, the performance of traditional GMM-HMM system can be greatly improved for noisy speech recognition. 8 Following research was further introduc
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1516630152	. Section III-B). Employing the SA based objective function was empirically examined to perform better than the MA-based one for source separation [94]. Furthermore, the conclusions found in [94] and [99] indicate that combining the two objective functions (i.e., MA and SA) can further improve the speech enhancement performance in both the magnitude and the Mel spectral domains. Due to the importance
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1790748249	), the terms of yn and xn are in the complex domain,makingthe networklearn to shrinkthe mask estimates when the noise is high [96]. Additionally, a multi-task learning framework was proposed in [97], [98] to jointly learn multiple sources (i.e., speech and noise) and the mask simultaneously. The assumption behind this idea is that the relationship between noise and its caused speech distortion could b
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2290318471	. TIMIT [71] DBM add. Aurora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2nd CHiME [12] DRNN add. Aurora-2 [77] LSTM-RNN add. Buckeye [79] LSTM-RNN con. TV control [78] LSTM-RNN add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92]
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2515753980	98] (see above) [105] LSTM-RNN add.&amp;con. 2nd CHiME joint model re-training [80], [78] (see above) cascaded [101] (see above) [106] DNN add.&amp;con. 2nd CHiME end-to-end [2] CNN+DNN add. multiple [16] Very Deep CNN add.&amp;con. Aurora-4, AMI [107] DNN add.&amp;con. TIMIT, WSJ or decoding. Analogous to this approach, Yu et al. [109] used the extracted i-vector to represent the noisy environment; h
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2062164080	adapt to various noisy conditions, an alternative approach aims to let the network-based AM be informed about the noise information when training, which is often termed as Noise-Aware Training (NAT) [100]. In this case, a noise estimate nˆ present in the signal serves as an augmented input and is incorporated with the original observation input y, i.e., [ ,nˆ]. In this way, the DNN is being given addi
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1897240248	add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92] DNN add. Aurora-4 IRM (MA, SA; Mel) [94] LSTM-DRNN add.&amp;con. 2nd CHiME IRM (PSA; Mel) [96] LSTM-RNN add.&amp;con. 2nd CHiME IRM (SA; mag) [98] DRNN add. TIMIT, etc. IBM, IRM (MA; mag/power) [91] DNN add. TIMIT IBM+IRM(MA,SA; mag) [93] DNN add. SiSEC2015 PSM (SA; logMel) [88] DRNN add.&amp;
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2398972335	add.&amp;con. 3rd CHiME superdirective [120] LSTM-RNN add.&amp;con. 4th CHiME PSD estimation [121] DNN add.&amp;con. 3rd CHiME DOA estimation [122] MLP add.&amp;con. WSJ beamform. weights estimation [123] DNN add.&amp;con. AMI post-ﬁltering estimation [124] MLP add.&amp;con. TIMIT back NIN-CNN [125] NIN-CNN add.&amp;con. 3rd CHiME joint channel concatenation [126] DNN add.&amp;con. AMI [127] DNN add.&
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2078528584	AEbased enhancementsmethodnotablyoutperformthe traditional methods like MMSE for enhancingspeech distorted by factory and car noises [68]. Analogous to this, another successful work has been shown in [69], where a DBM was utilised to estimate the complex mapping function. In the pre-training stage, noisy speech was used to train RBMs layer-by-layer in a standard unsupervised greedy fashion to obtain a
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2290318471	ally the LSTM-RNNs, have been frequently demonstrated to be highly capable of capturing the context information in a long sequence [75], [76], as mentioned in Section II-B. In this light, Maas et al. [12] introduced RNN to purify the distorted input features (i.e., MFCCs). Speciﬁcally, the model was trained to predict clean features given noisy input frame by frame. This enhancement model is shown to
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2110798204	amework. For more details about these networks, the reader is referred to [32]. A. Deep Boltzmann Machines and Stacked Autoencoders Deep Boltzmann Machines (DBMs) [33] and Stacked Autoencoders (SAEs) [34] are respectively constructed by stacking multiple layers of Restricted Boltzmann Machines (RBMs) or feedforward autoencoders, respectively. The essential idea of these networks is to utilise deep arc
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2141411743	approaches can for source separation. Rather than estimating the masks in the T-F domain, the masking-based approaches were also successfully applied to a reduced feature space – Mel frequency domain [92], [94] and its logarithmic scale [95] that have frequently been proven to be effective for ASR in deep learning. The experimental results in [94] showed that the masking-based approaches in the Mel fr
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2515753980	arallel way; the activations of the hidden layer of each network will be mutually concatenated as new inputs of its next hidden layer. A quite recent and well-developed framework has been reported in [16], where two tasks were evaluated: the Aurora-4 task with multiple additive noise types and channel mismatch, and the ‘AMI’ meeting transcription task with signiﬁcant reverberation. In this framework,
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1872348600	ary noise, multi-channel speech recognition I. INTRODUCTION Recently, following years of research, Automatic Speech Recognition (ASR) has achieved major breakthroughs and greatly improved performance [1]–[3]. Plenty of related applications, including smartphone assistants (e.g. Siri, Cortana, Google Now), and products such as Amazon Echo and Kinect Xbox One, have started to become part of our daily l
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2167204516	c models and is often implemented in practice using iterative approaches which base new estimates of the Wiener ﬁlter on the enhanced signal obtained by the previous iteration’s Wiener ﬁlter estimate [57]. Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener ﬁltering [4], [58],[59].Anotherpopularfamilyoftechniquescomprises the minimum mean square er
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1600744878	to the capability to capture the inherent representations embedded in the spectro-temporal feature space or in the raw signals, CNNs have attracted increasing interests in the most recent years [2], [82]. For image restoration and further image processing tasks, deep convolutional encoder-decoder networks were proposed in [83] and delivered promising performance. This network was further introduced f
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2046869671	ches can for source separation. Rather than estimating the masks in the T-F domain, the masking-based approaches were also successfully applied to a reduced feature space – Mel frequency domain [92], [94] and its logarithmic scale [95] that have frequently been proven to be effective for ASR in deep learning. The experimental results in [94] showed that the masking-based approaches in the Mel frequenc
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2405774341	ches typical publications NNs noise types databases front (mappingbased) mag [84] deep CNN add. TIMIT log mag [70] DBM add.&amp;con. 2nd CHiME log power [69] DBM add. TIMIT [71] DBM add. Aurora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2nd CHiME [12] DRNN add. Aurora-2 [77] LSTM-RNN a
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2031647436	ck to the timedomain signal sˆ(t)by an inverse STFT. Apart from the MA-based objective function, more and more studies have recently started to use Signal Approximation (SA) objective functions [94], [97], [98]. Such an alternative straightforwardly targets minimising the MSE between the estimated clean spectrum xˆ = y⊗ Mˆ(n,f) and the target clean spectrum xby J(θ)= 1 N XN n=1 kyn ⊗Mˆ(n,f)−xnk2. (15)
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1499999342	con. IEEE corpus back multi-condition training [90] (see above) [100] DNN add. Aurora-4 model adaptation [71] (see above) noise-aware training [71], [100] (see above) [101] DNN add.&amp;con. Aurora-5 [102] DNN con. REVERB i-vector [103] DNN add. WSJ multi-stream [42] (see above) [15] LSTM-RNN add.&amp;con. 2nd CHiME hybrid [104] LSTM-RNN add.&amp;con. 2nd CHiME multi-task [98] (see above) [105] LSTM-RN
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2515753980	convolutional layers were implemented, and each of them is followed by four fully connected layers and one softmax output layer for senone prediction. Compared with DBMs, the CNNs have the advantages [16]: 1) They are well suited to model the local correlations in both time and frequencyin speech spectrogram;and 2) Translational invariance, such as the frequency shift due to speaker or speaking style
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2296167893	criminative representations for speech recognition when reconstructing the clean features from the noisy ones by feature enhancement in the front-end. A similar work has also been done by Wang et al. [106], who concatenated a DNN-based speech separation front-end and a DNN-based AM back-end to build a larger neural network, and jointly adjusted the weights in each model. In doing this, the separationfr
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2603646652	d CHiME joint model re-training [80], [78] (see above) cascaded [101] (see above) [106] DNN add.&amp;con. 2nd CHiME end-to-end [2] CNN+DNN add. multiple [16] Very Deep CNN add.&amp;con. Aurora-4, AMI [107] DNN add.&amp;con. TIMIT, WSJ or decoding. Analogous to this approach, Yu et al. [109] used the extracted i-vector to represent the noisy environment; however, the i-vectors are calculated from the Ve
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2099471712	d deep neural network structures have further been proposed, such as residual neural networks [30] that use skip connections to connect non-adjacent hidden layers, and generative adversarial networks [31] that introduce two neural networks competing against each other in a zero-sum game framework. For more details about these networks, the reader is referred to [32]. A. Deep Boltzmann Machines and Sta
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2044893557	DBN. stages typical approaches typical publications NNs noise types databases front (mappingbased) mag [84] deep CNN add. TIMIT log mag [70] DBM add.&amp;con. 2nd CHiME log power [69] DBM add. TIMIT [71] DBM add. Aurora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2nd CHiME [12] DRNN ad
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2042071805	here the deepness comes from layers through time. Furthermore, similar to DBM and SAE, stacking RNN blocks 3 into a deep structure has attracted increased attention, leading to a Deep RNN (DRNN) [41]–[43]. Recently, the Gated Recurrent Unit (GRU) has also emerged as a computationally simpler alternative to the LSTM block [44]. C. Convolutional Neural Networks (CNNs) CNNs are biologically inspired vari
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2078528584	DNN REFERS TO DBM OR DBN. stages typical approaches typical publications NNs noise types databases front (mappingbased) mag [84] deep CNN add. TIMIT log mag [70] DBM add.&amp;con. 2nd CHiME log power [69] DBM add. TIMIT [71] DBM add. Aurora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2n
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2147437692	dvanced variations include exemplar-based approaches to obtaining large dictionaries [66], and semi-supervised approaches to releasing the requirement of both speech and noise for dictionary training [67]. B. Mapping-based Deep Enhancement Methods The mapping-based methods aim to learn a non-linear mapping function F from the observed noisy speech y(t)into the desired clean speech s(t), as y(t)−→F s(t
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2158143227	e/interference, respectively.When appliedto broadbandsignals such as audio, this principle leads to considerable distortion of the speech due to independent SNR optimisation of each frequency bin. In [117], this is compensated by single-channel post-ﬁltering in order to achieve a performance comparable to the MVDR beamformer. B. Neural-Network-Supported Beamformers and Post-Filter To calculate the spat
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1897240248	e., MA and SA) can further improve the speech enhancement performance in both the magnitude and the Mel spectral domains. Due to the importance of phase information as aforementioned, Weninger et al. [96] took the phase information in the objective function, which is called Phase-sensitive SA (PSA). Speciﬁcally, the network does not predict the phase, but still predicts a masking. However, in the obje
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2044893557	EC2015 PSM (SA; logMel) [88] DRNN add.&amp;con. 2nd CHiME cIRM (MA; complex) [89] DNN add.&amp;con. IEEE corpus back multi-condition training [90] (see above) [100] DNN add. Aurora-4 model adaptation [71] (see above) noise-aware training [71], [100] (see above) [101] DNN add.&amp;con. Aurora-5 [102] DNN con. REVERB i-vector [103] DNN add. WSJ multi-stream [42] (see above) [15] LSTM-RNN add.&amp;con. 2
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2046869671	ective function used for the mapping-basedmethods (cf. Section III-B). Employing the SA based objective function was empirically examined to perform better than the MA-based one for source separation [94]. Furthermore, the conclusions found in [94] and [99] indicate that combining the two objective functions (i.e., MA and SA) can further improve the speech enhancement performance in both the magnitude
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2101045344	re enhancement approaches. VI. MULTI-CHANNEL TECHNIQUES Microphone arrays and multi-channel processing techniques have recently played an increasingly signiﬁcant role in the development of robust ASR [8], [9]. A central approach is acoustical beamforming, i.e., spatio-temporal ﬁltering that operates on the outputs of microphone arrays and converts it to a single-channel signal while focusing on the d
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1542280630	ers and post-ﬁlters for speech enhancement, joint front- and back-end multi-channel ASR systems have recently attracted considerable attention with a goal of decreasing the WER directly [126], [128], [129]. In [127], the individual features extracted from each microphone channel are concatenated as a long single feature vector and fed into a DNN for AM. Whilst such a feature concatenation operation is
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	160800111	essionfunctionfroma noisyspeech spectrum Y(n,f)to a Time-Frequency (T-F) mask M(n,f). That is, Y(n,f)−→F M(n,f). (8) 1) Masks: Two most commonlyused masks in the literature include: binary based mask [85] and ratio based mask [86]. Typical binary based mask often refers to Ideal Binary Mask (IBM), where a T-F mask unit is set to 1 if the local SNR is greater than a threshold R (indicating clean speech
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2078528584	etworks (i.e., SAE, DBM, and SDAE) are evaluated to be less capable in this aspect, although certain naive solutions were performed, such as expanding several sequential frames as a long vector input [69]. RNNs, especially the LSTM-RNNs, have been frequently demonstrated to be highly capable of capturing the context information in a long sequence [75], [76], as mentioned in Section II-B. In this light
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2193413348	exploring the potential of deep neural networks at different stages in the speech recognition chain, end-to-end systems have recently attracted increasing interest and have shown great promise in ASR [2], [82]. The central idea is to jointly optimise the parameters of the networks at the front-endwhich automatically learn the inherent representations for the task at hand, and the networks at the back
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2115055618	frame by frame. This enhancement model is shown to be competitive with other DNN-based mapping models at various levels of SNR when evaluated by ASR systems. Following from this work, Wo¨llmer et al. [77] further proposed to use LSTMRNNs to handle highly non-stationary additive noise, which was then extended to coping with reverberation in [42], [78]– [81]. With the help of LSTM-RNN, the speech recogn
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2242685705	hancement approaches. VI. MULTI-CHANNEL TECHNIQUES Microphone arrays and multi-channel processing techniques have recently played an increasingly signiﬁcant role in the development of robust ASR [8], [9]. A central approach is acoustical beamforming, i.e., spatio-temporal ﬁltering that operates on the outputs of microphone arrays and converts it to a single-channel signal while focusing on the desire
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2193413348	hanks to the capability to capture the inherent representations embedded in the spectro-temporal feature space or in the raw signals, CNNs have attracted increasing interests in the most recent years [2], [82]. For image restoration and further image processing tasks, deep convolutional encoder-decoder networks were proposed in [83] and delivered promising performance. This network was further introd
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2141411743	icated that the ratio masking (e.g., IRM) is superior to the binary masking (e.g., IBM) in terms of objective intelligibility and quality metrics. This conclusion was further supported by the work in [92], where the obtained results suggested that IRM achieves better ASR performance than IBM. Further, the work done by Grais et al. [93] shows that combining IBM and IRM can deliver better performance th
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2603646652	he inherent representations for the task at hand, and the networks at the back-end which provide ﬁnal predictions. Rather than constructing the end-to-end networks in a cascaded way, Ravanelli et al. [107] proposed to join the enhancement network and speech recognition network in a parallel way; the activations of the hidden layer of each network will be mutually concatenated as new inputs of its next
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2141520175	ionaries of speech and noise is by training them separately on the clean speech and noise sounds [64], [65]. More advanced variations include exemplar-based approaches to obtaining large dictionaries [66], and semi-supervised approaches to releasing the requirement of both speech and noise for dictionary training [67]. B. Mapping-based Deep Enhancement Methods The mapping-based methods aim to learn a
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2044893557	ired clean speech was set as the target by minimising the objective function as Eq. (15). Similar research efforts were also extensively made on the log magnitude [70] and the logMel spectral domains [71], respectively. Motivated by the fact that the same distortion in different frequency bands has different effects on speech quality, a weighted SAE was proposed in [72] which has shown positive perfor
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2127851351	isy) signal in different representation domains (e.g., spectral or cepstral); and Source-toDistortion Ratio (SDR) is also frequently used in evaluating the performance of source separation algorithms [49]. Further, Perceptual Evaluation of Speech Quality (PESQ) [50] and its more recent successor Perceptual Objective Listening Quality Assessment (POLQA) [51] are test methodologies developed for quality
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1992475611	ition system divided into frontend and back-end. the prominent techniques. Whilst several related surveys on environmentally robust speech recognition are available in the literature (e.g., [5], [25]–[29]), none of these works focuses on the usage of deep learning. The emergence of deep learning is, however, deemed as one of the most signiﬁcant advances in the ﬁeld of speech recognition in the past de
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	189596042	k (DBN). Different from DBMs of which all connections are undirected, the top two layers of DBNs form an undirected graph and the remaining layers form a belief net with directed top-down connections [39]. At the early development stage of deep learning, the term Deep Neural Network (DNN) often refers to DBMs or DBNs. B. Recurrent Neural Networks (RNNs) In contrast to the aforementioned neural network
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1897240248	l frequency domain perform better than the ones in the T-F domain in terms of SDR. 6 Further, another trend in masking-based approaches is replacing DNNs with LSTM-RNNs as the mask learning model[94]–[96], since LSTM-RNNshaveshownto be capable of learning the speech and noise context information in a long temporal range as mentioned in Sections II-B and III-B likewise often being able to cover up for
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2405774341	l noisy speech, and evaluated by the objective measures as aforementioned. 1) Based on SAE or DBM: Speciﬁcally, in 2013 SAE was employed to map noisy speech to clean speech in the Mel spectral domain [68]. Given an AE that includes one nonlinear encoding stage and one linear decoding stage for real valued speech as h(y)=g(W1y+b) xˆ=W2h(y)+b, (6) where W1 and W2 are the weight matrices of encoding and
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2059045901	litude MMSE (Log-MMSE) short-time spectral amplitude (STSA) estimators [61]. Their performance is still considered to be among the best of the published general-purpose speech noise reduction methods [62]. Particularly, a supervised-learning-based method, namely Non-negative Matrix Factorisation (NMF), has been frequently utilised [63]. The main idea is to ﬁnd a dictionary (basis) matrix W and an acti
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2520164769	ls, CNNs have attracted increasing interests in the most recent years [2], [82]. For image restoration and further image processing tasks, deep convolutional encoder-decoder networks were proposed in [83] and delivered promising performance. This network was further introduced for speech enhancement [84], where the timefrequency spectrum (spectrogram) is viewed as an image. Speciﬁcally, the encoder ne
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2062164080	MIT IBM+IRM(MA,SA; mag) [93] DNN add. SiSEC2015 PSM (SA; logMel) [88] DRNN add.&amp;con. 2nd CHiME cIRM (MA; complex) [89] DNN add.&amp;con. IEEE corpus back multi-condition training [90] (see above) [100] DNN add. Aurora-4 model adaptation [71] (see above) noise-aware training [71], [100] (see above) [101] DNN add.&amp;con. Aurora-5 [102] DNN con. REVERB i-vector [103] DNN add. WSJ multi-stream [42] (
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1872348600	the MSE between the estimated mask and the target mask as follows J(θ)= 1 N XN n=1 kF(yn)−M(n,f)k2, (13) where k· 2 is the squared loss, n denotes the frame index, and F(yn)is restricted to the range [0,1]. In the test stage, to ﬁlter out the noise, the estimated mask Mˆ(n,f)=F(yn)is sequentially applied to the spectrum of the mixed noisy signal yby ˆxn =yn ⊗Mˆ(n,f), (14) where ⊗ denotes the elementwis
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2296167893	n. 2nd CHiME hybrid [104] LSTM-RNN add.&amp;con. 2nd CHiME multi-task [98] (see above) [105] LSTM-RNN add.&amp;con. 2nd CHiME joint model re-training [80], [78] (see above) cascaded [101] (see above) [106] DNN add.&amp;con. 2nd CHiME end-to-end [2] CNN+DNN add. multiple [16] Very Deep CNN add.&amp;con. Aurora-4, AMI [107] DNN add.&amp;con. TIMIT, WSJ or decoding. Analogous to this approach, Yu et al. [
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1790748249	N add.&amp;con. Aurora-5 [102] DNN con. REVERB i-vector [103] DNN add. WSJ multi-stream [42] (see above) [15] LSTM-RNN add.&amp;con. 2nd CHiME hybrid [104] LSTM-RNN add.&amp;con. 2nd CHiME multi-task [98] (see above) [105] LSTM-RNN add.&amp;con. 2nd CHiME joint model re-training [80], [78] (see above) cascaded [101] (see above) [106] DNN add.&amp;con. 2nd CHiME end-to-end [2] CNN+DNN add. multiple [16
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2242685705	n practice [5]–[10]. To address these issues, a new wave of research efforts has emerged over the past few years, as showcased in the robust speech recognition challenges such as REVERB and CHiME [7]–[9], [11]. In this research, data-driven approaches based on a supervised machine learning paradigm have received increasing attention, and have emerged as viable methods for enhancing robustness of ASR
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2398972335	N was studied in [122] and a follow-up study investigated the implementation of DS beamforming in an ASR system directly using a feedforward DNN that is used to predict the beamformer’s weight vector [123]. In both cases, the network was trained with generalised cross correlation from simulated multi-channel data from a given array geometry using all possible DOA angles. In the latter paper, the future
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2515753980	nals and features from noisy speech audio, or directly perform recognition of noisy speech. To this end, deep neural networks (”deep learning”) have had a central role in the recent developments [13]–[16]. Deep learning has been consistently found to be a powerful learning approach in exploiting large-scale training data to build complex and dedicated analysis systems [17], and has achieved considerab
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2101045344	nced system and the one evaluated in a degradation-free, clean environment. Therefore, further efforts are still required for speech recognition to overcome the adverse effect of environmental noises [8]–[10]. We hope that this review could help researchers and developers to stand on the frontier of the developments in this ﬁeld and to make greater breakthroughs. ACKNOWLEDGEMENTS This work was suppor
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2194775991	Networks, Convolutional Neural Networks, andothers. Building onthese basic neuralnetworks, diverse advanced deep neural network structures have further been proposed, such as residual neural networks [30] that use skip connections to connect non-adjacent hidden layers, and generative adversarial networks [31] that introduce two neural networks competing against each other in a zero-sum game framework.
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2070707809	ng in practice [5]–[10]. To address these issues, a new wave of research efforts has emerged over the past few years, as showcased in the robust speech recognition challenges such as REVERB and CHiME [7]–[9], [11]. In this research, data-driven approaches based on a supervised machine learning paradigm have received increasing attention, and have emerged as viable methods for enhancing robustness of
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2153894152	nhanced signal obtained by the previous iteration’s Wiener ﬁlter estimate [57]. Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener ﬁltering [4], [58],[59].Anotherpopularfamilyoftechniquescomprises the minimum mean square error (MMSE) [60] and log-spectral amplitude MMSE (Log-MMSE) short-time spectral amplitude (STSA) estimators [61]. Their perform
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1542280630	back NIN-CNN [125] NIN-CNN add.&amp;con. 3rd CHiME joint channel concatenation [126] DNN add.&amp;con. AMI [127] DNN add.&amp;con. AMI cross-channel max-pooling [128] CNN add.&amp;con. AMI end-to-end [129] CNN-DNN add.&amp;con. voice search factoring spatial &amp; spectral ﬁlterings [130] CLDNN add.&amp;con. voice search adaptive spatial ﬁltering [131] LSTM-RNN, CLDNN add.&amp;con. voice search the use
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1542280630	in each node. This algorithm performs better than the one by applying a CNN after a DS beamformer [128]. Encouraged by this work as well as the research trend of end-to-end ASR systems, Hosen et al. [129] extended this work [128] to raw speech signals and without the operation of cross-layer max pooling. The advantage of these extensions is that the system can automatically exploit the spatial informa
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2117539524	oach in exploiting large-scale training data to build complex and dedicated analysis systems [17], and has achieved considerable success in a variety of ﬁelds, such as gaming [18], visual recognition [19], [20], language translation [21], music information retrieval [22], and ASR [23], [24]. These achievements have encouragedincreasing research efforts on deep learning with the goal of improving the r
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2193413348	on. 2nd CHiME multi-task [98] (see above) [105] LSTM-RNN add.&amp;con. 2nd CHiME joint model re-training [80], [78] (see above) cascaded [101] (see above) [106] DNN add.&amp;con. 2nd CHiME end-to-end [2] CNN+DNN add. multiple [16] Very Deep CNN add.&amp;con. Aurora-4, AMI [107] DNN add.&amp;con. TIMIT, WSJ or decoding. Analogous to this approach, Yu et al. [109] used the extracted i-vector to represe
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1499999342	or phoneme or state predictions can further improve the robustness of speech recognition systems. Recently, a multi-task learning based AM has attracted increasing attention. For example, the work of [102] and [105] respectively introduced similar multi-task learning architectures but different network types (i.e., one is a feedforward DNN and the other one is a LSTM-RNN) for noisy speech recognition,
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2025768430	from a random initialisation of weights. Moredetails on the advantagesof pretraining with autoencoders and RBMs can be found in [36]. An extensionof SAE is called Stacked Denoising AutoEncoder (SDAE) [37], [38], where the initial inputs y are corrupted into a partially destroyed version ye by means of stochastic mapping ey ∼ qd(ey|y). By doing this, the robustness of the formed high-level representati
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2290318471	In this research, data-driven approaches based on a supervised machine learning paradigm have received increasing attention, and have emerged as viable methods for enhancing robustness of ASR systems [12]. The primary objective of these approaches is, by means of learning from large amounts of training data, to either obtain cleaner signals and features from noisy speech audio, or directly perform rec
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1600744878	ring the potential of deep neural networks at different stages in the speech recognition chain, end-to-end systems have recently attracted increasing interest and have shown great promise in ASR [2], [82]. The central idea is to jointly optimise the parameters of the networks at the front-endwhich automatically learn the inherent representations for the task at hand, and the networks at the back-end w
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2138857742	re robust than the one learnt by training the whole network in ensemble from a random initialisation of weights. Moredetails on the advantagesof pretraining with autoencoders and RBMs can be found in [36]. An extensionof SAE is called Stacked Denoising AutoEncoder (SDAE) [37], [38], where the initial inputs y are corrupted into a partially destroyed version ye by means of stochastic mapping ey ∼ qd(ey
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2115055618	rora-2 mel [68] DSAE add. Japan. speech logMel [73] DBM con. CENSREC-4 [80] LSTM-DRNN add.&amp;con. 2nd CHiME [42] LSTM-DRNN con. REVERB MFCC [74] SDAE add.&amp;con. 2nd CHiME [12] DRNN add. Aurora-2 [77] LSTM-RNN add. Buckeye [79] LSTM-RNN con. TV control [78] LSTM-RNN add.&amp;con. 2nd CHiME front (maskingbased) IBM (MA; cochleagram) [90] DBN-SVM add. TIMIT IRM (MA; Mel) [92] DNN add. Aurora-4 IRM (
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2172140247	ructure has attracted increased attention, leading to a Deep RNN (DRNN) [41]–[43]. Recently, the Gated Recurrent Unit (GRU) has also emerged as a computationally simpler alternative to the LSTM block [44]. C. Convolutional Neural Networks (CNNs) CNNs are biologically inspired variants of Multilayer Perceptrons (MLPs) originally developed for visual perception tasks [45], [46]. Typically, it consists o
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1499999342	s beneﬁcial to predict phonetic targets [100]. Experimental results on the ‘Aurora-4’ database show that the NAT-based AM has a remarkablenoise robustness in ASR. A similar work has also been done in [102], where the noise estimation was replaced by the room information as an augmented input for dereverberation. Further, a more general way to take the noise information into the network input is employi
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2062164080	s, a drawback of the back-end techniques is that they cannot be applied with arbitrary ASR engines. The most widely used approach in robust ASR at the back end often involves multi-condition training [100]. In doing this, various acoustic variations caused by different noises are provided in the training process, reducing the acoustic distribution mismatch between the training and the test speech. Anot
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1506255978	r signals and features from noisy speech audio, or directly perform recognition of noisy speech. To this end, deep neural networks (”deep learning”) have had a central role in the recent developments [13]–[16]. Deep learning has been consistently found to be a powerful learning approach in exploiting large-scale training data to build complex and dedicated analysis systems [17], and has achieved consi
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2046869671	the speech and noise context information in a long temporal range as mentioned in Sections II-B and III-B likewise often being able to cover up for non-stationaryevents. The research efforts made in [94] have demonstrated that LSTM-RNNs can notably outperform DBM/SAE alternatives in the mask estimation for source separation. 2) Objective Functions and Training Strategies: In the neural network traini
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2158143227	st against an inaccurately estimated steering vector d [116]. In contrast, Generalised EigenValue (GEV) beamforming requires no DOA estimate and is based on maximising the output signal-tonoise ratio [117]. The beamformer ﬁlter coefﬁcients for a given frequency bin are found as the principal eigenvector of a generalised eigenvalue problem as required by [117] wˆGEV =argmax w wHR SSw wHRV V w , (20) whe
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2616139854	st-ﬁlters for speech enhancement, joint front- and back-end multi-channel ASR systems have recently attracted considerable attention with a goal of decreasing the WER directly [126], [128], [129]. In [127], the individual features extracted from each microphone channel are concatenated as a long single feature vector and fed into a DNN for AM. Whilst such a feature concatenation operation is simple, it
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2616139854	stimation [123] DNN add.&amp;con. AMI post-ﬁltering estimation [124] MLP add.&amp;con. TIMIT back NIN-CNN [125] NIN-CNN add.&amp;con. 3rd CHiME joint channel concatenation [126] DNN add.&amp;con. AMI [127] DNN add.&amp;con. AMI cross-channel max-pooling [128] CNN add.&amp;con. AMI end-to-end [129] CNN-DNN add.&amp;con. voice search factoring spatial &amp; spectral ﬁlterings [130] CLDNN add.&amp;con. vo
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2009934439	t; however, the i-vectors are calculated from the Vector Taylor Series (VTS) enhanced features and bottleneck features rather than the commonly used MFCCs. Recently, a successful work was reported by [15], where a double-stream HMM architecture was used for fusing two AMs. Given the HMM emission state s and the input vector y, at every time frame n the double stream HMM has access to two independent i
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2143612262	ta, where the deepness comes from layers through time. Furthermore, similar to DBM and SAE, stacking RNN blocks 3 into a deep structure has attracted increased attention, leading to a Deep RNN (DRNN) [41]–[43]. Recently, the Gated Recurrent Unit (GRU) has also emerged as a computationally simpler alternative to the LSTM block [44]. C. Convolutional Neural Networks (CNNs) CNNs are biologically inspired
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2405774341	are tied, i.e., W1 =WT 2 =W. The empirical results indicate that SAEbased enhancementsmethodnotablyoutperformthe traditional methods like MMSE for enhancingspeech distorted by factory and car noises [68]. Analogous to this, another successful work has been shown in [69], where a DBM was utilised to estimate the complex mapping function. In the pre-training stage, noisy speech was used to train RBMs l
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1790748249	the timedomain signal sˆ(t)by an inverse STFT. Apart from the MA-based objective function, more and more studies have recently started to use Signal Approximation (SA) objective functions [94], [97], [98]. Such an alternative straightforwardly targets minimising the MSE between the estimated clean spectrum xˆ = y⊗ Mˆ(n,f) and the target clean spectrum xby J(θ)= 1 N XN n=1 kyn ⊗Mˆ(n,f)−xnk2. (15) This
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1973681148	unction is employed to train SAE on the power spectrum as J(θ)= 1 N XN n=1 λwkF(yn)−xnk2, (7) where λw is a weight for the w-th frequency band. Further, related approaches were also shown in [73] and [74], where the authors utilised SDAEs to enhance the Mel ﬁlterbankfeatures corruptedby eitheradditiveorconvolutional noise for ASR. The networks were pre-trained with multicondition data, and ﬁne-tuned b
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2009934439	urora-4 model adaptation [71] (see above) noise-aware training [71], [100] (see above) [101] DNN add.&amp;con. Aurora-5 [102] DNN con. REVERB i-vector [103] DNN add. WSJ multi-stream [42] (see above) [15] LSTM-RNN add.&amp;con. 2nd CHiME hybrid [104] LSTM-RNN add.&amp;con. 2nd CHiME multi-task [98] (see above) [105] LSTM-RNN add.&amp;con. 2nd CHiME joint model re-training [80], [78] (see above) cascad
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2110878662	of W are dictionary atoms w, representing the spectra of acoustic events. A typical way to acquire dictionaries of speech and noise is by training them separately on the clean speech and noise sounds [64], [65]. More advanced variations include exemplar-based approaches to obtaining large dictionaries [66], and semi-supervised approaches to releasing the requirement of both speech and noise for dictio
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	2046869671	xˆ back to the timedomain signal sˆ(t)by an inverse STFT. Apart from the MA-based objective function, more and more studies have recently started to use Signal Approximation (SA) objective functions [94], [97], [98]. Such an alternative straightforwardly targets minimising the MSE between the estimated clean spectrum xˆ = y⊗ Mˆ(n,f) and the target clean spectrum xby J(θ)= 1 N XN n=1 kyn ⊗Mˆ(n,f)−xnk2
2618099328	Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments	1897240248	y [97]. Although the masking-based approaches were initially designed forremovingadditivenoise, recent research has showed that they are capable of eliminating convolutional noise as well [88], [95], [96]. IV. BACK-END TECHNIQUES The back-end techniques are also known as model-based techniques. They leave the noisy observation unchanged, and instead let the neural networks automatically ﬁnd out the re
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2158899491	; based on word embeddings trained on NIPS articles, and on the large generic Google News corpus (Mikolov et al., 2013a,b). Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015). However, as we shall see here, this standard practice might not always be eective, as the size of a dataset does not correspond
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2120861206	(KD) complexity for both the E- and M-steps for each token, where Kand Dare the number of topics/dictionary words, respectively, and even O(D) per token is considered impractical for word embeddings (Mnih and Teh, 2012; Mikolov et al., 2013a). Instead, I propose an approximation to EM that is sublinear time in both K and D. We rst impute z using a reparameterization technique, thereby reducing the task to standard
2619688110	Mixed Membership Word Embeddings for Computational Social Science	1566289585	ata of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al., 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015). A standard practice in the literature is to train word embedding models on a generic large corpus such as arX
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2107743791	dictionary. These models follow a long line of research in data-driven semantic representations of text, including latent semantic analysis (Deerwester et al., 1990) and its probabilistic extensions (Hofmann, 1999a; Griths et al., 2007). In particular, topic models (Blei et al., 2003) have found broad applications in computational social science (Wallach, 2016; Roberts et al., 2014) and the digital humanities
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2052261215	distribution. The proposal is implemented eciently by sampling from the experts via the alias table data structure, in amortized O(1) time, rather than in time linear in the sparsity pattern, as in (Li et al., 2014), since the proposal does not involve the sparse term (which is less important in our case). We perform simulated annealing to optimize over the posterior, which is very natural for Metropolis-Hasting
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2153579005	both the E- and M-steps for each token, where Kand Dare the number of topics/dictionary words, respectively, and even O(D) per token is considered impractical for word embeddings (Mnih and Teh, 2012; Mikolov et al., 2013a). Instead, I propose an approximation to EM that is sublinear time in both K and D. We rst impute z using a reparameterization technique, thereby reducing the task to standard word embedding. This c
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2052261215	e scale this algorithm up to thousands of topics using an adapted version of the recently proposed MetropolisHastings-Walker algorithm for high-dimensional topic models, which scales sublinearly in K(Li et al., 2014). The method uses a data structure called an alias table, which allows for amortized O(1) time sampling from discrete distributions. A Metropolis-Hastings update is used to correct for approximating t
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2153579005	esentations of words, independently of whether a full joint probabilistic language model is learned, and that alternative training schemes can be benecial for learning the embeddings. In particular, Mikolov et al. (2013a,b) proposed the skip-gram model, which inverts the language model prediction task and aims to predict the context given an input word. The skip-gram model is a log-bilinear discriminative probabilis
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2238728730	esian inference further aids data eciency, as uncertainty over (d) can be managed for shorter documents. Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here. I provide a more detailed discussion of related work in the supplementary. 3 The Mixed
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2164019165	the estimate ^z, learning the word and topic vectors corresponds to nding the optimal vectors for encoding the ˚’s. This topic model pre-clustering step is reminiscent of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an o-the-shelf clustering algorithm (or LDA) to initially identify dierent clusters of contexts, and then apply word embedding algorithms on the cluster assignments. H
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2158899491	hich can process fewer documents. In this work, I oer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplied by the work of Mikolov et al. (2013a,b); Collobert et al. (2011), and others, who argue that massive datasets are sucient to allow language models to automatically resolve many challenging NLP tasks. Note that \big&quot; datasets are not always available, particu
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2153579005	learner centered emergent literacy kinesthetic learning Table 1: Most similar words to \learning,&quot; based on word embeddings trained on NIPS articles, and on the large generic Google News corpus (Mikolov et al., 2013a,b). Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015). However, as we shall see
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2153579005	ls provide a exible yet ecient latent representation, in which entities are associated with shared, global representations, but to uniquely varying degrees. I identify the skipgram word2vec model of Mikolov et al. (2013a,b) as corresponding to a certain naive Bayes topic model, which leads to mixed membership extensions, allowing the use of fewer vectors than words. I show that this leads to better modeling performa
2619688110	Mixed Membership Word Embeddings for Computational Social Science	1486649854	neric Google News corpus (Mikolov et al., 2013a,b). Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015). However, as we shall see here, this standard practice might not always be eective, as the size of a dataset does not correspond to its degree of relevance for a particular analysis. Even very large
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2238728730	ning the word and topic vectors corresponds to nding the optimal vectors for encoding the ˚’s. This topic model pre-clustering step is reminiscent of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an o-the-shelf clustering algorithm (or LDA) to initially identify dierent clusters of contexts, and then apply word embedding algorithms on the cluster assignments. However, our cluster
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2096974619	and its probabilistic extensions (Hofmann, 1999a; Griths et al., 2007). In particular, topic models (Blei et al., 2003) have found broad applications in computational social science (Wallach, 2016; Roberts et al., 2014) and the digital humanities (Mimno, 2012), where interpretable representations reveal meaningful insights. Despite widespread success at NLP tasks, word embeddings have not yet supplanted topic models
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2147152072	resentation of words than a simple indicator vector into a dictionary. These models follow a long line of research in data-driven semantic representations of text, including latent semantic analysis (Deerwester et al., 1990) and its probabilistic extensions (Hofmann, 1999a; Griths et al., 2007). In particular, topic models (Blei et al., 2003) have found broad applications in computational social science (Wallach, 2016;
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2164973920	s. Then, with the z’s xed to the estimate ^z, learning the word and topic vectors corresponds to nding the optimal vectors for encoding the ˚’s. This topic model pre-clustering step is reminiscent of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an o-the-shelf clustering algorithm (or LDA) to initially identify dierent clusters of contexts, and then apply word embedding algorithms on the c
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2483215953	s trained on the massive, generic Google News corpus, the most similar words relate to learning and teaching in the classroom. Evidently, domain-specic data can be important. Even more concerningly, Bolukbasi et al. (2016) show that word embeddings can encode implicit sexist assumptions. This suggests that when trained on large generic corpora they could also encode the hegemonic worldview, which is inappropriate for s
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2120861206	tion, i.e. optimizing logp(w;^zjV~;b;) = logp(wj^z;V~;b) + const, (7) where V~ is the vector of all word and topic embeddings. This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noisecontrastive estimation (NCE) algorithm of Gutmann and Hyvarinen (2010, 2012). NCE avoids the expensive normalization step, making the algor
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2250753706	unique degree. Bayesian inference further aids data eciency, as uncertainty over (d) can be managed for shorter documents. Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here. I provide a more detailed discussion of related work in the suppleme
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2153579005	w models to address both of these limitations. Word embeddings have risen in popularity for NLP applications due to the success of models designed specically for the big data setting. In particular, Mikolov et al. (2013a,b) showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models
2619688110	Mixed Membership Word Embeddings for Computational Social Science	2250753706	word embeddings parameterize discrete probability distributions over words p(w cjw i) which tend to co-occur, and tend to be semantically coherent { a property leveraged by the Gaussian LDA model of Das et al. (2015). This suggests that these discrete distributions can be reinterpreted as topics ˚(w i). We thus reinterpret the skip-gram as a parameterization of a certain supervised naive Bayes topic model (Table
2622039625	Deep learning evaluation using deep linguistic processing	2181199551	, artiﬁcial data is relatively little used in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al., 2015). Here we focus on the problem of visually grounded language understanding in the context of the recently popular task of visual question answering (VQA). In p
2622039625	Deep learning evaluation using deep linguistic processing	1933349210	. For instance, MS COCO (Lin et al., 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. There are, however, various problems related to this practice. Data obtained this way tends to be comparatively simple in terms of syntax and compositional semantics, despite exh
2622039625	Deep learning evaluation using deep linguistic processing	2560730294	correctly – is not a good reﬂection of a system’s general ability to understand visually grounded language. 2.2 The Clever Hans effect Crowd-sourced visual questions have other unexpected properties. Goyal et al. (2016) and Mahendru et al. (2017) note how questions rarely talk about objects that are not present in the image, hence an existential question like “Do you see a...?” is often true. Agrawal et al. (2016) a
2622039625	Deep learning evaluation using deep linguistic processing	2026942141	despite exhibiting a degree of lexical complexity due to its real-world breadth. Moreover, repurposed photos do not – and were never intended to – reﬂect the visual complexity of every-day scenarios (Pinto et al., 2008). Humans given the task of captioning such images will mostly produce descriptions which are syntactically simple. The way that workers on crowd-sourcing platforms are paid gives them an incentive to
2622039625	Deep learning evaluation using deep linguistic processing	1861492603	essful training led to the practice of adopting online data, such as the Flickr photo sharing platform, and leveraging crowd-sourcing, usually via Amazon Mechanical Turk (AMT). For instance, MS COCO (Lin et al., 2014) is an image caption dataset which contains more than 300,000 images annotated with more than 2 million human-written captions, while the popular VQA dataset Antol et al. (2015) is based on MS COCO. T
2622039625	Deep learning evaluation using deep linguistic processing	2416885651	evaluated system to achieve competitive performance without truly learning these abilities. To address this, several artiﬁcial VQA datasets have been released recently, including the SHAPES dataset (Andreas et al., 2016), the CLEVR dataset (Johnson et al., 2017a), the dataset of Suhr et al. (2017), and our ShapeWorld framework (Kuhnle and Copestake, 2017). They all consist of images showing abstract scenes with color
2622039625	Deep learning evaluation using deep linguistic processing	2560730294	or example a combined quantiﬁcation and spatial relation captioner module. Flexibility &amp; reusability Real-world or human-created data essentially has to be obtained again for every change/update (Goyal et al., 2016)5. In contrast to that, modularity and detailed conﬁgurability makes our approach easily reusable for a wide range of potentially unforeseen changes in evaluation focus (or more general usage shifts).
2622039625	Deep learning evaluation using deep linguistic processing	2410217169	ion, and have led to an entire research subﬁeld on adversarial instances in vision. Such investigations are not yet as prominent in the NLP community, although see, e.g., Sproat and Jaitly (2016) and Arthur et al. (2016). The ability to work with raw input data and to pick up correlations/biases, which humans cannot always manifest in explicit symbolic rules, is precisely the strength of DNNs as feature extractors. B
2622039625	Deep learning evaluation using deep linguistic processing	2560730294	larly interesting from a semantic perspective, since it combines general language understanding, reference resolution and grounded language reasoning in a simple and clear task. However, recent work (Goyal et al., 2016; Agrawal et al., 2016) has suggested that the popular VQA datasets are inadequate, due to various issues which allow an evaluated system to achieve competitive performance without truly learning thes
2622039625	Deep learning evaluation using deep linguistic processing	1525961042	in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al., 2015). Here we focus on the problem of visually grounded language understanding in the context of the recently popular task of visual question answering (VQA). In principle, this task is particularly inter
2622039625	Deep learning evaluation using deep linguistic processing	2515741950	in NLP, suggest similarly problematic conclusions: Is the bag-of-words model actually able to encode sequential information, as its surprisingly strong performance in comparison to an LSTM suggests (Adi et al., 2017)? Is visual information really not as important to answer visually grounded questions, as the strong performance of text-only systems suggests (Jabri et al., 2016)? Or are these results indicating an
2622039625	Deep learning evaluation using deep linguistic processing	2416885651	ortcomings. 3.2 Controlled and syntactically rich language generation Of the recent abstract datasets mentioned in the introduction, Suhr et al. (2017) use human-written captions, the SHAPES dataset (Andreas et al., 2016) a minimalist grammar, and the CLEVR dataset (Johnson et al., 2017a) a more complex one based on functional building blocks, both speciﬁcally designed for their microworlds. For the ShapeWorld framewo
2622039625	Deep learning evaluation using deep linguistic processing	2174196774	of spoken dialogue systems (e.g., Schefﬂer and Young (2001)), artiﬁcial data is relatively little used in NLP. There are, nonetheless, a few recent examples, including the MazeBase game environment (Sukhbaatar et al., 2015), the long-term research proposal of Mikolov et al. (2015), or the bAbI tasks (Weston et al., 2015). Here we focus on the problem of visually grounded language understanding in the context of the rece
2622039625	Deep learning evaluation using deep linguistic processing	1933349210	th biases which relate to what people choose to photograph rather than to what they see. Animal images in the VQA dataset are predominantly cats and dogs, sport images mainly baseball and tennis (see Antol et al. (2015) for more statistics). Considering all these biases both in language and vision, the common evaluation measure – simple accuracy of questions answered correctly – is not a good reﬂection of a system’s
2622804218	Concept Transfer Learning for Adaptive Language Understanding	2024632416	ions. At ﬁrst,Yao et al.(2013) used RNN outperforming CRF (conditional random ﬁeld) on the ATIS dataset.Mesnil et al.(2013) tried bidirectional and hybrid RNN to investigate using RNN for slot ﬁlling.Yao et al. (2014) introduced LSTM (long-short memory networks) and deep LSTM architecture for this task and obtained a marginal improvement over RNN. Vu et al.(2016) proposed to use the ranking loss to train a bidirec
2622804218	Concept Transfer Learning for Adaptive Language Understanding	2475662749	prove the memory capability of RNN. Inspired by the encoder-decoder nerual network (Bahdanau et al.,2014),Kurata et al.(2016) proposed an encoder-labeler model for slot ﬁlling, andLiu and Lane (2016);Zhu and Yu 2017) adapted the attention model to the slot ﬁlling task. However, these work only predicted a joint semantic slot (one-hot vector), not a structure of atomic concepts. Domain Adaptation in LU For the dom
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	1902237438	~ d 0 = tanh(W d[h P 1 ;h Q ] + b) (15) where W d is the weight matrix and bis the bias vector. The context vector c t for current time step tis computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state d t with each encoder hidden state h t to get the weighted sum representation. Here h i consists of the passage representation hP and the question representat
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2609482285	-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al., 2015b), response generation (Gu et al., 2016), and summarization generation (Zhou et al., 2017). We use it to generate the synthetic answer with the start and end positions of the evidence snippet as features. 3 OUR APPROACH Following the overview in Figure 1, our approach consists of two parts
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2427527485	2015) and CBT (Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	1902237438	AD dataset may also be applied on the MS-MARCO dataset (Yu et al., 2016; Lee et al., 2016; Yang et al., 2016). The sequence-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al., 2015b), response generation (Gu et al., 2016), and summarization generation (Zhou et al., 2017). We use it to generate the synthetic answer with the start and end positions
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2427527485	ce neural networks with extracted evidences as features. Experiments show that our extraction-then-synthesis method outperforms state-of-the-art methods. 1 INTRODUCTION Machine reading comprehension (Rajpurkar et al., 2016; Nguyen et al., 2016), which attempts to enable machines to answer questions after reading a passage or a set of passages, attracts great attentions from both research and industry communities in rec
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2304113845	e et al., 2016; Yang et al., 2016). The sequence-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al., 2015b), response generation (Gu et al., 2016), and summarization generation (Zhou et al., 2017). We use it to generate the synthetic answer with the start and end positions of the evidence snippet as features. 3 OUR APPROACH Following the overvi
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2125436846	n methods and all other existing methods on the MS-MARCO dataset. 2 RELATED WORK Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage. CNN/Daily-Mail (Hermann et al., 2015) and CBT (Hill et al., 2016) are the cloze-style data
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	1544827683	ng comprehension and question answering research. Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage. CNN/Daily-Mail (Hermann et al., 2015) and CBT (Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpur
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2126209950	nswering research. Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage. CNN/Daily-Mail (Hermann et al., 2015) and CBT (Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answ
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2101105183	set (10,047 pairs) and a test set (9,650 pairs). The answers are human-generated and not necessarily sub-spans of the passages so that the metrics in the ofﬁcial tool of MS-MARCO evaluation are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). In the ofﬁcial evaluation tool, the ROUGE-L is calculated by averaging the score per question, however, the BLEU is normalized with all questions. We hold that the answer sho
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	2250539671	ta to obtain the extracted span. Then we treat the passage to which this span belongs as the input. 4.2.2 PARAMETER For answer extraction, we use 300-dimensional uncased pre-trained GloVe embeddings (Pennington et al., 2014)2 for both question and passage without update during training. We use zero vectors to represent all out-of-vocabulary words. Hidden vector length is set to 150 for all layers. We also apply dropout (
2625541525	S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension	6908809	word embedding size to 300, set the feature embedding size of start and end positions of the extracted snippet to 50, and set all GRU hidden state sizes to 150. The model is optimized using AdaDelta (Zeiler, 2012) with initial learning rate of 1.0. All hyperparameters are selected on the MS-MARCO development set. 4.2.3 DECODING When decoding, we ﬁrst run our extraction model to obtain the extracted span, and r
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	109203355	Chali and Golestanirad (2016); Labutov, Basu, and Vanderwende (2015) or syntactic transformation heuristics Agarwal and Mannem (2011); Ali, Chali, and Hasan (2010) (e.g., subject-auxiliary inversion Heilman and Smith (2010a)). These techniques can be inadequate to capture the diversity of natural language questions. To address this limitation, end-to-end-trainable neural models have recently been proposed for question
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	95207536	Existing studies formulate key phrase extraction as a two-step process. In the ﬁrst step, lexical features (e.g., part-of-speech tags) are used to extract a key phrase candidate list of certain types Liu et al. (2011); Wang, Zhao, and Huang (2016); Le, Nguyen, and Shimazu (2016); Yang et al. (2017). In the second step, ranking models are often used to select a key phrase. Medelyan, Frank, and Witten; Lopez and Rom
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	109203355	ey phrases. We used two different evaluation approaches: an ambitious one that compares our generated question-answer pairs to human generated ones from SQuAD and another that compares our model with Heilman and Smith (2010a) (henceforth refered to as H&amp;S). Comparison to human generated questions - We presented annotators with documents from the SQuAD ofﬁcial development set and two sets of question-answer pairs, on
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2296194829	ibatch size of 32 using the ADAM optimization algorithm. 4.2.1 Key Phrase Detection Key phrase detection models used pretrained word embeddings of 300 dimensions, generated using a word2vec extension Ling et al. (2015) trained on the English Gigaword 5 corpus. We used bidirectional LSTMs of 256 dimensions (128 forward and backward) to encode the document and an LSTM of 256 dimensions as our decoder in the pointer n
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2081580037	ms to improve generation quality, including parsing Heilman and Smith (2010a); Mitkov and Ha (2003), semantic role labeling Lindberg et al. (2013), and the use of lexicographic resources like WordNet Miller (1995); Mitkov and Ha (2003). However, the majority of proposed methods resort to simple rule-based techniques such as slot-ﬁlling with templates Lindberg et al. (2013); Chali and Golestanirad (2016); Labut
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2540636186	n beneﬁt from automatic question generation, including vocabulary assessment Brown, Frishkoff, and Eskenazi (2005), writing support Liu, Calvo, and Rus (2012), and assessment of reading comprehension Mitkov and Ha (2003); Kunichika et al. (2004). Formulating questions that test for certain skills at certain levels requires signiﬁcant human effort that is difﬁcult to scale, e.g., to massive open online courses (MOOCs)
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	109203355	of questions to assess reading comprehension Mitkov and Ha (2003); Kunichika et al. (2004). Various NLP techniques have been adopted in these systems to improve generation quality, including parsing Heilman and Smith (2010a); Mitkov and Ha (2003), semantic role labeling Lindberg et al. (2013), and the use of lexicographic resources like WordNet Miller (1995); Mitkov and Ha (2003). However, the majority of proposed meth
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2540636186	Related Work 2.1 Question Generation Automatic question generation systems are often used to alleviate (or even eliminate) the burden of human generation of questions to assess reading comprehension Mitkov and Ha (2003); Kunichika et al. (2004). Various NLP techniques have been adopted in these systems to improve generation quality, including parsing Heilman and Smith (2010a); Mitkov and Ha (2003), semantic role lab
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2586581008	st step, lexical features (e.g., part-of-speech tags) are used to extract a key phrase candidate list of certain types Liu et al. (2011); Wang, Zhao, and Huang (2016); Le, Nguyen, and Shimazu (2016); Yang et al. (2017). In the second step, ranking models are often used to select a key phrase. Medelyan, Frank, and Witten; Lopez and Romary used bagged decision trees, while Lopez and Romary used a Multi-Layer Perceptr
2626182443	Neural Models for Key Phrase Extraction and Question Generation.	2609237822	ttend on when it has generated all key phrases. This provides the ﬂexibility to learn the number of key phrases the model should extract from a particular document. This is in contrast to the work of Meng et al. (2017), where a ﬁxed number of key phrases was generated per document. A pointer network is an extension of sequence-to-sequence models Sutskever, Vinyals, and Le (2014), where the target sequence consists
2654535376	Lexical representation explains cortical entrainment during speech comprehension	2036931463	which in turn will be quite distinct from the vector for the action verb “rubs”. Psycholinguists have argued that information about linguistic distributions partly underlies knowledge of word meaning [7]. Distributional semantics is also increasingly inﬂuential in semantic theory [8,9] and widely applied in computational linguistics where it yields state-of-the-art results in applications for natural
2661761953	Neural Machine Translation with Gumbel-Greedy Decoding	2542835211	ion, and yet it has been shown to be effective if the generator is continually learned from the initialization of a pretrained discriminator (Ranzato et al.,2015;Shen et al.,2015;Bahdanau et al.,2016;Lamb et al., 2016). Because our learning algorithm requires sampling from the generator, the searching space is extensive for a randomly initialized generator to output any meaningful translation to learn from. In our
2661761953	Neural Machine Translation with Gumbel-Greedy Decoding	2099471712	ork and the GGD algorithm are also directly motivated by Generative Adversarial Networks (GANs), which are one of the popular generative models that consist of discriminative and generative networks (Goodfellow et al., 2014). Energy-based GAN was later introduced, which uses the energy as the score function rather than binary score (i.e., predicting whether the input is real or fake) (Zhao et al.,2016). The GAN style of
2729046720	Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation	1566289585	fashion and uses a recurrent network to encode a given sentence into an embedding and then decode it to predict the preceding and following sentences. The model was trained on the BookCorpus dataset (Zhu et al., 2015). The embeddings produced by the encoder have a robust performance on semantic relatedness tasks. We use the pre-trained Skip-Thought encoder provided by the authors1. We also compute other embedding-
2729046720	Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation	2250645967	ons of the embedding and Cis the candidate sentence. 3.2.4 Greedy matching Greedy matching does not compute a sentence embedding but directly a similarity score between a candidate Cand a reference r(Rus and Lintean, 2012). This similarity score is computed as follows: G(C;r) = P w2Cmax w^2r cossim(e w;w w^) jCj GM(C;r) = G(C;r) + G(r;C) 2 : (4) In other words, each word in the candidate sentence is greedily matched to
2729046720	Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation	2427764808	s and might suffer from overﬁtting issues on these relatively small datasets. The hld-scLSTM is considered to consistently outperform the other models based on the wordoverlap metrics. As explained bySharma et al. (2017), this improvement results from the model’s access to the lexicalized slot values, due to which it can take into account the grammatical associations of the generated words near the output tokens, the
2733769132	Automatic Generation of Natural Language Explanations	1508116597	ation to a target user in a style of a user-generated review. To measure the quality of the presented text, we used a suite of natural language readability metrics : Automated Readability index (ARI) [16], Flesch reading ease (FRE) [20], FleschKincaidgradelevel(FGL)[21],Gunning-Fogindex(GFI)[20],simple measure of gobbledygook (SMOG) [17], Coleman Liau index (CLI) [21], LIX [3], and RIX[21]. Flesch rea
2733769132	Automatic Generation of Natural Language Explanations	2036490790	atures and aspects of a specific item. There are many approaches to generating explanations for different types of recommender systems, including collaborative filtering [9] and case-based approaches [18]. Explanations showed to increase the effectiveness of the recommendation and the user’s satisfaction [26] in various evaluations methods. Current state of the art in explainable recommender systems d
2733769132	Automatic Generation of Natural Language Explanations	2557508245	different domains such as text generation for images as in [11]. Learning the rules for generating the reviews can be accomplished by representing input as sentences, words or characters. In [19] and [24] they propose a tree-based neural network model for Automatic Generation of Natural Language Explanations DLRS’17, August 2017, Como, Italy natural-language inference based on words and their context.
2733769132	Automatic Generation of Natural Language Explanations	1969245231	e recommender system with experiments showing good performance. Despite good performance, this example of work suffers from the same problem as the other works that it is not explainable. The work of [2] is among the first where a recommender system is utilising the review text as side information to improve the performance of recommender system and the solutions are rooted in recurrent neural networ
2733769132	Automatic Generation of Natural Language Explanations	2011501238	e of a user-generated review. To measure the quality of the presented text, we used a suite of natural language readability metrics : Automated Readability index (ARI) [16], Flesch reading ease (FRE) [20], FleschKincaidgradelevel(FGL)[21],Gunning-Fogindex(GFI)[20],simple measure of gobbledygook (SMOG) [17], Coleman Liau index (CLI) [21], LIX [3], and RIX[21]. Flesch reading ease score is considered th
2733769132	Automatic Generation of Natural Language Explanations	2129642308	easure the quality of the presented text, we used a suite of natural language readability metrics : Automated Readability index (ARI) [16], Flesch reading ease (FRE) [20], FleschKincaidgradelevel(FGL)[21],Gunning-Fogindex(GFI)[20],simple measure of gobbledygook (SMOG) [17], Coleman Liau index (CLI) [21], LIX [3], and RIX[21]. Flesch reading ease score is considered the oldest method to calculate the r
2733769132	Automatic Generation of Natural Language Explanations	2130868038	ent are important factors in the creation of user generated on-line reviews and should not be neglected. Recent research on recommender systems demonstrated improvements achieved by including context [1].Thispaperincorporatesthisinformationtoenrichthegenerated sentences with particular contextual features. In this paper, we propose a technique for the automatic generation of explanations, based on ge
2733769132	Automatic Generation of Natural Language Explanations	2064675550	eight matrixWh will be changed to suit the input value. RNN’s suffer a vanishing gradient problem, that depending on the activation functions, sequential informationgetslostovertime.Tohandlethisissue,[10]introduced Long short term memory (LSTM) cells, and was later improved by [7] using forget gates to discard some information. LSTM is an improved version of RNNs controlled by sequential connection of
2733769132	Automatic Generation of Natural Language Explanations	2130868038	rder to generate explanations for different ratings, we concatenate the input characters with the ratings of the review the character belongs to. In addition, we normalise the scale of the ratings to [0,1]. 5.4 Evaluation Metrics Current methods to explain recommendations do not have a natural language way to present the information to the user. Our proposed method explains the recommendation to a targ
2733769132	Automatic Generation of Natural Language Explanations	2078021145	r reviews and trusted social relations. Explanations generated in this manner lack natural language expressions, since the sentences are generated in a modular way. However, it is well established by [25] that a good explanation must be clear, and interesting to the target user, since this information has a significant influence on the user’s decision. On-line usergenerated reviews present clear and i
2733769132	Automatic Generation of Natural Language Explanations	2050113838,2116354394,2125330369	rmation about items, since they describe personal usage experience from users. Furthermore, this source plays an important role on the user side, since he/she tend to trust the opinion of other users [5, 13, 22]. arXiv:1707.01561v1 [cs.CL] 4 Jul 2017 DLRS’17, August 2017, Como, Italy Felipe Costa, Sixun Ouyang, Peter Dolog, and Aonghus Lawlor Recurrent neural networks (RNNs) have recently demonstrated to sho
2733769132	Automatic Generation of Natural Language Explanations	2130868038	rmations which relies on the review, as shown in Eq. 4. Meanwhile, in terms of the auxiliary information, our model uses a set of numeric values of the users’ ratings, which are rescaled to the range [0,1]. X′ t = [onehot(xchar);xauxiliary] (4) 4.4 Generative Explanation As mentioned previously, [15] proposed a GCN model concatenating characters with some auxiliary information, i.e. overall rating or t
2733769132	Automatic Generation of Natural Language Explanations	1905882502	the text composition are learned from the existing reviews. Therefore, we would like to achieve similar alignment as others have achieved in different domains such as text generation for images as in [11]. Learning the rules for generating the reviews can be accomplished by representing input as sentences, words or characters. In [19] and [24] they propose a tree-based neural network model for Automat
2733769132	Automatic Generation of Natural Language Explanations	1579090648	types of recommender systems, including collaborative filtering [9] and case-based approaches [18]. Explanations showed to increase the effectiveness of the recommendation and the user’s satisfaction [26] in various evaluations methods. Current state of the art in explainable recommender systems does not offer human-oriented explanations. To address this particular issue, our model is defined to targe
2733769132	Automatic Generation of Natural Language Explanations	2152184085	this work we develop an approach using character-level neural networks to generate readable explanations. Current explainable recommendations propose to mine user reviews to generate explanations. In [27] they propose an explicit factor model, where they first extracts aspects and user opinions by phrase-level sentiment analysis on user generated reviews, then generate both recommendations and disreco
2734458248	Representation Learning for Grounded Spatial Reasoning	2157104268	(Moratz and Tenbrink, 2006; Levit and Roy, 2007; Tellex et al., 2011) and human-robot interaction (Skubic et al., 2004). Previous computational approaches include techniques such as proximity ﬁelds (Kelleher et al., 2006), spatial templates (Levit and Roy, 2007) and geometrically deﬁned mappings (Moratz and Tenbrink, 2006; Kollar et al., 2010). More recent work in robotics has integrated text containing position infor
2734458248	Representation Learning for Grounded Spatial Reasoning	1962416794	atial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only driven by the ﬁnal rewards
2734458248	Representation Learning for Grounded Spatial Reasoning	2040145958	er, our methods also apply in the stochastic case. Text instructions Prior work has investigated human usage of different types of referring expressions to describe spatial relations (Levinson, 2003; Viethen and Dale, 2008). In order to build a robust instruction following system, we examine several categories of spatial expressions that exhibit the wide range of natural language goal descriptions. Speciﬁcally, we consi
2734458248	Representation Learning for Grounded Spatial Reasoning	2064675550	es the instruction text to drive the learning of the environment representation. We start by converting the instruction text into a realvalued vector using a recurrent neural network with LSTM cells (Hochreiter and Schmidhuber, 1997). Using this vector as a kernel in a convolution operation, we obtain an instruction-conditioned representation of the state. This allows the model to reason about immediate local neighborhoods in ref
2734458248	Representation Learning for Grounded Spatial Reasoning	2069809153	ious computational approaches include techniques such as proximity ﬁelds (Kelleher et al., 2006), spatial templates (Levit and Roy, 2007) and geometrically deﬁned mappings (Moratz and Tenbrink, 2006; Kollar et al., 2010). More recent work in robotics has integrated text containing position information with spatial models of the environment to obtain accurate maps for navigation (Walter et al., 2013; Hemachandra et al
2734458248	Representation Learning for Grounded Spatial Reasoning	2111526438	ith the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path
2734458248	Representation Learning for Grounded Spatial Reasoning	1522301498	man annotation and a reward of 1 is given for falling in a puddle cell. The only terminal state is when the agent is at the goal. Rewards are discounted by a factor of 0.95. We use Adam optimization (Kingma and Ba, 2014) for training all models. 6 Results Comparison with state-of-the-art We ﬁrst investigate the ability of our model to learn solely from environment simulation. Figure 3 shows the discounted reward achi
2734458248	Representation Learning for Grounded Spatial Reasoning	2236233024	nd Dale, 2008; Byrne and Johnson-Laird, 1989; Li and Gleitman, 2002). The practical implications of this research are related to autonomous navigation (Moratz and Tenbrink, 2006; Levit and Roy, 2007; Tellex et al., 2011) and human-robot interaction (Skubic et al., 2004). Previous computational approaches include techniques such as proximity ﬁelds (Kelleher et al., 2006), spatial templates (Levit and Roy, 2007) and ge
2734458248	Representation Learning for Grounded Spatial Reasoning	2107537133	nd Gleitman, 2002). The practical implications of this research are related to autonomous navigation (Moratz and Tenbrink, 2006; Levit and Roy, 2007; Tellex et al., 2011) and human-robot interaction (Skubic et al., 2004). Previous computational approaches include techniques such as proximity ﬁelds (Kelleher et al., 2006), spatial templates (Levit and Roy, 2007) and geometrically deﬁned mappings (Moratz and Tenbrink,
2734458248	Representation Learning for Grounded Spatial Reasoning	2189089430	nvironment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our set
2734458248	Representation Learning for Grounded Spatial Reasoning	2040145958	oth theoretical and practical interest. From the linguistic and cognitive perspectives, research has focused on the wide range of mechanisms speakers use to express spatial relations (Tenbrink, 2007; Viethen and Dale, 2008; Byrne and Johnson-Laird, 1989; Li and Gleitman, 2002). The practical implications of this research are related to autonomous navigation (Moratz and Tenbrink, 2006; Levit and Roy, 2007; Tellex et al.
2734458248	Representation Learning for Grounded Spatial Reasoning	2064675550	by projecting each cell to a low-dimensional embedding (˚) as a function of the objects contained in that cell. In parallel, the text instruction xis passed through an LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997) to obtain a continuous vector representation h(x). This vector is then split into local and global components h(x) = [h 1(x);h 2(x)]. The local component, h 1(x), is reshaped into a kernel to perform
2734458248	Representation Learning for Grounded Spatial Reasoning	1602500555	rence via interaction with the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstration
2734458248	Representation Learning for Grounded Spatial Reasoning	2118781169	ring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with prov
2734458248	Representation Learning for Grounded Spatial Reasoning	2104238610	rink, 2007; Viethen and Dale, 2008; Byrne and Johnson-Laird, 1989; Li and Gleitman, 2002). The practical implications of this research are related to autonomous navigation (Moratz and Tenbrink, 2006; Levit and Roy, 2007; Tellex et al., 2011) and human-robot interaction (Skubic et al., 2004). Previous computational approaches include techniques such as proximity ﬁelds (Kelleher et al., 2006), spatial templates (Levit
2734458248	Representation Learning for Grounded Spatial Reasoning	2138455848	struction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	1494198834	our experiment including the dataset, model setup, and the baseline model. 5.1. Dataset Two corpora across ﬁve languages were used in the experiment. One of the corpora we used is LibriSpeech corpus [47] (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousa
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	1501669607	fer 1. INTRODUCTION Embedding audio word segments into ﬁxed-length vectors has many useful applications in natural language processing such as speaker identiﬁcation [1], audio emotion classiﬁ- cation [2], and spoken term detection (STD) [3, 4, 5]. In these applications, audio segments are usually represented as feature vectors to be applied to a standard classiﬁers which determines the speaker’s iden
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2030422732,2295088914	ord segments into ﬁxed-length vectors has many useful applications in natural language processing such as speaker identiﬁcation [1], audio emotion classiﬁ- cation [2], and spoken term detection (STD) [3, 4, 5]. In these applications, audio segments are usually represented as feature vectors to be applied to a standard classiﬁers which determines the speaker’s identiﬁcation, emotion or whether the input que
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2190506272	ort for indexing, accelerate the speed of calculation, and improve the efﬁciency for the retrieval task [6, 7, 8]. Recently, deep learning has been used for encoding acoustic information into vectors [9, 10, 11]. Existing works have shown that it is possible to transform audio word segments into ﬁxed dimensional vectors. The transformation successfully produces vector space where word audio segments with sim
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	139960808,2293634267	ot seem to learn long-term dependencies due to the vanishing gradient problem [18, 19]. To conquer such difﬁculties, LSTM [20] and GRU [21, 22] were proposed. While LSTM achieves many amazing results [23, 24, 25, 26, 27, 21, 28], the relative new GRU performs just as well with less parameters and training effort [29, 30, 31, 32]. RNN Encoder-Decoder [27, 33] consists of an Encoder RNN and a Decoder RNN.The Encoder RNN reads
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2555428947	is problem is not yet to be fully examined in Audio Word2Vec, works in neural machine translation (NMT) successfully transfer the model learned on highresource languages to low-resource languages. In [16, 17], the authors ﬁrst train a source model with high-resource language pair. The source model is used to initialize the target model which is then trained by low-resource language pairs. For audio, all l
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2336585117	rs. Human annotated data is required under this supervised learning scenario. Besides supervised approaches [12, 11, 13, 14], unsupervised approaches are also proposed to reduce the annotation effort [15]. As for the unsupervised learning for the audio embedding, LSTM-based sequence-to-sequence autoencoder demonstrates a promising result [15]. The model is trained to minimize the reconstruction error
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2239904444	s, LSTM [20] and GRU [21, 22] were proposed. While LSTM achieves many amazing results [23, 24, 25, 26, 27, 21, 28], the relative new GRU performs just as well with less parameters and training effort [29, 30, 31, 32]. RNN Encoder-Decoder [27, 33] consists of an Encoder RNN and a Decoder RNN.The Encoder RNN reads the input sequence x = (x 1;x 2;:::;x T ) sequentially and the hidden state h t of the RNN is updated
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2190506272	th side information to obtain embeddings that separate sameword pairs and different-word pairs. Human annotated data is required under this supervised learning scenario. Besides supervised approaches [12, 11, 13, 14], unsupervised approaches are also proposed to reduce the annotation effort [15]. As for the unsupervised learning for the audio embedding, LSTM-based sequence-to-sequence autoencoder demonstrates a p
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2030422732,2295088914	ugh concatenating the maverage vectors sequentially into a vector representation of dimensionality 39 m. Although NEis simple, similar approaches have been used in STD and achieved successful results [3, 4, 5]. 6. EXPERIMENTS In this section, we ﬁrst examine how changing the hidden layer size of the RNN Encoder/Decoder, the dimension of Audio Word2Vec, affects the MAP performance of queryby-example STD (Se
2736350121	Language Transfer of Audio Word2Vec: Learning Audio Segment Representations Without Target Language Data	2064675550	work to process sequences of variable length. However, in practice, RNN does not seem to learn long-term dependencies due to the vanishing gradient problem [18, 19]. To conquer such difﬁculties, LSTM [20] and GRU [21, 22] were proposed. While LSTM achieves many amazing results [23, 24, 25, 26, 27, 21, 28], the relative new GRU performs just as well with less parameters and training effort [29, 30, 31,
2736946518	Spherical Paragraph Model	2250539671	s in distributed word representations have succeeded in capturing semantic regularities in language. Speciﬁcally, neural embedding models, e.g., Word2Vec model (Mikolov et al.,2013a) and Glove model (Pennington et al., 2014), learn word vectors (also called word embeddings) efﬁciently from very large text corpus. The learned word vectors can reveal the semantic relatedness between words and perform word analogy tasks suc
2736946518	Spherical Paragraph Model	2114524997	sentences) modeling originally, cannot work well on long texts. 4.4 Sentiment Analysis We run the sentiment classiﬁcation experiments on two publicly available datasets. Subj, Subjectivity dataset (Pang and Lee, 2004)10 which contains 5;000 subjective instances and 5;000 objective instances. The task is to classify a sentence as being subjective or objective; MR, Movie reviews (Pang and Lee,2005) with one sentenc
2737619627	Iris: A Conversational Agent for Complex Tasks	836999996	a [40]. Similarly, structured systems assume a logical representation for the information exchanged in conversation (for example, slot filling systems [31]), which unstructured systems do not require [27]. Iris is a modular, structured system in that it is trained for a concrete set of tasks and contains structural information provided by its conversational type system. To our knowledge, Iris is the f
2737619627	Iris: A Conversational Agent for Complex Tasks	889023230	-end, and structured versus unstructured. Modular systems, like Siri, train multiple models to support a set of tasks [15], while end-to-end systems train a single learning algorithm on dialogue data [40]. Similarly, structured systems assume a logical representation for the information exchanged in conversation (for example, slot filling systems [31]), which unstructured systems do not require [27].
2737619627	Iris: A Conversational Agent for Complex Tasks	2141311545	. For example, P2 said: “There are certain things, like selecting data columns, that I’d prefer to do by clicking on things.” Prior work has demonstrated strong results with mixed modality interfaces [21], and we plan to incorporate other modalities into Iris moving forward. Manipulating and transforming data, for example, is perhaps best accomplished through a spreadsheet-like interface, and we plan
2737619627	Iris: A Conversational Agent for Complex Tasks	2106999256,2125990861	: for example, using word embedding models to connect user vocabulary with system keywords [1, 5] or programming syntax [38], and statistical language models to predict functions a user wants to call [9, 26, 6]. All of these systems solve the vocabulary problem [10] through statistical models that map natural language to the domain language of a system. While Iris takes a similar tack, it extends user reque
2737619627	Iris: A Conversational Agent for Complex Tasks	2099517310	he AI literature, these systems fall along two primary axes: modular versus end-to-end, and structured versus unstructured. Modular systems, like Siri, train multiple models to support a set of tasks [15], while end-to-end systems train a single learning algorithm on dialogue data [40]. Similarly, structured systems assume a logical representation for the information exchanged in conversation (for exa
2737619627	Iris: A Conversational Agent for Complex Tasks	2274505579	al language interfaces, systems must manage the ambiguity of user language. DataTone and PixelTone provide guidance here, illustrating how systems can be designed to surface decisions about ambiguity [11] (at a point where these decisions matter), and constrain some of these decisions by direct manipulation [21] or other modalities [42]. Iris combines these ideas with an understanding of how humans re
2737619627	Iris: A Conversational Agent for Complex Tasks	2044102377	case this architecture in Iris, a conversational agent that helps users with data science tasks (Figure 1). Data science represents a domain with complex tasks that standalone commands cannot support [18]. To interact with Iris, you type natural language requests into a chat window (1.1) and receive real-time feedback on what command your request will trigger (1.2). When you enter a command, Iris conv
2737619627	Iris: A Conversational Agent for Complex Tasks	889023230	equests for Iris, the model is updated to incorporate them. As Iris gains more data, it is possible to swap out this statistical model with something more sophisticated, such as a deep neural network [40]. In this paper, the classifier Iris uses is important primarily to enable the other, novel parts of its architecture. Executing Requests via Iris Commands To connect user requests with commands, Iris
2737619627	Iris: A Conversational Agent for Complex Tasks	2141311545	idance here, illustrating how systems can be designed to surface decisions about ambiguity [11] (at a point where these decisions matter), and constrain some of these decisions by direct manipulation [21] or other modalities [42]. Iris combines these ideas with an understanding of how humans resolve ambiguity (e.g., through clarification requests [28]) to manage the execution of user commands. Systems
2737619627	Iris: A Conversational Agent for Complex Tasks	2167213883	lying functionality such as commands, code snippets, and APIs. Query-feature graphs set the foundation for these methods, connecting user requests with system commands in an image editing application [10]. Others have since extended this approach: for example, using word embedding models to connect user vocabulary with system keywords [1, 5] or programming syntax [38], and statistical language models
2737619627	Iris: A Conversational Agent for Complex Tasks	2130848543	mand has just returned an array, us - ers can ask Iris to “take the mean of that”. This simple model works well in practice; in the future, it is possible to swap in more advanced co-reference models [33]. Iris also provides an internal API that commands can call to add new, named variables to the Iris runtime environment. We have used this API to create an general purpose command to save {value} to {
2737619627	Iris: A Conversational Agent for Complex Tasks	2116680608,2163986367,2591734415	practical lens on the challenges of implementing speech interfaces, providing tools for rapid prototyping [20], triggering user queries [48], or extending the limits of machine reasoning with a crowd [4, 22, 23]. One of Iris’s goals is similarly to provide a framework (through its DSL and command API) that allows others to more quickly design conversational agents. Iris also draws on insights from tools desi
2737619627	Iris: A Conversational Agent for Complex Tasks	2127058057,2165254944	provide a framework (through its DSL and command API) that allows others to more quickly design conversational agents. Iris also draws on insights from tools designed to help with data science tasks [34, 44, 41]. Wrangler, for example, combines spreadsheet visualizations with natural language command descriptions to help users manipulate data [17]. Iris leverages similar natural language descriptions in the
2737619627	Iris: A Conversational Agent for Complex Tasks	2161968588,2281258985	requests with system commands in an image editing application [10]. Others have since extended this approach: for example, using word embedding models to connect user vocabulary with system keywords [1, 5] or programming syntax [38], and statistical language models to predict functions a user wants to call [9, 26, 6]. All of these systems solve the vocabulary problem [10] through statistical models tha
2737619627	Iris: A Conversational Agent for Complex Tasks	2064766209	s from tools designed to help with data science tasks [34, 44, 41]. Wrangler, for example, combines spreadsheet visualizations with natural language command descriptions to help users manipulate data [17]. Iris leverages similar natural language descriptions in the hints it displays as a user formulates a command. Other tools such as Burrito and Variolite are oriented more towards organizing data scie
2737619627	Iris: A Conversational Agent for Complex Tasks	2273847690	scenario, Iris is seeded with a large set of statistical commands based on the scipy.stats and sklearn libraries. We have also implemented commands that enable text analysis through popular lexicons [35, 7]. The original study collected dogmatism labels for Reddit posts through a crowdsourcing task. The dataset has two col - umns: post, the text of a post; and score, a dogmatism label between 0 (non-dog
2737619627	Iris: A Conversational Agent for Complex Tasks	2099529102	support dynamic modification through transformation into an automata-based programming model. In automata-based programming, execution of a program is broken down into explicit states of computation [14]. These states encapsulate a section of code to be run, after which the program will transition (much like a finite state machine) to the next state of computation. The DSL we have designed extends th
2737619627	Iris: A Conversational Agent for Complex Tasks	2099517310	support human conversa - tional behaviors that are largely missing in today’s systems. TODAY’S CONVERSATIONAL AGENTS Iris is inspired by commercial agents such as Siri and their open source variants [15]. We examine the interactions these agents are designed to support in Table 1. Iris has much in common with existing conversational agents. All systems have the ability to execute standalone commands,
2739255396	On the State of the Art of Evaluation in Neural Language Models	2571859396	78:1 75:6 NAS 1 73:0 69:8 Table 2: Validation and test set perplexities on Wikitext-2. All results are with shared input and output embeddings. y: parallel work. favourably even to the Neural Cache (Grave et al., 2016) whose innovations are fairly orthogonal to the base model. Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a signiﬁcant marg
2739255396	On the State of the Art of Evaluation in Neural Language Models	1810943226	est human designed and one machine optimised cell that was the top performer among thousands of candidates. 5 Under review as a conference paper at ICLR 2018 Model Size Depth Valid Test Stacked LSTM, Graves (2013) 21M 7 - 1.67 Grid LSTM, Kalchbrenner et al. (2015) 17M 6 - 1.47 MI-LSTM, Wu et al. (2016) 17M 1 - 1.44 LN HM-LSTM, Chung et al. (2016) 35M 3 - 1.32 ByteNet, Kalchbrenner et al. (2016) - 25 - 1.31 VD
2739255396	On the State of the Art of Evaluation in Neural Language Models	1771459135	imised cell that was the top performer among thousands of candidates. 5 Under review as a conference paper at ICLR 2018 Model Size Depth Valid Test Stacked LSTM, Graves (2013) 21M 7 - 1.67 Grid LSTM, Kalchbrenner et al. (2015) 17M 6 - 1.47 MI-LSTM, Wu et al. (2016) 17M 1 - 1.44 LN HM-LSTM, Chung et al. (2016) 35M 3 - 1.32 ByteNet, Kalchbrenner et al. (2016) - 25 - 1.31 VD RHN, Zilly et al. (2016) 23M 5 - 1.31 VD RHN, Zilly
2739255396	On the State of the Art of Evaluation in Neural Language Models	1810943226	a remarkable overlap with ours, Merity et al. (2017) demonstrate the utility of adding a Neural Cache (Grave et al., 2016). Building on their work, Krause et al. (2017) show that Dynamic Evaluation (Graves, 2013) contributes similarly to the ﬁnal perplexity. As pictured in Fig. 1a, our models with LSTM or NAS cells have all the standard components: an input embedding lookup table, recurrent cells stacked as l
2739255396	On the State of the Art of Evaluation in Neural Language Models	2571859396	rthogonal to the question of the relative merits of these recurrent cells. In parallel work with a remarkable overlap with ours, Merity et al. (2017) demonstrate the utility of adding a Neural Cache (Grave et al., 2016). Building on their work, Krause et al. (2017) show that Dynamic Evaluation (Graves, 2013) contributes similarly to the ﬁnal perplexity. As pictured in Fig. 1a, our models with LSTM or NAS cells have
2739255396	On the State of the Art of Evaluation in Neural Language Models	1499864241	Treebank dataset. the input gate as in Eq. 3. c t = f t c t 1 +i t j t (1) c t = f t c t 1 +(1 f t)j t (2) c t = f t c t 1 +min(1 f t;i t)j t (3) Where the equations are based on the formulation of Sak et al. (2014). All LSTM models in this paper use the third variant, except those titled “Untied gates” and “Tied gates” in Table 4 corresponding to Eq. 1 and 2, respectively. The results show that LSTMs are insens
2747272186	HANDLING HOMOGRAPHS IN NEURAL MACHINE TRANSLATION	2130942839	age pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs. 1 Introduction Neural machine translation (NMT;Sutskever et al. (2014);Bahdanau et al.(2015), x2), a method for MT that performs translation in an end-toend fashion using neural networks, is quickly becoming the de-facto standard in MT applications due to its impressive
2747272186	HANDLING HOMOGRAPHS IN NEURAL MACHINE TRANSLATION	2139183784	rmulated the task of WSD for Statistical Machine Translation (SMT) as predicting possible target translations which directly improves the accuracy of machine translation. Following this reformulation,Chan et al. (2007);Carpuat and Wu(2007a,b) integrated WSD systems into phrase-based systems.Xiong and Zhang(2014) breaks the process into two stages. First predicts the sense of the ambiguous source word. The predicted
2751279010	Learning Neural Word Salience Scores	2295800168	-products as follows: h(s i;s j) = exp s i &gt;s j P Sk2C exp(s i &gt;s k) (2) Ideally, the normalisation term in the denominator in the softmax must be taken over all the sentences S k in the corpus [6]. However, this is computationally expensive in most cases except for extremely small corpora. Therefore, following noise-contrastive estimation [18], we approximate the normalisation term using a ran
2751279010	Learning Neural Word Salience Scores	2251861449	5 scores and human ratings, then it can be considered as empirical support for the accuracy of the NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2]. Note that the tasks with the same name in different years actually represent different tasks. We use Pearson correlation coefﬁcient as the evaluation measure. For a
2751279010	Learning Neural Word Salience Scores	2251861449	arning process. We use the NWS scores to compute sentence embeddings and measure the similarity between two sentences using 18 benchmark datasets for semantic textual similarity in past SemEval tasks [4]. Experimental results show that the sentence similarity scores computed using the NWS scores and pre-trained word embeddings show a high degree of correlation with human similarity ratings in those b
2751279010	Learning Neural Word Salience Scores	2126400076	atings, then it can be considered as empirical support for the accuracy of the NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2]. Note that the tasks with the same name in different years actually represent different tasks. We use Pearson correlation coefﬁcient as the evaluation measure. For a list of n ordered p
2751279010	Learning Neural Word Salience Scores	71795751	e salience of words, then we can develop better representations of texts that can be used in downstream NLP tasks such as similarity measurement [7] or text (e.g. sentiment, entailment) classiﬁcation [34]. As described later in section 2, existing methods for detecting word salience can be classiﬁed into two groups: (a) lexicon-based ﬁltering methods such as stop word lists, or (b) word frequency-base
2751279010	Learning Neural Word Salience Scores	2149684865	ence, irrespective of the semantic contribution to the topic of the sentence. Following the success of Inverse Document Frequency (IDF) in ﬁltering out high frequent words in text classiﬁcation tasks [24], we deﬁne Inverse Sentence Frequency (ISF) of a word as the reciprocal of the number of sentences in which that word appears in a corpus. Speciﬁcally, ISF is computed as follows: ISF(w) = log  1 + t
2751279010	Learning Neural Word Salience Scores	2175723921	esults in the original papers. Because [27] did not report results for skip-thought on all 18 benchmark datasets used here, we report the re-evaluation of skip-thought on all 18 benchmark datasets by [38]. 6Corresponds to the GloVe-W method in the original publication. 7We use the GloVe implementation by the original authors available at https://nlp.stanford.edu/ projects/glove/ 6 Table 1: Performance
2751279010	Learning Neural Word Salience Scores	2518578398	the other hand, prior work studying the relationship between human reading patterns using eyetracking devices show that there exist a high positive correlation between word salience and reading times [16, 19]. For example, humans pay more attention to words that carry meaning as indicated by the longer ﬁxation times. Therefore, an interesting open question is that what psycholinguistic properties of words
2751279010	Learning Neural Word Salience Scores	2137607259	mantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor [8, 29]. Alternatively to applying nonparametric operators on word embeddings to create sentence embeddings, recurrent neural networks can learn the optimal weight matrix that can produce an accurate sentenc
2751279010	Learning Neural Word Salience Scores	2211192759,2518578398	nce embeddings even when sentence embeddings are computed by averaging the individual word embeddings. Instead of considering all words equally for sentence embedding purposes, attention-based models [19, 39, 36] learn the amount of weight (attention) we must assign to each word in a given context. Our proposed method for learning NWS scores are based on the prior observation that averaging is an effective he
2751279010	Learning Neural Word Salience Scores	2133458109	t can be considered as empirical support for the accuracy of the NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2]. Note that the tasks with the same name in different years actually represent different tasks. We use Pearson correlation coefﬁcient as the evaluation measure. For a list of n ordered pairs of rating
2751279010	Learning Neural Word Salience Scores	2147152072	word arouses images) have been successfully predicted using word embeddings [35, 31]. For example, [35] used the cosine similarity between word embeddings obtained via Latent Semantic Analysis (LSA) [13] to predict the concreteness and imageability ratings of words. On the other hand, prior work studying the relationship between human reading patterns using eyetracking devices show that there exist a
2751448157	Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning	2101964891	atasets. The datasets are GeoQuery880 [Tang and Mooney,2001], ATIS [Price,1990], Free917 [Cai and Yates,2013], Overnight [Wang et al.,2015], WebQuestions [Berant et al.,2013], and WikiTableQuestions [Pasupat and Liang, 2015]. “Size” denotes the number of examples in the dataset. “LF” indicates whether it has annotated logical forms. “Schema” denotes the number of table schemas. ATIS is presented as a slot ﬁlling task. E
2751448157	Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning	2546950329	leverage existing database engines to perform efﬁcient query execution. Furthermore, in contrast toDong &amp; Lapata (2016) andNeelakantan et al.(2017), we use policy-based RL in a fashion similar toLiang et al. (2017),Mou et al.(2017), andGuu et al.(2017), which helps Seq2SQL achieve state-of-the-art performance. UnlikeMou et al.(2017) andYin et al.(2016), we generalize across natural language questions and table
2751448157	Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning	2133564696	during training. 3 Model The Seq2SQL model generates a SQL query from a natural language question and table schema.1 One powerful baseline model is the Seq2Seq model utilized for machine translation [Bahdanau et al., 2015]. However, the output space of the standard softmax in a Seq2Seq model is unnecessarily large for this task. In particular, we note that the problem can be dramatically simpliﬁed by limiting the outp
2752081959	Query-by-Example Spoken Term Detection Using Attention-Based Multi-Hop Networks	1494198834	approaches (1) and (2): a neural network takes as input the query vector V Q, utterance vector V Sn , and cosine similarity, and outputs a score. 4. EXPERIMENTAL SETUP We used the LibriSpeech corpus [30] as the data for the experiments. To train the attention-based multi-hop network, some querysegment pairs were needed as training examples. 70,000 training examples were used in the experiments, inclu
2752081959	Query-by-Example Spoken Term Detection Using Attention-Based Multi-Hop Networks	2030422732	e used to compute the new attention values to obtain a new story vector V S2 . This can be seen as 1The LSTMs used in Figs 2 (A) and (B) are the same. Fig. 3. The hopping operation relevance feedback [13,29], where the machine goes over the audio segment again, extracting information to expand the query to form a new query vector. Again, V Q1 and V S1 are summed to form V Q2 (hop 2). After nhops (nis pre
2752081959	Query-by-Example Spoken Term Detection Using Attention-Based Multi-Hop Networks	2126203737	most intuitive way to search over spoken content for a spoken query is to directly match the audio signals to ﬁnd those audio snippets that sound like the spoken query, and dynamic time warping (DTW) [7] is widely used. Despite DTW’s wide use, it has several drawbacks. As typical DTW does not have trainable parameters, even in an online system that collects the training data from user feedback, the d
2753797950	Spoken English Intelligibility Remediation with Pocketsphinx Alignment and Feature Extraction Improves Substantially Over the State of the Art	2156885227	f three (T n) and two (D n) phonemes are used to measure phoneme substitutions, and insertions and deletions, respectively. 2. ADAPTING POCKETSPHINX FOR FEATURE EXTRACTION We chose to use PocketSphinx[11] system’s alignment routines. We tried a two-pass alignment approach over a ﬁxed grammar by using the time endpoints from recognizing the phonemes of the expected utterance in sequence, using a ﬁ- nit
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2250770256	an conversations. It’s worth noting that ConceptNet 5 is also noisy due to uncertainties in the constructing process, where 15.5% of all entries are considered “false” or “vague” by human evaluators (Speer and Havasi 2012). Our max-pooling strategy used in Tri-LSTM encoder and supervised word embeddings is partly designed to alleviate this weekness. and offers no performance boost to the rest 27%. 6https://conceptnet.i
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2175256910	e A c = ;, i.e., no commonsense knowledge is recalled, m(A c;r) = 0 and the model degenerates to dual-LSTM encoder. Comparison Approaches Supervised word embeddings We follow (Bordes and Weston 2016; Dodge et al. 2015) and use supervised word embeddings as another baseline. Word embeddings are most well-known in the context of unsupervised training on raw text as in (Mikolov et al. 2013), yet they can also be used
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2175256910	ly, such as in question answering (Weston et al. 2015) and language modeling (Sukhbaatar et al. 2015). It has also been employed on dialog modeling in several limited settings. With Memory Networks, (Dodge et al. 2015) used a set of fact triples about movies as long-term memory when modeling reddit dialogs, movie recommendation and factoid question answering. Similarly in a restaurant reservation setting, (Bordes a
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2250770256	mmonsense knowledge base in the open-domain retrieval-based dialog setting. Commonsense knowledge Several commonsense knowledge bases have been constructed during the past decade, such as ConceptNet (Speer and Havasi 2012) and SenticNet (Cambria, Olsher, and Rajagopal 2014). The aim is to give a foundation of real-world knowledge to a variety of AI applications. Typically a commonsense knowledge base can be seen as a s
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	94800547	ntial concept2. If the n-gram is a key in H, the corresponding value, i.e., all assertions in A concerning the concept, is added to A x (Figure 4). 1More sophisticated methods such as concept parser (Rajagopal et al. 2013) are also possible. Here, we chose n-gram for better speed and recall. N is set to 5. 2For unigrams, we exclude a set of stopwords. Both the original version and stemmed version of every word are cons
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2114686809	ure work includes extending the commonsense knowledge with common (or factual) knowledge, e.g., to extend the knowledge base coverage by linking more named entities to commonsense knowledge concepts (Cambria et al. 2014), and developing a better mechanism for utilizing such knowledge instead of the simple max-pooling scheme used in this paper. We would also like to explore the memory of the model for multiple message
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	2175256910	ve all relevant information that may help with the task. In our dialog modeling setting, we use A c as the memory component. Our implementation of Memory Networks, similar to (Bordes and Weston 2016; Dodge et al. 2015), differs from supervised word embeddings in Section 3.5.1 in only one aspect: how to treat multiple entries in memory. PIn memory networks, output memory representation ~o= i p i~a i, where~a i is th
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	889023230	wo categories: retrieval-based methods (Lowe et al. 2015b; 2016a; Zhou et al. 2016), which select a response from a predeﬁned repository, and generation-based methods (Ritter, Cherry, and Dolan 2011; Serban et al. 2016; Vinyals and Le 2015), which employ an encoder-decoder framework where the message is encoded into a vector representation and, then, fed to the decoder to generate the response. The latter is more n
2754573465	Augmenting End-to-End Dialogue Systems with Commonsense Knowledge	1511480444	xt of artiﬁcial intelligence (AI), commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate (Minsky 1986; Cambria et al. 2009; Cambria and Hussain 2015). Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather
2755148833	Acquiring Background Knowledge to Improve Moral Value Prediction	9292421	edicted as “Non-moral” if all classiﬁers return False. 3.2 Learning Model In previous studies on predicting attributes such as gender, personality, power, and political orientation (Gomez et al.,2003;Burger et al., 2011;Schwartz et al.,2013;Park et al.,2015; Katerenchuk and Rosenberg,2016), a document is usually modeled as a bag of words and represented by counting the frequency of each feature or aggregating embedd
2755148833	Acquiring Background Knowledge to Improve Moral Value Prediction	2340954483	ise (Horne et al.,2016), leadership role (Tyshchuk et al.,2013), personality (Golbeck et al.,2011;Schwartz et al.,2013), gender (Burger et al.,2011;Rao et al.,2010), hate speech (Waseem and Hovy,2016;Nobata et al., 2016), and social interaction (Althoff et al.,2014; Tan et al.,2016). This work has extensively studied textual (e.g., n-gram and LIWC) and structural features (e.g., Twitter relationships) on a variety of
2755148833	Acquiring Background Knowledge to Improve Moral Value Prediction	2017729405	rimarily relied on textual features directly derived from target texts; these features have ranged from n-grams, word embedarXiv:1709.05467v1 [cs.CL] 16 Sep 2017 dings, emoticons, to word categories (Rao et al., 2010;Tumasjan et al.,2010;Golbeck et al.,2011; Conover et al.,2011;Schwartz et al.,2013;Dehghani et al.,2014,2016). While such approaches can yield powerful representations of text, they fall far short of
2755957574	StarSpace: Embed All The Things!	2101802482	. 2013). We compare with a number of methods, including transE 5http://www.freebase.com presented in (Bordes et al. 2013).TransE was shown to outperform RESCAL (Nickel, Tresp, and Kriegel 2011), RFM (Jenatton et al. 2012), SE (Bordes et al. 2011) and SME (Bordes et al. 2014) and is considered a standard benchmark method. TransE uses an L2 similarity ||head + relation - tail||2 and SGD updates with single entity corrup
2755957574	StarSpace: Embed All The Things!	2247119764	3 https://code.google.com/archive/p/word2vec/ 4 https://github.com/facebookresearch/fastText/blob/master/ pretrained-vectors.md Metric Hits@10 r. Mean Rank r. Hits@10 f. Mean Rank f. Train Time SE* (Bordes et al. 2011) 28.8% 273 39.8% 162 - SME(LINEAR)* (Bordes et al. 2014) 30.7% 274 40.8% 154 - SME(BILINEAR)* (Bordes et al. 2014) 31.3% 284 41.3% 158 - LFM* (Jenatton et al. 2012) 26.0% 283 33.1% 164 - RESCAL† (Nick
2755957574	StarSpace: Embed All The Things!	2247119764	number of methods, including transE 5http://www.freebase.com presented in (Bordes et al. 2013).TransE was shown to outperform RESCAL (Nickel, Tresp, and Kriegel 2011), RFM (Jenatton et al. 2012), SE (Bordes et al. 2011) and SME (Bordes et al. 2014) and is considered a standard benchmark method. TransE uses an L2 similarity ||head + relation - tail||2 and SGD updates with single entity corruptions of head or tail tha
2755957574	StarSpace: Embed All The Things!	2608603130	thodsthengivesimilarperformance. Note there are some recent improved results on this dataset using larger embeddings (Kadlec, Bajgar, and Kleindienst 2017) or more complex, but less general, methods (Shen et al. 2017). Inﬂuence of k In this section, we ran experiments on the Freebase 15k dataset to illustrate the complexity of our model in terms of the number of negative search examples. We set dim = 50, and the m
2755957574	StarSpace: Embed All The Things!	2493916176	training data consisting of raw text. We select a as a window of words (e.g.,fourwords,two either side ofa middleword),andb as the middle word, following (Collobert et al. 2011; Mikolov et al. 2013; Bojanowski et al. 2017). Learning Sentence Embeddings Learning word embeddings (e.g. as above) and using them to embed sentences does not seem optimal when you can learn sentence embeddings directly. Given a training set of
2755957574	StarSpace: Embed All The Things!	2101802482	ts@10 f. Mean Rank f. Train Time SE* (Bordes et al. 2011) 28.8% 273 39.8% 162 - SME(LINEAR)* (Bordes et al. 2014) 30.7% 274 40.8% 154 - SME(BILINEAR)* (Bordes et al. 2014) 31.3% 284 41.3% 158 - LFM* (Jenatton et al. 2012) 26.0% 283 33.1% 164 - RESCAL† (Nickel, Tresp, and Kriegel 2011) - - 58.7% - TransE (dim=50) 47.4% 212.4 71.8% 63.9 1m27m TransE (dim=100) 51.1% 225.2 82.8% 72.2 1h44m TransE (dim=200) 51.2% 234.3 83.
2755957574	StarSpace: Embed All The Things!	2129921015	r unsupervised embeddings; we also compare on the SentEval tasks (Conneau et al. 2017) against a wide range of unsupervised models for sentence embedding. In the domain of supervised embeddings, SSI (Bai et al. 2009) and WSABIE (Weston, Bengio, and Usunier 2011) areearly approachesthat showedpromisein NLP and information retrievaltasks ((Weston et al. 2013),(Hermannet al. 2014)).Severalmorerecentworksincluding(Ta
2755957574	StarSpace: Embed All The Things!	2493916176	in an unsupervised way over large corpora. Work on neural embeddings in this domain includes (Bengio et al. 2003), (Collobert et al. 2011), word2vec (Mikolov et al. 2013) and more recently fastText (Bojanowski et al. 2017). In our experiments we compare to word2vec and fastText as representative scalable models for unsupervised embeddings; we also compare on the SentEval tasks (Conneau et al. 2017) against a wide range
2756244695	Limitations of Cross-Lingual Learning from Image Search	2047221353	to correct translation pairs are labeled as positive while all other pairs are labeled as negative. In the prediction step, translations are ranked based on the predicted distance to the hyperplane (Joachims, 2002). 3.6 Evaluation Metrics Ranking performance is evaluated by computing the Mean Reciprocal Rank (MRR) as MRR = 1 M XM i=1 1 rank(w s;w t) M is the number of words to be translated and rank(w s;w t) is
2756244695	Limitations of Cross-Lingual Learning from Image Search	2250976127	ented by SIFT and color features averaged over all language pairs. First, we observe that our results for translating nouns using CNN features are slightly lower than those reported in previous work (Kiela et al. (2015) report P@1 = 0.567 for a dataset containing 500 nouns). This may be due to a higher proportion of more abstract nouns in our dataset. Second, our experiments conﬁrm that the CNN features outperform t
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2612881151	! #!&quot; !#! &quot;# Figure 1: The architecture of attention-based NMT external syntactic knowledge or connects words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to reﬁne the NMT based on the above two points. The main idea is to learn relationship between the source word-pairs.
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	1816313093	! ! !!!! !!! ! &quot; ! #!&quot; !#! &quot;# Figure 1: The architecture of attention-based NMT external syntactic knowledge or connects words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to reﬁne the NMT based on the above two points. The main idea is to learn relationship between
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2612881151	ch.org Systems test15 test16 test17 Avg. RNNsearch 17.3 20.9 16.6 18.3 RNNsearch? 21.4 25.6 20.1 22.4 RNMT 22.7 27.8 21.8 24.1 Table 2: Performance comparison on WMT17 En-De datasets. Systems test16 (Bastings et al., 2017) 23.9 RNMT 25.4 Table 3: Performance comparison with related work on the WMT16 English-German dataset. On the WMT17 English-German dataset, as shown in Table 2, RNMT shows superiority on three test da
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2130942839	cts and the relations between them are reasoned. Speciﬁcally, our method introduces a relation network (RN) component between the encoder and the attention layer in the RNN encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015). The RN component is composed of three layers: ﬁrst, the convolutional neural network (CNN) layer slides window along the output of the encoder to capture information among mu
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2133564696	d then performs decoding based on this space, employing attention to indicate the relevance of each source word to the current translation. Figure 1 shows the architecture of the attention-based NMT (Bahdanau et al., 2015), which is composed of three components: the encoder, the attention layer and the decoder. The Encoder The encoder uses a pair of GRUs to run through source words bidirectionally to get two sequences
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	1816313093	d as the test sets: newstest2015 (2169 sentences), newstest2016 (2999 sentences) and newstest2017 (3004 sentences). Besides, 8000 merging operations were performed to learn byte-pair encodings (BPE) (Sennrich et al., 2016) on the target side of the parallel training data. WMT16 We used this data set for we wanted to compare with the work of Bastings (2017) which used WMT16 as the training set. We kept the same settings
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2130942839	encoder-decoder model, and can even outperform the approach involving supervised syntactic knowledge. 1 Introduction In recent years, neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the stateof-the-art statistical machine translation (SMT). The RNN encoder-decoder architecture is widely used fra
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2610661645	f attention-based NMT external syntactic knowledge or connects words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to reﬁne the NMT based on the above two points. The main idea is to learn relationship between the source word-pairs. Corresponding to the ﬁrst point, our method e
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2609011624	Figure 1: The architecture of attention-based NMT external syntactic knowledge or connects words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to reﬁne the NMT based on the above two points. The main idea is to learn relationship between the source word-pairs. Corresponding to the ﬁrst p
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	1836465849	hinese-English translation task, two convolution layers with kernel width of 1 and 3 were stacked, the output channel sizes of CNN were 128 and 256 respectively, followed by batch normalization (BN) (Ioffe and Szegedy, 2015) with learnable parameters, and MLP contained 256 units. For English-German translation task, only one convolution layer was used with kernel width of 3, the output channel size was 96, 128 was adopte
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2130942839	iple behind which is that: encoding the meaning of the input bidirectionally into a concept space via recurrent neural networks (RNNs) and decoding into target words with RNNs based on this encoding (Sutskever et al., 2014; Bahdanau et al., 2015) . This meaning encoding principle leads to a deeper understanding and learning of the translation rules, and hence better translation than conventional statistic machine trans
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2133564696	methods employing external syntax knowledge explicitly and achieved comparable performance. 2 Background As the main idea of our method is to introduce relation networks into the attention-based NMT (Bahdanau et al., 2015) to learn word relationship and keep all source words in memory, in this section we will brieﬂy describe the baseline model – the attention-based NMT ﬁrst and the technique used in this paper – relati
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2511730936	n is conducted to get the ﬁnal output of the l-th RNL in the following two steps. First, the input and the output of the l-th RNL are added together: hl= hl in+h l out (10) Next, dense concatenation (Huang et al., 2017) is applied to receives features from all previous RNLs and then the ﬁnal output of the l-th RNL is produced by hl dc= W h h1;h2; ;hl i +b (11) where weight matrix W dcand bias b dcare adjusted to map
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2153653739	n, e.g., for the source sentence “take the heavy box away”, when translating “away”, “take” should be considered together. In addition, it has been proven that using phrases rather than words in SMT (Koehn et al., 2003) brings performance improvement, while in NMT the attention is only modeled in the unit of words. In the same sense, improvement is expected if attention is operated on more words rather than one. Mor
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2133564696	nd can even outperform the approach involving supervised syntactic knowledge. 1 Introduction In recent years, neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the stateof-the-art statistical machine translation (SMT). The RNN encoder-decoder architecture is widely used framework for NMT, the prin
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	1753482797	niﬁcantly over the conventional encoder-decoder model, and can even outperform the approach involving supervised syntactic knowledge. 1 Introduction In recent years, neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the stateof-the-art statistical machine translation (SMT). The RNN encoder-decoder archite
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2624614404	omputing relations is inherent without needing to be learned specially. Formally, given a set of input “objects” denoted as O = fo 1;o 2; ;o ng, RN can be formed as a composition function of objects (Santoro et al., 2017), represented as RN(O) = f ˚ P i;jg (o i;o j)  , where o iis the i-th object, and f ˚and g are functions used to calculate relations. multi-layer perceptrons are often used for f ˚and g , as thei
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2162245945	RNMT 38.08 41.07 36.82 36.72 38.17 RNMT 39.24 42.01 37.79 37.81 39.21 Table 1: Performance comparison on NIST datasets. is used to indicate the improvement over RNNsearch?is statistically signiﬁcant (Collins et al., 2005) (p&lt;0:01). We compared our system RNMT with the two baseline systems: RNNsearch and RNNsearch?both on the Chinese-English and the WMT17 English-German translation tasks. As RNMT was implemented on
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2624614404	round one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. For the second point, it employs relation networks (RNs) (Santoro et al., 2017) to build pairwise relations between words but does not require external syntactic knowledge input. In this way, our method can memorize all words ahead and behind via additional connection between wo
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2131774270	s expected if attention is operated on more words rather than one. Moreover, NMT produces the representation for the source by running through the source words sequentially with a bidirectional RNNs (Schuster and Paliwal, 1997), so it only employs word information and ignores the relation between words. Although some researchers have demonstrated that NMT is able to capture certain syntactic phenomena (e.g. subject-verb agr
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2549835527	s the relation between words. Although some researchers have demonstrated that NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other work which has shown their superior performance by modeling word relationship explicitly. However, these work usually needs to employ arXiv:1805.11154v1 [cs.C
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2133564696	t: encoding the meaning of the input bidirectionally into a concept space via recurrent neural networks (RNNs) and decoding into target words with RNNs based on this encoding (Sutskever et al., 2014; Bahdanau et al., 2015) . This meaning encoding principle leads to a deeper understanding and learning of the translation rules, and hence better translation than conventional statistic machine translation (SMT) that consid
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2064675550	tic machine translation (SMT) that considers only surface forms, e.g., words and phrases. The RNNs with gating, such as Gated Recurrent Unit (GRU) (Cho et al., 2014) or Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), are designed to memorize useful history information and meanwhile forget irrelative information. Together with attention technique which makes the decoding process only focus on the most related sou
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2101105183	tokenization. newstest2015 and newstest2016 were used as the validation set and test set, respectively. 16000 BPE merging operations were conducted in this data set. Case-sensitive 4-gram BLEU score (Papineni et al., 2002) was reported by using the multi-bleu.pl script. Except WMT16, the results on the other two data sets were evaluated with case-insensitive 4-gram BLEU score. 4.2 Systems RNNsearch We implemented the a
2756303778	Refining Source Representations with Relation Networks for Neural Machine Translation	2133564696	tween them are reasoned. Speciﬁcally, our method introduces a relation network (RN) component between the encoder and the attention layer in the RNN encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2015). The RN component is composed of three layers: ﬁrst, the convolutional neural network (CNN) layer slides window along the output of the encoder to capture information among multiple words around one
2756386045	Natural Language Inference over Interaction Space	2118463056	., 2016; Vendrov et al., 2015; Mou et al., 2015; Liu et al., 2016; Munkhdalai &amp; Yu, 2016). (ii) Joint feature models which use the cross sentence feature or attention from one sentence to another(Rocktaschel et al., 2015; Wang &amp; Jiang, 2015; Cheng et al., 2016; Parikh et al., 2016; Wang¨ et al., 2017; Yu &amp; Munkhdalai, 2017; Sha et al., 2016). After neural attention mechanism is successfully applied on the mac
2756386045	Natural Language Inference over Interaction Space	2118463056	(Mou et al., 2015) 82.1 5. SPINN-PI encoders(Bowman et al., 2016) 83.2 6. BiLSTM intra-attention encoders(Liu et al., 2016) 84.2 7. NSE encoders(Munkhdalai &amp; Yu, 2016) 84.6 8. LSTM with attention(Rocktaschel et al., 2015) 83.5¨ 9. mLSTM(Wang &amp; Jiang, 2015) 86.1 10. LSTMN with deep attention fusion(Cheng et al., 2016) 86.3 11. decomposable attention model(Parikh et al., 2016) 86.3 12. Intra-sentence attention + (11
2756386045	Natural Language Inference over Interaction Space	2118463056	e sentence representation, and Munkhdalai &amp; Yu (2016) proposes an memory augmented neural network to encode the sentence. The next group of model, experiments (8-18), uses cross sentence feature. Rocktaschel et al. (2015) aligns each sentence word-by-word with attention on top of LSTMs. Wang¨ &amp; Jiang (2015) enforces cross sentence attention word-by-word matching with the proprosed mLSTM model. Cheng et al. (2016)
2756386045	Natural Language Inference over Interaction Space	2250539671	ial decayed keep rate during training, where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10,000 step. We initialize our word embeddings with pre-trained 300D GloVe 840B vectors Pennington et al. (2014) while the out-of-vocabulary word are randomly initialized with uniform distribution. The character embeddings are randomly initialized with 100D. We crop or pad each token to have 16 characters. The
2756386045	Natural Language Inference over Interaction Space	2250539671	r phrase to a vector representation and construct the representation matrix for sentences. In embedding layer, a model can map tokens to vectors with the pre-trained word representation such as GloVe(Pennington et al., 2014), word2Vec(Mikolov et al., 2013) and fasttext(Joulin et al., 2016). It can also utilize the preprocessing tool, e.g. named entity recognizer, part-of-speech recognizer, lexical parser and coreference
2756386045	Natural Language Inference over Interaction Space	2531207078	ures. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (840B GloVe). The word embedding is updated during training. As in (Kim et al., 2016; Lee et al., 2016), we ﬁlter character embedding with 1D convolution kernel. The character convolutional feature maps are then max pooled over time dimension for each token to obtain a vector. The character features su
2756540778	Generating Sentences by Editing Prototypes	2210838531	)k ;0))) by rotating to the ﬁrst canonical basis vector. Hence r q L KL = 0. Comparison with existing VAE encoders. Our design of q differs from the typical choice of a standard normal distribution (Bowman et al., 2016; Kingma and Welling, 2014) for two reasons: First, by construction, edit vectors are sums of word vectors and since cosine distances are traditionally used to measure distances between word vectors,
2756540778	Generating Sentences by Editing Prototypes	2066350381	(Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their ﬂexibility and performance across a wide range of NLP tasks (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tas
2756540778	Generating Sentences by Editing Prototypes	10957333	014), neural language models (Bengio et al., 2003) have been widely used due to their ﬂexibility and performance across a wide range of NLP tasks (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more i
2756540778	Generating Sentences by Editing Prototypes	2210838531	ces, while prior retrieval models attend to examples based on similarity of the input sequences. In terms of generation techniques that capture semantics, the sentence variational autoencoder (SVAE) (Bowman et al., 2016) is closest to our work in that it attempts to impose semantic structure on a latent vector space. However, the SVAE’s latent vector is meant to represent the entire sentence, whereas the neural edito
2756540778	Generating Sentences by Editing Prototypes	2210838531	e the SVAE to “edit” a target sentence into a semantically similar sentence, we perturb its underlying latent sentence vector and then decode the result back into a sentence — the same method used in Bowman et al. (2016). Semantic smoothness. A good editing system should have ﬁne-grained control over the semantics of a sentence: i.e., each edit should only alter the semantics of a sentence by a small and well-control
2756540778	Generating Sentences by Editing Prototypes	2467604901	in Equation 1) by only summing over x0that are lexically similar to x. 2.We lower bound the expectation over the edit prior (in Equation 2) using the evidence lower bound (ELBO) (Jordan et al., 1999; Doersch, 2016) which can be effectively approximated. We describe and motivate these approximations in Sections 3.1 and 3.2, respectively. In Section 3.3, we combine the two approximations to give the ﬁnal objectiv
2756540778	Generating Sentences by Editing Prototypes	2210838531	er L gen. We ﬁnd a good balance by tuning . In contrast, when using a Gaussian variational encoder, the KL term takes a different value per example and cannot be explicitly controlled. Consequently, Bowman et al. (2016) and others have observed that training tends to aggressively drive these KL terms to zero, leading to uninformative values of z— even when multiplying L KL by a carefully tuned and annealed importanc
2756540778	Generating Sentences by Editing Prototypes	1958706068	erate sentences from scratch, often in a left-to-right manner (Bengio et al., 2003). It is often observed that such NLMs suffer from the problem of favoring generic utterances such as “I don’t know” (Li et al., 2016). At the same time, naive strategies to increase diversity have been shown to compromise grammaticality (Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully repr
2756540778	Generating Sentences by Editing Prototypes	2133564696	erformance across a wide range of NLP tasks (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more information from the input sequence (Vaswani et al., 2017). Our work extends the applicability of attenti
2756540778	Generating Sentences by Editing Prototypes	2510403706	generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics (Tenenbaum et al., 2000; Hashimoto et al., 2016). From a generative modeling perspective, editing randomly sampled training sentences closely resembles nonparametric kernel density estimation (Parzen, 1962) where one samples points from a training
2756540778	Generating Sentences by Editing Prototypes	2141599568	ich randomly samples an edit vector z^ ˘p(z), instead using ^zderived from f(x 1;x 2). We also compare our accuracies to the simpler task of solving the word, rather than sentence-level analogies in (Mikolov et al., 2013a) using GloVe. This task is substantially simpler, since the goal is to identify a single word (such as “good:better::bad:?”) instead of an entire sentence. Despite this, the top10 performance of our
2756540778	Generating Sentences by Editing Prototypes	2141599568	top kcandidate outputs of p edit (using beam search) and evaluate whether the gold y 2 appears among the top kelements. We generate the semantic relations rusing prior evaluations for word analogies (Mikolov et al., 2013a; Mikolov et al., 2013b). We leverage these to generate a new dataset of sentence analogies, using a simple strategy: given an analogous word pair (w 1;w 2), we mine the Yelp corpus for sentence pair
2756540778	Generating Sentences by Editing Prototypes	2130942839	neural models, retrieval-augmented text generation, semantically meaningful representations, and nonparametric statistics. Based upon recurrent neural networks and sequence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their ﬂexibility and performance across a wide range of NLP tasks (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritt
2756540778	Generating Sentences by Editing Prototypes	2210838531	oves perplexity by 13 points on the Yelp corpus and 7 points on the One Billion Word Benchmark. For the latter, we show that latent edit vectors outperform standard sentence variational autoencoders (Bowman et al., 2016) on semantic similarity, locally-controlled text generation, and a sentence analogy task. 2 Problem statement Our primary goal is to learn a generative model of sentences for use as a language model.1
2756540778	Generating Sentences by Editing Prototypes	1753482797	quence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their ﬂexibility and performance across a wide range of NLP tasks (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various seq
2756540778	Generating Sentences by Editing Prototypes	2467604901	sampling z ˘q(z jx0;x). The second term, L KL, penalizes the difference between q(z jx0;x) and p(x), which is necessary for the lower bound to hold. A thorough introduction to the ELBO is provided in Doersch (2016). Note that q(zjx0;x) and p edit(xjx0;z) combine to form a variational autoencoder (VAE) (Kingma and Welling, 2014), where q(zjx0;x) is the variational encoder and p edit(xjx0;z) is the variational de
2756540778	Generating Sentences by Editing Prototypes	1958706068	sentences. Many NLM systems have noted an undesirable tradeoff between grammaticality and diversity, where a temperature low enough to enforce grammaticality results in short and generic utterances (Li et al., 2016). Figure 4 illustrates that both the grammaticality and plausibility of NEURALEDITOR without any temperature annealing is on par with the best tuned temperature for NLM, with a far higher diversity, a
2756540778	Generating Sentences by Editing Prototypes	2141599568	suggest that local variation over edits is easier to model than global variation over sentences. Our use of lexical similarity neighborhoods is comparable to context windows in word vector training (Mikolov et al., 2013a). More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics (Tenenbaum et al
2756540778	Generating Sentences by Editing Prototypes	2133564696	tivation is that sentences from the corpus provide a high quality starting point: they are grammatical, naturally diverse, and exhibit no bias towards shortness or vagueness. The attention mechanism (Bahdanau et al., 2015) of the neural editor strongly biases the generation towards the prototype, and therefore it needs to solve a much easier problem than generating from scratch. We train the neural editor by maximizing
2760343428	Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals	2293634267	r is its applicability in the real-world. To tackle the aforementioned challenges, we propose a novel hybrid deep neural network that combines the beneﬁts of both Convolutional Neural Networks (CNNs) [7] and recurrent neural networks (RNNs) [8] for effective EEG signal decoding. Our model is capable of modeling high-level, robust and salient feature representations hidden in the raw EEG signal stream
2760343428	Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals	2074772891	convolutional layer, so the shape of Xc 2 is [1;64;2]. The pooling layer is a non-linear down-sampling transformation layer. There are several pooling options, with max pooling being the most popular [11]. The max pooling layer scans through the inputs along with a sliding window with a designed stride. Then it outputs the maximum value in every sub-region that the window is scanned. The pooling layer
2760343428	Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals	2151669316	s (Section IV-E). A. Experiment Setting We select the widely used EEG data from PhysioNet eegmmidb (EEG motor movement/imagery database) database3. This data is collected using the BCI200 EEG system4 [14] which records the brain signals using 64 channels at a sampling rate of 160Hz. The subject is asked to wear the EEG device and sit in front of a computer screen and perform certain typing actions in
2760343428	Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals	2295598076	t hand right hand both hands both feet label 0 1 2 3 4 emotiv intent up arrow down arrow left arrow right arrow eye closed label 0 1 2 3 4 command up down left right conﬁrmation (XGBoost) is employed [13] to classify the EEG streams. It fuses a set of classiﬁcation and regression trees (CART) and exploits detailed information from the input data. It builds multiple trees and each tree has its leaves a
2762484717	Word translation without parallel data	1828724394	, their performance is signiﬁcantly below supervised methods. To sum up, current methods have either not reached competitive performance, or they still require parallel data, such as aligned corpora (Gouws et al., 2015; Vulic &amp; Moens, 2015) or a seed parallel lexicon (Duong et al., 2016). In this paper, we introduce a model that either is on par, or outperforms supervised state-of-the-art methods, without emplo
2762484717	Word translation without parallel data	630532510	5 for English-Esperanto and Esperanto-English is of 46.5% and 43.9% respectively. To show the impact of such a dictionary on machine translation, we apply it to the English-Esperanto Tatoeba corpora (Tiedemann, 2012). We remove all pairs containing sentences with unknown words, resulting in about 60k pairs. Then, we translate sentences in both directions by doing word-byword translation. In Table 5, we report the
2762484717	Word translation without parallel data	2475191182	current methods have either not reached competitive performance, or they still require parallel data, such as aligned corpora (Gouws et al., 2015; Vulic &amp; Moens, 2015) or a seed parallel lexicon (Duong et al., 2016). In this paper, we introduce a model that either is on par, or outperforms supervised state-of-the-art methods, without employing any cross-lingual annotated data. We only use two large monolingual c
2762484717	Word translation without parallel data	2493916176	embeddings and dictionaries are publicly available1. 1 INTRODUCTION Most successful methods for learning distributed representations of words (e.g. Mikolov et al. (2013c;a); Pennington et al. (2014); Bojanowski et al. (2017)) rely on the distributional hypothesis of Harris (1954), which states that words occurring in similar contexts tend to have similar meanings. Levy &amp; Goldberg (2014) show that the skip-gram with n
2762484717	Word translation without parallel data	2156365280	ity nearest neighbors of many other points, while others (anti-hubs) are not nearest neighbors of any point. This problem has been observed in different areas, from matching image features in vision (Jegou et al., 2010) to translating words in text understanding applications (Dinu et al., 2015). Various solutions have been proposed to mitigate this issue, some being reminiscent of pre-processing already existing in
2762484717	Word translation without parallel data	2294774419	l. (2013b) obtained better results on the word translation task using a simple linear mapping, and did not observe any improvement when using more advanced strategies like multilayer neural networks. Xing et al. (2015) showed that these results are improved by enforcing an orthogonality constraint on W. In that case, the equation (1) boils down to the Procrustes problem, which advantageously offers a closed form so
2762484717	Word translation without parallel data	1542713999	l translation tool to alleviate this issue. We make these dictionaries publicly available as part of the MUSE library3. We report results on these bilingual dictionaries, as well on those released by Dinu et al. (2015) to allow for a direct comparison with previous approaches. For each language pair, we consider 1,500 query source and 200k target words. Following standard practice, we measure how many times one of
2762484717	Word translation without parallel data	1542713999	lished as a conference paper at ICLR 2018 English to Italian Italian to English P@1 P@5 P@10 P@1 P@5 P@10 Methods with cross-lingual supervision Mikolov et al. (2013b) y 10.5 18.7 22.8 12.0 22.1 26.7 Dinu et al. (2015) y 45.3 72.4 80.7 48.9 71.3 78.3 Smith et al. (2017) y 54.6 72.7 78.2 42.9 62.2 69.2 Procrustes - NN 42.6 54.7 59.0 53.5 65.5 69.5 Procrustes - CSLS 66.1 77.1 80.7 69.5 79.6 83.5 Methods without cross
2762484717	Word translation without parallel data	2250600644	mghani &amp; Knight, 2017). Although initially not based on distributional semantics, recent studies show that the use of word embeddings can bring signiﬁcant improvement in statistical decipherment (Dou et al., 2015). The rise of distributed word embeddings has revived some of these approaches, now with the goal of aligning embedding spaces instead of just aligning vocabularies. Cross-lingual word embeddings can
2762484717	Word translation without parallel data	2250539671	ne translation. Our code, embeddings and dictionaries are publicly available1. 1 INTRODUCTION Most successful methods for learning distributed representations of words (e.g. Mikolov et al. (2013c;a); Pennington et al. (2014); Bojanowski et al. (2017)) rely on the distributional hypothesis of Harris (1954), which states that words occurring in similar contexts tend to have similar meanings. Levy &amp; Goldberg (2014) show
2762484717	Word translation without parallel data	1542713999	nearest neighbors of any point. This problem has been observed in different areas, from matching image features in vision (Jegou et al., 2010) to translating words in text understanding applications (Dinu et al., 2015). Various solutions have been proposed to mitigate this issue, some being reminiscent of pre-processing already existing in spectral clustering algorithms (Zelnik-manor &amp; Perona, 2005). However, m
2762484717	Word translation without parallel data	2118090838	ous cross-lingual language processing systems. More recently, several approaches have been proposed to learn bilingual dictionaries mapping from the source to the target space (Mikolov et al., 2013b; Zou et al., 2013; Faruqui 9 Published as a conference paper at ICLR 2018 &amp; Dyer, 2014; Ammar et al., 2016). In particular, Xing et al. (2015) showed that adding an orthogonality constraint to the mapping can sign
2762484717	Word translation without parallel data	2294774419	or points to learn this mapping and evaluated their approach on a word translation task. Since then, several studies aimed at improving these cross-lingual word embeddings (Faruqui &amp; Dyer (2014); Xing et al. (2015); Lazaridou et al. (2015); Ammar et al. (2016); Artetxe et al. (2016); Smith et al. (2017)), but they all rely on bilingual word lexicons. Recent attempts at reducing the need for bilingual supervisio
2762484717	Word translation without parallel data	2294774419	ries mapping from the source to the target space (Mikolov et al., 2013b; Zou et al., 2013; Faruqui 9 Published as a conference paper at ICLR 2018 &amp; Dyer, 2014; Ammar et al., 2016). In particular, Xing et al. (2015) showed that adding an orthogonality constraint to the mapping can signiﬁcantly improve performance, and has a closed-form solution. This approach was further referred to as the Procrustes approach in
2762484717	Word translation without parallel data	1542713999	rocrustes-ISF, while being computationally faster and not requiring hyper-parameter tuning. In Table 2, we compare our Procrustes-CSLS approach to previous models presented in Mikolov et al. (2013b); Dinu et al. (2015); Smith et al. (2017); Artetxe et al. (2017) on the English-Italian word translation task, on which state-of-the-art models have been already compared. We show that our Procrustes-CSLS approach obtain
2762484717	Word translation without parallel data	1542713999	s a conference paper at ICLR 2018 English to Italian Italian to English P@1 P@5 P@10 P@1 P@5 P@10 Methods with cross-lingual supervision (WaCky) Mikolov et al. (2013b) y 33.8 48.3 53.9 24.9 41.0 47.4 Dinu et al. (2015)y 38.5 56.4 63.9 24.6 45.4 54.1 CCAy 36.1 52.7 58.1 31.0 49.9 57.0 Artetxe et al. (2017) 39.7 54.7 60.5 33.8 52.4 59.1 Smith et al. (2017)y 43.1 60.7 66.4 38.0 58.5 63.6 Procrustes - CSLS 44.9 61.8 66
2762484717	Word translation without parallel data	1731081199	ximizing its ability to identify the origin of an embedding, and Waims at preventing the discriminator from doing so by making WXand Yas similar as possible. This approach is in line with the work of Ganin et al. (2016), who proposed to learn latent representations invariant to the input domain, where in our case, a domain is represented by a language (source or target). Discriminator objective We refer to the discr
2762715843	A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments	2756778986	lso includes enriching our dataset with some alignments at the word level, in order to evaluate a bilingual lexicon discovery task. This is possible with encoder-decoder approaches, as shown in (ZanonBoito et al., 2017). As we distribute this corpus,we hope that this will helpthe communityto strengthenits effortto improvethe technolomethod P R F gold FA phones + dpseg 7.5 13.9 9.7 (Jansen and Van Durme, 2011) 4.6 4.
2762715843	A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments	2122228338	ures (Mu¨ller et al., 2017b). So, the number of clusters (pseudo phones) can be controlled during this process. Unsupervised word discovery (UWD) To perform unsupervised word discovery, we use dpseg (Goldwater et al., 2006; Goldwater et al., 2009).9 It implements a Bayesian non-parametric approach, where (pseudo)-morphs are generated by a bigram model over a non-ﬁniteinventory,throughthe useofaDirichlet-Process. Estima
2762917925	Can Machines Think in Radio Language	1562336775	capacity of radio language not based on these mechanisms. Thereby, high-level in telligence may not be brain-like. The brain-like intelligence tries to achieve intelligence as demonstrated by brains [11], preferably those of highly evolved creatures. But the nature of intelligence can be understood in a brain-different way, just like the secret of how to fly. Indeed, without flapping its wings, an ai
2762917925	Can Machines Think in Radio Language	2194775991	thinking in radio language (radio thinking), is a different way to implement intelligence than people can. One may argue that, even without language, artificial intelligence (e.g. by residual network [7], deep Q-network [8] and AlphoGo [9]) could equal or even beat human intelligence in deep learning performance of some tasks such as object recognition, video games and board games. In practical reali
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	2293453011	15). Several researchers adopted this architecture for the reading comprehension (RC) style QA tasks, because it can extract contextual information from each sentence and use it in ﬁnding the answer (Xiong et al., 2016;Kumar et al.,2016). However, none of this research is applied to the QA pair ranking task directly. 3 Model In this section, we depict a previously released neural text ranking model, and then introd
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	836999996	ence similarity rankings (Baudiˇs et al. ,2016;Lowe et al.,2015). As of now, the Ubuntu Dialogue dataset is one of the largest corpus openly available for text ranking. To tackle the Ubuntu dataset, (Lowe et al., 2015) adopted the “term frequency-inverse document frequency” approach to capture important words among context and next utterances (Ramos et al.,2003). (Bordes et al.,2014;Yu et al., 2014) proposed deep n
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	1993378086	l as an extension of the RNN-CNN model. Recently, the hierarchical recurrent encoder-decoder model was proposed to embed contextual information in user query prediction and dialogue generation tasks (Sordoni et al. 2015; Serban et al. 2016). A lower-level RNN embedded sentence level information from sequence inputs of the words in each sentence while an upper-level RNN embedded sentence turns level information from
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	836999996	lable via web repository 4. 5 Empirical Results 5.1 Evaluation Metrics We regards all the tasks as selecting the best answer among text candidates for the given question. Following the previous work (Lowe et al., 2015), we report model performance as recall at k(R@k) relevant texts among given 2 or 10 candidates (e.g., 1 in 2 R@1). Though this metric is useful for ranking task, R@1 metric is also meaningful for cla
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	32002537	nd to user requests. Another type of advanced QA systems is IBM’s Watson who builds knowledge bases from unstructured data. These raw data are also indexed in search clusters to support user queries (Fan et al. 2012; Chu-Carroll et al. 2012). In academic literature, researchers have recently studied reading comprehension (RC) style QA tasks. With the release of sufﬁcient datasets in terms of both large quantitie
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	889023230	the RNN-CNN model. Recently, the hierarchical recurrent encoder-decoder model was proposed to embed contextual information in user query prediction and dialogue generation tasks (Sordoni et al. 2015; Serban et al. 2016). A lower-level RNN embedded sentence level information from sequence inputs of the words in each sentence while an upper-level RNN embedded sentence turns level information from the sequence input of
2763613715	LEARNING TO RANK QUESTION-ANSWER PAIRS USING HIERARCHICAL RECURRENT ENCODER WITH LATENT TOPIC CLUSTERING	1694079474	ts. Another type of advanced QA systems is IBM’s Watson who builds knowledge bases from unstructured data. These raw data are also indexed in search clusters to support user queries (Fan et al. 2012; Chu-Carroll et al. 2012). In academic literature, researchers have recently studied reading comprehension (RC) style QA tasks. With the release of sufﬁcient datasets in terms of both large quantities and human validated qual
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2527845440	al.,2015a;Gehring et al.,2017) are appealingfortheirsingle-model,end-to-endtrainingprocess, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al.,2007;Junczys-Dowmunt et al., 2016). However, there are still many open problems in NMT (Koehn and Knowles,2017). One particularissueismistranslationofrarewords.For example, consider the Uzbek sentence: Source: Ammo muammolar hali ko&a
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2410217169	Anthony or Margaret . Because NMT learns word representations in continuous space, it tends to translate words that seem natural in the context, but do not reect the content of the source sentence (Arthur et al., 2016). This coincides with other observations that NMT&apos;s translations are often uent but lack accuracy (Wang et al.,2017b;Wu et al.,2016). Why does this happen? At each time step, the model&apos;s dis
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2613904329	BLEU, surpassing phrase-based translation in nearly all settings. 1 Introduction Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealingfortheirsingle-model,end-to-endtrainingprocess, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2304113845	cing the number of UNK s by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNK s by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al.
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2625092622	confused between similar words, cos W l e ;hl signicantly favors Fauci . 6.3 Alignment and unknown words Both our baseline NMT and xnorm models suffer from the problem of shifted alignments noted by Koehn and Knowles (2017). As seen in Figure 2a and 2b, the alignments for those two systems seemtoshiftbyonewordtotheleft(onthesource side). For example, nói should be aligned to said instead of Telekom , and so on. Although
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2154124206	cosine similarity cos W e ;h measures how well e ts into the context. The bias be controls how much the word e is generated; it is analogous to the language model in a log-linear translation model (Och and Ney, 2002). Finally, kW ek also controls how much e is generated. Figure 1 shows that it generally correlates with frequency. But because it is multiplied by cos W e ;h, it has a stronger e ect on words whose
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	1591801644	ems with Adadelta (Zeiler, 2012). All parameters were initializeduniformlyfrom[ 0:01 ;0:01].Whenagradient&apos;snormexceeded5,wenormalizeditto5.We also used dropout on non-recurrent connections only (Zaremba et al., 2014), with probability 0.2. We used minibatches of size 32. We trained for 50 epochs,validatingonthedevelopmentsetafterevery epoch, except on English-Japanese, where we validatedtwiceperepoch.Wekeptthebes
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2311921240	et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benet from more advanced UNK - replaceme
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2100664567	gin. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNK s by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNK s by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, n
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2194775991	ings (not the hidden states, as in the main model) to give an averagesource-wordembeddingateachdecodingtime step t: f` t = tanh X s at(s)fs: Then we use a one-hidden-layer FFNN with skip connections (He et al., 2016): h` t = tanh( Wf ` t ) + f ` t and combine its output with the decoder output to getthepredictivedistributionoveroutputwordsat time step t: p(yt jy&lt; t;x) = softmax( W o ht + bo + W `h` t + b `):
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2130942839	nging from 100k to 8M words, and achieve improvements of up to + 4.5 BLEU, surpassing phrase-based translation in nearly all settings. 1 Introduction Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealingfortheirsingle-model,end-to-endtrainingprocess, and have demonstrated competitive performance compared to earlier stati
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2625092622	ocess, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016). However, there are still many open problems in NMT (Koehn and Knowles, 2017). One particularissueismistranslationofrarewords.For example, consider the Uzbek sentence: Source: Ammo muammolar hali ko&apos;p, deydi amerikalik olim Entoni Fauchi. Reference: But still there are ma
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2514713644	ought of as embeddings of the output vocabulary, and sometimes are in fact tied to the embeddings in the input layer, reducing model size while often achieving similar performance (Inan et al., 2017; Press and Wolf, 2017). We veried this claim on some language pairs and found out that this approach usually performs better than without tying, as seen in Table 1. For this reason, we always tie the target embeddings and
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2133564696	rds, and achieve improvements of up to + 4.5 BLEU, surpassing phrase-based translation in nearly all settings. 1 Introduction Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealingfortheirsingle-model,end-to-endtrainingprocess, and have demonstrated competitive performance compared to earlier statistical approaches (Koeh
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2410217169	text but doesn&apos;t necessarily correspond to the source word(s). Count-based statistical models, by contrast, don&apos;t have this problem, because they simply don&apos;t model any of this context.Arthur et al. (2016) try to alleviate this issue by integrating a count-based lexicon into an NMT system. However, this lexicon must be trained separately using GIZA ++ (Och and Ney,2003), and its parameters formalarge,s
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2339995566	ver, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benet from more advanced UNK - replacement or subword-level techni
2764124976	IMPROVING LEXICAL CHOICE IN NEURAL MACHINE TRANSLATION	2372425001	words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNK s by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNK s by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An
2765159719	One-shot and few-shot learning of word embeddings	2141599568	e this broad issue in the speciﬁc context of creating a useful representation for a new word based on its context. 1.1 BACKGROUND Continuous representations of words have proven to be very effective (Mikolov et al., 2013; Pennington et al., 2014, e.g.). These approaches represent words as vectors in a space, which are learned from 1 arXiv:1710.10280v2 [cs.CL] 2 Jan 2018 Under review as a conference paper at ICLR 2018
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2612364175	., 2012). Existing DDI efforts have focused on explicit mentions of interactions in single sentences (Gurulingappa et al., 2012; Percha et al., 2012; Segura-Bedmar et al., 2013). However, as shown by Peng et al. (2017), cross-sentence relation extraction increases the number of available relations. It is thus likely that cross-document interactions would further improve recall, which is of particular importance con
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1544827683	, and batch size 64 for 50 epochs. 6.2 Lexical Abstraction: Candidate Masking The presence of lexical regularities among answers is a problem in RC dataset assembly – a phenomenon already observed by Hermann et al. (2015). When comprehending a text, the correct answer should become clear from its context – rather than from an intrinsic property of the answer expression. To evaluate the ability of models to rely on con
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1525961042	015; Kumar et al., 2016) deﬁne a generic model class that iteratively attends over memory items deﬁned via text, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2015). One common characteristic of neural multi-hop models is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al.,
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2126209950	al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datasets are conﬁned to single d
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2738674657	al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datasets are conﬁned to single documents, and RC typically does not require a combination o
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2325237720	al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017b). All these methods show promise in single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evalua
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2586358499	ates are discarded, resulting in a loss of ˇ1% of the data. 3.2 Mitigating Dataset Biases Dataset creation is always fraught with the risk of inducing unintended errors and biases (Chen et al., 2016; Schwartz et al., 2017). As Hewlett et al. (2016) only carried out limited analysis of their WIKIREADING dataset, we present an analysis of the downstream effects we observe on WIKIHOP. Candidate Frequency Imbalance A ﬁrst
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2541244035	context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017b). All these methods show promise in single-document RC, and by design should be cap
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2262178610	or a cross-document multi-step RC task – as in this work. Learning Search Expansion Other research addresses expanding the document set available to a QA system, either in the form of web navigation (Nogueira and Cho, 2016), or via query reformulation techniques, which often use neural reinforcement learning (Narasimhan et al., 2016; Nogueira and Cho, 2017; Buck et al., 2017). While related, this work ultimately aims at
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1544827683	d based on FREEBASE (Berant et al., 2013; Bordes et al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2126276057	d that has been undergoing exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There is a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2131494463	ding Comprehension A rich collection of neural network models tailored towards multi-step RC has been developed over the last few years. Memory networks (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2016) deﬁne a generic model class that iteratively attends over memory items deﬁned via text, and they show promising performance on synthetic tasks requiring multi-step reasoning (Weston et al., 2015). On
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2738015883	document. Note that including other type-consistent candidates alongside a as end points in the graph traversal – and thus into the support documents – renders the task considerably more challenging (Jia and Liang, 2017). Models could otherwise identify a in the documents by simply relying on type consistency heuristics. It is worth pointing out that by introducing false candidates we counterbalance a type consistenc
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1544827683	e identify a in the documents by simply relying on type consistency heuristics. It is worth pointing out that by introducing false candidates we counterbalance a type consistency bias, in contrast to Hermann et al. (2015) and Hill et al. (2015). We will next describe how we apply this generic dataset construction methodology in two domains to create the WIKIHOP and MEDHOP datasets. 3 WIKIHOP WIKIPEDIA contains an abun
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2427527485	ed QA has witnessed a surge in interest with the advent of largescale datasets, which have been assembled based on FREEBASE (Berant et al., 2013; Bordes et al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et a
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2512077205	erant et al., 2013; Bordes et al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 2017). Besides TriviaQA (Joshi et al., 2017), all these datase
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2427527485	ext, we must move beyond a scenario where relevant information is coherently and explicitly stated within a single document. Methods with 2 Although annotators are encouraged to pose complex queries (Rajpurkar et al., 2016). arXiv:1710.06481v1 [cs.CL] 17 Oct 2017 this capability would beneﬁt search and Question Answering (QA) applications where the required information cannot be found in one location. They would also ai
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2521709538	f a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017; Shen et al., 2017a) have demonstrated that end-to-end language understanding approaches can infer answers directly from text – sidestepping intermediate query parsing and IE steps. Our work aims to evaluate whether en
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2127978399	hes centre around learning how to combine facts from a KB, i.e. in a structured form with pre-deﬁned schema. That is, they work as part of a pipeline, and either rely on output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017; Shen et al., 2017a) have demonstrated tha
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2427527485	IA has thus been used for a wealth of research to build datasets posing queries about a single sentence (Morales et al., 2016; Levy et al., 2017) or article (Yang et al., 2015a; Hewlett et al., 2016; Rajpurkar et al., 2016). However, no attempt has been made to construct a cross-document multi-step RC dataset based on WIKIPEDIA. A recently proposed RC dataset is WIKIREADING (Hewlett et al., 2016), where WIKIDATA tuples
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1525961042	igned for cross-document RC and multistep inference. There exist other multi-hop RC resources, but they are either very limited in size, such as the FraCaS test suite, or based on synthetic language (Weston et al., 2015). Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneﬁcial for re-ranking answers in opendomain non-factoid QA.
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2094728533	a KB, i.e. in a structured form with pre-deﬁned schema. That is, they work as part of a pipeline, and either rely on output of a previous IE step (Banko et al., 2007), or on direct human annotation (Bollacker et al., 2008) which tends to be costly and biased in coverage. However, recent neural RC methods (Seo et al., 2017; Shen et al., 2017a) have demonstrated that end-to-end language understanding approaches can infer
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2107598941	l as their interactions, found across multiple MEDLINE abstracts. For both datasets we draw upon existing Knowledge Bases (KBs), WIKIDATA and DRUGBANK, as ground truth, utilising distant supervision (Mintz et al., 2009) to induce the data – similar to Hewlett et al. (2016) and Joshi et al. (2017). We establish that for 74.1% and 68.0% of the samples, the answer can be inferred from the given documents by a human ann
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2558203065	le datasets, which have been assembled based on FREEBASE (Berant et al., 2013; Bordes et al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 2016), books (Hill et al., 2015; Paperno et al., 2016), science exams (Welbl et al., 2017), and trivia (Boyd-Graber et al., 2012; Dunn et al., 201
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2411480514	ments or 100 candidates are discarded, resulting in a loss of ˇ1% of the data. 3.2 Mitigating Dataset Biases Dataset creation is always fraught with the risk of inducing unintended errors and biases (Chen et al., 2016; Schwartz et al., 2017). As Hewlett et al. (2016) only carried out limited analysis of their WIKIREADING dataset, we present an analysis of the downstream effects we observe on WIKIHOP. Candidate Fre
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	154351976	ng exponential growth in the number of publications (Cohen and Hunter, 2004). The promise of applying NLP methods to cope with this increase has led to research efforts in IE (Hirschman et al., 2005; Kim et al., 2011) and QA for biomedical text (Hersh et al., 2007; Nentidis et al., 2017). There is a plethora of manually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can e
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2125444198	nt RC and multistep inference. There exist other multi-hop RC resources, but they are either very limited in size, such as the FraCaS test suite, or based on synthetic language (Weston et al., 2015). Fried et al. (2015) have demonstrated that exploiting information from other related documents based on lexical semantic similarity is beneﬁcial for re-ranking answers in opendomain non-factoid QA. Their method is relat
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2126209950	nts by simply relying on type consistency heuristics. It is worth pointing out that by introducing false candidates we counterbalance a type consistency bias, in contrast to Hermann et al. (2015) and Hill et al. (2015). We will next describe how we apply this generic dataset construction methodology in two domains to create the WIKIHOP and MEDHOP datasets. 3 WIKIHOP WIKIPEDIA contains an abundance of humancurated c
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1954715867	nually curated structured resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision for NLP systems (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1894439495	oducing synthetic links via dense latent embeddings. Several other multi-fact inference methods based on dense representations have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2016), and memory networks (Jain, 2016). Another approach is the Neural Theorem Prover (Rockt¨aschel and Riedel, 2017), which uses dense rule and symbol e
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2477209458	ommon characteristic of neural multi-hop models is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping m
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2534274346	p models is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2521709538	and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017b). All these methods show promise in single-document RC, and by design should be capable of integrating multiple facts across documents. However, thus far they have not been evaluated for a cross-doc
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2738674657	of the query, for MEDHOP there is only the single query type – interacts with. TF-IDF Retrieval-based models are known to be strong QA baselines if candidate answers are provided (Clark et al., 2016; Welbl et al., 2017). They search for individual documents based on keywords in the question, but typically do not combine information across documents. The purpose of this baseline is to see if it is possible to identif
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1576450670	resources (Ashburner et al., 2000; The UniProt Consortium, 2017) which can either serve as ground truth or to induce training data using distant supervision for NLP systems (Craven and Kumlien, 1999; Bobic et al., 2012). Existing RC datasets are either severely limited in size (Hersh et al., 2007) or cover a very diverse set of query types (Nentidis et al., 2017), complicating the application of neural models that h
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2471900581	s based on dense representations have been proposed, using composition functions such as vector addition (Bordes et al., 2014), RNNs (Neelakantan et al., 2015; Das et al., 2016), and memory networks (Jain, 2016). Another approach is the Neural Theorem Prover (Rockt¨aschel and Riedel, 2017), which uses dense rule and symbol embeddings to learn a differentiable backward chaining algorithm. All these previous a
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2252136820	s-document information. 7 Related Work Related Datasets End-to-end text-based QA has witnessed a surge in interest with the advent of largescale datasets, which have been assembled based on FREEBASE (Berant et al., 2013; Bordes et al., 2015b), WIKIPEDIA (Yang et al., 2015b; Rajpurkar et al., 2016; Hewlett et al., 2016), web search queries (Nguyen et al., 2016), news articles (Hermann et al., 2015; Onishi et al., 201
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	114118985	more scalable approach to composite rule learning is the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent the sparsity problems in PRA by introducing synthetic links via dense latent embeddings. Several other multi-fact inference methods based on dense representations have been proposed, using
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2133585753	t al., 2015). One common characteristic of neural multi-hop models is their rich structure that enables matching and interaction between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain t
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1756422141	t sparsity have been undertaken (Schoenmackers et al., 2008; Schoenmackers et al., 2010). A more scalable approach to composite rule learning is the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which performs random walks to identify salient paths between entities. Gardner et al. (2013) circumvent the sparsity problems in PRA by introducing synthetic links via dense latent embeddings. Seve
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1793121960	Text-Based Multi-Step Reading Comprehension A rich collection of neural network models tailored towards multi-step RC has been developed over the last few years. Memory networks (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2016) deﬁne a generic model class that iteratively attends over memory items deﬁned via text, and they show promising performance on synthetic tasks requiring multi-step reasoning (Wes
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	2417356443	tion between question, context, answer candidates and combinations thereof (Peng et al., 2015; Weissenborn, 2016; Xiong et al., 2016; Liu and Perez, 2017), which often is iterated over several times (Sordoni et al., 2016; Mark et al., 2016; Seo et al., 2016; Hu et al., 2017) and may contain trainable stopping mechanisms (Graves, 2016; Shen et al., 2017b). All these methods show promise in single-document RC, and by d
2765383698	Constructing Datasets for Multi-hop Reading Comprehension Across Documents	1604644367	y more complex than most previous tasks where distant supervision has been applied, the distant supervision assumption is only violated for 20% of the samples – a proportion similar to previous work (Riedel et al., 2010). These cases can either be due to conﬂicting information between WIKIDATA and WIKIPEDIA (8%), e.g. when the date of birth for a person differs between WIKIDATA and what is stated in the WIKIPEDIA art
2765627424	Phase Conductor on Multi-layered Attentions for Machine Comprehension	2427527485	6 71.07 ( 0:28) y 80.76 80.53 ( 0.22) y PhaseCond, QPAtt+ 71.85 71.60 ( 0.22) z 81.13 81.04 ( 0.17) z 3 EXPERIMENTS AND ANALYSIS This paper focuses on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to train and evaluate our model. SQuAD, which has gained a signiﬁcant attention recently, is a largescale dataset consisting of more than 100,000 questions manually created through crowdsourcing on 5
2765627424	Phase Conductor on Multi-layered Attentions for Machine Comprehension	2250539671	answers at the character level. 3.1 TRAINING DETAILS Our input for the encoding layer in Section 2.1 includes a list of commonly used features. We use pre-trained GloVe 100-dimensional word vectors (Pennington et al., 2014), parts-of-speech tag features, named-entity tag feature, and binary features of exact matching (Chen et al., 2017) which indicate if a passage word can be exactly matched to any question word and vic
2765627424	Phase Conductor on Multi-layered Attentions for Machine Comprehension	2171278097	ndancy between answer candidates (e.g., ”J.F.K” and ”Kennedy” can, in fact, be equivalent despite their different surface forms) that have been shown to be very effective during answer merging stage (Ferrucci et al., 2010). More generally, propagating evidence among the passage words allows correct answers to have better evidence for the question than the rest part of the passage. For a single self-attention layer, we
2765627424	Phase Conductor on Multi-layered Attentions for Machine Comprehension	2427527485	nslation (Bahdanau et al., 2015), image captioning (Xu et al., 2015), and speech recognition (Chorowski et al., 2015). Beneﬁting from the availability of large-scale benchmark datasets such as SQuAD (Rajpurkar et al., 2016), the attention-based neural networks has spread to machine comprehension and question answering tasks to allow the model to attend over past output vectors (Wang &amp; Jiang, 2017; Seo et al., 2017;
2765671566	Learning Neural Trans-Dimensional Random Field Language Models with Noise-Contrastive Estimation	1938755728	module, which contains a set of 1-D convolutional ﬁlters with widths ranging from 1to K. These ﬁlters explicitly model local contextual information (akin to modeling unigrams, bigrams, up to K-grams) [20]. Third, the output feature maps from multiple ﬁlters withvarying widths are spliced together, and fed into a few ﬁxed-width 1-D convolutions to further extract hierarchical features, which resembles
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2126209950	., ﬁnding the answer to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicting a blanked-out word of a sentence (Hill et al., 2015; Hermann et al., 2015). Visual question answering (VQA; Antol et al. (2015)) is a another related task where the aim is to provide a natural language answer to a question about an image. Our inferenc
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2274287116	4 We then map each frame to a 1,536-dimensional visual feature vector xv using the ﬁnal hidden layer of a pre-trained convolutional network which was optimized for object classiﬁcation (inception-v4; Szegedy et al. (2016)). Acoustic modality For each sentence, we extract the audio track from the video which includes all sounds and background music but no spoken dialog. We then obtain Mel-frequency cepstral coefﬁcient
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2427527485	achine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ Edinbu
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1525961042	ailment (Bowman et al., 2015; Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural lan
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2143449221	al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2118463056	alities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or v
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1895577753	of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al.
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1895577753	curate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed sever
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1486723856	e in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving ima
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2124660252	eed et al., 2005; Sang and Xu, 2010; Dimitrova et al., 2000). Although visual features are used mostly in isolation, in some cases they are combined with audio in order to perform video segmentation (Boreczky and Wilcox, 1998) or semantic movie indexing (Naphide and Huang, 2001). A few datasets have been released recently which include movies and textual data. MovieQA (Tapaswi et al., 2016) is a large-scale dataset which c
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2295759824	ervice (DVS), a narration service for the visually impaired. MovieDescription (Rohrbach et al., 2017) is a related dataset which contains sentences aligned to video clips from 200 movies. Scriptbase (Gorinski and Lapata, 2015) is another movie database which consists of movie screenplays (without video) and has been used to generate script summaries. In contrast to the story comprehension tasks envisaged in MovieQA and Mov
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1734113335	et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns re
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2119031011	as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to screenplays (Cour et al., 2008), plot synopses (Tapaswi et al., 2015), and books (Zhu et al., 2015) with the aim of imp
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1840435438	gether with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2136036867	l is, however, not static, it learns representations which evolve over time. VideoUnderstanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2144125753	ld-standard entity mention annotations are in color. Perpetrator mentions (e.g., Peter Berglund) are in green, while words referring to other entities are in red. classiﬁcation (Rasheed et al., 2005; Sang and Xu, 2010; Dimitrova et al., 2000). Although visual features are used mostly in isolation, in some cases they are combined with audio in order to perform video segmentation (Boreczky and Wilcox, 1998) or seman
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2064675550	lds. The sequential nature of the inference task lends itself naturally to recurrent network modeling. We adopt a generic architecture which combines a one-directional long-short term memory network (Hochreiter and Schmidhuber, 1997) with a softmax output layer over binary labels indicating whether the perpetrator is mentioned. Based on this architecture, we investigate the following questions: 1.What type of knowledge is necessa
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2064675550	licable to any episode or indeed any crime series. A sketch of our inference task is shown in Figure 4. The core of our model (see Figure 5) is a one-directional long-short term memory network (LSTM; Hochreiter and Schmidhuber (1997; Zaremba et al. (2014)). LSTM cells are a variant of recurrent neural networks with a more complex Figure 4: Overview of the perpetrator prediction task. The model receives input in the form of text,
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2148154194	music but no spoken dialog. We then obtain Mel-frequency cepstral coefﬁcient (MFCC) features from the continuous signal. MFCC features were originally developed in the context of speech recognition (Davis and Mermelstein, 1990; Sahidullah and Saha, 2012), but 4We also experimented with multiple frames per sentence but did not observe any improvement in performance. have also been shown to work well for more general sound c
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2125436846	n Answering A variety of question answering tasks (and datasets) have risen in popularity in recent years. Examples include reading comprehension, i.e., reading text and answering questions about it (Richardson et al., 2013; Rajpurkar et al., 2016), open-domain question answering, i.e., ﬁnding the answer to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question c
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2088850873	nd Saha, 2012), but 4We also experimented with multiple frames per sentence but did not observe any improvement in performance. have also been shown to work well for more general sound classiﬁcation (Chachada and Kuo, 2014). We extract a 13-dimensional MFCC feature vector for every ﬁve milliseconds in the video. For each input sentence, we sample ﬁve MFCC feature vectors from its associated time interval, and concatenat
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2028175314	ng text and answering questions about it (Richardson et al., 2013; Rajpurkar et al., 2016), open-domain question answering, i.e., ﬁnding the answer to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicting a blanked-out word of a sentence (Hill et al., 2015; Hermann et al., 2015). Visual question answering (VQA; Antol et al. (2015)) i
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2190067570	notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understan
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1566289585	ns in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to screenplays (Cour et al., 2008), plot synopses (Tapaswi et al., 2015), and books (Zhu et al., 2015) with the aim of improving scene prediction and semantic browsing. Other work uses low-level features (e.g., based on face detection) to establish social networks of main characters in order to summar
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2112184938	nterest in the problem of grounding language in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of g
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1591801644	ny crime series. A sketch of our inference task is shown in Figure 4. The core of our model (see Figure 5) is a one-directional long-short term memory network (LSTM; Hochreiter and Schmidhuber (1997; Zaremba et al. (2014)). LSTM cells are a variant of recurrent neural networks with a more complex Figure 4: Overview of the perpetrator prediction task. The model receives input in the form of text, images, and audio. Eac
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1933349210	oorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicting a blanked-out word of a sentence (Hill et al., 2015; Hermann et al., 2015). Visual question answering (VQA; Antol et al. (2015)) is a another related task where the aim is to provide a natural language answer to a question about an image. Our inference task can be viewed as a form of question answering over multi-modal data,
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2190067570	perform video segmentation (Boreczky and Wilcox, 1998) or semantic movie indexing (Naphide and Huang, 2001). A few datasets have been released recently which include movies and textual data. MovieQA (Tapaswi et al., 2016) is a large-scale dataset which contains 408 movies and 14,944 questions, each accompanied with ﬁve candidate answers, one of which is correct. For some movies, the dataset also contains subtitles, vi
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2427527485	question answering tasks (and datasets) have risen in popularity in recent years. Examples include reading comprehension, i.e., reading text and answering questions about it (Richardson et al., 2013; Rajpurkar et al., 2016), open-domain question answering, i.e., ﬁnding the answer to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicti
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1544827683	r to a question from a large collection of documents (Voorhees and Tice, 2000; Yang et al., 2015), and cloze question completion, i.e., predicting a blanked-out word of a sentence (Hill et al., 2015; Hermann et al., 2015). Visual question answering (VQA; Antol et al. (2015)) is a another related task where the aim is to provide a natural language answer to a question about an image. Our inference task can be viewed as
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1933349210	Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For exam
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	1514535095	s which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott a
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2145056192	Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural langu
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2130942839	tegy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The pr
2765715516	Whodunnit? Crime Drama as a Case for Natural Language Understanding	2125436846	tical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rocktaschel et al., 2016), and notably¨ question answering based on text (Hermann et al., 1Our dataset is available at h
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2479423890	age information into a visual semantic space [13] and image retrieval [14] have also been investigated. For more precise comprehension [15], captioning images densely for each object [16] or relation [17] has been done. Further work includes work on tasks for referring expressions [6], [18]–[20], and generating or comprehending discriminative sentences in a scene where similar objects exist. Visual qu
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2108598243	a candidate object detection model, which is described in Section IV, on our dataset. As a base CNN network [3] for feature extraction, we used a VGG16 model which was pretrained on ImageNet dataset [27]. The model is trained for 60,000 iterations by SGD (stochastic gradient descent) with momentum.4 Additionally, we employ data augmentation by randomly ﬂipping the images vertically, since our image d
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2123024445	The capability of comprehending images has been improved and measured through image captioning [11], [12] using neural network based models. Mapping language information into a visual semantic space [13] and image retrieval [14] have also been investigated. For more precise comprehension [15], captioning images densely for each object [16] or relation [17] has been done. Further work includes work on
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2193145675	will be described in the following sections. A. Candidate Object Detection To detect a bounding box for each object, we train an object detection model based on a Single Shot Multibox Detector (SSD) [3], which encodes regions in the input image efﬁciently and effectively. The model scores a large number of cropped regions in the image and outputs bounding boxes for candidate objects by ﬁltering out
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2194775991	and Destination Box Selection Model: We also trained an object comprehension model, described in Section IV, on our dataset. A CNN in the model uses the ﬁnal layer (after pooling) of 50-layer ResNet [28] pre-trained on ImageNet dataset [27]5 Training is performed by minimizing a max-margin loss with a margin m = 0:1 between correct sentence–object pairs and randomly-sampled incorrect pairs. Optimizat
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2193145675	g discriminative sentences in a scene where similar objects exist. Visual question answering [21] is also a related task that is answering single open-ended questions about an image. Object detection [3], [22], which plays an important role in our work, has been improved through several grand challenges such as PASCAL VOC [23] and MSCOCO [24]. In our task setting, the system tries to identify particu
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	1861492603	h region is scored purely based on its objectness [4], and also allows us to take advantage of existing large-scale data created for different domains or object classes (e.g., VOC PASCAL [23], MSCOCO [24]). Our model will also be able to detect unknown objects that do not appear in the training data by generalizing over all training objects. Our implementation of the modiﬁed SSD is built on top of rei
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2489434015	handle complex structures and cope with the diversity of unconstrained language, we combine and modify existing state-of-the-art models for object detection [3], [4] and object-referring expressions [5], [6] into an integrated system that can handle a wide variety of spoken expressions and their mapping to miscellaneous objects in a real-world environment. This modiﬁcation makes it possible to train
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	1861492603	ingle open-ended questions about an image. Object detection [3], [22], which plays an important role in our work, has been improved through several grand challenges such as PASCAL VOC [23] and MSCOCO [24]. In our task setting, the system tries to identify particular objects within an image described with a spoken expression. However, it is naive to assume that the referred object is always uniquely de
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2571175805	so been investigated. For more precise comprehension [15], captioning images densely for each object [16] or relation [17] has been done. Further work includes work on tasks for referring expressions [6], [18]–[20], and generating or comprehending discriminative sentences in a scene where similar objects exist. Visual question answering [21] is also a related task that is answering single open-ended
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	1895577753	iﬁcant progress in multimodal research related to computer vision and natural language processing. The capability of comprehending images has been improved and measured through image captioning [11], [12] using neural network based models. Mapping language information into a visual semantic space [13] and image retrieval [14] have also been investigated. For more precise comprehension [15], captioning
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2193145675	l-and-error process on the validation data. 1) Candidate Object Detection Model: We trained a candidate object detection model, which is described in Section IV, on our dataset. As a base CNN network [3] for feature extraction, we used a VGG16 model which was pretrained on ImageNet dataset [27]. The model is trained for 60,000 iterations by SGD (stochastic gradient descent) with momentum.4 Additional
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2240541372	l-world tasks, use of spoken language instructions is more intuitive than programming, and is more versatile than alternative communication methods such as touch panel user interfaces [1] or gestures [2] due to the possibility of referring to abstract concepts or the use of high-level instructions. Hence, using natural language as a means to interact between humans and robots is desirable. However, t
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2571175805	le complex structures and cope with the diversity of unconstrained language, we combine and modify existing state-of-the-art models for object detection [3], [4] and object-referring expressions [5], [6] into an integrated system that can handle a wide variety of spoken expressions and their mapping to miscellaneous objects in a real-world environment. This modiﬁcation makes it possible to train the
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2254252455	ls. Mapping language information into a visual semantic space [13] and image retrieval [14] have also been investigated. For more precise comprehension [15], captioning images densely for each object [16] or relation [17] has been done. Further work includes work on tasks for referring expressions [6], [18]–[20], and generating or comprehending discriminative sentences in a scene where similar objects
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2171361956	n signiﬁcant progress in multimodal research related to computer vision and natural language processing. The capability of comprehending images has been improved and measured through image captioning [11], [12] using neural network based models. Mapping language information into a visual semantic space [13] and image retrieval [14] have also been investigated. For more precise comprehension [15], capt
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2571175805	ng the relational features, our module compares the geometric features with all other objects in the environment, rather than with those in the same object class, as was done in the original model by [6]. This modiﬁcation allows us to build a generalized model that can recognize both seen and unseen objects, and also simpliﬁes our data creation process as we do not need to annotate each object with i
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2193145675	nstrained spoken language instructions. To handle complex structures and cope with the diversity of unconstrained language, we combine and modify existing state-of-the-art models for object detection [3], [4] and object-referring expressions [5], [6] into an integrated system that can handle a wide variety of spoken expressions and their mapping to miscellaneous objects in a real-world environment. T
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2571175805	tracted from the vision and text (speech) input are fed into the target object detection module, while only the text feature is fed into the destination box selection model. expression listener model [6], but we have additionally made a modiﬁcation to support zero-shot recognition of unseen objects. The target object recognition is formulated as a task to ﬁnd the best bounding box ^b from a set of pr
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2108598243	We also trained an object comprehension model, described in Section IV, on our dataset. A CNN in the model uses the ﬁnal layer (after pooling) of 50-layer ResNet [28] pre-trained on ImageNet dataset [27]5 Training is performed by minimizing a max-margin loss with a margin m = 0:1 between correct sentence–object pairs and randomly-sampled incorrect pairs. Optimization is performed for 120,000 iteratio
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2751439922	the training data by generalizing over all training objects. Our implementation of the modiﬁed SSD is built on top of reimplementation of the original SSD algorithm2 with Chainer [25] and Chainer-CV [26]. B. Target Object Selection After bounding boxes for all objects are recognized, we need to identify where the speciﬁed target object is located. Our object comprehension module is based on a referri
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	1933349210	urther work includes work on tasks for referring expressions [6], [18]–[20], and generating or comprehending discriminative sentences in a scene where similar objects exist. Visual question answering [21] is also a related task that is answering single open-ended questions about an image. Object detection [3], [22], which plays an important role in our work, has been improved through several grand cha
2765804957	Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions	2505639562	vestigated. For more precise comprehension [15], captioning images densely for each object [16] or relation [17] has been done. Further work includes work on tasks for referring expressions [6], [18]–[20], and generating or comprehending discriminative sentences in a scene where similar objects exist. Visual question answering [21] is also a related task that is answering single open-ended questions a
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	1731081199	. In order to add such a constraint, we train a neural network, which we will refer to as the discriminator, to classify between the encoding of source sentences and the encoding of target sentences (Ganin et al., 2016). The discriminator operates on the output of the encoder, which is a sequence of latent vectors (z 1;:::;z m), with z i 2Rn, and produces a binary prediction about the language of the encoder input s
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2101105183	we do not have access to parallel sentences to judge how well our model translates, not even at validation time. Therefore, we propose the surrogate criterion which we show correlates well with BLEU (Papineni et al., 2002), the metric we care about at test time. For all sentences xin a domain ‘ 1, we translate these sentences to the other domain ‘ 2, and then translate the resulting sentences back to ‘ 1. The quality o
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	1508577659	amounts of monolingual data. There have been several attempts at leveraging monolingual data to improve the quality of machine translation systems in a semi-supervised setting (Munteanu et al., 2004; Irvine, 2013; Irvine &amp; Callison-Burch, 2015; Zheng et al., 2017). Most notably, Sennrich et al. (2015a) proposed a very effective data-augmentation scheme, dubbed “back-translation”, whereby an auxiliary tran
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2133564696	ility of being the next one. The process is repeated until the decoder generates a stop symbol indicating the end of the sequence. In this article, we use a sequence-to-sequence model with attention (Bahdanau et al., 2015). The encoder is a bidirectional-LSTM which returns a sequence of hidden states z = (z 1;z 2;:::;z m). At each step, the decoder, which is an LSTM, takes the previous hidden state, the current word an
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2422843715	nal training data for the original translation system. Another way to leverage monolingual data on the target side is to augment the decoder with a language model (Gulcehre et al., 2015). And ﬁnally, Cheng et al. (2016); He et al. (2016a) have proposed to add an auxiliary auto-encoding task on monolingual data, which ensures that a translated sentence can be translated back to the original one. All these works still
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2610245951	rk on zero-resource machine translation has also relied on labeled information, not from the language pair of interest but from other related language pairs (Firat et al., 2016; Johnson et al., 2016; Chen et al., 2017) or from other modalities (Nakayama &amp; Nishida, 2017; Lee et al., 2017). The only exception is the work by Ravi &amp; Knight (2011); Pourdamghani &amp; Knight (2017), where the machine translation
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2443536229	s parallel sentences, however. Previous work on zero-resource machine translation has also relied on labeled information, not from the language pair of interest but from other related language pairs (Firat et al., 2016; Johnson et al., 2016; Chen et al., 2017) or from other modalities (Nakayama &amp; Nishida, 2017; Lee et al., 2017). The only exception is the work by Ravi &amp; Knight (2011); Pourdamghani &amp; Kni
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	1731081199	tribution using an adversarial regularization term, whereby the model tries to fool a discriminator which is simultaneously trained to identify the language of a given latent sentence representation (Ganin et al., 2016). This procedure is then iteratively repeated, giving rise to translation models of increasing quality. To keep our approach fully unsupervised, we initialize our algorithm by using a na¨ıve unsupervi
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2493916176	UPERVISED DICTIONARY LEARNING To implement our baseline and also to initialize the embeddings Zof our model, we ﬁrst train word embeddings on the source and target monolingual corpora using fastText (Bojanowski et al., 2017), and then we apply the unsupervised method proposed by Conneau et al. (2017) to infer a bilingual dictionary which can be use for word-by-word translation. Since WMT yields a very large-scale monolin
2765961751	Unsupervised Machine Translation Using Monolingual Corpora Only	2133564696	wo language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time. 1 INTRODUCTION Thanks to recent advances in deep learning (Sutskever et al., 2014; Bahdanau et al., 2015) and the availability of large-scale parallel corpora, machine translation has now reached impressive performance on several language pairs (Wu et al., 2016). However, these models work very well only
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2265289447	(Qiu and Huang 2015; Severyn and Moschitti 2015) can effectively capture compositions of n-grams and their relations in questions and answers. Inner-Attention (Wang, Liu, and Zhao 2016) and MV-LSTM (Wan et al. 2015) can model complex interaction betwen questions and answers through recurrent neural network based architectures. More studies on text matching for question answering can be found in (Tan, Xiang, and
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2399880602	; Serban et al. (Serban et al. 2016b) extended the sequenceto-sequence model to a hierarchical encoder-decoder structure; and under this structure, they further proposed two variants including VHRED (Serban et al. 2017) and MrRNN (Serban et al. 2016a) to introduce latent and explicit variables into the generation process. Upon these methods, reinforcement learning technique (Li et al. 2016b) and adversarial learning
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2131876387	found in (Tan, Xiang, and Zhou 2015; Liu et al. 2016; Liu, Qiu, and Huang 2016; Wan et al. 2016; He and Lin 2016; Yin et al. 2015; Yin and Schütze 2015). In web search, Shen et al. and Huang et al. (Shen et al. 2014; Huang et al. 2013) built a neural network with tri-letters to alleviate mismatching of queries and documents due to spelling errors. In textual entailment, the model in (Rocktäschel et al. 2015) uti
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	1910529161	ang, Lu, and Li 2015; Serban et al. 2016b; Vinyals and Le 2015; Li et al. 2016a; Xing et al. 2016; Mou et al. 2016) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Zhou et al. 2016; Yan, Song, and Wu 2016). Generation-based methods generate responses with natural language generation models learnt from the conversation data, while retrieval-based methods re-use
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	836999996	election because there will be different proper responses if we exchange the third turn and the last turn. Existing work, including the recurrent neural network architectures proposed by Lowe et al. (Lowe et al. 2015), the deep learning to respond architecture proposed by Yan et al. (Yan, Song, 2 Wu et al.A Sequential Matching Framework for Multi-turn Response Selection in Retrieval-based Chatbots and Wu 2016), an
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2891416139	erformed matching with a deep neural network architecture; Zhou et al. (Zhou et al. 2016) adopted an utterance view and a word view in matching to model relationships among utterances; and Wu et al. (Wu et al. 2017) proposed a sequential matching network that can capture important information in contexts and model relationships among utterances in a uniﬁed form. Our work belongs to retrieval based methods. It is
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	836999996	as follows: g(s;r) = m(h(f(u 1);:::;f(u n));f0(r)): (1) The existing models are special cases under the framework with different deﬁnitions of f(), h(), f0() and m(;). Speciﬁcally, the RNN models in (Lowe et al. 2015) can be deﬁned as m rnn(s;r) = ˙  h rnn (f rnn(u 1);:::;f rnn(u n)) &gt;Mf0 rnn (r) + b  ; (2) where M is a linear transformation, b is a bias, and ˙() is a sigmoid function. 8u i = fw u i;1;:::;w u
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	836999996	under the framework. 4. A Framework for the Existing Models Before us, there are a few studies on context-response matching for response selection in multiturn conversation. For example, Lowe et al. (Lowe et al. 2015) match a context and a response with recurrent neural networks (RNNs); Yan et al. (Yan, Song, and Wu 2016) present a deep learning to respond architecture for multi-turn response selection; and Zhou e
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2891416139	nclude that SCN is faster and easier to parallelize than SAN. We test the performance of SCN and SAN on two public data sets: Ubuntu Dialogue Corpus (Lowe et al. 2015) and Douban Conversation Corpus (Wu et al. 2017). The Ubuntu corpus is a large scale English data set in which negative instances are randomly sampled and dialogues are collected from a speciﬁc domain; while the Douban corpus is a newly published C
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	1966443646	ng techniques have proven effective on capturing semantic relations between text pairs in a variety of NLP tasks. For example, in question answering, covolutional neural networks (Qiu and Huang 2015; Severyn and Moschitti 2015) can effectively capture compositions of n-grams and their relations in questions and answers. Inner-Attention (Wang, Liu, and Zhao 2016) and MV-LSTM (Wan et al. 2015) can model complex interaction be
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	1910529161	ngle-turn scenario, matching is conducted between a message and a response. For example, Hu et al. (Hu et al. 2014) proposed message-response matching with convolutional neural networks; Wang et al. (Wang et al. 2015) incorporated syntax information into matching; Ji et al. (Ji, Lu, and Li 2014) combined a bunch of matching features, such as cosine, topic similarity, and translation score, to rank response candida
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2127426251	nn() in RNN models with a composition of CNN and RNN to model both composition of n-grams and their sequential relationship, and we can replace the m rnn() with a more powerful neural tensor network (Socher et al. 2013). Third, the framework unveils the limitations the existing models 9 Computational Linguistics Volume 1, Number 1 u 1 u n r Matching prediction Sequential Matching Framework u 2 h() m() Matching accum
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	889023230	nted sequence-to-sequence model. In multi-turn conversation, Sordoni et al. (Sordoni et al. 2015) compressed a context to a vector with a multi-layer perceptron in response generation; Serban et al. (Serban et al. 2016b) extended the sequenceto-sequence model to a hierarchical encoder-decoder structure; and under this structure, they further proposed two variants including VHRED (Serban et al. 2017) and MrRNN (Serb
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	836999996	osine, topic similarity, and translation score, to rank response candidates. In multi-turn conversation, matching requires taking the entire context into consideration. In this scenario, Lowe et al. (Lowe et al. 2015) employed a dual LSTM model to match a response with the literal concatenation of utterances in a context; Yan et al. (Yan, Song, and Wu 2016) reformulated the input message with the utterances in its
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2127426251	performance by ablating SCN last and SAN last. Table 6 reports the results of ablation on the test data. First, we replaced the utterance-response matching module in SCN and SAN with a neural tensor (Socher et al. 2013) (denoted as Replace M) which matches an utterance and a response by feeding their representations to a neural tensor network (NTN). The result is that the performance of the two models dropped dramat
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2891416139	plore the logic consistency problem in retrieval-based chatbots by leveraging more features. (2) No valid candidates. Another serious issue is the quality of candidates after retrieval. According to (Wu et al. 2017), the candidate retrieval method can be described as follows: given a message u n with fu 1;:::;u n 1gutterances in its previous turns, the top 5 keywords are extracted from fu 1;:::;u n 1gbased on th
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2170738476	ply to a new input. The key to response selection is how to match the input with a response. In a single-turn scenario, matching is conducted between a message and a response. For example, Hu et al. (Hu et al. 2014) proposed message-response matching with convolutional neural networks; Wang et al. (Wang et al. 2015) incorporated syntax information into matching; Ji et al. (Ji, Lu, and Li 2014) combined a bunch o
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2170738476	proposed generation-based methods (Shang, Lu, and Li 2015; Serban et al. 2016b; Vinyals and Le 2015; Li et al. 2016a; Xing et al. 2016; Mou et al. 2016) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Zhou et al. 2016; Yan, Song, and Wu 2016). Generation-based methods generate responses with natural language generation models learnt from the conversation dat
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2153579005	rameters on the validation sets. All models were implemented using the Theano framework (Theano Development Team 2016). Word embeddings in neural networks were initialized by the results of word2vec (Mikolov et al. 2013) 5 pre-trained on the training data. We did not use Glove (Pennington, Socher, and Manning 2014) because the Ubuntu corpus contains many technical words that are not covered by Twitter or Wikipedia. T
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	836999996	retically analyze efﬁciency of SCN and SAN, and conclude that SCN is faster and easier to parallelize than SAN. We test the performance of SCN and SAN on two public data sets: Ubuntu Dialogue Corpus (Lowe et al. 2015) and Douban Conversation Corpus (Wu et al. 2017). The Ubuntu corpus is a large scale English data set in which negative instances are randomly sampled and dialogues are collected from a speciﬁc domain
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2110485445	t r= (w r;1;:::;w r;n r ), then f0 rnn (r) is deﬁned as f0 rnn (r) = RNN(w~ r;1;:::;w~ r;k;:::;w~ r;n r ); (4) where w~ r;k is the embedding of the k-the word in r, and RNN() is either a vanilla RNN (Elman 1990) or an RNN with long short-term memory (LSTM) units (Hochreiter and Schmidhuber 1997). RNN() takes a sequence of vectors as an input, and outputs the last hidden state of the network. Finally, the con
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	889023230	topics. To address the problem, researchers have considered leveraging the large amount of conversation data available on the internet, and proposed generation-based methods (Shang, Lu, and Li 2015; Serban et al. 2016b; Vinyals and Le 2015; Li et al. 2016a; Xing et al. 2016; Mou et al. 2016) and retrieval-based methods (Wang et al. 2013; Hu et al. 2014; Ji, Lu, and Li 2014; Wang et al. 2015; Zhou et al. 2016; Yan,
2765963752	A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots	2891416139	ve responses and negative responses is 1 : 1 in the training set, and 1 : 9 in both the validation set and the test set. In addition to Ubuntu Dialogue Corpus, we selected Douban Conversation Corpus (Wu et al. 2017) as another data set. The data is a recently released large scale open domain conversation corpus in which conversations are crawled from a popular Chinese forum Douban Group 4. The training set conta
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2152180407	ed independent of the word embedding method and dimension. As an additional test we evaluated our model on the OnWN, FNWN (SemEval 2013) datasets. These datasets have been reported to be challenging (Agirre et al., 2013). On the one hand the OnWN dataset was shown to be well represented by the BoW baseline, ρ = 0.8431; however for SemEval competing systems it is hard to surpass such a performance. Our best result was
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2251803266	edding methods. These statistical estimates provide word embeddings performing well enough in general purpose NLP applications (Baroni and Lenci, 2010; Mikolov et al., 2013a; Pennington et al., 2014; Baroni et al., 2014; Bojanowski et al., 2016). The problem of modeling sentences is still open. For the cases of documents or words, most applications expect representations encoding text content or word use. Nonetheles
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	113724690	these issues is to perform massive comparisons by considering the content of sentences or short snippets of text. These comparisons can be done by means of Semantic Textual Similarity (STS) systems (Hatzivassiloglou et al., 1999; Agirre et al., 2012). An STS system computes a similarity score (a real value) between a pair of sentences. This score indicates how similar the sentences of the pair are. Most STS systems incorpora
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2251861449	sive comparisons by considering the content of sentences or short snippets of text. These comparisons can be done by means of Semantic Textual Similarity (STS) systems (Hatzivassiloglou et al., 1999; Agirre et al., 2012). An STS system computes a similarity score (a real value) between a pair of sentences. This score indicates how similar the sentences of the pair are. Most STS systems incorporate a number of supervi
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2251861449	tems. There are a number of proposed methods to assess semantic similarity between pairs of sentences. Most of these proposals emerge from benchmarks like the one used in the SemEval STS competition (Agirre et al., 2012). Given a pair of sentences, the aim of the STS task is to determine a similarity score (a real value). This score indicates just how similar the sentences of the pair are. The higher the score is, th
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2251291469	ty scores (Partial={Yes/No}), use of external (knowledge) resources (Resources). System/method sts/ repr. Parsing Resources Partial Access Rychalska et al. (2016) sts Dependency WordNet Supervised No Han et al. (2013) sts Chunking WordNet Yes No Sultan et al. (2014) sts Chunking, named entities, dependency WordNet Yes No UWBunsup (Brychcın and Svoboda, 2016) sts Chunking No No No LSA repr. No No No Yes BoW repr. N
2766034339	Unsupervised sentence representations as word information series: Revisiting TF–IDF	2161186120	u¨tze, 2015), semantic similarity/ 2 relatednessandsentiment classiﬁcation(Arroyo-Ferna´ndez and Meza Ruiz,2017; Chen et al., 2017; De Boom et al., 2016; Kalchbrenner et al., 2014; Onan et al., 2017; Yazdani and Popescu-Belis, 2013). The usefulness of vector representation methods mainly depends on the characteristics of the text to be embedded into vector spaces (Salton and Buckley, 1988). On the one hand, most embedding method
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	1816313093	07) to tokenize and lowercase both sides of the texts. Then we concatenate all ﬁve TED/TEDx development and test corpus to form a test set containing 6750 sentence pairs. We apply byte-pair encoding (Sennrich et al., 2016) to transform the texts to subword level so that the vocabulary has a size of 20K for each language. For evaluation, we report tokenized BLEU using “multi-bleu.perl”. The ASPEC dataset contains 300M b
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2250539671	argmin C;E 1 jVj X w2V jj XM i=1 Ei(Ci w) E~(w)jj2 ; (4) where jVjis the vocabulary size. The baseline embeddings can be a set of pre-trained vectors such as word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014) embeddings. In Eq. 3, the baseline embedding matrix E~ is approximated by Mcodewords selected from Mcodebooks. The selection of codewords is controlled by the code Cw. Such problem of learning compac
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2113459411	ccl package, so that one round of code learning takes around 15 minutes to complete. 5.2 SENTIMENT ANALYSIS Dataset: For sentiment analysis, we use a standard separation of IMDB movie review dataset (Maas et al., 2011), which contains 25k reviews for training and 25K reviews for testing purpose. We lowercase and tokenize all texts with the nltk package. We choose the 300-dimensional uncased Glove word vectors (trai
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2513419314	Courbariaux et al., 2014; Anwar et al., 2015), quantization (Chen et al., 2015; Han et al., 2016; Zhou et al., 2017), network pruning (LeCun et al., 1989; Hassibi &amp; Stork, 1992; Han et al., 2015; Wen et al., 2016) and knowledge distillation (Hinton et al., 2015). Network quantization such as HashedNet (Chen et al., 2015) forces the weight matrix to have few real weights, with a hash function to determine the w
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2114766824	ion computation (Vanhoucke et al., 2011; Hwang &amp; Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015), quantization (Chen et al., 2015; Han et al., 2016; Zhou et al., 2017), network pruning (LeCun et al., 1989; Hassibi &amp; Stork, 1992; Han et al., 2015; Wen et al., 2016) and knowledge distillation (Hinton et al., 2015). Network quantization such as HashedNet (Chen et al., 2015) forces the weight matrix t
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2153579005	jVj X w2V jjE(Cw) E~(w)jj2 (3) =argmin C;E 1 jVj X w2V jj XM i=1 Ei(Ci w) E~(w)jj2 ; (4) where jVjis the vocabulary size. The baseline embeddings can be a set of pre-trained vectors such as word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014) embeddings. In Eq. 3, the baseline embedding matrix E~ is approximated by Mcodewords selected from Mcodebooks. The selection of codewords is controlled by the code
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	1938755728	ness with the Gumbel-Softmax trick for producing compositional codes. As an alternative to our approach, one can also reduce the number of unique word types by forcing a character-level segmentation. Kim et al. (2016) proposed a character-based neural language model, which applies a convolutional layer after the character embeddings. Botha et al. (2017) propose to use char-gram as input features, which are further
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2119144962	Net (Chen et al., 2015) forces the weight matrix to have few real weights, with a hash function to determine the weight assignment. To capture the non-uniform nature of the networks, DeepCompression (Han et al., 2016) groups weight values into clusters based on pre-trained weight matrices. The weight assignment for each value is stored in the form of Huffman codes. However, as the embedding matrix is tremendously
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2460130460	ork sparse. Iterative pruning (Han et al., 2015) prunes a weight value if its absolute value is smaller than a threshold. The remaining network weights are retrained after pruning. Some recent works (See et al., 2016; Zhang et al., 2017) also apply iterative pruning to prune 80% of the connections for neural machine translation models. In this paper, we compare the proposed method with iterative pruning. The prob
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2119144962	orks for compressing neural networks include low-precision computation (Vanhoucke et al., 2011; Hwang &amp; Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015), quantization (Chen et al., 2015; Han et al., 2016; Zhou et al., 2017), network pruning (LeCun et al., 1989; Hassibi &amp; Stork, 1992; Han et al., 2015; Wen et al., 2016) and knowledge distillation (Hinton et al., 2015). Network quantization such as
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	587794757	s is fairly easy to implement, and does not require modiﬁcations to other parts in the neural network. 2 RELATED WORK Existing works for compressing neural networks include low-precision computation (Vanhoucke et al., 2011; Hwang &amp; Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015), quantization (Chen et al., 2015; Han et al., 2016; Zhou et al., 2017), network pruning (LeCun et al., 1989; Hassibi &amp; Stork
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2194775991	s to improve the performance. The model has a standard bi-directional encoder composed of two LSTM layers similar to Bahdanau et al. (2015). The decoder contains two LSTM layers. Residual connection (He et al., 2016) with a scaling factor of p 1=2 is applied to the two decoder states to compute the outputs. All LSTMs and embeddings have 256 hidden units in the 7 Under review as a conference paper at ICLR 2018 IWS
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2172166488	TED WORK Existing works for compressing neural networks include low-precision computation (Vanhoucke et al., 2011; Hwang &amp; Sung, 2014; Courbariaux et al., 2014; Anwar et al., 2015), quantization (Chen et al., 2015; Han et al., 2016; Zhou et al., 2017), network pruning (LeCun et al., 1989; Hassibi &amp; Stork, 1992; Han et al., 2015; Wen et al., 2016) and knowledge distillation (Hinton et al., 2015). Network qu
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2409591106	tly linearly transformed to 600-dimensional vectors before computing the ﬁnal softmax. Dropout with a rate of 0.2 is applied everywhere except the recurrent computation. We apply Key-Value Attention (Miller et al., 2016) to the ﬁrst decoder, where the query is the sum of the feedback embedding and the previous decoder state and the keys are computed by linear transformation of encoder states. Training details: All mo
2766061613	Compressing Word Embeddings via Deep Compositional Code Learning	2739997338	of unique word types by forcing a character-level segmentation. Kim et al. (2016) proposed a character-based neural language model, which applies a convolutional layer after the character embeddings. Botha et al. (2017) propose to use char-gram as input features, which are further hashed to save space. Generally, using characterlevel inputs requires modiﬁcations to the model architecture. Moreover, some Asian langua
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1895577753,1948566616	[20]. Adaptations of this framework have demonstrated state-of-the-art performance in many generative tasks, such as machine translation [5, 6, 21], and conversation modelling and response generation [22, 17]. Implementations based on the encoder-decoder framework work by mapping sequences of source tokens to sequences of target tokens. We adapt the Sequence-to-Sequence model to the requirements of Semant
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1836465849	agate vectors of dierent orders of magnitude leading to the explosion of the gradients phenomenon. However, nding the appropriate values to initialise the models’ parameters is certainly not trivial [30]. In order to sidestep this problem, we use Batch Normalisation before each nonlinear activation function and after each fully-connected layer both on the encoder and the decoder side, and we initiali
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2154764394	for data-to-text generation either focus mainly on creating a small, domain-specic corpus where data and text are manually aligned by a small group of experts, such as the WeatherGov [7] and RoboCup [8] datasets, or rely heavily on crowdsourcing [9], which makes them costly to apply for large domains. Our second contribution is an automatic approach for building a large datato-text corpus of rich li
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2130942839	employed in machine translation problems, they tend to produce low quality approximations [27]. A compromise between a strictly-greedy decoding algorithm and Viterbi is to adopt a beam-search decoder [6], which provides us with the B-most-probable summaries or hypotheses given a set of triples Fas input. The decoder maintains only a small number of Bhypotheses (i.e. partially completed summaries) and
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2157331557	equence-to-Sequence framework, within which an RNN-based encoder encapsulates the information that exists in a sequence, and an RNN-based decoder that generates a new sequence from this encapsulation [5, 6]. However, since the triples that we use in our problem are not sequentially correlated, we propose a concatenation-based formulation that enables us to capture the information across all the triples
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1645937837	evaluation of the model. Section 6 summarises the contributions of this work and outlines future plans. 2. Related Work Models for NLG can be divided into two groups: statistical and rule-based ones [13]. The latter employ linguistic expertise and work in three dierent phases:(i) document planning or content selection, (ii) microplanning and (iii) surface realisation [13, 4]. During document plannin
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1982430640	Examples include systems that generate text in domains with limited linguistic variability, such as clinical narratives [14], summaries of football matches [2], and, descriptions of museum’s exhibits [3]. Further Semantic Web oriented NLG applications can be found in [4]. Our work naturally lies on the path opened by recent unsupervised [15] and distant-supervision [16] based approaches for the extra
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2304545146	included in the xed target vocabulary would substantially limit the model, causing unnecessary repetition of this particular token in the generated summaries. Inspired by the Multi-Placeholder model [11], we rst attempt to match a rare entity that has been annotated in the text, in the subjects or the objects of the allocated triples. In case it exists in the triples, then it is replaced by a placeho
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2157331557	out the inclusion of any hand-coded rules or templates. Previous work on neural network approaches shows their great potential at tackling a wide variety of NLP tasks ranging from machine translation [5, 6] to automatic response generation [17, 18], and to computing vector representations of words in a continuous semantic space [19]. Our approach is inspired by the general encoder-decoder framework [5,
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2101105183	ines. First, we compute expected 13Perplexity measures the cross-entropy between the predicted sequence of words and the actual, empirical, sequence of words. 14BLEU (Bilingual Evaluation Understudy) [31] is precisionoriented metric for measuring the quality of generated text by comparing it to the actual, empirical text. BLEU-ncalculates a similarity scores based on the co-occurrence of up to n-grams
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2146579954	ly correct summary [34]. 18Adapted from [34] where they use an identical metric to evaluate the comprehensibility and readability of a generated question in natural language. 19Similar to coverage in [16] which measures the number of included sub-graphs in the text. 11 (a) DBpedia (b) Wikidata Figure 3: Performance of our models with the BLEU 4 metric across the dierence number of input triples on DB
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1924770834	nd to the input gate, the forget gate, the output gate and the cell respectively. 3.2.2. Gated Recurrent Unit (GRU). The GRU is a less complex variant of the LSTM cell [5] with comparable performance [25].  rl t u l t  =  sigm sigm  Wl  hl 1 h 1  ; (9) hel t = tanh(W l in h l 1 t + W l h!h (r l t h l t 1 )) ; (10) hl t = (1 u l t ) h l t 1 + u l t he t ; (11) where Wl : R2m !R2m is a biased line
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2064675550	nspired by the general encoder-decoder framework [5, 6] with multi-gated Recurrent Neural Network (RNN) variants, such as the Gated Recurrent Unit (GRU) [5] and the Long Short-Term Memory (LSTM) cell [20]. Adaptations of this framework have demonstrated state-of-the-art performance in many generative tasks, such as machine translation [5, 6, 21], and conversation modelling and response generation [22,
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1982430640	rate text from Semantic Web data. These systems worked in domains with small vocabularies and restricted linguistic variability, such as football match summaries [2] and museum exhibits’ descriptions [3]. However, the tedious repetition of their textual patterns along with the diculty of transferring the involved rules across dierent domains or languages prevented them from becoming widely accepted
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1591706642,2157331557	rent Unit (GRU) [5] and the Long Short-Term Memory (LSTM) cell [20]. Adaptations of this framework have demonstrated state-of-the-art performance in many generative tasks, such as machine translation [5, 6, 21], and conversation modelling and response generation [22, 17]. Implementations based on the encoder-decoder framework work by mapping sequences of source tokens to sequences of target tokens. We adapt
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2157331557	rom becoming widely accepted [4]. We address the above limitations by proposing a statistical model for NLG using neural networks. Our work explores how an adaptation of the encoder-decoder framework [5, 6] could be used to generate textual summaries for triples. More specically, given a set of triples about an entity (i.e. the entity appears as the subject or the object of the triples), our task consi
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1895577753	s or templates. Previous work on neural network approaches shows their great potential at tackling a wide variety of NLP tasks ranging from machine translation [5, 6] to automatic response generation [17, 18], and to computing vector representations of words in a continuous semantic space [19]. Our approach is inspired by the general encoder-decoder framework [5, 6] with multi-gated Recurrent Neural Netwo
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2146579954	scriptions of museum’s exhibits [3]. Further Semantic Web oriented NLG applications can be found in [4]. Our work naturally lies on the path opened by recent unsupervised [15] and distant-supervision [16] based approaches for the extraction of RDF verbalisation templates using parallel data-to-text corpora. However, rather than making a prediction about the template that would be the most appropriate
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2116716943	sting solutions for data-to-text generation either focus mainly on creating a small, domain-specic corpus where data and text are manually aligned by a small group of experts, such as the WeatherGov [7] and RoboCup [8] datasets, or rely heavily on crowdsourcing [9], which makes them costly to apply for large domains. Our second contribution is an automatic approach for building a large datato-text c
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	1843891098	t each timestep of the generation process. Even though, such greedy decoders have proven to be very fast when employed in machine translation problems, they tend to produce low quality approximations [27]. A compromise between a strictly-greedy decoding algorithm and Viterbi is to adopt a beam-search decoder [6], which provides us with the B-most-probable summaries or hypotheses given a set of triples
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2153579005	tackling a wide variety of NLP tasks ranging from machine translation [5, 6] to automatic response generation [17, 18], and to computing vector representations of words in a continuous semantic space [19]. Our approach is inspired by the general encoder-decoder framework [5, 6] with multi-gated Recurrent Neural Network (RNN) variants, such as the Gated Recurrent Unit (GRU) [5] and the Long Short-Term
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2157331557	tand layer depth lthat correspond to the input gate, the forget gate, the output gate and the cell respectively. 3.2.2. Gated Recurrent Unit (GRU). The GRU is a less complex variant of the LSTM cell [5] with comparable performance [25].  rl t u l t  =  sigm sigm  Wl  hl 1 h 1  ; (9) hel t = tanh(W l in h l 1 t + W l h!h (r l t h l t 1 )) ; (10) hl t = (1 u l t ) h l t 1 + u l t he t ; (11) whe
2766078582	Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples	2304545146	the verbalisation of the predicted entities in the generated summary. Conventional systems based on neural networks when employed on NLG tasks, such as Machine Translation [6] or Question Generation [11] are incapable of learning high quality vector representation for the infrequent tokens (i.e. either words or entities) in their training dataset. Inspired by [12, 11], we address this problem by adap
2766179782	Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks	2110485445	the encoder-decoder RNN are provided in the Appendix. Using the seq2seq framework, we tested a range of standard recurrent neural network models from the literature: simple recurrent networks (SRNs; Elman, 1990), long short-term memory networks (LSTMs; Hochreiter &amp; Schmidhuber, 1997), and gated recurrent units (GRUs; Chung et al., 2014). Recurrent networks with attention have become increasingly popular
2766179782	Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks	2613904329	mple inefﬁcient, requiring very large training sets, which suggests they may lack the same alge1Very recently, convolutional networks have reached comparable or superior performance on the same task (Gehring et al., 2017). We leave the investigation of their systematicity to future work. 1 arXiv:1711.00350v1 [cs.CL] 31 Oct 2017 Under review jump ) JUMP jump left ) LTURN JUMP jump around right ) RTURN JUMP RTURN JUMP R
2766179782	Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks	2133564696	s use it to test a wide range of modern recurrent network architectures in terms of their compositional skills. Our results suggest that, although standard architectures such as LSTMs with attention (Bahdanau et al., 2015) do generalize when novel examples feature a mixture of constructions that have been observed in training, the models are catastrophically affected by systematic differences between training and test
2766179782	Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks	2133564696	urrent networks with attention have become increasingly popular in the last few years, and thus we also tested each network with and without an attentional mechanism, using the attentional model from Bahdanau et al. (2015) (see Appendix for more details). Finally, to make the evaluations as systematic as possible, a large-scale hyperparameter search was conducted that varied the number of layers (1 or 2), the number of
2766182427	Unsupervised Neural Machine Translation	1816313093,2284660317	ges. In order to train our system in a true translation setting without violating the constraint of using nothing but monolingual corpora, we propose to adapt the backtranslation approach proposed by Sennrich et al. (2016a) to our scenario. More concretely, given an input sentence in a given language, we use the system in inference mode with greedy decoding to translate it to the other language (i.e. apply the shared
2766182427	Unsupervised Neural Machine Translation	2531207078	glish). 3 Under review as a conference paper at ICLR 2018 2. Shared encoder. Our system makes use of one and only one encoder that is shared by both languages involved, similarly to Ha et al. (2016), Lee et al. (2017) and Johnson et al. (2017). For instance, the exact same encoder would be used for both French and English. This universal encoder is aimed to produce a language independent representation of the inpu
2766182427	Unsupervised Neural Machine Translation	2133564696	l sentences, respectively. Our implementation is released as an open source project1. 1 INTRODUCTION Neural machine translation (NMT) has recently become the dominant paradigm to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014). As opposed to the traditional statistical machine translation (SMT), NMT systems are trained end-to-end, take advantage of continuous representations that greatly alleviate
2766182427	Unsupervised Neural Machine Translation	2133564696	the method to train it in an unsupervised manner. 3.1 SYSTEM ARCHITECTURE As shown in Figure 1, the proposed system follows a fairly standard encoder-decoder architecture with an attention mechanism (Bahdanau et al., 2014). More concretely, we use a two-layer bidirectional RNN in the encoder, and another two-layer RNN in the decoder. All RNNs use GRU cells with 600 hidden units (Cho et al., 2014), and the dimensionalit
2766182427	Unsupervised Neural Machine Translation	1816313093,2284660317	ng this translated sentence with the shared encoder and recovering the original sentence with the L1 decoder. Training alternates between sentences in L1 and L2, with analogous steps for the latter. (Sennrich et al., 2016a) into the training procedure to further improve results. Figure 1 summarizes this general schema of the proposed system. In spite of the simplicity of the approach, our experiments show that the pro
2766182427	Unsupervised Neural Machine Translation	2555428947	nnrich et al., 2016a). At the same time, Currey et al. (2017) showed that training an NMT system to directly copy target language text is also helpful and complementary with backtranslation. Finally, Ramachandran et al. (2017) pre-train the encoder and the decoder in language modeling. To the best of our knowledge, the more ambitious scenario where an NMT model is trained from monolingual corpora alone has never been explo
2766182427	Unsupervised Neural Machine Translation	1816313093,2284660317	parameters without any rigorous exploration. As for the corpus preprocessing, we perform tokenization and truecasing using standard Moses tools.3 We then apply byte pair encoding (BPE) as proposed by Sennrich et al. (2016b) using the implementation provided by the authors.4 Learning was done on the monolingual corpus of each language independently, using 50,000 operations. While BPE is known to be an effective way to
2766182427	Unsupervised Neural Machine Translation	1828724394	rios. 2.1 UNSUPERVISED CROSS-LINGUAL EMBEDDINGS Most methods for learning cross-lingual word embeddings rely on some bilingual signal at the document level, typically in the form of parallel corpora (Gouws et al., 2015; Luong et al., 2015a). Closer to our scenario, embedding mapping methods independently train the embeddings in different languages using monolingual corpora, and then learn a linear transformation th
2766182427	Unsupervised Neural Machine Translation	2130942839	sed NMT, and opens exciting opportunities for future research. 1 INTRODUCTION Neural machine translation (NMT) has recently become the dominant paradigm to machine translation (Bahdanau et al., 2014; Sutskever et al., 2014). As opposed to the traditional statistical machine translation (SMT), NMT systems are trained end-to-end, take advantage of continuous representations that greatly alleviate the sparsity problem, and
2766182427	Unsupervised Neural Machine Translation	2250600644	timated using either expectation maximization or Bayesian inference. This approach was shown to beneﬁt from the incorporation of syntactic knowledge of the languages involved (Dou &amp; Knight, 2013; Dou et al., 2015). More in line with our proposal, the use of word embeddings has also been shown to bring signiﬁcant improvements in statistical decipherment for machine translation (Dou et al., 2015). 2.3 LOW-RESOUR
2766182427	Unsupervised Neural Machine Translation	2250646737	train the embeddings in different languages using monolingual corpora, and then learn a linear transformation that maps them to a shared space based on a bilingual dictionary (Mikolov et al., 2013a; Lazaridou et al., 2015; Artetxe et al., 2016; Smith et al., 2017). While the dictionary used in these earlier work typically contains a few thousands entries, Artetxe et al. (2017) propose a simple self-learning extension
2766184602	Emergent Translation in Multi-Agent Communication	2738919465	al., 2016). It is also related to previous work on multiagent translation´ for low-resource language pairs (without grounding) (He et al., 2016a) and learning multimodal multilingual representations (Gella et al., 2017). It was recently shown that zero-resource translation is possible by separately learning an image encoder and a language decoder (Nakayama &amp; Nishida, 2017). The main difference to our work is tha
2766184602	Emergent Translation in Multi-Agent Communication	2402402867	better translators. 2 PRIOR WORK Recent work has used neural networks and reinforcement learning in multi-agent settings to solve a variety of tasks with communication, including simple coordination (Sukhbaatar et al., 2016), logic riddles (Foerster et al., 2016), complex coordination with verbal and physical interaction (Lowe et al., 2017), cooperative dialogue (Das et al., 2017) and negotiation (Lewis et al., 2017). At
2766184602	Emergent Translation in Multi-Agent Communication	2564324149	gotiation (Lewis et al., 2017). At the same time, there has been a surge of interest in communication protocols or languages that emerge from multi-agent communication in solving these various tasks. Lazaridou et al. (2017) ﬁrst showed that simple neural network agents can learn to coordinate in an image referential game with single-symbol bandwidth. This work has been extended to induce communication protocols that are
2766184602	Emergent Translation in Multi-Agent Communication	1816313093	lish and German captions using preprocessing scripts from Moses.2 In addition, we tokenize German captions into subword symbols using the byte pair encoding (BPE) algorithm with 10k merge operations (Sennrich et al., 2015). Baselines We compare against several baselines that similarly only make use of paired imagelanguage data. In increasing order of sophistication: Nearest neighbor To translate an English sentence int
2766184602	Emergent Translation in Multi-Agent Communication	2133564696	a longstanding challenge in artiﬁcial intelligence. Remarkable successes have been achieved in natural language processing (NLP) via the use of supervised learning approaches on large-scale datasets (Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Sennrich et al., 2017). Machine translation is no exception: most translation systems are trained to derive statistical patterns from huge parallel corpora. Pa
2766184602	Emergent Translation in Multi-Agent Communication	2564324149	simultaneously, and the agents are jointly trained to solve this task. We only allow agents to send a sequence of discrete symbols to each other, and never a continuous vector. Our task is similar to Lazaridou et al. (2017), but with the following differences: communication (1) is bidirectional and (2) of variable length; (3) the speaker is trained on both the listener’s feedback and ground-truth annotations; and (4) th
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2159640018	) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g.,Cho et al. (2014 );Wu et al. 2016 Shang et al. 2015 Vinyals and Le(2015);Rush et al.(2015);Yin et al.(2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017;Prakash et al.,2016;Gupta et al.,2018;Su and Yan,201
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2434014514	arning, and training of generator in RankGAN relies on parallel data while the training of RbM-IRL can use non-parallel data. There are connections between GAN and IRL as pointed byFinn et al.(2016a);Ho and Ermon (2016). However, there are signiﬁcant differences between GAN and our RbM-IRL model. GAN employs the discriminator to distinguish generated examples from real examples, while RbMIRL employs the evaluator as
2766462485	Paraphrase Generation with Deep Reinforcement Learning	174630521	e generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov &amp; Gelbukh, 2004; Kauchak &amp; Barzilay, 2006) and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-to-sequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation (Cho et al., 2014; Su
2766462485	Paraphrase Generation with Deep Reinforcement Learning	648786980	e during training. This technique is called teacher forcing. Learning with teacher forcing, the discrepancy between training and prediction can quickly accumulate errors along the generated sequence (Bengio et al., 2015; Ranzato et al., 2015). Therefore, the generator G is further ﬁne-tuned to maximize the expected cumulative reward in reinforcement learning (RL), which is given by the evaluator. 5 In the RL settin
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2581637843	erated examples from real examples, and they are trained in an adversarial way. There are applications of GAN on NLP, such as text generation (Yu et al.,2017;Guo et al.,2018) and dialogue generation (Li et al., 2017). RankGAN (Lin et al.,2017) is the one most similar to RbM-IRL that employs a ranking model as the discriminator. However, RankGAN works for text generation rather than sequenceto-sequence learning, a
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2098774185	the expert policy outperforms the other policies. There are different approaches to IRL. One approach is feature expectation matching (FEM), proposed in Abbeel &amp; Ng (2004) and further studied in Ziebart et al. (2008); Finn et al. (2016b); Ho et al. (2016). FEM assumes the reward function expressed as a linear combination of known features, and is solved by min-max optimization. Our work is based on another approa
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2167170026	hesaurus-based methods (Bolshakov and Gelbukh,2004;Kauchak and Barzilay,2006), grammar-based methods (Narayan et al.,2016), and statistical machine translation (SMT) based methods (Quirk et al. ,2004;Zhao et al. 2008 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarizati
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2401592218	icies. There are different approaches to IRL. One approach is feature expectation matching (FEM), proposed in Abbeel &amp; Ng (2004) and further studied in Ziebart et al. (2008); Finn et al. (2016b); Ho et al. (2016). FEM assumes the reward function expressed as a linear combination of known features, and is solved by min-max optimization. Our work is based on another approach, referred to as maximum-margin IRL (
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2101105183	on measure. Cross entropy can only work as a loose approximation of the semantic similarity, however. To tackle this problem, Ranzato et al. (2015) propose using sequence-level measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) and employing reinforcement learning (RL), speciﬁcally the REINFORCE algorithm (Williams, 1992), to optimize the measures as reward functions. However, the measures are calculat
2766462485	Paraphrase Generation with Deep Reinforcement Learning	1776056560	mplexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh,2004;Kauchak and Barzilay,2006), grammar-based methods (Narayan et al.,2016), and statistical machine translation (SMT) based methods (Quirk et al. ,20
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2170738476	n two sentences. A variety of learning techniques have been developed for matching short texts, from linear models (e.g., Wu et al. (2013)) to neural network based models (e.g., Socher et al. (2011); Hu et al. (2014)). Our idea in this work is to ﬁrst train a matching model that can identify paraphrases, and then tune the paraphrase generator based on the feedback from the matching model. In our implementation we
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2170738476	ng models are proposed for semantic matching. Most of them can also be adapted to paraphrase identiﬁcation, including but not limited to RNN-based methods (Wang &amp; Jiang, 2015), CNN-based methods (Hu et al., 2014) and attention-based methods (Parikh et al., 2016). We use the attention-based matching model (Parikh et al., 2016) in this work, since it is more efﬁcient and capable of capturing the phrase-level si
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2119717200	nt states and action. In our experiment, we try to use ROUGE-2 score as the reward, namely R(Y^) = ROUGE-2(Y;Y^ ), where Ydenotes the ground-truth reference. According to the policy gradient theorem (Williams, 1992; Sutton et al., 2000), the gradients of the expected return can be estimated by r Lb RL() = XT t=1 r logp (^y tjY^ 1:t 1;X)Q(^y t;X;Y^ 1:t 1) (11) 5 In order to reduce the variance of the above g
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2101105183	o that in Ranzato et al. (2015). We conduct both automatic and manual evaluation on the models. For the automatic evaluation, we adopt four evaluation measures: ROUGE-1, ROUGE-2 (Lin, 2004)1, BLEU-2 (Papineni et al., 2002)2 and METEOR (Lavie &amp; Agarwal, 2007)3. For the manual evaluation, we conduct evaluation on the generated paraphrases aggregated from all methods. As pointed out, ideally it would be better not to
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2487501366	or, only a positive reward Ris given at the end of sentence. This provides sparse supervision signals and can make the model greatly degenerate. Inspired by the idea of reward shaping (Ng et al.,1999;Bahdanau et al., 2017), we estimate the intermediate cumulative reward (value function) for each position, that is Q t= E p (Y t+1:TjY^ 1:t;X) R(X;[Y^ 1:t;Y t+1:T]); by Monte-Carlo simulation, in the same way as inYu et a
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2103305545	two sentences as paraphrases of each other. A variety of learning techniques have been developed for matching sentences, from linear models (e.g.,Wu et al.(2013)) to neural network based models (e.g.,Socher et al. (2011);Hu et al.(2014)). We choose a simple yet effective neural network architecture, called the decomposable-attention model (Parikh et al., 2016), as the evaluator. The evaluator can calculate the semant
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2119717200	put sequences, which can induce discrepancy between training and testing. To tackle this problem, Ranzato et al. (2015) propose using reinforcement learning (RL), speciﬁcally the REINFORCE algorithm (Williams, 1992), to directly optimize the sequence-level metric such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). However, using these metrics as reward functions for paraphrasing can be problematic, becaus
2766462485	Paraphrase Generation with Deep Reinforcement Learning	2154652894	tackle this problem,Ranzato et al.(2016) propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based measures such as BLEU (Papineni et al.,2002) and ROUGE (Lin, 2004) as a reward function. However, these lexical measures may not perfectly represent semantic similarity. It is likely that a correctly generated sequence gets a low ROUGE score due to lexical mismatch.
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2740663516	of dynamically updating wordrepresentations. Tracking and updating concepts, entities or sentences with dynamic memories is a very active research direction (Kumar et al., 2016; Henaff et al., 2017; Ji et al., 2017; Kobayashi et al., 2017). However, those works typically focus on particular tasks whereas our approach is taskagnostic and most importantly allows for the integration of external background knowledg
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2604685013	ical models of AI (Schank &amp; Abelson, 1977; Minsky, 2000); however, it has only recently begun to play a role in neural network models of NLU (Ahn et al., 2016; Xu et al., 2016; Long et al., 2017; Dhingra et al., 2017). However, previous efforts have focused on speciﬁc tasks or certain kinds of knowledge, whereas we take a step towards a more general-purpose solution for the integration of heterogeneous knowledge f
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2604685013	ion model. Xu et al. (2016) created a recall mechanism into a standard LSTM cell that retrieves pieces of external knowledge encoded by a single representation for a conversation model. Concurrently, Dhingra et al. (2017) exploit linguistic knowledge using MAGE-GRUs, an adapation of GRUs to handle graphs, however, external knowledge has to be present in form of triples. The main difference to our approach is that we i
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	1840435438	ller (on the order of hundreds) human veriﬁed part. We report results on both. See Appendix A.1 for implementation details. Recognizing Textual Entailment We test on the frequently used SNLI dataset (Bowman et al., 2015), a collection of 570ksentence pairs, and the more recent MultiNLI dataset (433ksentence pairs) (Williams et al., 2017). Given two sentences, a premise p and a hypothesis q, the task is to determine w
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2563734883	pproach is the notion of dynamically updating wordrepresentations. Tracking and updating concepts, entities or sentences with dynamic memories is a very active research direction (Kumar et al., 2016; Henaff et al., 2017; Ji et al., 2017; Kobayashi et al., 2017). However, those works typically focus on particular tasks whereas our approach is taskagnostic and most importantly allows for the integration of external ba
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2131494463	tant aspect of our approach is the notion of dynamically updating wordrepresentations. Tracking and updating concepts, entities or sentences with dynamic memories is a very active research direction (Kumar et al., 2016; Henaff et al., 2017; Ji et al., 2017; Kobayashi et al., 2017). However, those works typically focus on particular tasks whereas our approach is taskagnostic and most importantly allows for the integ
2766508367	Dynamic Integration of Background Knowledge in Neural NLU Systems	2740714844	ts about mentioned entities for neural language models. Bahdanau et al. (2017) and Long et al. (2017) create word embeddings on-the-ﬂy by reading word deﬁnitions prior to processing the task at hand. Pilehvar et al. (2017) seamlessly incorporate information about word senses into their representations before solving the downstream NLU task, which is similar. We go one step further by seamlessly integrating all kinds of
2766569996	Understanding Grounded Language Learning Agents	2729557715	both, as determined by the training regime) that narrows the space of possible referents, permitting faster word learning as training progresses.2 These conclusions can be incorporated with those of Ritter et al. (2017), who observe a shape bias in convolutional networks trained on the ImageNet Challenge training set. Our experiments with a single canonical architecture exposed to different training stimuli indicate
2766569996	Understanding Grounded Language Learning Agents	2015861736	guish between objects of interest. It is well established that convolutional networks trained to classify images also exhibit differential specialisation of feature detectors between layers (see e.g. LeCun et al. (2010)). Layerwise attention provides a means to quantify the magnitude of this specialisation, and to measure the importance of each layer with respect 7Humans also learn basic level categories before supe
2766569996	Understanding Grounded Language Learning Agents	2620290674	mportance of curriculum training may be far greater for agents learning to act conditioned on language than those learning to map between linguistic inputs and outputs. Both Hermann et al. (2017) and Oh et al. (2017) observed that curricula were essential for agents learning to execute linguistic instructions that require both resolving of referring expressions (get the red ball..) and non-trivial action policies
2766569996	Understanding Grounded Language Learning Agents	2620290674	nd sequences of linguistic symbols not only in terms of the contemporaneous raw visual input, but also in terms of past visual input and the actions required to execute an appropriate motor response (Oh et al., 2017; Chaplot et al., 2017; Hermann et al., 2017; Misra et al., 2017). The most advanced such agents learn to execute a range of phrasal and multi-task instructions, such as ﬁnd the green object in the re
2766569996	Understanding Grounded Language Learning Agents	2611884151	ontemporaneous raw visual input, but also in terms of past visual input and the actions required to execute an appropriate motor response (Oh et al., 2017; Chaplot et al., 2017; Hermann et al., 2017; Misra et al., 2017). The most advanced such agents learn to execute a range of phrasal and multi-task instructions, such as ﬁnd the green object in the red room, pick up the pencil in the third room on the right or go t
2766569996	Understanding Grounded Language Learning Agents	2729557715	ral network-based models of grounded language learning, noting the parallels with research in neuroscience and psychology that aims to understand human language acquisition. Extending the approach of Ritter et al. (2017), we adapt various experimental techniques initially developed by experimental psychologists (Landau et al., 1988; Markman, 1990; Hollich et al., 2000). In line with typical experiments on humans, our
2766569996	Understanding Grounded Language Learning Agents	2293453011	ses, in many cases aligned to words, from unstructured pixel representations of large quantities of photographs (Krizhevsky et al., 2012). Visual question answering (VQA) systems (Antol et al., 2015; Xiong et al., 2016; Xu &amp; Saenko, 2016) must reconcile raw images with (arbitrary-length) sequences of symbols, in the form of natural language questions, in order to predict lexical or phrasal answers. Recently, si
2766569996	Understanding Grounded Language Learning Agents	2260756217	a simpliﬁed version of that proposed by Hermann et al. (2017), without auxiliary learning components. Weight updates are computed according to the asynchronous advantage actor-critic (A3C) algorithm (Mnih et al., 2016), in conjunction with the RMSProp update rule (Tieleman &amp; Hinton, 2012). During training, a single parameter vector is shared across 16 CPU cores, which offers a suitable tradeoff between training
2766569996	Understanding Grounded Language Learning Agents	2108598243	t to respond. Many neural network models also overcome a learning task that is – to varying degrees – analogous to early human word learning. Image classiﬁcation tasks such as the ImageNet Challenge (Deng et al., 2009) require models to induce discrete semantic classes, in many cases aligned to words, from unstructured pixel representations of large quantities of photographs (Krizhevsky et al., 2012). Visual questi
2766569996	Understanding Grounded Language Learning Agents	2729557715	tion, for better understanding semantic and visual processing in such agents. The application of experimental paradigms from cognitive psychology to better understand deep neural nets was proposed by Ritter et al. (2017), who observed that convolutional architectures exhibit a shape bias when trained on the ImageNet Challenge data. The ability to control precisely both training and test stimuli in our simulated envir
2767019613	EVALUATING DISCOURSE PHENOMENA IN NEURAL MACHINE TRANSLATION	2606032440	;Huang et al.,2016), multiencoder translation models have recently been used to incorporate extra-sentential linguistic context in purely textual NMT (Zoph and Knight, 2016;Libovicky and Helcl´ ,2017;Wang et al., 2017). Unlike multi-modal translation, which typically uses two complementary representations of the main input, for example a textual description and an image, linguistically contextual NMT has focused on
2767019613	EVALUATING DISCOURSE PHENOMENA IN NEURAL MACHINE TRANSLATION	2606032440	(see Fig.4c). We study three combination strategies here: concatenation, an attention gate and hierarchical attention. We also tested using the auxiliary context to initialise the decoder, similar toWang et al. (2017), which was ineffective in our experiments and which we therefore do not report in this paper. Attention concatenation The two context vectors c(1) i and c (2) i are concatenated and the resulting vec
2767019613	EVALUATING DISCOURSE PHENOMENA IN NEURAL MACHINE TRANSLATION	2606032440	er(2017), but with mixed results. A variety of multi-encoder strategies have also been tested, including using a representation of the previous sentence to initialise the main encoder and/or decoder (Wang et al., 2017) and using multiple attention mechanisms, with different strategies to combine the resulting context vectors, such as concatenation (Zoph and Knight,2016), hierarchical attention (Libovicky´ and Helcl
2767019613	EVALUATING DISCOURSE PHENOMENA IN NEURAL MACHINE TRANSLATION	2222949842	on multi-modal translation (Caglayan et al.,2016;Huang et al.,2016), multiencoder translation models have recently been used to incorporate extra-sentential linguistic context in purely textual NMT (Zoph and Knight, 2016;Libovicky and Helcl´ ,2017;Wang et al., 2017). Unlike multi-modal translation, which typically uses two complementary representations of the main input, for example a textual description and an image
2767019613	EVALUATING DISCOURSE PHENOMENA IN NEURAL MACHINE TRANSLATION	2222949842	ode? // Still some bugs... Et le code ? // Encore quelques bugs/#insectes... Recent work on multi-encoder neural machine translation (NMT) appears promising for the integration of linguistic context (Zoph and Knight, 2016;Libovicky and Helcl´ ,2017;Jean et al., 2017a;Wang et al.,2017). However models have almost only been evaluated using standard automatic metrics, which are poorly adapted to evaluating discourse phen
2767020241	LEARNING WITH LATENT LANGUAGE	2613312549	, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used t
2767020241	LEARNING WITH LATENT LANGUAGE	2119717200	.4 The learner is trained from task-speciﬁc expert policies using DAgger (Ross et al., 2011) during the language-learning phase, and adapts to individual environments using “vanilla” policy gradient (Williams, 1992) during the concept learning phase. The environment implementation and linguistic annotations are in this case adapted from a natural 4In the case of RL in particular, the contribution from L3 are ort
2767020241	LEARNING WITH LATENT LANGUAGE	1999121513	age strings. However, many closely-related ideas have been explored in the literature. String-valued latent variables are widely used in language processing tasks ranging from morphological analysis (Dreyer and Eisner, 2009) to sentence compression (Miao and Blunsom, 2016). Natural language annotations have been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et
2767020241	LEARNING WITH LATENT LANGUAGE	2601273560	anguage of strings is discrete, and whatever structure the interpretation function has is wrapped up inside the black box of f. Inspired by related techniques aimed at making synthesis more efﬁcient (Devlin et al., 2017), we use learning to help us develop an effective optimization procedure for natural language parameters. In particular, we simply use the language-learning datasets, consisting of pairs (x(‘i) j ;y (
2767020241	LEARNING WITH LATENT LANGUAGE	2604763608	ar, the contribution from L3 are orthogonal to those of meta-learning—one could imagine using a technique like RL2 (Duan et al., 2016) to generate candidate descriptions more efﬁciently, or use MAML (Finn et al., 2017) rather than zero-shot reward as the training criterion for the interpretation model. language navigation dataset originally introduced by Janner et al. (2017). In our version of the problem (Figure 7
2767020241	LEARNING WITH LATENT LANGUAGE	2601273560	always correct) we can do better than any of the models that have to do inference. Coupling our inference procedure with an oracle RE evaluator, we essentially recover the synthesis-based approach of Devlin et al. (2017). Our ﬁndings are consistent with theirs: when a complete and accurate execution engine is available, there is no reason not to use it. But we can get almost 90% of the way there with an execution mod
2767020241	LEARNING WITH LATENT LANGUAGE	2049311030	e P wj 1 n j rep(x j;y j) Baselines are analogous to those for classiﬁcation. While string editing tasks of the kind shown in Figure 5 are popular in both the programming by demonstration literature (Singh and Gulwani, 2012) and the semantic parsing literature (Kushman and Barzilay, 2013), we are unaware of any datasets that support both learning paradigms at the same time. We have thus created a new dataset of string ed
2767020241	LEARNING WITH LATENT LANGUAGE	2432717477	f(x(c) j ; ; (c));y(c) j (3) on the new dataset, then make predictions for new inputs using f(x(e); ;(c)). Closely related meta-learning approaches (e.g. Schmidhuber, 1987; Santoro et al., 2016; Vinyals et al., 2016) make use of the same data, but collapse the inner optimization over (c) and subsequent prediction of y(e) into a single learned model. 3 Learning with Language In this work, we are interested in dev
2767020241	LEARNING WITH LATENT LANGUAGE	1947481528	where qprovides a (suitably normalized) approximation to the distribution of descriptions given task data. In the running example, this proposal distribution is essentially an image captioning model (Donahue et al., 2015). By sampling from q, we expect to obtain candidate descriptions that are likely to obtain small loss. But our ultimate inference criterion is still the true model f: at evaluation time we perform the
2767020241	LEARNING WITH LATENT LANGUAGE	1792831685	t of the continuous space of weight vectors and into a discrete space of formal program descriptors (e.g. regular expressions or Prolog queries). Domain-speciﬁc structure like version space algebras (Lau et al., 2003) or type systems (Kitzelmann and Schmid, 2006) can be brought to bear on the search problem, and the bias inherent in the syntax of the formal language provides a strong prior. But while program synth
2767020241	LEARNING WITH LATENT LANGUAGE	2132525863	thm with additional kinds of training data. The approach we present in this paper combines elements of both, so we begin with a review of existing work. (Inductive) program synthesis approaches (e.g. Gulwani, 2011) reduce the effective size of the parameter space H by moving the optimization problem out of the continuous space of weight vectors and into a discrete space of formal program descriptors (e.g. regul
2767020241	LEARNING WITH LATENT LANGUAGE	2553882142	for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Andreas et al., 2017; Denil et al., 2017). Finally, natural language instructions available at concept-learning time (rather than languagelearning time) have been used to provide side information to reinforcement learner
2767020241	LEARNING WITH LATENT LANGUAGE	2123024445	ve been used in conjunction with training examples to guide the discovery of logical descriptions of concepts (Ling et al., 2017; Srivastava et al., 2017), and used as an auxiliary loss for training (Frome et al., 2013), analogously to the Meta+Joint baseline in this paper. Structured language-like annotations have been used to improve learning of generalizable structured policies (Andreas et al., 2017; Denil et al.
2767020241	LEARNING WITH LATENT LANGUAGE	1580222235	y, natural language instructions available at concept-learning time (rather than languagelearning time) have been used to provide side information to reinforcement learners about high-level strategy (Branavan et al., 2011), environment dynamics (Narasimhan et al., 2017) and exploration (Harrison et al., 2017). 8 Conclusion We have presented an approach for optimizing models in a space parameterized by natural language.
2767020241	LEARNING WITH LATENT LANGUAGE	2122223050	y proportional to a bilinear function of the encoded description and world state. f is effectively an instruction following model of a kind well-studied in the natural language processing literature (Branavan et al., 2009); the proposal model allows it to generate its own instructions without external direction. 20 40 60 80 100 Timestep ( 1000) 1:0 0:5 0:0 0:5 1:0 1:5 2:0 2:5 3:0 Average reward L3 Multitask Scratch Fig
2767289498	Dual Language Models for Code Switched Speech Recognition.	2099808146,2123162330,2536834305	dels to the individual segments [1, 2, 3]. (2) Employing a universal phone set to build acoustic models for the mixed speech and pairing it with standard language models trained on code-switched text [4, 5, 6, 7, 8]. There have been many past efforts towards enhancing the capability of language models for code-switched speech using additional sources of information such as part-of-speech (POS) taggers and statis
2767289498	Dual Language Models for Code Switched Speech Recognition.	1934041838	exities on the validation and test sets using both Good Turing [16] and 1We note that choosing fewer speakers in the development and test sets led to high variance in the observed results. Kneser-Ney [17] smoothing techniques. DLMs clearly outperform mixed LMs on both the datasets. All subsequent experiments use Kneser-Ney smoothed bigram LMs as they perform better than the Good Turing smoothed bigram
2767289498	Dual Language Models for Code Switched Speech Recognition.	2013489815	language models during decoding. Li et al. [10] propose combining a code-switch boundary predictor with both a translation model and a reconstruction model to build language models. (Solorio et. al. [11] were one of the ﬁrst works on learning to predict code-switching points.) Adel et al. [12] investigated how to effectively use syntactic and semantic features extracted from code-switched data within
2767289498	Dual Language Models for Code Switched Speech Recognition.	1524333225	nt of 1). This analysis provides further evidence for the need for LMs that generalize better at code-switching boundaries. 4.3. ASR experiments All the ASR systems were built using the Kaldi toolkit [15]. We used standard MFCC+delta+double-delta features with fMLLR transforms to build speaker-adapted triphone models with 4200 tied-state triphones, henceforth referred to as “SAT” models. We also build
2767289498	Dual Language Models for Code Switched Speech Recognition.	2094655846	redictor with both a translation model and a reconstruction model to build language models. (Solorio et. al. [11] were one of the ﬁrst works on learning to predict code-switching points.) Adel et al. [12] investigated how to effectively use syntactic and semantic features extracted from code-switched data within factored language models. Combining recurrent neural network-based language models with su
2767289498	Dual Language Models for Code Switched Speech Recognition.	2114569717	roadly categorized into two sets of approaches: (1) Detecting code-switching points in an utterance, followed by the application of monolingual acoustic and language models to the individual segments [1, 2, 3]. (2) Employing a universal phone set to build acoustic models for the mixed speech and pairing it with standard language models trained on code-switched text [4, 5, 6, 7, 8]. There have been many pas
2767289498	Dual Language Models for Code Switched Speech Recognition.	2134237567	wo monolingual bigram LMs. (The choice of bigram LMs instead of trigram LMs will be justiﬁed later in Section 5). Table 2 shows the perplexities on the validation and test sets using both Good Turing [16] and 1We note that choosing fewer speakers in the development and test sets led to high variance in the observed results. Kneser-Ney [17] smoothing techniques. DLMs clearly outperform mixed LMs on bot
2767321762	Breaking the Softmax Bottleneck: A High-Rank RNN Language Model	1938755728	8 Model #Param Validation Test Mikolov &amp; Zweig (2012) – RNN-LDA + KN-5 + cache 9Mz - 92.0 Zaremba et al. (2014) – LSTM 20M 86.2 82.7 Gal &amp; Ghahramani (2016) – Variational LSTM (MC) 20M - 78.6 Kim et al. (2016) – CharCNN 19M - 78.9 Merity et al. (2016) – Pointer Sentinel-LSTM 21M 72.4 70.9 Grave et al. (2016) – LSTM + continuous cache pointery - - 72.1 Inan et al. (2016) – Tied Variational LSTM + augmented
2767321762	Breaking the Softmax Bottleneck: A High-Rank RNN Language Model	2150884987	ontext of learning word embeddings. In a general sense, Mixture of Softmaxes proposed in this work can be seen as a particular instantiation of the long-existing idea called Mixture of Experts (MoE) (Jacobs et al., 1991). However, there are two core differences. Firstly, MoE has usually been instantiated as mixture of Gaussians to model data in continuous domains (Jacobs et al., 1991; Graves, 2013; Bazzani et al., 20
2767554854	Learning Filterbanks from Raw Speech for Phone Recognition	1542280630	, we believe that future end-to-end speech recognition system will learn directly from the waveform. There have been several attempts at learning directly from the raw waveform for speech recognition [5, 6, 7, 8]. [6, 7] propose an architecture composed of a convolutional layer followed by max-pooling and a nonlinearity, so that gammatone ﬁlterbanks correspond to a particular conﬁguration of the network. [8]
2767554854	Learning Filterbanks from Raw Speech for Phone Recognition	2148154194	of them remain almost analytic. 1. INTRODUCTION Speech features such as gammatones or mel-ﬁlterbanks (MFSC, for mel-frequency spectral coefﬁcients) were designed to match the human perceptual system [1, 2], and contain invaluable priors for speech recognition tasks. However, even if a consensus has been reached on the proper setting of the hyperparameters of these ﬁlterbanks along the years, there is n
2767634090	Towards Language-Universal End-to-End Speech Recognition	2005708641	+ EN 300 univ 12.9 5.8 ES + EN + DE 450 univ 13.1 3.9 ES 300 stl 11.7 14.4 4.2. Training and decoding Our language-universal encoder was a 4-layer Bidirectional Long Short-Term Memory (BLSTM) network [23, 24] with 320 cells in each layer and direction. A linear projection layer followed each BLSTM layer. All the weights in the models were initialized with a uniform distribution in the range of [-0.05, 0.0
2767634090	Towards Language-Universal End-to-End Speech Recognition	2521999726	tors to input to the network and employ frame-skipping [22] which decimates the original frame rate by a factor of three. Thus, each feature vector is presented to the network exactly once. Following [15], we used a label symbol inventory consisting of the individual characters and their double-letter units. An initial capitalized letter rather than a space symbol was used to indicate word boundaries.
2767634090	Towards Language-Universal End-to-End Speech Recognition	587565084,2130414229	to training acoustic models for low resource languages have used MTL as a means of sharing some of the acoustic model parameters across languages, e.g. all the parameters up to the ﬁnal output layer [1,5,6]. This forces the model to learn commonalities across languages which provides effective regularization and prevents over-ﬁtting. To the best of our knowledge, this is the ﬁrst effort to evaluate MTL
2767709712	Object Referring in Visual Scene with Spoken Language	1933349210	U [31], ROUGE [23], METEOR [2] and CIDEr [41], which are the well-known evaluation metrics in NLP domain. In Figure 3, we plot the histogram of the scores of the metrics having mapped to the range of [0;1]. The google speech alternatives have little differences with the ground truth expressions. We experiment to determine which metric captures these subtle differences. In Figure 3, the lesser the skewn
2767709712	Object Referring in Visual Scene with Spoken Language	2489434015,2505639562	16] for effective interaction between vision and language; 2) modeling contextual information to better understand a speaker’s intent, be it global context [24, 14], or local among ‘similar’ objects [28, 45, 24]. Our work use natural speech rather than clean texts as the input. Similar work has been conducted in Robotics Community. For instance, a joint model is learned from gestures, languages, and visual s
2767709712	Object Referring in Visual Scene with Spoken Language	2144960104,2241785942	annotate object location, issue language description and record speech description. A few choices are made in this work to scale the data collection: 1) Following previous work for language-based OR [24, 14], we use a simple bounding box to indicate the location of object; and 2) Following the success of synthetic data in many other vision tasks[8, 10], we construct hybrid speech recordings which contain
2767709712	Object Referring in Visual Scene with Spoken Language	2250539671	ant object class. In particular, we train a two-layer LSTM with 300 hidden layer nodes each. The input of the model is the embedded representation of the expressions by the pre-trained word2vec model [32]. The output layer is a classiﬁcation layer to classify expressions to the corresponding classes of the referred objects. The model lets us rank proposals by not only detection scores, but also the re
2767709712	Object Referring in Visual Scene with Spoken Language	1895577753	cessitates formulating and addressing more AI-complete research problems. This is evidenced by the recent trend of a joint understanding of vision and language in tasks such as image/video captioning [42], visual question answering [1], and object referring [24]. This work addresses the task of object referring (OR). OR is heavily used in human communication; the speaker issues a referring expression;
2767709712	Object Referring in Visual Scene with Spoken Language	2038484192	ch. To further study the performance of the method, we add different levels of noise to the speech ﬁles. The noise we mixed with the original speech ﬁles is selected randomly from the Urban8K dataset [37]. This dataset contains 10 categories of ambient noise: air conditioner, car horn, children playing, dog bark, drilling, enging idling, gun shot, jackhammer, siren, and street music. For each original
2767709712	Object Referring in Visual Scene with Spoken Language	2123301721	dered as a true detection if IoU computed between the predicted bounding box and the ground truth box is more than 0.5. We evaluate the performance of VGSR based on standard criterions such as METEOR [2], ROUGE [23], BLEU [31] and CIDEr [41]. We evaluate the results of VGSR in comparison to the ground truth referring expressions generated by humans using which we record/generate the speech ﬁles. For
2767709712	Object Referring in Visual Scene with Spoken Language	1686810756	e use Google Cloud Speech API service 1 for transcribing speech to text. We have tried Kaidi framework as well, but it generally gives worse results. The image features are extracted using CNN of VGG [38] network while we use a two-layer LSTM with 300 hidden layer nodes each for textual features. Subsequently, image and textual features are subjected to element-wise multiplication and a fully connecte
2767709712	Object Referring in Visual Scene with Spoken Language	2144960104	earch problems. This is evidenced by the recent trend of a joint understanding of vision and language in tasks such as image/video captioning [42], visual question answering [1], and object referring [24]. This work addresses the task of object referring (OR). OR is heavily used in human communication; the speaker issues a referring expression; the co-observers then identify the referred object and co
2767709712	Object Referring in Visual Scene with Spoken Language	2302548814	ferent names in Computer Vision. Notable ones are referring expressions [45 ,24], phrase localization [33 43], grounding of textual phrases [35], language-based object retrieval [14] and segmentation [13]. Recent research focus on LOD can be put into 2 groups: 1) learning embedding functions [9, 16] for effective interaction between vision and language; 2) modeling contextual information to better und
2767709712	Object Referring in Visual Scene with Spoken Language	639708223	ferred class. There are numerous object proposal techniques in the literature. For instance, [14] uses EdgeBox [47] for the object proposals; [24] and [16] use the faster RCNN (FRCNN) object detector [34] and recently Mask-RCNN [12] to propose the candidates. However, a direct use of the object proposal methods or object detectors is far from being optimal. General Object Proposal and Detection method
2767709712	Object Referring in Visual Scene with Spoken Language	2144960104	focusing on scenarios for assistive robots and the other for automated cars. 4.1. GoogleRef Object Annotation. As the ﬁrst testbed, we choose to enrich the standard object referring dataset GoogleRef [24] with speeches. GoogleRef contains 24;698 images with Dataset #Images #Objects Synthetic Speech Real Speech On-site Noise GoogleRef Train 24698 85474 3 7 7 Test 4650 9536 3 3 7 DrivingRef Train 250 75
2767709712	Object Referring in Visual Scene with Spoken Language	2154652894	gnition alternatives. For training, we use the alternative transcriptions of all the synthetic speech ﬁles of GoogleRef train set. For objective scores, we use the following metrics: BLEU [31], ROUGE [23], METEOR [2] and CIDEr [41], which are the well-known evaluation metrics in NLP domain. In Figure 3, we plot the histogram of the scores of the metrics having mapped to the range of [0;1]. The google
2767709712	Object Referring in Visual Scene with Spoken Language	2065749455,2141311545	and Google Now, and a large body of academic publications [6 ,22 29 26 15 44]. Speaking has been proven faster than typing on mobile devices for exchanging information [36]. There are academic works [39, 20, 4, 7] which use speech for image description and annotation. The main merit is that speech is very natural and capable of conveying rich content, lively emotion, and human intelligence – all in a hands-fre
2767709712	Object Referring in Visual Scene with Spoken Language	2144960104	groups: 1) learning embedding functions [9, 16] for effective interaction between vision and language; 2) modeling contextual information to better understand a speaker’s intent, be it global context [24, 14], or local among ‘similar’ objects [28, 45, 24]. Our work use natural speech rather than clean texts as the input. Similar work has been conducted in Robotics Community. For instance, a joint model is
2767709712	Object Referring in Visual Scene with Spoken Language	7746136	ingRef. Later, LOP ﬁlters them to yield candidates of the relevant class predicted by the speciﬁcally trained LSTM model producing class speciﬁc object proposals. Following object proposals literature[47, 40], we evaluate our LOP by the average recall of target objects under a ﬁxed IoU criterion. It is evaluated under multiple candidate budgets. Figure 8 shows the results of LOP on GoogleRef dataset, with
2767709712	Object Referring in Visual Scene with Spoken Language	2101105183	ion if IoU computed between the predicted bounding box and the ground truth box is more than 0.5. We evaluate the performance of VGSR based on standard criterions such as METEOR [2], ROUGE [23], BLEU [31] and CIDEr [41]. We evaluate the results of VGSR in comparison to the ground truth referring expressions generated by humans using which we record/generate the speech ﬁles. For LOP, we use Intersectio
2767709712	Object Referring in Visual Scene with Spoken Language	2076462394	l Analysis. Our work also shares similarity with visually grounded speech understanding. Notable examples include visually-grounded instruction understanding [18, 27], audio-visual speech recognition [30], and a ‘unsupervised’ speech understanding via a joint embedding with images [11, 5]. The ﬁrst vein of research investigates heavily how to gain the beneﬁt of visual cues to ground highlevel commands
2767709712	Object Referring in Visual Scene with Spoken Language	1905882502	lization [33 43], grounding of textual phrases [35], language-based object retrieval [14] and segmentation [13]. Recent research focus on LOD can be put into 2 groups: 1) learning embedding functions [9, 16] for effective interaction between vision and language; 2) modeling contextual information to better understand a speaker’s intent, be it global context [24, 14], or local among ‘similar’ objects [28,
2767709712	Object Referring in Visual Scene with Spoken Language	2241785942	m of Language Grounded Object Proposal (LOP) is to propose a set of object candidates that belong to the referred class. There are numerous object proposal techniques in the literature. For instance, [14] uses EdgeBox [47] for the object proposals; [24] and [16] use the faster RCNN (FRCNN) object detector [34] and recently Mask-RCNN [12] to propose the candidates. However, a direct use of the object p
2767709712	Object Referring in Visual Scene with Spoken Language	2343052201	n: 1) Following previous work for language-based OR [24, 14], we use a simple bounding box to indicate the location of object; and 2) Following the success of synthetic data in many other vision tasks[8, 10], we construct hybrid speech recordings which contain sounding-realistic synthetic speeches for training and real human speeches for evaluation. It is to be noted that real speeches are still recorded
2767709712	Object Referring in Visual Scene with Spoken Language	2123301721	natives. For training, we use the alternative transcriptions of all the synthetic speech ﬁles of GoogleRef train set. For objective scores, we use the following metrics: BLEU [31], ROUGE [23], METEOR [2] and CIDEr [41], which are the well-known evaluation metrics in NLP domain. In Figure 3, we plot the histogram of the scores of the metrics having mapped to the range of [0;1]. The google speech alter
2767709712	Object Referring in Visual Scene with Spoken Language	7746136	nded Object Proposal (LOP) is to propose a set of object candidates that belong to the referred class. There are numerous object proposal techniques in the literature. For instance, [14] uses EdgeBox [47] for the object proposals; [24] and [16] use the faster RCNN (FRCNN) object detector [34] and recently Mask-RCNN [12] to propose the candidates. However, a direct use of the object proposal methods or
2767709712	Object Referring in Visual Scene with Spoken Language	2144960104	o propose a set of object candidates that belong to the referred class. There are numerous object proposal techniques in the literature. For instance, [14] uses EdgeBox [47] for the object proposals; [24] and [16] use the faster RCNN (FRCNN) object detector [34] and recently Mask-RCNN [12] to propose the candidates. However, a direct use of the object proposal methods or object detectors is far from b
2767709712	Object Referring in Visual Scene with Spoken Language	1686810756	ound-truth expressions. candidate which maximizes the generative probability of the expression for the target object. The model architecture is same as that of [14]. We extract fc7 features of VGG-16 [38] net from bounding box location for object features and from whole image for global contextual feature. Textual features are extracted using a embedding layer which is also learned in an end-to-end fa
2767709712	Object Referring in Visual Scene with Spoken Language	2241785942	proposals in the image, our aim of Language Grounded Instance Detection (LID) is to identify the exact instance referred by the given expression. We employ a generative approach developed by Hu et al [14] for this task. Here, the model learns a scoring function that takes features from object candidate regions, their spatial conﬁgurations, whole image as global context along with the given referring e
2767709712	Object Referring in Visual Scene with Spoken Language	1956340063	raining, we use the alternative transcriptions of all the synthetic speech ﬁles of GoogleRef train set. For objective scores, we use the following metrics: BLEU [31], ROUGE [23], METEOR [2] and CIDEr [41], which are the well-known evaluation metrics in NLP domain. In Figure 3, we plot the histogram of the scores of the metrics having mapped to the range of [0;1]. The google speech alternatives have li
2767709712	Object Referring in Visual Scene with Spoken Language	2241785942	rded speech (with 10% added noise) to ground-truth expressions. candidate which maximizes the generative probability of the expression for the target object. The model architecture is same as that of [14]. We extract fc7 features of VGG-16 [38] net from bounding box location for object features and from whole image for global contextual feature. Textual features are extracted using a embedding layer w
2767709712	Object Referring in Visual Scene with Spoken Language	2046970591	rk architectures have been investigated for interpreting contextually grounded natural language commands [3]; and the appropriateness of natural language dialogue with assistive robots is examined in [19]. While sharing similarity, our work are very different. Our method works with wild, natural scenes, instead of rather controlled lab environments. Joint Speech-Visual Analysis. Our work also shares s
2767709712	Object Referring in Visual Scene with Spoken Language	1905882502	a set of object candidates that belong to the referred class. There are numerous object proposal techniques in the literature. For instance, [14] uses EdgeBox [47] for the object proposals; [24] and [16] use the faster RCNN (FRCNN) object detector [34] and recently Mask-RCNN [12] to propose the candidates. However, a direct use of the object proposal methods or object detectors is far from being opti
2767709712	Object Referring in Visual Scene with Spoken Language	2101105183	speech recognition alternatives. For training, we use the alternative transcriptions of all the synthetic speech ﬁles of GoogleRef train set. For objective scores, we use the following metrics: BLEU [31], ROUGE [23], METEOR [2] and CIDEr [41], which are the well-known evaluation metrics in NLP domain. In Figure 3, we plot the histogram of the scores of the metrics having mapped to the range of [0;1].
2767709712	Object Referring in Visual Scene with Spoken Language	1933349210	ssing more AI-complete research problems. This is evidenced by the recent trend of a joint understanding of vision and language in tasks such as image/video captioning [42], visual question answering [1], and object referring [24]. This work addresses the task of object referring (OR). OR is heavily used in human communication; the speaker issues a referring expression; the co-observers then identify
2767709712	Object Referring in Visual Scene with Spoken Language	2241785942	been tackled under different names in Computer Vision. Notable ones are referring expressions [45 ,24], phrase localization [33 43], grounding of textual phrases [35], language-based object retrieval [14] and segmentation [13]. Recent research focus on LOD can be put into 2 groups: 1) learning embedding functions [9, 16] for effective interaction between vision and language; 2) modeling contextual inf
2767709712	Object Referring in Visual Scene with Spoken Language	2154652894	true detection if IoU computed between the predicted bounding box and the ground truth box is more than 0.5. We evaluate the performance of VGSR based on standard criterions such as METEOR [2], ROUGE [23], BLEU [31] and CIDEr [41]. We evaluate the results of VGSR in comparison to the ground truth referring expressions generated by humans using which we record/generate the speech ﬁles. For LOP, we use
2767709712	Object Referring in Visual Scene with Spoken Language	2247513039	uage-based object detection (LOD) has been tackled under different names in Computer Vision. Notable ones are referring expressions [45 ,24], phrase localization [33 43], grounding of textual phrases [35], language-based object retrieval [14] and segmentation [13]. Recent research focus on LOD can be put into 2 groups: 1) learning embedding functions [9, 16] for effective interaction between vision an
2767709712	Object Referring in Visual Scene with Spoken Language	1956340063	uted between the predicted bounding box and the ground truth box is more than 0.5. We evaluate the performance of VGSR based on standard criterions such as METEOR [2], ROUGE [23], BLEU [31] and CIDEr [41]. We evaluate the results of VGSR in comparison to the ground truth referring expressions generated by humans using which we record/generate the speech ﬁles. For LOP, we use Intersection over Union (I
2767727041	Tracking of Enriched Dialog States for Flexible Conversational Information Access	2055537935,2250297846	ict. 46.3 82.5 Table 4. Results on Iqiyi dialog dataset. 5. RELATED WORK Recent DST studies mainly focus on using deep neural networks. Initially, a word-based RNN with n-gram features is proposed in [11, 12]. It has been shown in [4] that employing CNN features with RNN can yield better results. In [6], given ﬁne-trained semantic word vectors and turn-level labels, rule-based DST with CNNs outperforms tr
2767906378	Multilingual Speech Recognition with a Single End-to-End Model	2033436836,2106440210	st of the previous work on multilingual speech recognition has been limited to making the acoustic model (AM) multilingual [3–6, 9–11, 13, 14]. Some of the multilingual AMs require a common phone set [3, 4, 13] while others share some of the acoustic model parameters [9–11,15]. A hat swap structure is proposed in [9–11], where the lower layers of a deep neural network (DNN) are shared across languages and t
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2138660131	. As a result our model achieve state-of-the-art performance and signiﬁcantly 7 Published as a conference paper at ICLR 2018 Model BPC Norm-stabilized RNN (Krueger &amp; Memisevic, 2015) 1.48 CW-RNN (Koutnik et al., 2014) 1.46 HF-MRNN (Mikolov et al., 2012) 1.41 MI-RNN (Wu et al., 2016) 1.39 ME n-gram (Mikolov et al., 2012) 1.37 BatchNorm LSTM (Cooijmans et al., 2016) 1.32 Zoneout RNN (Krueger et al., 2016) 1.27 Hyper
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2157762871	bed in Appendix A. Model UF 1 LBRANCH 28.7 RANDOM 34.7 DEP-PCFG (Carroll &amp; Charniak, 1992) 48.2 RBRANCH 61.7 CCM (Klein &amp; Manning, 2002) 71.9 DMV+CCM (Klein &amp; Manning, 2005) 77.6 UML-DOP (Bod, 2006) 82.9 PRPN 70.02 UPPER BOUND 88.1 Table 5: Parsing Performance on the WSJ10 dataset Table 5 summarizes the results. Our model signiﬁcantly outperform the RANDOM baseline indicate a high consistency wi
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2469894155	cantly 7 Published as a conference paper at ICLR 2018 Model BPC Norm-stabilized RNN (Krueger &amp; Memisevic, 2015) 1.48 CW-RNN (Koutnik et al., 2014) 1.46 HF-MRNN (Mikolov et al., 2012) 1.41 MI-RNN (Wu et al., 2016) 1.39 ME n-gram (Mikolov et al., 2012) 1.37 BatchNorm LSTM (Cooijmans et al., 2016) 1.32 Zoneout RNN (Krueger et al., 2016) 1.27 HyperNetworks (Ha et al., 2016) 1.27 LayerNorm HM-LSTM (Chung et al., 2
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2157762871	ency parsing tree is not limited to binary tree. Previous unsupervised constituency parsing model also generate binary trees (Klein 9 Published as a conference paper at ICLR 2018 &amp; Manning, 2002; Bod, 2006). Our model is compared with the several baseline methods, that are explained in Appendix E. Different from the previous experiment setting, the model treat each sentence independently during train an
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2157762871	he generalization capability of supervised parsers. Unsupervised syntactic structure induction has been among the longstanding challenges of computational linguistic (Klein &amp; Manning, 2002; 2004; Bod, 2006). Researchers are interested in this 1 arXiv:1711.02013v2 [cs.CL] 19 Feb 2018 Published as a conference paper at ICLR 2018 problem for a variety of reasons: to be able to parse languages for which no
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2123893795	of higher layers directly, while ours control it softly through an attention mechanism. In terms of language modeling, syntactic language modeling can be dated back to Chelba (1997). Charniak (2001); Roark (2001) have also proposed language models with a top-down parsing mechanism. Recently Dyer et al. (2016); Kuncoro et al. (2016) have introduced neural networks into this space. It learns both a discriminati
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2251939518	information (Tai et al., 2015). Developing a deep neural network that can leverage syntactic knowledge to form a better semantic representation has received a great deal of attention in recent years (Socher et al., 2013; Tai et al., 2015; Chung et al., 2016). Integrating syntactic structure into a language model is important for different reasons: 1) to obtain a hierarchical representation with increasing levels of
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2138660131	Lin et al. (1998)). The NARX RNN (Lin et al., 1998) is another example which used a feed forward net taking different inputs with predeﬁned time delays to model long-term dependencies. More recently, Koutnik et al. (2014) also used multiple layers of recurrent networks with different pre-deﬁned updating frequencies. Instead, our model tries to learn the structure from data, rather than predeﬁning it. In that respect,
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2251939518	man brain (Bengio et al., 2009; LeCun et al., 2015; Schmidhuber, 2015); 2) to capture complex linguistic phenomena, like long-term dependency problem (Tai et al., 2015) and the compositional effects (Socher et al., 2013); 3) to provide shortcut for gradient back-propagation (Chung et al., 2016). A syntactic parser is the most common source for structure information. Supervised parsers can achieve very high performanc
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2157762871	is outperformed by DMV+CCM and UML-DOP models. The DMV+CCM model has extra information from a dependency parser. The UML-DOP approach captures both contiguous and non-contiguous lexical dependencies (Bod, 2006). 7 CONCLUSION In this paper, we propose a novel neural language model that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2155693943	rols the updates of higher layers directly, while ours control it softly through an attention mechanism. In terms of language modeling, syntactic language modeling can be dated back to Chelba (1997). Charniak (2001); Roark (2001) have also proposed language models with a top-down parsing mechanism. Recently Dyer et al. (2016); Kuncoro et al. (2016) have introduced neural networks into this space. It learns both
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	1793121960	t on input/output embeddings, between Recurrent Layers and on recurrent states were (0.4, 0.2, 0.2) respectively. Model PPL LSTM-500 (Mikolov et al., 2014) 156 SCRNN (Mikolov et al., 2014) 161 MemNN (Sukhbaatar et al., 2015) 147 LSTM-1024 (Grave et al., 2016) 121 LSTM + continuous cache pointer (Grave et al., 2016) 99.9 PRPN 81.64 Table 4: PPL on the Text8 valid set In Table 2, our results are comparable to the state-of-
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2113075298	tated treebanks exist (Marecek, 2016); to create a dependency structure to better suit a particular NLP application (Wu et al., 2017); to empirically argue for or against the poverty of the stimulus (Clark, 2001; Chomsky, 2014); and to examine cognitive issues in language learning (Solan et al., 2003). In this paper, we propose a novel neural language model: Parsing-Reading-Predict Networks (PRPN), which can
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	1938755728	tween recurrent layers, and on recurrent states were (0.7, 0.5, 0.5) respectively. Model PPL RNN-LDA + KN-5 + cache (Mikolov &amp; Zweig, 2012) 92.0 LSTM (Zaremba et al., 2014) 78.4 Variational LSTM (Kim et al., 2016) 78.9 CharCNN (Kim et al., 2016) 78.9 Pointer Sentinel-LSTM (Merity et al., 2016) 70.9 LSTM + continuous cache pointer (Grave et al., 2016) 72.1 Variational LSTM (tied) + augmented loss (Inan et al.,
2768091364	Neural Language Modeling by Jointly Learning Syntax and Lexicon	2251939518	vided by human experts. 2 RELATED WORK The idea of introducing some structures, especially trees, into language understanding to help a downstream task has been explored in various ways. For example, Socher et al. (2013); Tai et al. (2015) learn a bottom-up encoder, taking as an input a parse tree supplied from an external parser. There are models that are able to infer a tree during test time, while still need super
2768195931	MojiTalk: Generating Emotional Responses at Scale	2740582239	same classiﬁer in training for automated evaluation, as is in (Hu et al.,2017). We can obtain meaningful results as long as the classiﬁer is able to capture the semantic relationship between emojis (Felbo et al., 2017). As is shown in Table2, CVAE signiﬁcantly reduces the perplexity and increases the emoji accuracy over base model. Reinforced CVAE also adds to the emoji accuracy at the cost of a slight increase in
2768195931	MojiTalk: Generating Emotional Responses at Scale	2188365844	n generated from the decoder. 4.2 Conditional Variational Autoencoder (CVAE) Having similar encoder-decoder structures, SEQ2SEQ can be easily extended to a Conditional Variational Autoencoder (CVAE) (Sohn et al., 2015). Figure3illustrates the model: response encoder, recognition network, and prior network are added on top of the SEQ2SEQ model. Response encoder has the same structure to original tweet encoder, but i
2768195931	MojiTalk: Generating Emotional Responses at Scale	2581637843	r training deep learning models with a large number of parameters. In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al. ,2016) and dialog (Li et al. 2017b) are proposed. However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.” Such coarse-grai
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2250539671	∈u (1− Ön i=1 1[ai ,yi]) (15) 3.3 Implemental Details We preprocess each utterance using the library of nltk [29] and exploit the popular pretrained word embedding Glove with 100 dimensional vectors [34]. The size of char-level embedding is also set as 100-dimensional and is obtained by CNN filters under the instruction of Kim [23]. The Gated Recurrent Unit [7] which is variant from LSTM [15] is empl
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1544827683	at adopt the attention mechanism in neural machine translation, attention mechanism based neural networks have become a major trend in diverse text researching field, such as in machine comprehension [13] [45] [19] [8], machine translation [31] [9], abstract summarization [36] [1], text classification [42] [47] [44] and so on. The principle of attention mechanism is to select the most pertinent piece
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2257123346	anism based neural networks have become a major trend in diverse text researching field, such as in machine comprehension [13] [45] [19] [8], machine translation [31] [9], abstract summarization [36] [1], text classification [42] [47] [44] and so on. The principle of attention mechanism is to select the most pertinent piece of information, rather than using all available information, a large part of
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2143017621	CRF-ASN method by a. We now introduce the evaluation criteria below. Accuracy= 1 |u| Õ u i ∈u (1− Ön i=1 1[ai ,yi]) (15) 3.3 Implemental Details We preprocess each utterance using the library of nltk [29] and exploit the popular pretrained word embedding Glove with 100 dimensional vectors [34]. The size of char-level embedding is also set as 100-dimensional and is obtained by CNN filters under the ins
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	18806914	or dialog act tagging in the HCRC MapTask corpus. Lendvai et al. [28] explore two sequence learners with a memory-based tagger and conditional random fields into turn-internal DA chunks. Boyer et al. [5] also applied HMM to discover internal dialogue strategies inherent in the structure of the sequenced dialogue acts. Galley et al. [10] use skip-chain conditional random field to model non-local pragm
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2017187090	ditional random fields into turn-internal DA chunks. Boyer et al. [5] also applied HMM to discover internal dialogue strategies inherent in the structure of the sequenced dialogue acts. Galley et al. [10] use skip-chain conditional random field to model non-local pragmatic dependencies between paired utterances. Zimmermann et al. [49] investigate the use of conditional random fields for joint segmenta
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1526096287	ialog acts exploiting both word and prosodic features. Recently, approaches based on deep learning methods improved manystate-of-the-arttechniquesinNLPincludingDARaccuracyon open-domain conversations [20] [48] [17] [26] [27]. Kalchbrenner et al. [20] used a mixture of CNN and RNN. CNNs were used to extract local features from each utterance and RNNs were used to create a general view of the whole dial
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2257123346	lity of transiting from state s to state s 2: Construct emission matrix B, each element stores the probability of observing oj from state si 3: for each state i ∈{1,2,...,K}do 4: T1[i,1]←πiBiy 1 5: T2[I,1]←0 6: end for 7: for each observation i = 2,3,...,T do 8: for each state j ∈{1,2,...,K}do 9: T1[j,i]←max(T1[k,i −1]·Akj ·Bjyi) 10: T2[j,i]←arдmax(1[k,i −1]·Akj ·Bjy i ) 11: end for 12: end for 13: zT
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2130119531	n the dialogue act classification using a Bayesian approach. Serafin et al. [37] employ Latent Semantic Analysis (LSA) proper and augmented method to work for dialogue act classification. Chen et al. [6] had an empirical investigation of sparse log-linear models for improved dialogue act classification. Milajevs et al. [33] investigate a series of compositional distributional semantic models to dialo
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1964929739	nsional approach to distinguish and annotate units in dialogue act segmentation and classification. Grau et al. [12] focus on the dialogue act classification using a Bayesian approach. Serafin et al. [37] employ Latent Semantic Analysis (LSA) proper and augmented method to work for dialogue act classification. Chen et al. [6] had an empirical investigation of sparse log-linear models for improved dial
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2401527985	odel states. Tavafi et al. [41] study the effectiveness of supervised learning algorithms SVM-HMM for DA modeling across a comprehensive set of conversations. Similar to the SVM-HMM, Surendran et al. [40] also use a combination of linear support vector machines and hidden markov models for dialog act tagging in the HCRC MapTask corpus. Lendvai et al. [28] explore two sequence learners with a memory-ba
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2106925883	oper and augmented method to work for dialogue act classification. Chen et al. [6] had an empirical investigation of sparse log-linear models for improved dialogue act classification. Milajevs et al. [33] investigate a series of compositional distributional semantic models to dialogue act classification. Regarding the DAR as a sequence labeling problem. Stolcke et al. [39] treat the discourse structur
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1539256681	plications have benefited from the use of automatic dialogue act recognition such as dialogue systems, machine translation, automatic speech recognition, topic identification and talking avatars [21] [24] [14]. One of the primary applications of DAR is to support task-oriented discourse agent system. Knowing the past utterances of DA can help ease the prediction of the current DA state, thus help to n
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1526096287	rewell&quot; examples. To tackle these two problems, some works have turn to structured prediction algorithm along with deep learning tactics such as DRLM-Conditional [17], LSTM-Softmax [21] and RCNN [20]. However, most of them failed to utilize the empirical effectiveness of attention in the graphical structured network and relies completely on the hidden layers of the network, which may cause the st
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2295434193	s follows: •Bi-LSTM-CRF [25] method builds a hierarchical bidirectional LSTM as a base unit and the conditional random field as the top layer to do the dialogue act recognition task. •DRLM-Conditional[18] method combines postive aspects of neural network architectures with probabilistic graphical models. The model combines a recurrent neural network language model with a latent variable model over sha
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	1579930314	a statistically based language model. Webb et al. [43] apply diverse intra-utterance features involving word n-gram cue phrases to understand the utterance and do the classification. Geertzen et al. [11] propose a multidimensional approach to distinguish and annotate units in dialogue act segmentation and classification. Grau et al. [12] focus on the dialogue act classification using a Bayesian appro
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2146785422	t scale up well across different datasets. Furthermore, they abandon the useful correlation information among contextual utterances. Typical multi-classification algorithms like SVM, Naive Bayes [12] [2] [39] can not account for the contextual dependencies and classify the DA label in isolation. It is evident that during a conversation, the speaker’s intent is influenced by the former utterance such
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2517194566	ve become a major trend in diverse text researching field, such as in machine comprehension [13] [45] [19] [8], machine translation [31] [9], abstract summarization [36] [1], text classification [42] [47] [44] and so on. The principle of attention mechanism is to select the most pertinent piece of information, rather than using all available information, a large part of it being irrelevant to compute
2768498165	Dialogue Act Recognition via CRF-Attentive Structured Network	2064675550	vectors [34]. The size of char-level embedding is also set as 100-dimensional and is obtained by CNN filters under the instruction of Kim [23]. The Gated Recurrent Unit [7] which is variant from LSTM [15] is employed throughout our model. We adopt the AdaDelta [46] optimizer for training with an initial learning rate of 0.005. We also apply dropout [38]between layers with a dropout rate of 0.2. For th
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2610403318	0 L 1 72.0 72.7 69.6 accuracy gain +2.3 +1.8 +0.0 Table 2: Instruction-following results in the SCONE domains. The table shows accuracy on the test set. For reference, we also show prior results fromGuu et al. (2017) (GPLL), although our models use more supervision at training time. a red guy appears on the far left then to orange’s other side base listener, L 0 rational listener, 0 L 1 Figure 3: Action traces pr
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	1933065844	1 Base listener Our base listener model, L 0, predicts action sequences conditioned on an encoded representation of the directions and the current world state. In the SAIL domain, this is the model ofMei et al. (2016) (illustrated in green inFigure 2b for a single sentence and its associated actions), see “domain speciﬁcs” below. Encoder Each direction sentence is encoded separately with a bidirectional LSTM (Hoch
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2420948438	agent navigates through a twodimensional grid of hallways with patterned walls and ﬂoors and a discrete set of objects (Figure 1 shows a portion of one of these hallways). In the three SCONE domains (Long et al., 2016), the world contains a number of objects with various properties, such as colored beakers which an agent can combine, drain, and mix. Instructions describe how these objects should be manipulated. The
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2118781169	directions to the intended ﬁnal state. We evaluate models for both tasks in four domains. The ﬁrst domain is the SAIL corpus of virtual environments and navigational directions (MacMahon et al.,2006;Chen and Mooney, 2011), where an agent navigates through a twodimensional grid of hallways with patterned walls and ﬂoors and a discrete set of objects (Figure 1 shows a portion of one of these hallways). In the three SCON
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2530850764	ing the rational speaker’s directions is substantially higher than for humanproduced directions in the Tangrams domain. In the SAIL evaluation, we also include the directions produced by the system ofDaniele et al. (2017) (DBW), and ﬁnd that the rational speaker’s directions are followable to comparable accuracy. We also compare the directions produced by the systems to the reference instructions given by humans in th
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2574790321	ing” task introduced byKazemzadeh et al.(2014). In this family are approaches that model the listener at training time (Mao et al.,2016), at evaluation time (Andreas and Klein,2016;Monroe et al.,2017;Vedantam et al., 2017;Su et al.,2017) or both (Yu et al.,2017b; Luo and Shakhnarovich,2017). Other conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics includ
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2009314958	ins (Long et al.,2016). Instruction generation Previous work has also investigated the instruction generation task, in particular for navigational directions. The GIVE shared tasks (Byron et al.,2009;Koller et al., 2010;Striegnitz et al.,2011) have produced a large number of interactive direction-giving systems, both rule-based and learned. The work most immediately related to the generation task in this paper is th
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2189089430	eight matrices Wand vector vare learned. Domain speciﬁcs For SAIL, we use the alignments between sentences and route segments annotated byChen and Mooney(2011), which were also used in previous work (Artzi and Zettlemoyer, 2013;Artzi et al.,2014;Mei et al.,2016). FollowingMei et al.(2016), we reset the decoder’s hidden state for each sentence. In the SCONE domains, which have a larger space of possible outputs than SAIL, we
2768660351	UNIFIED PRAGMATIC MODELS FOR GENERATING AND FOLLOWING INSTRUCTIONS	2133564696	t]) q t= W o(W yy t+W hh d t +W zz t) p(a tja 1:t 1;y 1:t;w 1:M) /exp(q t) where all weight matrices Ware learned parameters. The sentence representation z tis produced using an attention mechanism (Bahdanau et al., 2015) over the representation vectors he 1 :::h e M 3We also experimented with sampling from the base models to produce these candidate lists, as was done in previous work (Andreas and Klein,2016;Monroe et
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2125297418,2162723805	, 2010) along with probabilistic logic (Richardson & Domingos, 2006; Broecheler et al., 2010; Wang et al., 2013) combine machine learning and logic but these approaches operate on symbols rather than vectors and hence do not enjoy the generalization properties of embedding based approaches.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1533230146	, 2017), which do logical rule learning in KBs, and also state-of-the-art embedding based methods such as DistMult (Yang et al., 2015) and ComplEx (Trouillon et al.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2127795553	(Bordes et al., 2013; Neelakantan et al., 2015; Xiong et al., 2017), we add the inverse relation of every edge, i.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2409591106	d ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018). (c) We also extend MINERVA to handle partially structured natural language queries and test it on the WikiMovies dataset (§ 3.3) (Miller et al., 2016). We also compare to DeepPath (Xiong et al., 2017) which uses reinforcement learning to pick paths between entity pairs. The main difference is that the state of their RL agent includes the answer ent
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2725395424	d scores are directly comparable to NTP and ComplEx (Trouillon et al., 2016). NTP-l is a NTP model trained with an additional objective function of ComplEx. We also compare MINERVA against Neural LP (Yang et al., 2017) on the UMLS and KINSHIP datasets. The evaluation metric we report is HITS@k - which is the percentage of correct entities ranked in top-k. For the COUNTRIES dataset, we report the area under the prec
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2548746141	Dataset We use three standard datasets: COUNTRIES (Bouchard et al., 2015), KINSHIP, and UMLS (Kok & Domingos, 2007).
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1756422141	DeepPath, additionally feeds its gathered paths to Path Ranking Algorithm (Lao et al., 2011), whereas MINERVA is a complete system trained to do query answering.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1756422141	DeepPath feeds the paths its agent gathers as input features to the path ranking algorithm (PRA) (Lao et al., 2011), which trains a per-relation classifier.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2521709538	has many desirable properties. First, MINERVA has the built-in ﬂexibility to take paths of variable length, which is important for answering harder questions that require complex chains of reasoning (Shen et al., 2017). Secondly, MINERVA needs no pretraining and trains on the knowledge graph from scratch with reinforcement learning; no other supervision or ﬁne-tuning is required representing a signiﬁcant advance ov
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2101848544	is a directed labeled multigraph G=(V;E;R), where V and E denote the vertices and edges of the graph respectively. Note thatV =Eand E V R V. Also, following previous approaches (Bordes et al., 2013; Neelakantan et al., 2015; Xiong et al., 2017), we add the inverse relation of every edge, i.e. for an edge (e 1;r;e 2) 2E, we add the edge (e 2;r 1;e 1) to the graph. (If the set of binary relations Rdoes not contain the inv
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2127426251	by distributed vector representations. Learning embedding of entities and relations using tensor factorization or neural methods has been a popular approach (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013; inter alia), but these methods cannot capture chains of reasoning expressed by KB paths. Neural multi-hop models (Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016) address the afor
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2107598941	dom walking or it fails to aggregate predictive features from all the k paths, many of which would be irrelevant to answer the given query. The latter is akin to the problem with distant supervision (Mintz et al., 2009), where important evidence gets lost amidst a plethora of irrelevant information. However, by taking each step conditioned on the query relation, MINERVA can effectively reduce the search space and fo
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	205829674	e approaches have largely been superceded by distributed vector representations. Learning embedding of entities and relations using tensor factorization or neural methods has been a popular approach (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013; inter alia), but these methods cannot capture chains of reasoning expressed by KB paths. Neural multi-hop models (Neelakantan et al., 2015; Guu et al., 2015
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2251960799	e are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010). KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges for automated reasoning. For example, consider the small knowledge gra
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2029111147	From its early days, the focus of automated reasoning approaches has been to build systems that can learn crisp symbolic logical rules (McCarthy, 1960; Nilsson, 1991).
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1999138184	Early work in ILP such as FOIL (Quinlan, 1990), PROGOL (Muggleton, 1995) are either rule-based or require negative examples which is often hard to find in KBs (by design, KBs store true facts).
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2101848544	en a popular approach (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013; inter alia), but these methods cannot capture chains of reasoning expressed by KB paths. Neural multi-hop models (Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016) address the aforementioned problems to some extent by operating on KB paths in vector space. However, these 1https://github.com/shehzaadzd/MINERVA 1 arXiv:1
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2250635077	entities and relations using tensor factorization (Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al., 2013; Toutanova et al., 2015; Verga et al., 2016) has been a popular approach to reasoning with a knowledge base. However, these methods cannot capture more complex reasoning patterns such as those found by following inference p
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2094728533	erved evidence, has been a long-standing goal of artiﬁcial intelligence. We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010). KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opportunities and challenges f
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2127795553	Following Bordes et al. (2013), we categorized the query relations into (M)any to 1, 1 to M or 1 to 1 relations.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2466714650	graph via “logical” paths between entity ‘Malala Yousafzai’ and the corresponding answer. independent of the query relation. Additionally, models such as those developed in Neelakantan et al. (2015); Das et al. (2017) use the same set of initially collected paths to answer a diverse set of query types (e.g. MarriedTo, Nationality, WorksIn etc.). This paper presents a method for efﬁciently searching the graph for a
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2725395424	h computes the average of the embeddings of the question words. The word embeddings are learned from scratch and we do not use any pretrained embeddings. We compare our results with those reported in Yang et al. (2017) (table 7). We got the best result using T =1, suggesting that WikiMovies is not the best testbed for multihop reasoning, but this experiment is a promising ﬁrst step towards the realistic setup of ha
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2738442461	hs such as (A — B — C) to predict the answer of the query, i.e. the third node (C). The clustering coefﬁcient also extends from triangles to cliques of arbitrary size (Watts &amp; 2We are grateful to Xiong et al. (2017) for releasing the negative examples used in their experiments. 3We are aware of the high variance of DistMult scores reported on FB15k-237 by several papers, but to ensure fairness we report the high
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2738442461	igraph G=(V;E;R), where V and E denote the vertices and edges of the graph respectively. Note thatV =Eand E V R V. Also, following previous approaches (Bordes et al., 2013; Neelakantan et al., 2015; Xiong et al., 2017), we add the inverse relation of every edge, i.e. for an edge (e 1;r;e 2) 2E, we add the edge (e 2;r 1;e 1) to the graph. (If the set of binary relations Rdoes not contain the inverse relation r 1, it
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2725395424	ing reinforcement learning, speciﬁcally policy gradients (§ 2). (b) We evaluate MINERVA on several benchmark datasets and compare favorably to Neural Theorem aschel &amp; Riedel, 2017) and Neural LP (Yang et al., 2017), which do logical¨ rule learning in KBs, and also state-of-the-art embedding based methods such as DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016). (c) We also extend MINERVA to han
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1512387364	We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010).
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1934264538	kel et al., 2011; Bordes et al., 2013; Socher et al., 2013; inter alia), but these methods cannot capture chains of reasoning expressed by KB paths. Neural multi-hop models (Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016) address the aforementioned problems to some extent by operating on KB paths in vector space. However, these 1https://github.com/shehzaadzd/MINERVA 1 arXiv:1711.05851v1 [cs.CL
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2101848544	knowledge base. However, these methods cannot capture more complex reasoning patterns such as those found by following inference paths in KBs. Multi-hop link prediction approaches (Lao et al., 2011; Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Das et al., 2017) address the problems above, but the reasoning paths that they operate on are gathered by 8 (i) Can learn general rules: (S1) LocatedIn(X,
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1533230146,1852412531,2127795553	Learning vector representations of entities and relations using tensor factorization (Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2738442461	lon et al., 2016). (c) We also extend MINERVA to handle partially structured natural language queries and test it on the WikiMovies dataset (§ 4.3) (Miller et al., 2016). We also compare to DeepPath (Xiong et al., 2017) which uses reinforcement learning to pick paths between entity pairs. The main difference is that the state of their RL agent includes the answer entity since it is designed for the simpler task of p
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2738442461	ly attractive. MINERVA uses a similar hard selection of relation edges to walk on the graph. More importantly, MINERVA outperforms both these methods on their respective benchmark datasets. DeepPath (Xiong et al., 2017) uses RL based approaches to ﬁnd paths in KBs. However, the state of their MDP requires the target entity to be known in advance and hence their path ﬁnding strategy is dependent on knowing the answer
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1934264538	these methods cannot capture more complex reasoning patterns such as those found by following inference paths in KBs. Multi-hop link prediction approaches (Lao et al., 2011; Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Das et al., 2017) address the problems above, but the reasoning paths that they operate on are gathered by 8 (i) Can learn general rules: (S1) LocatedIn(X, Y) (X, Z) &amp; Lo
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1756422141	Multi-hop link prediction approaches (Lao et al., 2011; Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Das et al., 2017) address the problems above, but the reasoning paths that they operate on are gathered by performing random walks independent of the type of query relation.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	205829674	nds whereas that of a GPU implementation of DistMult is 211 seconds (with the maximum batch size). 6 RELATED WORK Learning vector representations of entities and relations using tensor factorization (Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al., 2013; Toutanova et al., 2015; Verga et al., 2016) has been a popular approac
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1793121960	Neural LP introduces a differential rule learning system using operators deﬁned in TensorLog (Cohen, 2016). It has a LSTM based controller with a differentiable memory component (Graves et al., 2014; Sukhbaatar et al., 2015) and the rule scores are calculated via attention. Even though, differentiable memory allows end to end training, it necessitates accessing the entire memory, which can be computationally expensive. R
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2022166150	new inferences from observed evidence, has been a long-standing goal of artiﬁcial intelligence. We are interested in automated reasoning on large knowledge bases (KB) with rich and diverse semantics (Suchanek et al., 2007; Bollacker et al., 2008; Carlson et al., 2010). KBs are highly incomplete (Min et al., 2013), and facts not directly stored in a KB can often be inferred from those that are, creating exciting opport
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1533230146	In particular we compare against embedding based models - DistMult (Yang et al., 2015), ComplEx (Trouillon et al.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2119717200	raph G, the agent learns to traverse the environment/graph to mine the answer and stop when it determines the answer (§ 2.2). The agent is trained using policy gradient more speciﬁcally by REINFORCE (Williams, 1992) with control variates (§ 2.3). Let us begin by describing the environment. 2.1 ENVIRONMENT - STATES, ACTIONS, TRANSITIONS AND REWARDS Our environment is a ﬁnite horizon, deterministic and partially o
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2409591106	since for most real applications, the queries appear in natural language. As a ﬁrst step in this direction, we extend MINERVA to take in “partially structured” queries. We use the WikiMovies dataset (Miller et al., 2016) which contains questions in natural language albeit generated by templates created by human annotators. An example question is “Which is a ﬁlm written by Herb Freed?”. WikiMovies also has an accompan
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2466714650	reasoning patterns such as those found by following inference paths in KBs. Multi-hop link prediction approaches (Lao et al., 2011; Neelakantan et al., 2015; Guu et al., 2015; Toutanova et al., 2016; Das et al., 2017) address the problems above, but the reasoning paths that they operate on are gathered by performing random walks independent of the type of query relation. Lao et al. (2011) further ﬁlters paths from
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2127426251	r representations of entities and relations using tensor factorization (Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al., 2013; Toutanova et al., 2015; Verga et al., 2016) has been a popular approach to reasoning with a knowledge base. However, these methods cannot capture more complex reasoning patterns such as those found
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2119717200	reward: J(q)=E (e 1;r;e2)˘DE A;::; T ˘p q [R(S T )jS 1=(e ;e ;r;e 2)] where we assume there is a true underlying distribution (e 1;r;e 2) ˘D. To solve this optimization problem, we employ REINFORCE (Williams, 1992) as follows: The ﬁrst expectation is replaced with empirical average over the training dataset. For the second expectation, we approximate by running multiple rollouts for each training example. The
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	65714572	Similar to WikiNav is Wikispeedia (West et al., 2009) in which an agent needs to learn to traverse to a given target entity node (wiki page) as quickly as possible.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2174833404	sing tensor factorization (Nickel et al., 2011; 2012; Bordes et al., 2013; Riedel et al., 2013; Nickel et al., 2014; Yang et al., 2015) or neural methods (Socher et al., 2013; Toutanova et al., 2015; Verga et al., 2016) has been a popular approach to reasoning with a knowledge base. However, these methods cannot capture more complex reasoning patterns such as those found by following inference paths in KBs. Multi-ho
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1543747524	Statistical relational learning methods (Getoor & Taskar, 2007; Kok & Domingos, 2007; Schoenmackers et al., 2010) along with probabilistic logic (Richardson & Domingos, 2006; Broecheler et al.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1756422141	Symbolic representations have also been integrated with machine learning especially in statistical relational learning (Muggleton et al., 1992; Getoor & Taskar, 2007; Kok & Domingos, 2007; Lao et al., 2011), but due to poor generalization performance, these approaches have largely been superceded by distributed vector representations.
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2725395424	tems (UMLS) (Kok &amp; Domingos, 2007) (d) WN18RR (Dettmers et al., 2017), (e) NELL-995, (f) FB15k-237 (g) WikiMovies (Miller et al., 2016). We also test on a synthetic grid world dataset released by Yang et al. (2017) to test the ability of the model to learn rules of long length. The COUNTRIES dataset is carefully designed to explicitly test the logical rule learning and reasoning capabilities of link prediction
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2101848544	tity ‘Colin Kaepernick’ and the corresponding answer. models take as input a set of paths which are gathered by performing random walks independent of the query relation. Additionally, models such as Neelakantan et al. (2015); Das et al. (2017) use the same set of initially collected paths to answer a diverse set of query types (e.g. MarriedTo, Nationality, WorksIn etc.). This paper presents a method for efﬁciently search
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	1850531616	tperform them. MINERVA is also similar to methods for learning to search for structured prediction (Collins &amp; Roark, 2004; Daume III &amp; Marcu, 2005;´ Daume III et al., 2009; Ross et al., 2011; Chang et al., 2015). These methods are based on imitating a´ reference policy (oracle) which make near-optimal decision at every step. In our problem setting, it is unclear what a good reference policy would be. For exa
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2725395424	the types of various query relation in FB15k-237 in the appendix. 4.2 GRID WORLD PATH FINDING As we empirically ﬁnd and also noted by previous work (Rocktaschel &amp; Riedel, 2017; Das et al.,¨ 2017; Yang et al., 2017), often the reasoning chains required to answer queries in KB is not too long (restricted to 3 or 4 hops). To test if our model can learn long reasoning paths, we test our model on a synthetic 16-by-1
2769099080	Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning	2466714650	ural LP Figure 3: Grid world experiment: We signiﬁcantly outperform NeuralLP for longer path lengths. While chains in KB need not be very long to get good empirical results (Neelakantan et al., 2015; Das et al., 2017; Yang et al., 2017), in principle MINERVA can be used to learn long reasoning chains. To evaluate the same, we test our model on a synthetic 16-by-16 grid world dataset created by Yang et al. (2017),
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2250539671	able 1: POS tagging accuracy on the PTB-WSJ test set, with other top-performing systems. 4.2Training &amp; Evaluation Details Model settings. We initialize word embeddings with 100-dimensional GloVe (Pennington et al., 2014) for English, and with 64-dimensional Polyglot (Al-Rfou et al.,2013) for other languages. We use 30-dimensional character embeddings, and set the state sizes of character/word-level BiLSTM to be 50, 2
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2338266296	ceptLing et al.(2015). The improvement over the baseline is statistically signiﬁcant, with p-value &lt; 0.05 on the t-test. We provide additional analysis on this result in later sections. Our Models Plank et al. (2016) BerendNguyen et BaselineAdversarial BiLSTMTNT CRF (2017) al.(2017) bg 98.34 98.53 97.97 96.84 96.36 95.63 97.4 cs 98.70 98.81 98.24 96.82 96.56 95.83  da 96.63 96.74 96.35 94.29 93.83 93.32 95.8 de
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2131774270	each language. Major neural character-level models include the character-level CNN (Ma and Hovy,2016) and (Bi)LSTM (Dozat et al.,2017). A Bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber,1997;Schuster and Paliwal, 1997) processes each sequence both forward and backward to capture sequential information, while preventing the vanishing / exploding gradient problem. We observed that the character-level BiLSTM outperfor
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2295030615	lighting a well-studied core problem of NLP, we propose and carefully analyze a neural part-of-speech (POS) tagging model that exploits adversarial training. With a BiLSTMCRF model (Huang et al.,2015;Ma and Hovy, 2016) as our baseline POS tagger, we apply adversarial training by considering perturbations to input word/character embeddings. In order to demystify the effects of adversarial training in the context of
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2099471712	nerable to, and thus is claimed to be effective (Goodfellow et al.,2015). It should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). GANs have already been applied to NLP tasks such as dialogue generation (Li et al.,2017) and transfer learning (Kim et al.,2017;Gui et al., 2017). Adversarial training also differs from adversarial
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	1673923490	old POS tags still yields better results, bolstering the view that POS tagging is an essential task in NLP that needs further development. 2.2Adversarial Training The concept of adversarial training (Szegedy et al., 2014;Goodfellow et al.,2015) was originally introduced in the context of image classiﬁcation to improve the robustness of a model by training on input images with malicious perturbations. Previous work (G
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2616177386	ow tagging accuracy of different models. For Plank et al. (2016), we include the traditional baselines TNT and CRF, and their state-ofthe-art model that employs a multi-task BiLSTM. Berend (2017) and Nguyen et al. (2017) are two recent works reporting POS tagging performance on UD v1.2. Languages with ¥ are morphologically rich, and those at the bottom (ÔelÕ to ÔtaÕ) are low-resourced, containing less than 60k tokens
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2579831847	s, and the rest show tagging accuracy of different models. For Plank et al. (2016), we include the traditional baselines TNT and CRF, and their state-ofthe-art model that employs a multi-task BiLSTM. Berend (2017) and Nguyen et al. (2017) are two recent works reporting POS tagging performance on UD v1.2. Languages with ¥ are morphologically rich, and those at the bottom (ÔelÕ to ÔtaÕ) are low-resourced, contai
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2153579005	sentation learning. For evaluating the tightness of word vector distribution, we employ the cosine similarity metric, which is widely used as a measure of the closeness between two word vectors (e.g.,Mikolov et al. (2013);Pennington et al.(2014)). To measure the tightness of each cluster, we compute the cosine similarity for every pair of words within, and then take the average. We also report the average tightness ac
2769107230	ROBUST MULTILINGUAL PART-OF-SPEECH TAGGING VIA ADVERSARIAL TRAINING	2295030615	ted Work 2.1POS Tagging Part-of-speech (POS) tagging is a fundamental NLP task that facilitates downstream tasks such as syntactic parsing. While current state-of-theart POS taggers (Ling et al.,2015;Ma and Hovy, 2016) yield accuracy over 97.5% on PTB-WSJ, there still remain issues. The per token accuracy metric is easy since taggers can easily assign correct POS tags to highly unambiguous tokens, such as punctuati
2769180089	Speech recognition for medical conversations	648786980	l use multi-headed attention [14], which extends the conventional attention mechanism to have multiple heads, where each head can generate different attention distributions. We use scheduled sampling [15] for training the decoder. At the beginning we use the ground truth as the previous prediction and as training proceeds we linearly ramp up the probability of sampling from model’s prediction up to 30
2769180089	Speech recognition for medical conversations	2622301280	nd music, cafenoises) ataSNRranging from5dBto25dB. We found that best results are obtained when noise is added when using CTC/CE training criteria and original audio is used while doing EMBR training [11]. 4.1. CTC For this task, we trained CTC models with context-dependent (CD) phoneme outputs. The model architecture is a stack of LSTM layers feeding a softmax output predicting 8K CD phone units plus
2769180089	Speech recognition for medical conversations	2626778328	s to the whole utterance for distinguishing noise, the attention mechanism can learn to focus mainly on the frames corresponding to the current prediction target. Our model use multi-headed attention [14], which extends the conventional attention mechanism to have multiple heads, where each head can generate different attention distributions. We use scheduled sampling [15] for training the decoder. At
2769180089	Speech recognition for medical conversations	2296545762	t T frames and then repeat the last input frame T times. During decoding, we used a 5-gram Language model trained on a mixture of medical and voice search/dictation data, using bayesian interpolation [13] for combining data from differentdomains. Forpronunciations weusedsupervisedpronunciation dictionaries (77%of pronunciations) and grapheme-tophoneme (G2P) models (23%of pronunciations). 4.2. LAS Our
2769180089	Speech recognition for medical conversations	2033256038	udiowiththeconversationtranscripts using a two-pass forced alignment approach. First pass: conﬁdence islands. We align the conversation to the transcript using conﬁdence-islands approach described in [9]. We recognized the audio using a constrained FST grammar G constructed as follows. 1. construct a linear FST where each arc represent a word in the transcript. 2. allow ǫ transitions from start state
2769180089	Speech recognition for medical conversations	2140943358	ve been evaluated on a clinical question answering task and it has been shown that domain adaptation with a language model improves the accuracy in interpreting spoken clinical questions signiﬁcantly [4]. Language model adaptation using crowdsourced input data has been shown to improve the accuracy of a medical speech recognition system [5]. The efﬁ- ciency and safety of using speech recognition assi
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2250539671	18: ct += 1 19: end while 20: epoch = epoch + 1 21: end while 2.6 Implementation Details In our full transfer learning model, we initialize the lookup table E with the pre-trained vectors from GloVe [26] by setting l as 300. For BCNN, the window size and activation function are set to be 4 and ReLU, and the feature map sizes are set to be 50 and 100 for PI and NLI; for the two convolution layers of P
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1648933886	4. Note that each element Ωi,j indicates the correlation between Wi and Wj, where i and j are one of s, sc, t and tc. Inspired by a general multi-task relationship learning framework as introduced in [46], we consider confining the output layer’s weights with Ω by using tr(WΩ−1WT ), where tr(·)is the trace of a square matrix. This means that if Ωi,j is a large positive/negative value, Wi and j will be
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1965667542	system architecture. Specifically, we first build an indexing for all the questions in our knowledge base (KB) using Apache Lucene 5. Next, given a query question, we employ TF-IDF ranking algorithm [39] in Lucene to compute its similarities to all the questions in the KB, and call back the top-K candidate questions. We then use a reranking algorithm to compute the similarities between the query and
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2767802162	ationships Learning, Adversarial Training 1 INTRODUCTION Question Answering (QA) systems have been widely developed and used in many domains. Examples of industry applications include Alibaba’s AliME [18, 27], Microsoft’s SuperAgent [7], Apple’s Siri and Google’s Google Assistant. Generally speaking, there are two kinds of commonly-used techniques behind most QA systems: Information Retrieval (IR)-based m
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2211192759	For base models, we compared our hCNN model with the following models: •BCNN is the left component of our hCNN model, which incorporates element-wise comparisons on top of the base model proposed in [43]. •Pyramid is the right component of our hCNN model based on sentence interactions as in [24]. •ABCNN is the attention-based CNN model by [43]. •BiLSTMis similar to BCNN, but uses LSTM instead of CNN
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1648933886	combined objective function as follows: min Ω,W,W d,Θ c, Θ s,Θ t,bs,bt,b d L+λ0ℓ s.t. Ω ≥0, tr(Ω)= 1, where λ0 is a hyper-parameter for tuning the importance of the adversarial loss. As suggested by [46], it is not easy to optimize such a semi-definite programming problem. We will present an alternating training approach in Section 2.5 for solving it efficiently. 2.4 Base Model Although the proposed
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2118463056,2556553881	an efficient and effective base model for encoding a pair of sentences. On one hand, although various attention-based LSTM architectureshavebeenproposedtoachieveasuperiorperformanceonboth PI and NLI [6, 25, 29, 36], these models are very time-consuming due to the computation of memory cells and attention weights in each time step, which may not satisfy the industry demand, especially when QPS is high. On the ot
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1840435438	efficiently improve the performance on a resource-poor target domain by leveraging knowledge from a resource-rich source domain. 3Queries Per Second AHybridBaseModel.Observing that LSTM-based methods [3, 6] are much more time-consuming than CNN-based methods [21, 43], we focus on CNN-based methods in this study. Meanwhile, there are typically two types of CNN-based methods for the task, namely sentence
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1840435438	ent of our hCNN model based on sentence interactions as in [24]. •ABCNN is the attention-based CNN model by [43]. •BiLSTMis similar to BCNN, but uses LSTM instead of CNN to encode each sentence as in [3]. •ESIM is one of the state-of-the-art attention-based LSTM models on SNLI proposed by [6]. •hCNNis our hybrid CNN model as introduced in Section 2.4. For evaluating the proposed transfer learning fra
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2211192759	et domain by leveraging knowledge from a resource-rich source domain. 3Queries Per Second AHybridBaseModel.Observing that LSTM-based methods [3, 6] are much more time-consuming than CNN-based methods [21, 43], we focus on CNN-based methods in this study. Meanwhile, there are typically two types of CNN-based methods for the task, namely sentence encoding (SE)-based methods [22, 42] and sentence interaction
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2294860948	g than CNN-based methods [21, 43], we focus on CNN-based methods in this study. Meanwhile, there are typically two types of CNN-based methods for the task, namely sentence encoding (SE)-based methods [22, 42] and sentence interaction (SI)-based methods [13, 24]. We argue that these two types of methods may highly complement each other, and thus we propose a hybrid CNN model by combining an SE-based method
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1648933886	Hb ⊕Hp. 2.5 Inference In our combined objective function, we have nine parameters Ω, W, Wd, bs, bt, bd, Θc, Θs and Θt, and it is not easy to optimize them at the same time. Following the practice in [46], we employ an alternating stochastic method, i.e., first optimizing the other eight parameters by fixingΩ, and then alternatively optimizing Ω by fixing the others in each iteration. The details are
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2170738476,2286300105	hem into a single representation [22, 42], while the latter tries to directly model the interaction between two sentences at the beginning and then makes abstractions on top of the interaction output [13, 24]. Observing that the two lines of methods focus on differentperspectives tomodelsentencepairs, we expectthatacombination of them can capture both good sentence representations and rich interaction str
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2556553881	hus we propose a hybrid CNN model by combining an SE-based method [43] and an SI-based method [24]. Specifically, we modify the SE-based method using two element-wise comparison functions inspired by [21, 36] to match the two sentence embeddings, and then concatenate them together with sentence embeddings from the SI-based method. Transfer Learning Framework. Transfer learning aims to apply knowledge gain
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1731081199	iminate across two domains, we define the adversarial loss as minimizing the negative entropy of the predicted domain distribution, which is different from maximizing the negative cross-entropy as in [11, 19]: ℓ = Õ k∈s,t  1 nk Õn k i=1 Õ1 j=0 p(dij |Xi 1,X i 2 )logp(dij |Xi1 ,Xi 2 )  . (2) Finally, we obtain a combined objective function as follows: min Ω,W,W d,Θ c, Θ s,Θ t,bs,bt,b d L+λ0ℓ s.t. Ω ≥0, t
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1731081199	inate the noisy features, here we also consider incorporating an adversarial loss on the shared feature space so that the trained model can not distinguish between the source and target domains on it [11]. First, we assume that the shared layer zc is mapped to a binary domain label d, which indicates whether zc comes from the source or the target domain: p(d |zc)= softmax(W dzc +b ). Since the goal of
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2175723921	instead of using the output of the time-consuming LSTM model, we feed another three features, namely, Word Mover’s Distance [16], keywords features [34] and the cosine distance of sentence embeddings [37] to a gradient boosted regression tree (GBDT). To combine our model with the existing ranking method, we treat the probability of being paraphrases predicted by our model as an additional feature, and
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1731081199	layer to uncover both the inter-domain and the intradomain relationships. Besides, to make the shared representation more invariant across domains, we follow some recent work on adversarial networks [11, 19] and introduce an adversarial loss on the shared feature space in our method. Fig. 3 gives an outline of our full model. To evaluate our proposed method, we conduct both intrinsic evaluation and extri
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2251202616	ly consider formulating our question rerank module as a PI task, but one can also model it as an NLI task. Our existing reranking method is based on this ensemble method for the Answer Selection task [34]. But instead of using the output of the time-consuming LSTM model, we feed another three features, namely, Word Mover’s Distance [16], keywords features [34] and the cosine distance of sentence embed
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2294860948	n a pair of sentences, we would like to identify their semantic relation. For paraphrase identification (PI), the semantic relation indicates whether or not the two sentences express the same meaning [42]; for natural language inference (NLI), it indicates whether a hypothesis sentence can be inferred from a premise sentence [3]. Formally, assume there are two sentences X1 = (x1 1 ,x 1 2 ,...,x 1 m) a
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2111362445,2122838776	ng. The former focuses on mining from the source labeled data to find those instances that are similar to the distribution of the target domain, and combine them together with the target labeled data [8, 15]. The core idea of the latter line of work is to find a shared feature space, which can reduce the divergence between the distribution of the source and the target domains [1, 17, 28, 33]. Our work fo
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2556553881	NN[43],wefirstuse two separate 1-D convolutional (conv) and 1-D max-pooling layers to encode the two input sentences into two sentence embeddings: h1 = CNN(X1); h2 = CNN(X2). Furthermore,assuggestedby[21,36]thatelement-wisecomparison can work well on the problem, we use two comparison functions to match the two sentence embeddings, and then concatenate them together with the sentence embeddings as the se
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1840435438,2103305545,2211192759	nswer for question C2 in the KB as the query question’s answer. In the literature, paraphrase identification (PI) and natural language inference (NLI) have been extensively studied in the last decade [3, 4, 6, 30, 43]. However, when applying existing solutions to PI and NLI in chatbot systems in the E-commerce industry, there are at least two major challenges we face: (1) Lack of rich training data: All these solu
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1840435438,2103305545,2211192759	nt neural networks, including Recursive Neural Networks (ReNN), Reccurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), into Paraphrase Identification and Natural Language Inference [3, 30, 43]. Although all these models have been shown to significantly outperform the traditional methods without deep learning, most of them only focus on improving the performance of standard in-domain settin
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1932968309	o each other, and can work better when combined. Moreover, we verified that the improvements of hCNN over the other methods are significant withp &lt; 0.05 based on McNemar’s paired significance test [12]. Finally, while the computational cost of hCNN is slightly higher than BCNN and Pyramid, it can still serve 233 question pairs per second, which is able to satisfy the current demand of our industria
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	658020064	od is based on this ensemble method for the Answer Selection task [34]. But instead of using the output of the time-consuming LSTM model, we feed another three features, namely, Word Mover’s Distance [16], keywords features [34] and the cosine distance of sentence embeddings [37] to a gradient boosted regression tree (GBDT). To combine our model with the existing ranking method, we treat the probabili
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1648933886	its partial derivatives with respect to the eight parameters. UpdatingΩ. After fixing the eight parameters, the optimization problem is as follows: min Ω tr(WΩ−1WT ) s.t. Ω ≥0, tr(Ω)= 1. As proved by [46], the above optimization problem has an analytical solution Ω = (W T W) 1 2 tr (WT W) 1 2 . Finally, we present the whole procedure for training our full model as in Algorithm 1. Note that we only upd
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2286300105	ponent of our hCNN model, which incorporates element-wise comparisons on top of the base model proposed in [43]. •Pyramid is the right component of our hCNN model based on sentence interactions as in [24]. •ABCNN is the attention-based CNN model by [43]. •BiLSTMis similar to BCNN, but uses LSTM instead of CNN to encode each sentence as in [3]. •ESIM is one of the state-of-the-art attention-based LSTM
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2286300105	Pyramid: As shown in the rightmost part of Fig 3, we first produce an interaction matrix M ∈Rm×m, where Mi,j denotes the similarity score between the ith word in X1 and the jth word in X2. Following [24], we use dot-product to compute the similarity score. Next, by viewing the interaction matrix as an image, we stack two 2-D convolutional layers and two 2-D max-pooling layers on it to obtain the hidd
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2162355876	in Section 1, our online chatbot system is based on traditional information retrieval techniques, where the goal is to obtain the nearest question in the knowledge base for a given customer question [14]. Fig. 1 depicts the whole system architecture. Specifically, we first build an indexing for all the questions in our knowledge base (KB) using Apache Lucene 5. Next, given a query question, we employ
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2170738476,2286300105	sed methods in this study. Meanwhile, there are typically two types of CNN-based methods for the task, namely sentence encoding (SE)-based methods [22, 42] and sentence interaction (SI)-based methods [13, 24]. We argue that these two types of methods may highly complement each other, and thus we propose a hybrid CNN model by combining an SE-based method [43] and an SI-based method [24]. Specifically, we m
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2294860948	sentence interaction (SI)-based methods. The former aims to first learn good representations for each sentence, followed by using a comparison function to transform them into a single representation [22, 42], while the latter tries to directly model the interaction between two sentences at the beginning and then makes abstractions on top of the interaction output [13, 24]. Observing that the two lines of
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2278264165	sentence pairs across domains. Deep Transfer Learning: With the recent advances of deep learning, different NN-based TL frameworks have been proposed for image processing [44] and speech recognition [35] as well as NLP [19, 22, 41]. A simple but widely used framework is referred to as fine-tuningapproaches, which first use the parameters of the well trained models on the source domain to initialize t
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2120354757	st existing studies for TL can be generally categorized into two groups. The first line of work assumes that we have enough labeled data from a source domain andalsoalittlelabeleddatafromatargetdomain[9],andthesecond line assumes that we only have labeled data from source domain but may also have some unlabeled data from a target domain [2, 45]. Our study belongs to the first line of work, which is a
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2123261262,2166096645	target labeled data [8, 15]. The core idea of the latter line of work is to find a shared feature space, which can reduce the divergence between the distribution of the source and the target domains [1, 17, 28, 33]. Our work follows the latter one, and tries to leverage NN models to learn a shared hidden representation for sentence pairs across domains. Deep Transfer Learning: With the recent advances of deep l
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	2211192759	th good sentence representations and rich interaction structures. Hence, we propose a hybrid CNN (hCNN) model, which are based on some minor modifications of two existing models: a SEbased BCNN model [43] and a SI-based Pyramid model [24]. Fig. 3 depicts our full transfer learning framework, which contains one shared hCNN and two domain-specific hCNNs. Below we briefly go through the architecture of h
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1840435438	tion indicates whether or not the two sentences express the same meaning [42]; for natural language inference (NLI), it indicates whether a hypothesis sentence can be inferred from a premise sentence [3]. Formally, assume there are two sentences X1 = (x1 1 ,x 1 2 ,...,x 1 m) andX2 = (x2 1 ,x 2 2 ,...,x 2 n),wherex j i denotesanl-dimensionaldense embedding vector retrieved from a lookup table E ∈Rl×|V
2769216919	Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce	1731081199,2553897675	tly learn the shared feature representations and domain relationships in a unified model. Moreover, inspired by the recent success of applying adversarial networks into unsupervised domain adaptation [11, 31] and multi-task learning [19], we also incorporate the adversarial training into our transfer learning model in order to learn a more robust shared feature space across domains. 6 CONCLUSIONS In this
2769298630	Modeling Past and Future for Neural Machine Translation	2195405088	. The FUTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difﬁculty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We ﬁrst evaluate the proposed model on the Chin
2769298630	Modeling Past and Future for Neural Machine Translation	2167839676	!&quot;(# &amp; Figure 2: NMT decoder augmented with PAST and FUTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, w
2769298630	Modeling Past and Future for Neural Machine Translation	2410539690	1 improvements of BLEU scores in three tasks, respectively. In addition, it obtains an alignment error rate of 35.90%, signiﬁcantly lower than the baseline (39.73%) and the coverage model (38.73%) by Tu et al. (2016). We observe that in traditional attention-based NMT, most errors occur due to over- and under-translation, which is probably because the decoder RNN fails to keep track of what has been translated an
2769298630	Modeling Past and Future for Neural Machine Translation	1411230545	2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Boja
2769298630	Modeling Past and Future for Neural Machine Translation	2410539690	addition, we explicitly model both translated (with PAST-RNN) and untranslated (with FUTURERNN) instead of using a single coverage vector to indicate translated source words. Another difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed to not only the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a me
2769298630	Modeling Past and Future for Neural Machine Translation	1793121960	al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktaschel et¨ al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rocktaschel et al. (2017) propose a¨ keyvalue-predict attention model, which outputs three vectors at each step: the ﬁrst is used to predict the next-word distribution; the second serves as the key
2769298630	Modeling Past and Future for Neural Machine Translation	1753482797	the conventional coverage model in terms of both the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the give
2769298630	Modeling Past and Future for Neural Machine Translation	1522301498	dden sizes are 1024. In training, we set the batch size as 80 for ZhEn, and 64 for De-En and En-De. We set the beam size as 12 in testing. We shufﬂe the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the propose
2769298630	Modeling Past and Future for Neural Machine Translation	2594229957,2740433069	e, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the
2769298630	Modeling Past and Future for Neural Machine Translation	2594229957,2740433069	E + dropout 31.9 27.2 27.4 21.0 + name entity forcing + synthetic data 36.9 29.0 30.9 22.7 Escolano et al. (2017) Char2Char + Rescoring with inverse model 32.1 - 27.0 - + synthetic data - 28.1 - 21.2 Sennrich et al. (2017a) cGRU + BPE + synthetic data 38.0 32.0 32.2 26.1 this work BASE 32.0 27.8 28.3 23.3 COVERAGE 32.2 28.7 28.9 23.6 OURS 33.5 29.7 29.5 24.3 Table 5: Results of De-En and En-De “synthetic data” denotes
2769298630	Modeling Past and Future for Neural Machine Translation	2410539690	equence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. CoverageModeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities
2769298630	Modeling Past and Future for Neural Machine Translation	2409591106	evealed that the overloaded use of representations makes model training difﬁcult, and such problem can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktaschel et¨ al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rock
2769298630	Modeling Past and Future for Neural Machine Translation	2487501366	g to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017); in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future
2769298630	Modeling Past and Future for Neural Machine Translation	2415583245	ifference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed to not only the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder !# !&quot; Past Layer !$ %# %&quot; !&quot; &amp; Attention ( Present ) Layer Decoder Layer Future Layer !# !&quot; ! &apos; $ &apos; &apos; !$ &amp; source summariz
2769298630	Modeling Past and Future for Neural Machine Translation	2580192806	ight, thus failing to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017); in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making
2769298630	Modeling Past and Future for Neural Machine Translation	2552838200	re lacks explicit mechanisms to maintain translated and untranslated contents. Evidence show that attention-based NMT still suffers from serious over- and under-translation problems (Tu et al., 2016; Tu et al., 2017b). Examples of under-translation are shown in Table 1a. Another piece of evidence also shows the decoder may lack a holistic view of the source information, explained as below. We conduct a pilot exp
2769298630	Modeling Past and Future for Neural Machine Translation	2470713034	loaded use of representations makes model training difﬁcult, and such problem can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktaschel et¨ al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rocktaschel et al. (2017) p
2769298630	Modeling Past and Future for Neural Machine Translation	2743455151	n of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder’s hidden states to not only generate the current target word, but also predict the target words that remain untranslated. Along the direction of future modeling, we introduce a FUTU
2769298630	Modeling Past and Future for Neural Machine Translation	2131774270	nformation as described in Section 1. This process is illustrated in Figure 1. Formally, let x = fx 1;:::;x Igbe a given input sentence. The encoder RNN—generally implemented as a bi-directional RNN (Schuster and Paliwal, 1997)—transforms the sentence to a sequence c t h i h i h I h I h 1 h 1 x 1 x i Encoder + ! t,1 ! t, i ! t, I s s y t Decoder h on source vector for present translation x I Figure 1: Architecture of attent
2769298630	Modeling Past and Future for Neural Machine Translation	2594229957,2740433069	NMT systems of WMT17. Our proposed model improves the strong baseline on both De-En and En-De. This shows that our proposed model work well across different language pairs. Rikters et al. (2017) and Sennrich et al. (2017a) obtain higher BLEU scores than our model, because they use additional large scaled synthetic data (about 10M) for training. It maybe unfair to compare our model to theirs directly. 5.3 Analysis We
2769298630	Modeling Past and Future for Neural Machine Translation	2514713644	o 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences of length up to 50 words in the training data. The dimension of word embeddings is 512, an
2769298630	Modeling Past and Future for Neural Machine Translation	2522143790	odel (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. CoverageModeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities at each decoding step
2769298630	Modeling Past and Future for Neural Machine Translation	2133564696	opose to explicitly model the holistic source summarization by PAST and FUTURE contents at each decoding step. 3 Related Work Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. CoverageModeling. Tu et al. (2016) and Mi et al. (2016) maintain a
2769298630	Modeling Past and Future for Neural Machine Translation	1816313093	pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En 4, and the multi-bleu-detok script for De-En and EnDe 5. 3The corpora includ
2769298630	Modeling Past and Future for Neural Machine Translation	2410539690	practice, as there lacks explicit mechanisms to maintain translated and untranslated contents. Evidence show that attention-based NMT still suffers from serious over- and under-translation problems (Tu et al., 2016; Tu et al., 2017b). Examples of under-translation are shown in Table 1a. Another piece of evidence also shows the decoder may lack a holistic view of the source information, explained as below. We co
2769298630	Modeling Past and Future for Neural Machine Translation	2535697732	Recent work has revealed that the overloaded use of representations makes model training difﬁcult, and such problem can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktaschel et¨ al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaata
2769298630	Modeling Past and Future for Neural Machine Translation	2552838200	s described above, the FUTURE layer models the future semantics in a declining way: F t = sFt 1 sFt ˇc t. Since source and target sides contain equivalent semantic information in machine translation (Tu et al., 2017a): c t ˇE(y t), we directly measure the consistence between F t and E(y t), which guides the subtraction to learn the right thing: loss( F t;E(y t)) = log exp l( F t ;E(yt)) P y exp l( F t ;E(y)) l(u
2769298630	Modeling Past and Future for Neural Machine Translation	1902237438	for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separ
2769298630	Modeling Past and Future for Neural Machine Translation	2410539690	T contents indicate translated information, whereas the FUTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (Tu et al., 2016). Ideally, PAST grows and FUTURE declines during the translation process. However, it may be difﬁcult for a single RNN to explicitly model the above processes. In this paper, we propose a novel neural
2769298630	Modeling Past and Future for Neural Machine Translation	2101105183	t. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En 4, and the multi-bleu-detok script for De-En and EnDe 5. 3The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 a
2769298630	Modeling Past and Future for Neural Machine Translation	2133564696	T and FUTURE contents. 2 Motivation In this section, we ﬁrst introduce the standard attention-based NMT, and then motivate our model by several empirical ﬁndings. The attention mechanism, proposed in Bahdanau et al. (2015), yields a dynamic source context vector for the translation at a particular decoding step, modeling PRESENT information as described in Section 1. This process is illustrated in Figure 1. Formally, l
2769298630	Modeling Past and Future for Neural Machine Translation	2534200568	tention ( Present ) Layer Decoder Layer Future Layer !# !&quot; ! &apos; $ &apos; &apos; !$ &amp; source summarization !&quot;(# &amp; Figure 2: NMT decoder augmented with PAST and FUTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated c
2769298630	Modeling Past and Future for Neural Machine Translation	2594229957,2740433069	thub.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl Training Details. We use the Nematus 6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNSEARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocab
2769298630	Modeling Past and Future for Neural Machine Translation	2130942839	the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source. During translation, the decoder i
2769298630	Modeling Past and Future for Neural Machine Translation	2133564696	ver, it may be difﬁcult for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the atte
2769298630	Modeling Past and Future for Neural Machine Translation	2594990650	w 10) is used. This indicates that enhancing performance is non-trivial by simply adding more RNN layers into the decoder without any explicit instruction, which is consistent with the observation of Britz et al. (2017) Our model also outperforms the word-level COVERAGE (Tu et al., 2016), which considers the coverage information of the source words independently. Our proposed model can be regarded as a high-level co
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	179314280	been commonly used in NLP problems, such as in parsing and translation, in order to make use of high-order or global features that are too expensive for decoding algorithms (Collins &amp; Koo, 2005; Shen et al., 2004; Huang, 2008; Dyer et al., 2016). Here we apply the idea of re-ranking; for each answer candidate, we efﬁciently incorporate global information from multiple pieces of textual evidence without signiﬁ
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2104009457	early as (Green Jr et al., 1961) and was popularized by TREC-8 (Voorhees, 1999). The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al., 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017). Recent efforts (Chen et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b; Wang et al., 2017) beneﬁt from
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2289899728	ems, such as in parsing and translation, in order to make use of high-order or global features that are too expensive for decoding algorithms (Collins &amp; Koo, 2005; Shen et al., 2004; Huang, 2008; Dyer et al., 2016). Here we apply the idea of re-ranking; for each answer candidate, we efﬁciently incorporate global information from multiple pieces of textual evidence without signiﬁcantly increasing the complexity
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2427527485	the ground truth answers and the types of questions on TriviaQA and Quasar-T. We do not include the analysis on SearchQA because the Jeopardy! style 10Our evaluation is based on the tool from SQuAD (Rajpurkar et al., 2016). 8 Under review as a conference paper at ICLR 2018 Quasar-T SearchQA TriviaQA (open) Top-K EM F1 EM F1 EM F1 1 35.1 41.6 51.2 57.3 47.6 53.5 3 46.2 53.5 63.9 68.9 54.1 60.4 5 51.0 58.9 69.1 73.9 58.0
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2171278097	ims to answer questions from a broad range of domains by effectively marshalling evidence from large open-domain knowledge sources. Such resources can be Wikipedia (Chen et al., 2017), the whole web (Ferrucci et al., 2010), structured knowledge bases (Berant et al., 2013; Yu et al., 2017) or combinations of the above (Baudiˇs &amp; Sedivˇ y, 2015).` Recent work on open-domain QA has focused on using unstructured text r
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2134729743	in NLP problems, such as in parsing and translation, in order to make use of high-order or global features that are too expensive for decoding algorithms (Collins &amp; Koo, 2005; Shen et al., 2004; Huang, 2008; Dyer et al., 2016). Here we apply the idea of re-ranking; for each answer candidate, we efﬁciently incorporate global information from multiple pieces of textual evidence without signiﬁcantly increa
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2252136820	s by effectively marshalling evidence from large open-domain knowledge sources. Such resources can be Wikipedia (Chen et al., 2017), the whole web (Ferrucci et al., 2010), structured knowledge bases (Berant et al., 2013; Yu et al., 2017) or combinations of the above (Baudiˇs &amp; Sedivˇ y, 2015).` Recent work on open-domain QA has focused on using unstructured text retrieved from the web to build machine comprehens
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2427527485	top-N candidate passages, p 1, p 2, ..., p N, from the web. Then a reading comprehension (RC) model is used to extract the answer from these passages. This setting is different from closed-domain QA (Rajpurkar et al., 2016), where a single ﬁxed passage is given, from which the answer is to be extracted. When developing a closed-domain QA system, we can use the speciﬁc positions of the answer sequence in the given passag
2769395616	Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering	2252136820	zed by TREC-8 (Voorhees, 1999). The task is to produce the answer to a question by exploiting resources such as documents (Voorhees, 1999), webpages (Kwok et al., 2001) or structured knowledge bases (Berant et al., 2013; Bordes et al., 2015; Yu et al., 2017). Recent efforts (Chen et al., 2017; Dunn et al., 2017; Dhingra et al., 2017b; Wang et al., 2017) beneﬁt from the advances of machine reading comprehension (RC)
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2399880602	(LF) [5] encodes the image, dialog history and question separately and later concatenated together and linearly transformed to a joint representation. HRE [5] applies a hierarchical recurrent encoder [27] to encode the dialog history and the HREA [5] additionally adds an attention mechanism on the dialogs. MemoryNetwork(MN)[5] maintains each previous question and answer as a ‘fact’ in its memory bank
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2171810632,2463565445	-pair discriminative loss and a self-attention mechanism. AMEM [25] applies a more advanced memory network to model the dependency of current question on previous attention. Additional two VQA models [19,40] are used for comparison. Table2shows that our model outperforms the previous baseline and stateof-the-art models on all the evaluation metrics. 4.4. Human study Above experiments verify the effective
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2558809543	20] propose an Image Grounded Conversation (IGC) dataset and task that requires a model to generate natural-sounding conversations (both questions and responses) about a shared image. De Vries et al. [7] propose a GuessWhat game style dataset, where one person asks questions about an image to guess which object has been selected, and the second person answers questions in yes/no/NA. Das et al. [5] pr
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2542835211	discriminator has been used to adjust the generation process, we ﬁnd it is still important to feed human generated responses to the generator for the model updating. Hence, we apply a teacher forcing [14,16] strategy to update the parameters in the generator. Speciﬁcally, at each training iteration, we ﬁrst update the generator using the reward obtained from the sampled data with the generator policy. Th
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2171810632	E [18] model, Model MRR R@1 R@5 R@10 Mean LF [5] 0.5807 43.82 74.68 84.07 5.78 HRE [5] 0.5846 44.67 74.50 84.22 5.72 HREA [5] 0.5868 44.82 74.81 84.36 5.66 MN [5] 0.5965 45.55 76.22 85.37 5.46 SAN-QI [40] 0.5764 43.44 74.26 83.72 5.88 HieCoAtt-QI [19] 0.5788 43.51 74.49 83.96 5.84 AMEM [25] 0.6160 47.74 78.04 86.84 4.99 HCIAE-NP-ATT [18] 0.6222 48.48 78.75 87.59 4.81 Ours 0.6398 50.29 80.71 88.81 4.47
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	10957333	ebook have attracted signiﬁcant public attention. In NLP, dialog generation is typically viewed as a sequence-to-sequence (Seq2Seq) problem, or formulated as a statistical machine translation problem [23,30]. Inspired by the success of the Seq2Seq model [32] in the machine translation, [26,33] build end-to-end dialog generation models using an encoder-decoder model. Reinforcement learning (RL) has also b
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	648143168	espectively. Adversarial learning Generative adversarial networks [9] have enjoyed great successes in a wide range of applications in Computer Vision, [3 ,21 24], especially in image generation tasks [8,43]. The learning process is formulated as an adversarial game in which the generative model is trained to generate outputs to fool the discriminator, while the discriminator is trained not to be fooled.
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2119717200	f the encoder. are more ﬁtting with the dialog history. In order to consider the reward at the local (i.e. word and phase) level, we use a Monte Carlo (MC) search strategy and the REINFORCE algorithm [36] is used to update the policy gradient. An overview of our model can be found in the Fig.2. In the following sections, we will introduce each component of our model separately. 3.1. A sequential co-at
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	1895577753,1905882502,2404394533	idate response list is given) and achieve the state-of-the-art performance. 2. Related work Visual dialog is the latest in a succession of vision-andlanguage problems that began with image captioning [11, 34,37], and includes visual question answering [1,22,38]. However, in contrast to these classical vision-and-language tasks that only involve at most a single natural language interaction, visual dialog req
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2099471712	iminative model is used as a reward to the generative model, encouraging it to generate more human-like dialogue. Although our proposed framework is inspired by generative adversarial networks (GANs) [9], there are several technical contributions that lead to the ﬁnal success on the visual dialog generation task. First, we propose a sequential co-attention generative model that aims to ensure that at
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	1933349210,2176212817	language has enabled the development of a range of applications that have made interesting steps towards Artiﬁcial Intelligence, including Image Captioning [11,34,37], Visual Question Answering (VQA) [1,22,38], and Referring Expressions [10,12,41]. VQA, for example, requires an agent to answer a previously unseen question about a previously unseen image, and is recognised as being an AI-Complete problem [1
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2130942839	LP, dialog generation is typically viewed as a sequence-to-sequence (Seq2Seq) problem, or formulated as a statistical machine translation problem [23,30]. Inspired by the success of the Seq2Seq model [32] in the machine translation, [26,33] build end-to-end dialog generation models using an encoder-decoder model. Reinforcement learning (RL) has also been applied to train a dialog system. Li et al. [15
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	1895577753,1905882502,2404394533	n The combined interpretation of vision and language has enabled the development of a range of applications that have made interesting steps towards Artiﬁcial Intelligence, including Image Captioning [11,34,37], Visual Question Answering (VQA) [1,22,38], and Referring Expressions [10,12,41]. VQA, for example, requires an agent to answer a previously unseen question about a previously unseen image, and is re
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	1933349210,2176212817	of-the-art performance. 2. Related work Visual dialog is the latest in a succession of vision-andlanguage problems that began with image captioning [11, 34,37], and includes visual question answering [1,22,38]. However, in contrast to these classical vision-and-language tasks that only involve at most a single natural language interaction, visual dialog requires the machine to hold a meaningful dialogue in
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2241785942,2489434015	of a range of applications that have made interesting steps towards Artiﬁcial Intelligence, including Image Captioning [11,34,37], Visual Question Answering (VQA) [1,22,38], and Referring Expressions [10,12,41]. VQA, for example, requires an agent to answer a previously unseen question about a previously unseen image, and is recognised as being an AI-Complete problem [1]. Visual Dialogue [5] represents an e
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2119717200	rated dialog by the discriminator (i.e., r(f~v; ~u;Q; A^g)) is used as a reward for the generator, which is trained to maximize the expected reward of generated response using the REINFORCE algorithm [36]: J() = E A^˘ˇ(A^jV;U;Q) (r(f~v; ~u;Q; A^g)j) (7) Given the input visual information (V), question (Q) and dialog history utterances (U), the generator generates an response answer A^by sampling fro
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2171810632	and Robotics, which is to enable an agent capable of acting upon the world, that we might collaborate with through dialogue. Due to the similarity between the VQA and Visual Dialog tasks, VQA methods [19,40] have been directly applied to solve the Visual Dialog problem. The fact that the Visual Dialog challenge requires an ongoing conversation, however, demands more than just taking into consideration th
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2434741482	ses indistinguishable from those a human might produce. In this paper, we introduce an adversarial learning strategy, motivated by the previous success of adversarial learning in many computer vision [3,21] and sequence generation [4,42] problems. We particularly frame the task as a reinforcement learning problem that we jointly train two sub-modules: a sequence generative model to produce response sent
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	10957333	story of the dialogue thus far. As we show in our experiments in Sec.4, this procedure results in our generator producing more suitable responses. Dialog generation in NLP Text-only dialog generation [15,16,23,30,39] has been studied for many years in the Natural Language Processing (NLP) literature, and has leaded to many applications. Recently, the popular ‘Xiaoice’ produced by Microsoft and the ‘Its Alive’ cha
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2099471712	are well supported by the associated reasoning. More details about our generator and discriminator can be found in Sections3.1 and3.2respectively. Adversarial learning Generative adversarial networks [9] have enjoyed great successes in a wide range of applications in Computer Vision, [3 ,21 24], especially in image generation tasks [8,43]. The learning process is formulated as an adversarial game in
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	2558809543	tion In this section, we describe our adversarial learning approach to generating natural dialog responses based on an image. There are several ways of deﬁning the visual based dialog generation task [7,20]. We follow the one in [5], in which an image I, a ‘ground truth’ dialog history (including an image description C) H = (C;(Q 1;A 1);:::;(Q t 1;A t 1)) (we deﬁne each QuestionAnswer (QA) pair as an ut
2769430803	Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning	889023230	y viewed as a sequence-to-sequence (Seq2Seq) problem, or formulated as a statistical machine translation problem [23,30]. Inspired by the success of the Seq2Seq model [32] in the machine translation, [26,33] build end-to-end dialog generation models using an encoder-decoder model. Reinforcement learning (RL) has also been applied to train a dialog system. Li et al. [15] simulate two virtual agents and ha
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2115242108	; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classiﬁcation (Bhatia et al., 2015), we also represent documents via Rhetorical Structure Theory’s (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although deﬁnitions for EDUs vary in the literature, we follow standard pra
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2084046180	, against the following methods: Majority: Majority class applied to all instances. SO-CAL: State-of-the-art lexicon-based system that classiﬁes segments into positive, neutral, and negative classes (Taboada et al., 2011). Seg-CNN: Fully-supervised CNN segment classiﬁer trained on SPOT’s labels (Kim, 2014). GICF: The Group-Instance Cost Function model introduced in Kotzias et al. (2015). This is an unweighted average
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2044599851	(Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although deﬁnitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003). We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes d
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2148404145	14) and the underlying systems exhibit varying degrees of linguistic sophistication from identifying aspects (Lerman et al., 2009) to using RSTstyle discourse analysis, and manually deﬁned templates (Gerani et al., 2014; Di Fabbrizio et al., 2014). Our proposed method departs from previous work in that it focuses on detecting opinions in individual documents. Given a review, we predict the polarity of every segment,
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2093835839	assiﬁcation datasets, the use of CNNs results in a relative performance decrease of &lt; 2% compared Yang et al’s model (2016). Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014) assumes a set of documents that contain opinions about some entity of interest (e.g., camera). The goal o
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2165075008	classiﬁcation made the strong assumption that a bag is negative only if all of its instances are negative, and positive otherwise (Dietterich et al., 1997; Maron and Ratan, 1998; Zhang et al., 2002; Andrews and Hofmann, 2004; Carbonetto et al., 2008). Subsequent work relaxed this assumption, allowing for prediction combinations better suited to the tasks at hand. Weidmann et al. (2003) introduced a generalized MIL framew
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2142972908	CNN(X). A ﬁnal sentiment prediction is produced using a softmax classiﬁer and the model is trained via backpropagation using sentence-level sentiment labels. The availability of large-scale datasets (Diao et al., 2014; Tang et al., 2015) has also led to the development of document-level sentiment classiﬁers which exploit hierarchical neural representations. These are obtained by ﬁrst building representations of se
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2135336649	deﬁnitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003). We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its const
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2158681777	e since been applied to drug activity prediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classiﬁcation (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). When applied to sentim
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2165075008	ediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classiﬁcation (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). When applied to sentiment analysis, MIL takes advantage of supervision
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2133564696	ent feature vector (Tang et al., 2015). Yang et al. (2016) further acknowledge that words and sentences are deferentially important in different contexts. They present a model which learns to attend (Bahdanau et al., 2015) to individual text parts when constructing document representations. We describe such an architecture in more detail as we use it as a point of comparison with our own model. Given document d compris
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2160660844	entiment Classiﬁcation Sentiment classiﬁcation is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1Our code and SPOT dataset are publicly available at: https://github.com/stangelid/milnet-sent larity of a text can be comp
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	1501617060	er at distilling polarity compared to the LEAD and RANDOM (EDUs) baselines. We should point out that the LEAD system is not a strawman; it has proved hard to outperform by more sophisticated methods (Nenkova, 2005), particularly on the newswire domain. Example EDU- and sentence-based summaries produced by gated variants of HIERNET and MILNET are shown in Figure 8, with attention weights and polarity scores of t
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2124859243	et al. (2015) used sentence vectors obtained by a pre-trained hierarchical CNN (Denil et al., 2014) as features under an unweighted average MIL objective. Prediction averaging was further extended by Pappas and Popescu-Belis (2014; 2017), who used a weighted summation of predictions, an idea which we also adopt in our work. Applications of MIL are many and varied. MIL was ﬁrst explored by Keeler and Rumelhart (1992) for recogn
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2124859243	in the framework of MIL, T¨ackstr ¨om and McDonald (2011) show how sentence sentiment labels can be learned as latent variables from document-level annotations using hidden conditional random ﬁelds. Pappas and Popescu-Belis (2014) use a multiple instance regression model to assign sentiment scores to speciﬁc aspects of products. The Group-Instance Cost Function (GICF), proposed by Kotzias et al. (2015), averages sentence senti
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2251939518	g et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classiﬁcation tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2093835839	the system is to generate a summary that is representative of the average opinion and speaks to its important aspects (e.g., picture quality, battery life, value). Output summaries can be extractive (Lerman et al., 2009) or abstractive (Gerani et al., 2014; Di Fabbrizio et al., 2014) and the underlying systems exhibit varying degrees of linguistic sophistication from identifying aspects (Lerman et al., 2009) to using
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2133564696	iment clues. We opt for a segment attention mechanism which rewards text units that are more likely to be good sentiment predictors. Our attention mechanism is based on a bidirectional GRU component (Bahdanau et al., 2015) and The starters were quite bland. I didn’t enjoy most of them, but the burger was brilliant! h             Figure 3: Polarity scores (bottom) obtained from class probab
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2251939518	information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classiﬁcation. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnso
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2153702313	l., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classiﬁcation (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). When applied to sentiment analysis, MIL takes advantage of supervision signals on the document level in order to train segment-level sentiment pre
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	1506229096	lexicon with carefully deﬁned rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in part
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2133564696	literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classiﬁcation. Supervised models are typically trained on
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2142972908	ng et al. (2015) and contains customer reviews of local businesses, each associated with human ratings on a scale from 1 (negative) to 5 (positive). The IMDB corpus of movie reviews was obtained from Diao et al. (2014); each review is associated with user ratings ranging from 1 to 10. Both datasets are split into training (80%), validation (10%) and test (10%) sets. A summary of statistics for each collection is pr
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2251939518	o back because the food is good – The drive-thru was horrible – It took us at least 30 minutes to order Figure 1: An EDU-based summary of a 2-out-of-5 star review with positive and negative snippets. Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading int
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2307381258	ocument collections of rated reviews to train ﬁne-grained sentiment predictors, we also investigate the granularity of the extracted segments. Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classiﬁcation (Bhatia et al., 2015), we
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2148404145	ompared Yang et al’s model (2016). Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014) assumes a set of documents that contain opinions about some entity of interest (e.g., camera). The goal of the system is to generate a summary that is representative of the average opinion and speaks
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	6908809	pt) as well as models trained with the attention mechanism disabled, falling back to simple averaging (avg subscript). 5.3 Model Training and Evaluation We trained MIL NET and HIER ET using Adadelta (Zeiler, 2012) for 25 epochs. Mini-batches of 200 documents were organized based on the reviews’ segment and document lengths so the amount of padding was minimized. We used 300-dimensional pre-trained word2vec emb
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	1544144649	Ratan, 1998; Zhang et al., 2002; Andrews and Hofmann, 2004; Carbonetto et al., 2008). Subsequent work relaxed this assumption, allowing for prediction combinations better suited to the tasks at hand. Weidmann et al. (2003) introduced a generalized MIL framework, where a combination of instance types is required to assign a bag label. Zhou et al. (2009) used graph kernels to aggregate predictions, exploiting relations b
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2251939518	rawbacks seen in previous approaches, namely the need for expert knowledge in lexicon-based sentiment analysis (Taboada et al., 2011), expensive ﬁnegrained annotation on the segment level (Kim, 2014; Socher et al., 2013) or the inability to naturally predict segment sentiment (Yang et al., 2016). 3.1 Problem Formulation Under multiple instance learning (MIL), a dataset D is a collection of labeled bags, each of which
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2133564696	s m), a Hierarchical Network with attention (henceforth HIERNET; based on Yang et al., 2016) produces segment representations (v 1;:::;v m) which are subsequently fed into a bidirectional GRU module (Bahdanau et al., 2015), whose resulting hidden vectors (h 1;:::;h m) are used to produce attention weights (a 1;:::;a m) (see Section 3.2 for more details on the attention mechanism). A document is represented as the weigh
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	1939882552	s in a relative performance decrease of &lt; 2% compared Yang et al’s model (2016). Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014) assumes a set of documents that contain opinions about some entity of interest (e.g., camera). The goal of the system is to generate a summary that is
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2084046180	section we describe how multiple instance learning can be used to address some of the drawbacks seen in previous approaches, namely the need for expert knowledge in lexicon-based sentiment analysis (Taboada et al., 2011), expensive ﬁnegrained annotation on the segment level (Kim, 2014; Socher et al., 2013) or the inability to naturally predict segment sentiment (Yang et al., 2016). 3.1 Problem Formulation Under multi
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2084046180	and SPOT dataset are publicly available at: https://github.com/stangelid/milnet-sent larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully deﬁned rules over syntax trees to predict sentence sentiment. Supervised learning techniques have su
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2135336649	use of subsentential units as the basis of extraction. Speciﬁcally, our model was applied to sentences and Elementary Discourse Units (EDUs), obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). According to RST, documents are ﬁrst segmented into EDUs corresponding roughly to independent clauses which are then recursively combined into larger discourse spans. This results in a tree represen
2769436106	Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis	2166010828	vidual digits was unknown. MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classiﬁcation (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (H
2769570678	Learning an Executable Neural Semantic Parser	2251079237	) 39.9 AQQU (Bast and Haussmann 2015) 49.4 AGENDAIL (Berant and Liang 2015) 49.7 DEPLAMBDA (Reddy et al. 2016) 50.3 SUBGRAPH (Bordes,Chopra, and Weston 2014) 39.2 MCCNN (Dong et al. 2015) 40.8 STAGG (Yih et al. 2015) 48.4 (52.5) MCNN (Xu et al. 2016) 47.0 (53.3) Sequence-to-sequence 48.3 TNSP, soft attention, top down 50.1 TNSP, hard attention, top down 49.4 TNSP with structured attention, top down 49.8 TNSP, sof
2769570678	Learning an Executable Neural Semantic Parser	2307268612	) 86.1 Kwiatkowksi et al. (2010) 87.9 Kwiatkowski et al. (2011) 88.6 Kwiatkowski et al. (2013) 88.0 Zhao and Huang (2015) 88.9 Liang, Jordan, and Klein (2011) 91.1 Dong and Lapata (2016) 84.6 Jia and Liang (2016) 85.0 (89.1) Rabinovich, Stern, and Klein (2017) 87.1 TNSP, soft attention, top down 86.8 TNSP, hard attention, top down 85.3 TNSP, structured attention, top down 87.1 TNSP, soft attention, bottom up
2769570678	Learning an Executable Neural Semantic Parser	2252136820	agation. Moreover,the lack of supervision prevents us from training an accurate generative parser for logical forms. To this end, we combine the generative neural parser with a discriminative ranker (Berant et al. 2013a). The role of the neural parseris togeneratealist ofcandidatelogicalforms,andtheroleofthe rankeris toselect which candidate to use. Training of the system involves the following steps. Given an inpu
2769570678	Learning an Executable Neural Semantic Parser	2251287417	ARASEMPRE(Berantand Liang 2014) 39.9 AQQU (Bast and Haussmann 2015) 49.4 AGENDAIL (Berant and Liang 2015) 49.7 DEPLAMBDA (Reddy et al. 2016) 50.3 SUBGRAPH (Bordes,Chopra, and Weston 2014) 39.2 MCCNN (Dong et al. 2015) 40.8 STAGG (Yih et al. 2015) 48.4 (52.5) MCNN (Xu et al. 2016) 47.0 (53.3) Sequence-to-sequence 48.3 TNSP, soft attention, top down 50.1 TNSP, hard attention, top down 49.4 TNSP with structured atten
2769570678	Learning an Executable Neural Semantic Parser	2134036914	buffer bas b= [h1,···,h k], where kdenotes the length of the utterance. Generation History Encoding. The generation history, aka partially completed subtrees, is encoded with a variant of stack LSTM (Dyer et al. 2015). Such an encoder captures not only previously generated tree tokens but also tree structures. We ﬁrst discuss the stack-based LSTM in the top-down transition system and then present modiﬁcations to a
2769570678	Learning an Executable Neural Semantic Parser	1559723967	cale semantic parsing to large, open-domain problems, a few approaches have been proposed which learn from utterance-denotation pairs mostly in the context of question answering over knowledge bases (Clarke et al. 2010; Liang, Jordan, and Klein 2011; Berant et al. 2013a; Choi, Kwiatkowski, and Zettlemoyer 2015; Reddy et al. 2017). There also exists some work on learning semantic parsers with distant supervision (Re
2769570678	Learning an Executable Neural Semantic Parser	2307268612	ces paired with annotated logical forms. More recently, alternative forms of supervision have been proposedtoalleviatetheannotationburden,e.g.,trainingonutterance-denotationpairs (Clarke et al. 2010; Liang 2016; Kwiatkowski et al. 2013), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite the different supervision signals, the inference procedure in conventional seman
2769570678	Learning an Executable Neural Semantic Parser	2307268612	and Clark 2013). Many structured prediction tasks in NLP have seen a surge in the use of recurrent neural networks, and semantic parsing is no exception. Previous work (Dong and Lapata 2016; Jia and Liang 2016) has mostly treated semantic parsing as a sequence to sequence learning problem by successfully repurposing models developed for neural machine translation (Bahdanau, Cho, and Bengio 2015). The fact t
2769570678	Learning an Executable Neural Semantic Parser	2252136820	clude full supervision with questions paired with annotated logical forms using the GEOQUERY (Zettlemoyer and Collins 2005) dataset, weak supervision with questionanswer pairs using the WEBQUESTIONS (Berant et al. 2013a) and GRAPHQUESTIONS (Su et al.2016) datasetsand distantsupervision without question-answer pairs, using the SPADES (Bisk et al. 2016) dataset. Experimental results show that our neural semantic pars
2769570678	Learning an Executable Neural Semantic Parser	104184427	combined feature representation of the buffer and the stack (Equation (14)), which computes the softmax activation of the next action or token. The dropout rate was set to 0.5. Finally, momentum SGD (Sutskever et al. 2013) was used as the optimization method to update the parameters of the model. EntityResolution. Amongst thefourdatasetsdescribedabove,only GEOQUERY contains annotatedlogicalformswhichcanbeusedtodirectly
2769570678	Learning an Executable Neural Semantic Parser	2189089430	d Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). © 2017 Association for Computational Linguistics Computational Linguistics Volume xx, Number xx Table 1: Examples of questions, corresponding logical forms, and their answers. Environment: A databas
2769570678	Learning an Executable Neural Semantic Parser	2135754437	ecutable Neural Semantic Parser used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney 2006; Kate, Wong, and Mooney 2005; Kwiatkowksi et al. 2010; Lu et al. 2008; Dong and Lapata 2016; Jia and Liang 2016). This form of supervision is most effective but also expensive to obtain. In order to scale semantic parsing to large, open-domain problems, a few approache
2769570678	Learning an Executable Neural Semantic Parser	2610301736,2739893875	estructure-awareneuralnetwork models: it also generates tree-structured logical forms, however, following a transitionbased approach. Our semantic parser is a generalization of the model presented in Cheng et al. (2017); it generates logical forms recursively with a set of transition operations whose sequence is predicted via recurrent neural networks. Our parser generates logical forms following either a top-down o
2769570678	Learning an Executable Neural Semantic Parser	147290778	forms of supervision have been proposedtoalleviatetheannotationburden,e.g.,trainingonutterance-denotationpairs (Clarke et al. 2010; Liang 2016; Kwiatkowski et al. 2013), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite the different supervision signals, the inference procedure in conventional semantic parsers relies largely on domain-speciﬁc grammars, and involves computing the most li
2769570678	Learning an Executable Neural Semantic Parser	2227250678	he GEOQUERY dataset. Results with additional resources are shown in parentheses. Models Accuracy Zettlemoyer and Collins (2005) 79.3 Zettlemoyer and Collins (2007) 86.1 Kwiatkowksi et al. (2010) 87.9 Kwiatkowski et al. (2011) 88.6 Kwiatkowski et al. (2013) 88.0 Zhao and Huang (2015) 88.9 Liang, Jordan, and Klein (2011) 91.1 Dong and Lapata (2016) 84.6 Jia and Liang (2016) 85.0 (89.1) Rabinovich, Stern, and Klein (2017) 87
2769570678	Learning an Executable Neural Semantic Parser	2102258316	ing algorithm is used to predict the most likely semantic parse for a sentence. A few approaches have also been inﬂuenced by machine translation techniques either directly or indirectly. For example, Wong and Mooney (2006) use word alignment as the basis of extracting a synchronous grammarwhose rules aresubsequently scoredwithmaximum-entropy model,whereas Andreas, Vlachos, and Clark(2013)useaphrase-basedtranslationmode
2769570678	Learning an Executable Neural Semantic Parser	2295690548	n in parentheses. (a) WEBQUESTIONS Models F1 SEMPRE (Berant et al. 2013b) 35.7 JACANA (Yao and Van Durme 2014) 33.0 PARASEMPRE(Berantand Liang 2014) 39.9 AQQU (Bast and Haussmann 2015) 49.4 AGENDAIL (Berant and Liang 2015) 49.7 DEPLAMBDA (Reddy et al. 2016) 50.3 SUBGRAPH (Bordes,Chopra, and Weston 2014) 39.2 MCCNN (Dong et al. 2015) 40.8 STAGG (Yih et al. 2015) 48.4 (52.5) MCNN (Xu et al. 2016) 47.0 (53.3) Sequence-to-
2769570678	Learning an Executable Neural Semantic Parser	2163274265	nd ﬁnally report and analyze semantic parsing results. 4.1 Datasets We evaluated our model on the following datasets which cover different domains and require differenttypes of supervision. GEOQUERY (Zelle and Mooney 1996) contains 880 questions and database queries about US geography. The utterances are compositional, but the language is simple and vocabulary size small (698 entities and 24 relations). Model training
2769570678	Learning an Executable Neural Semantic Parser	147290778	ng has attracted a great deal of attention due to its utility in a wide range of applications such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettle
2769570678	Learning an Executable Neural Semantic Parser	2102258316	nment: Freebase Utterance: How many daughters does Obama have? Logical form: count(daughterOf(Barack Obama)) Denotation: 2 Early semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) have for the most part used machine learning techniques to train a parser on a collection of utterances paired with annotated logical forms. More recently, alternative forms
2769570678	Learning an Executable Neural Semantic Parser	2135754437	ntences to logical forms, and the model together with the parsing algorithm ﬁnd the most likely derivation. The model which can take the form of a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008) or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a) scores the set of candidate derivations generated by the grammar and is often trained on utterances paired with log3 Computat
2769570678	Learning an Executable Neural Semantic Parser	1559723967	ollection of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposedtoalleviatetheannotationburden,e.g.,trainingonutterance-denotationpairs (Clarke et al. 2010; Liang 2016; Kwiatkowski et al. 2013), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite the different supervision signals, the inference procedure in conven
2769570678	Learning an Executable Neural Semantic Parser	2252136820	orithm ﬁnd the most likely derivation. The model which can take the form of a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008) or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a) scores the set of candidate derivations generated by the grammar and is often trained on utterances paired with log3 Computational Linguistics Volume xx, Number xx ical forms. During inference, a C
2769570678	Learning an Executable Neural Semantic Parser	2227250678	reebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention due to its utility in a wide range of applications such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matus
2769570678	Learning an Executable Neural Semantic Parser	2163274265	river, location(Ohio))) Denotation: Ohio River Environment: Freebase Utterance: How many daughters does Obama have? Logical form: count(daughterOf(Barack Obama)) Denotation: 2 Early semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) have for the most part used machine learning techniques to train a parser on a collection of utterances paired with annot
2769570678	Learning an Executable Neural Semantic Parser	2252136820	rms containing only those entities. This greatly reduces the search space. 3.8.3 Distant Supervision. Despite allowing to scale semantic parsing to large opendomain problems (Kwiatkowski et al. 2013; Berant et al. 2013a; Yao and Van Durme 2014), the creation of utterance-denotation pairs still relies on labor-intensive crowdsourcing. A promising alternative is to train a semantic parser with distant supervision wit
2769570678	Learning an Executable Neural Semantic Parser	2252136820	s, a few approaches have been proposed which learn from utterance-denotation pairs mostly in the context of question answering over knowledge bases (Clarke et al. 2010; Liang, Jordan, and Klein 2011; Berant et al. 2013a; Choi, Kwiatkowski, and Zettlemoyer 2015; Reddy et al. 2017). There also exists some work on learning semantic parsers with distant supervision (Reddy, Lapata, and Steedman 2014; Bisk et al. 2016).
2769570678	Learning an Executable Neural Semantic Parser	104184427	s generated by the neural semantic parser. At test time, the generation process is accomplished by beam search with beam size 300. The ranker which is a log-linear model is trained with momentum SGD (Sutskever et al. 2013). As features, we consider the embedding cosine similarity between the utterance (excluding stop-words) and the logical form, the token overlap countbetweenthetwo,andalsosimilarfeaturesbetweenthelemma
2769570678	Learning an Executable Neural Semantic Parser	2307268612	tasks has provided strong impetus to treat semantic parsing as a sequence transduction problemwherean utteranceis mappedtoatargetmeaning representation in string format (Dong and Lapata 2016; Jia and Liang 2016; Kocˇiský et al. 2016). The neural semantic parser reduces the need for domain-speciﬁc assumptions, grammar learning,andmoregenerallyextensivefeatureengineering.Butthismodelingﬂexibility comes at a c
2769570678	Learning an Executable Neural Semantic Parser	2295690548	tems (see the ﬁrst block in Table6a). It is important to note that Bast and Haussmann (2015) develop a question answering system, which contrary to ours cannot produce meaning representations whereas Berant and Liang (2015) propose a sophisticated agenda-based parser which is trained borrowing ideasfrom imitation learning. Reddy et al. (2016) learn a semantic parservia intermediate representations which they generate ba
2769570678	Learning an Executable Neural Semantic Parser	2307268612	training data consisting of sentences and their corresponding logical forms (Kate and Mooney 2006; Kate, Wong, and Mooney 2005; Kwiatkowksi et al. 2010; Lu et al. 2008; Dong and Lapata 2016; Jia and Liang 2016). This form of supervision is most effective but also expensive to obtain. In order to scale semantic parsing to large, open-domain problems, a few approaches have been proposed which learn from utter
2769570678	Learning an Executable Neural Semantic Parser	2626778328	tree representations, resembling a tree-to-tree transduction model. To tackle long-term dependencies in the generation process, an intra-attention mechanism can be used (Cheng, Dong, and Lapata 2016; Vaswani et al. 2017). Secondly, in the learning from denotation setting, it is possible that the beam search output contains spurious logical forms which lead to correct answers accidentally but do not represent the actu
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2100833569	. Context-free grammars are also used to generate natural language sentences from formal meaning representations (Lu and Ng 2011; Belz 2008). Other effective approaches include hybrid alignment tree (Kim and Mooney 2010), tree conditional random ﬁelds (Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali 2016) and template extraction in a log-linear framework (Angeli, Liang, and Klein 2010). Recent work combines co
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2135363470	linguistic features to train sentence planners for sentence generation. Context-free grammars are also used to generate natural language sentences from formal meaning representations (Lu and Ng 2011; Belz 2008). Other effective approaches include hybrid alignment tree (Kim and Mooney 2010), tree conditional random ﬁelds (Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali 2016) and template extraction in
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2154764394	n pre-deﬁned schemas. In contrast to previous work experimented on small datasets which contain only a few tens of thousands of records such as WEATHERGOV (Liang, Jordan, and Klein 2009) and ROBOCUP (Chen and Mooney 2008), we focus on a more challenging task to generate biographies Copyright c 2018, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. Figure 1: The Wikipedia i
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2003170434	ons for this subset. Many approaches have been proposed to learn the individual modules. For content selection module, one approach builds a content selection model by aligning records and sentences (Barzilay and Lapata 2005; Duboue and McKeown 2002). A hierarchical semi-Markov method is proposed by (Liang, Jordan, and Klein 2009) which ﬁrst associates the text sequences to corresponding records and then generates corres
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2250220874	resentations (Lu and Ng 2011; Belz 2008). Other effective approaches include hybrid alignment tree (Kim and Mooney 2010), tree conditional random ﬁelds (Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali 2016) and template extraction in a log-linear framework (Angeli, Liang, and Klein 2010). Recent work combines content selection and surface realization in a uniﬁed framework (Ratnaparkhi 2002; Konstas and
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2154764394	t generation from structured data. Previous researches include weather forecast based on a set of weather records (Liang, Jordan, and Klein 2009) and sportscasting based on temporally ordered events (Chen and Mooney 2008). However, previous work models the structured data in the limited pre-deﬁned schemas. For example, a weather record rainChance(time:06:00-21:00, mode:SSE, value:20) is represented by a ﬁxed-length on
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	1514535095	t-th hidden state of the decoder calculated by the LSTM unit. The computational details can be referred in Equation 3, 4 and 5. a t is the attention vector which is widely used in many applications (Xu et al. 2015; Luong et al. 2014; Ma et al. 2017). Vanilla attention mechanism is proposed to encode the semantic relevance between the encoder states fh tg L t=1 and and the decoder state s t at time t. The atten
2769637628	Table-to-text Generation by Structure-aware Seq2seq Learning	2167247296	utilize various linguistic features to train sentence planners for sentence generation. Context-free grammars are also used to generate natural language sentences from formal meaning representations (Lu and Ng 2011; Belz 2008). Other effective approaches include hybrid alignment tree (Kim and Mooney 2010), tree conditional random ﬁelds (Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali 2016) and template e
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2325162016	(d) vs. (a)). In addition, with transfer learning, QACNN outperforms the previous best models on TOEFL-manual by 7%, TOEFLASR (Fang et al.,2016) by 6.5%, MC160 (Wang et al.,2015) by 1.1%, and MC500 (Trischler et al., 2016) by 1.3%, and becomes the state-of-the-art on all target datasets. Which QACNN parameters to transfer? For the QACNN, the training parameters are E;W(1) CNN ;W (2) CNN ;W (1) FC , and W (2) FC (Sectio
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2626667877	d to apply transfer learning for machine comprehension. The authors showed only limited transfer between two QA tasks, but the transferred system was still significantly better than a random baseline.Wiese et al. (2017) tackled a more speciﬁc task of biomedical QA with transfer learning from a large-scale dataset. The work most similar to ours is byMin et al.(2017), where the authors used a simple transfer learning
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2516995758	ments to investigate the transferability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is signiﬁcantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al.,2016). In particular, one of the models achieves the state-of-the-a
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2512720747	monstrate that we can effectively overcome these difﬁculties via transfer learning in Section5. 4 QA Neural Network Models Among numerous models proposed for multiplechoice QA (Trischler et al. ,2016;Fang et al. 2016;Tseng et al.,2016), we adopt the End-toEnd Memory Network (MemN2N)2 (Sukhbaatar et al.,2015) and Query-Based Attention CNN (QACNN)3 (Liu et al.,2017), both open-sourced, to conduct the experiments. B
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2125436846	rability of knowledge learned from a source QA dataset to a target dataset using two QA models. The performance of both models on a TOEFL listening comprehension test (Tseng et al., 2016) and MCTest (Richardson et al., 2013) is signiﬁcantly improved via a simple transfer learning technique from MovieQA (Tapaswi et al.,2016). In particular, one of the models achieves the state-of-the-art on all target datasets; for the TO
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2250539671	rforms the best when only the last fully-connected layer was ﬁne-tuned. Note that for training the QACNN, we followed the same procedure as inLiu et al.(2017), whereby pre-trained GloVe word vectors (Pennington et al., 2014) were used to initialize the embedding layer, which were not updated during training. Thus, the embedding layer does not depend on the training set, and the effective vocabularies are the same. Fine-t
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2516995758	stories like humans do. A story is a sequence of sentences, and can be in the form of plain text (Trischler et al., 2017;Rajpurkar et al.,2016;Weston et al.,2016; Yang et al.,2015) or spoken content (Tseng et al., 2016), where the latter usually requires the spoken content to be ﬁrst transcribed into text by automatic speech recognition (ASR), and the model will subsequently process the ASR output. To evaluate the e
2769693954	SUPERVISED AND UNSUPERVISED TRANSFER LEARNING FOR QUESTION ANSWERING	2557764419	uestion Answering One of the most important characteristics of an intelligent system is to understand stories like humans do. A story is a sequence of sentences, and can be in the form of plain text (Trischler et al., 2017;Rajpurkar et al.,2016;Weston et al.,2016; Yang et al.,2015) or spoken content (Tseng et al., 2016), where the latter usually requires the spoken content to be ﬁrst transcribed into text by automatic
2770056630	Simulating Action Dynamics with Neural Process Networks	2214429195	al., 2016). Our work seeks to provide a relatively more structured representation of domain-speciﬁc action knowledge to provide an inductive bias to the reasoning process. Work on Neural Programmers (Neelakantan et al., 2015; 2016) has also used functions to simulate reasoning, by building a model to select rows in a database and applying operation on those selected rows. While their work explicitly deﬁned the effect of
2770056630	Simulating Action Dynamics with Neural Process Networks	2550448043	arch that aims to model aspects of world state changes, such as language models and machine readers with explicit entity representations (Henaff et al., 1 arXiv:1711.05313v1 [cs.CL] 14 Nov 2017 2016; Yang et al., 2016; Ji et al., 2017), as well as other more general purpose memory network variants (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Seo et al., 2016). This worldcentric modeling of pro
2770056630	Simulating Action Dynamics with Neural Process Networks	2252269235	del to learn representations for how actions change the state space. Works on instructional language studied the task of building discrete graph representations of recipes using probabilistic models (Kiddon et al., 2015; Mori et al., 2014; 2012). We propose a complementary new model by integrating action and entity relations into the neural network architecture and also address the additional challenge of tracking t
2770056630	Simulating Action Dynamics with Neural Process Networks	2740663516	odel aspects of world state changes, such as language models and machine readers with explicit entity representations (Henaff et al., 1 arXiv:1711.05313v1 [cs.CL] 14 Nov 2017 2016; Yang et al., 2016; Ji et al., 2017), as well as other more general purpose memory network variants (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Seo et al., 2016). This worldcentric modeling of procedural language (
2770056630	Simulating Action Dynamics with Neural Process Networks	2410539690	or a particular entity at sentence tand I d is the number of entities in a document. P S t=1 a i t is upper bounded by 1. This is analogous to the coverage penalty used in neural machine translation (Tu et al., 2016). 4 EXPERIMENTAL SETUP We evaluate our model on two tasks: tracking and generation, and provide a qualitative analysis of the internal workings of our model. 5 Model Entity Selection State Change F1 U
2770056630	Simulating Action Dynamics with Neural Process Networks	1793121960	representations (Henaff et al., 1 arXiv:1711.05313v1 [cs.CL] 14 Nov 2017 2016; Yang et al., 2016; Ji et al., 2017), as well as other more general purpose memory network variants (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Seo et al., 2016). This worldcentric modeling of procedural language (i.e., understanding by simulation) abstracts away from the surface strings, complementing text-centric modeli
2770056630	Simulating Action Dynamics with Neural Process Networks	2169255714	tional work in tracking states with visual or multimodal context has focused on 1) building graph representations for how entities change in goal-oriented domains (Gao et al., 2016; Liu et al., 2016; Si et al., 2011) or 2) tracking visual state changes based on decisions taken by agents in environment simulators such as videos or games (Chiappa et al., 2017; Wahlstrom et al., 2015; Oh et al., 2015). Our work, in
2770056630	Simulating Action Dynamics with Neural Process Networks	1793121960	understanding of global patterns. 6 RELATED WORK Recent studies in machine comprehension have used a neural memory component to store a running representation of processed text (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Seo et al., 2016). Our model, in contrast, use the memory to remember effects of actions on entities and is trained by tracking a set of common sense state changes that are induce
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2489434015,2558809543	). The longer the expression is, the more information is provided, however, the harder the problem is because more details need to be analysed and more steps of reasoning are required. Previous works [6,44,45] have primarily appled a CNN-RNN pipeline that uses a CNN to encode the image content and an LSTM to encode the expression. The encoded features are then jointly embedded and used to to locate the obj
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2571175805	oN (PLAN) 81.67% 80.81% 81.32% 64.18% 66.31 % 61.46 % 69.47% Table 1: Accuracies on RefCOCO, RefCOCO+ and RefCOCOg datasets. Note that for RefCOCOg, we use the standard google split for testing while [45] divide the dataset by randomly partitioning objects into training and validation splits, which is not comparable. Methods val test Human 9.2% 9.2% Random 82.9% 82.9% LSTM [6] 37.9% 38.7% HRED [6] 38.
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	1686810756	1. Feature encoding The global visual feature To encode the full image while maintaining the spatial information therein, we rst rescale the image to 224x224 and then pass it through a VGG-16 network [36] pre-trained on Ima3 TM TM Y TM LSTM MLP V TM TM TM Y atten atten V atten atten atten Prediction h0 h1 h2 Y Y Is ita person? No does have wheels? Yes Y does the person carrying it have ashirt on? Yes
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2571175805	8% 76.59% - 59.17% 55.62% 64.02% Neg Bag [28] - 75.60% 78.00% - - - 68.40% Luo et al. [26] - 74.14% 71.46% - 59.87% 54.35% 63.39% Luo et al. (w2v) [26] - 74.04% 73.43% - 60.26% 55.03% 65.36% listener [45] 77.48% 76.58% 78.94% 60.50% 61.39% 58.11% 71.12% speaker+listener [45] 77.84% 77.50% 79.31% 60.97% 62.85% 58.58% 72.25% speaker+listener+reinforcer [45] 78.14% 76.91% 80.10% 61.34% 63.34% 58.42% 71.7
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2341047870,2552383788	that their appearance must remain xed. The limitations of this approach are visible in the rise of mediation strategies such as Domain Adaptation [8,23], Transfer Learning [29,33], Zero-Shot learning [32,47], and a subCNN Is it a person? Yes, Is the person a boy? Yes ! LSTM object Is is a person? Yes LSTM Is the person a boy? Yes LSTM ! ! ! ! object (a) One-step Reasoning (b) Region-wise &amp; Step-wise
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2052293776,2122922389	d long in advance, but also that their appearance must remain xed. The limitations of this approach are visible in the rise of mediation strategies such as Domain Adaptation [8,23], Transfer Learning [29,33], Zero-Shot learning [32,47], and a subCNN Is it a person? Yes, Is the person a boy? Yes ! LSTM object Is is a person? Yes LSTM Is the person a boy? Yes LSTM ! ! ! ! object (a) One-step Reasoning (b)
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	1861492603	e. 4.1. Datasets We evaluate the performance on four referring expression datasets, including RefCOCO, RefCOCO+, RefCOCOg [44] and GuessWhat?! [6]. All of the datasets are collected on MS-COCO images [21]. Overall, RefCOCO has 142,210 expressions for 50,000 objects in 19,994 images. Compared to RefCOCO, RefCOCO+ removes absolute location words in referring expressions and contains 141,565 expressions
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2571175805	ed proposals, which further promotes the grounding of the target object. Note that the proposed method only focus on improving the model that is similar to the listener in the latest state-of-the-art [45]. In [45], the speaker is a generative model that aims to produce referring expressions. The listener learns to embed the visual information and referring expression into a joint embedding space for c
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2489434015	erring expression so that a step-wise reasoning process is achieved. We evaluate our PLAN model on what is currently the largest referring expression dataset, the ReferCOCO, ReferCOCO+ and ReferCOCOg [44], which has a dierent length of the input expression. Our model outperforms the previous state-of-art by a large margin, for example, our single model even outperforms an ensemble model (and with Rei
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2254252455	erring Expressions: generation and comprehension. The generation task requires a model to generate a language expression for the given region, which is very similar to the dense image captioning task [14]. Referring expression comprehension aims to localize the regions being described by a given referring expression [12,13,27,28,44]. Given a set of extracted candidate regions, each region is scored by
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2558809543	for example, our single model even outperforms an ensemble model (and with Reinforcement Learning) on a test split on the ReferCOCO. We further evaluate our model on the recently released GuessWhat?! [6] dataset, which requires an agent to point out the object in an image that is being discussed by a ‘Questioner’ and an ‘Oracle’ via multiple rounds of dialog. Here our model also outperforms the previ
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	1773149199	the fragments of language into the embedding space with a structured max-margin objective and [16] further replace the dependency tree of the language parser with a bidirectional RNN. Some approaches [31,38] use a Canonical Correlation Analysis [10] based method to learn the correlation between visual and language modalities. Recently, Hu et al. [13] proposed an SCRC model to integrate spatial congurati
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2241785942	ge parser with a bidirectional RNN. Some approaches [31,38] use a Canonical Correlation Analysis [10] based method to learn the correlation between visual and language modalities. Recently, Hu et al. [13] proposed an SCRC model to integrate spatial conguration and global scene-level contextual information into the network. The Attention Mechanism An attention mechanism was rst successfully introduced
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2144960104,2241785942,2489434015,2505639562	on for the given region, which is very similar to the dense image captioning task [14]. Referring expression comprehension aims to localize the regions being described by a given referring expression [12,13,27,28,44]. Given a set of extracted candidate regions, each region is scored by the model with respect to the referring expression and the region with the highest score is selected as the nal grounding result.
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2176212817,2463565445,2597425697,2613526370,2616125804,2743640935	group of recent methods combining vision and language. Recent work in this area includes a variety of approaches to image captioning [9,14,19,20,22,30,37,39,41,42] and visual question answering (VQA) [2,7,11,15,25,40,46, 48]. Image captioning seeks to generate a natural language description for the whole given image, while the VQA requires the agent to answer previously unseen visual questions about an image. Most recent
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2558809543	h box are width and height of the bounding box. The image height and width are normalized to the range [ 1;1] and the center of the image is set as the origin. For the experiments on the GuessWhat?! [6] dataset, the additional object category information for each candidate is also used, for a fair comparison with the previous state-o-the-art. We denote the feature representation as P = f p 1;:::; N
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	1905882502	l and language descriptions. To solve this problem, Karpathy et al. [17] propose to align the objects and the fragments of language into the embedding space with a structured max-margin objective and [16] further replace the dependency tree of the language parser with a bidirectional RNN. Some approaches [31,38] use a Canonical Correlation Analysis [10] based method to learn the correlation between vi
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2505639562	ner model of [45], for instance, we outperform it by nearly 5% on TestA setting of RefCOCO+. Our nal single model is even better than a ensemble model in [45]. On the RefCOCOg, we outperforms the MMI [28] and other state-of-the-art on the same split. [45] divide the dataset by randomly partitioning objects into training and validation splits while we use the standard ‘google’ split for evaluation, whi
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2112912048	ng Phrase grounding aims to localize the objects described in the phrase. The main problem is to learn the correlation between visual and language descriptions. To solve this problem, Karpathy et al. [17] propose to align the objects and the fragments of language into the embedding space with a structured max-margin objective and [16] further replace the dependency tree of the language parser with a b
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2571175805	ns. By comparing our full model with the listener model of [45], for instance, we outperform it by nearly 5% on TestA setting of RefCOCO+. Our nal single model is even better than a ensemble model in [45]. On the RefCOCOg, we outperforms the MMI [28] and other state-of-the-art on the same split. [45] divide the dataset by randomly partitioning objects into training and validation splits while we use t
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2302086703	posed a co-attention model for VQA that jointly reasons about language and the image. Then in [24], an adaptive attention model with a visual sentinel was applied to decide when to attend. You et al. [43] run a set of attribute detectors to get a list of visual attributes and fuse them into the RNN hidden state. Rather than statically attending to the image using the given expressions, we instead prop
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2571175805	a pre-trained object detection model like Faster-RCNN [34], or an object proposal model such as Edgebox [49], Objectness [1] and so on. However, for a fair comparison with previous methods, we follow [44,45] use all the annotated entities in the image as the proposal bounding boxes at both training and test. The results are reported in Table1. Compared to previous state-of-the-art methods, our methods ha
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2739008552	rest is xed, and determined long in advance, but also that their appearance must remain xed. The limitations of this approach are visible in the rise of mediation strategies such as Domain Adaptation [8,23], Transfer Learning [29,33], Zero-Shot learning [32,47], and a subCNN Is it a person? Yes, Is the person a boy? Yes ! LSTM object Is is a person? Yes LSTM Is the person a boy? Yes LSTM ! ! ! ! object
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2144960104,2241785942,2489434015,2558809543	he rst is the CNN feature u i extracted as described for the global visual features, the Conv5 3 feature from the VGG-16 for each object proposal. To further increase the expressive power, similar to [6,12,13,27,44,45], we also embed the spatial representations of the proposal regions. Following [6,45], the spatial information of the bounding box of each object is encoded as the an 8-d vector: s i = [x min;y min;x
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2558809543	s that an agent participate intelligently in a dialog about an image. In our work, we extend the phase/sentence based referring expression task to dialogs, i.e., extending the dialog task proposed in [6] to the problem of identifying a specic object in an image. 3. The PLAN Model In this section, we describe our unied model that takes as input an image and a set of natural language expressions and
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	1895577753,2254252455,2404394533,2544271936,2560645892,2599772929,2756246624	s in a unied framework. Vision and Language Our work is part of a group of recent methods combining vision and language. Recent work in this area includes a variety of approaches to image captioning [9,14,19,20,22,30,37,39,41,42] and visual question answering (VQA) [2,7,11,15,25,40,46, 48]. Image captioning seeks to generate a natural language description for the whole given image, while the VQA requires the agent to answer p
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2463565445	scene-level contextual information into the network. The Attention Mechanism An attention mechanism was rst successfully introduced within the image captioning task by [41]. Based on this, Lu et al. [25] further proposed a co-attention model for VQA that jointly reasons about language and the image. Then in [24], an adaptive attention model with a visual sentinel was applied to decide when to attend.
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2100235303	space with a structured max-margin objective and [16] further replace the dependency tree of the language parser with a bidirectional RNN. Some approaches [31,38] use a Canonical Correlation Analysis [10] based method to learn the correlation between visual and language modalities. Recently, Hu et al. [13] proposed an SCRC model to integrate spatial conguration and global scene-level contextual infor
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	639708223	t from the set of all object candidates. 4.2. Evaluation on RefCOCO, RefCOCO+ and RefCOCOg datasets The object candidates can be obtained through a pre-trained object detection model like Faster-RCNN [34], or an object proposal model such as Edgebox [49], Objectness [1] and so on. However, for a fair comparison with previous methods, we follow [44,45] use all the annotated entities in the image as the
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	7746136	uation on RefCOCO, RefCOCO+ and RefCOCOg datasets The object candidates can be obtained through a pre-trained object detection model like Faster-RCNN [34], or an object proposal model such as Edgebox [49], Objectness [1] and so on. However, for a fair comparison with previous methods, we follow [44,45] use all the annotated entities in the image as the proposal bounding boxes at both training and test
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2575842049	uccessfully introduced within the image captioning task by [41]. Based on this, Lu et al. [25] further proposed a co-attention model for VQA that jointly reasons about language and the image. Then in [24], an adaptive attention model with a visual sentinel was applied to decide when to attend. You et al. [43] run a set of attribute detectors to get a list of visual attributes and fuse them into the RN
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2505639562	vel attention models are jointly optimized and the results are further improved. It can be attributed to that the reasonable 6 Method RefCOCO RefCOCO+ RefCOCOg val TestA TestB val TestA TestB val MMI [28] - 71.72% 71.09% - 58.42% 51.23% 62.14% visdif [44] - 67.57% 71.19% - 52.44% 47.51% 59.25% visdif+ MMI [44] - 73.98% 76.59% - 59.17% 55.62% 64.02% Neg Bag [28] - 75.60% 78.00% - - - 68.40% Luo et al.
2770129969	Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries	2489434015,2505639562	Several works [13,27] propose to use local visual features or global image features as the feature representation. To better employ the structural information between dierent candidates, some works [12,28,44] further explicitly incorporating modeling context between objects into referring expression. In [12,28], the models are proposed to handle inter-object relationships for grounding a referential expre
2770165365	On the Automatic Generation of Medical Imaging Reports	2101105183	. 4.4. Quantitative results We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table1) results using the following captioning evaluation tools: BLEU [14], METEOR [4], ROUGE [13] and CIDER [18]. For paragraph generation, as shown in the upper part of Table1, it is clear that models with single LSTM decoder perform much worse than hierarchical LSTM deco
2770165365	On the Automatic Generation of Medical Imaging Reports	1947481528	0.418 0.286 Ours-Visual-only 0.507 0.373 0.297 0.238 0.211 0.426 0.300 Ours-CoAttention 0.517 0.386 0.306 0.247 0.217 0.447 0.327 PEIR Gross CNN-RNN[19] 0.247 0.178 0.134 0.092 0.129 0.247 0.205 LRCN[5] 0.261 0.184 0.136 0.088 0.135 0.254 0.203 Soft ATT[20] 0.283 0.212 0.163 0.113 0.147 0.271 0.276 ATT-RK[21] 0.274 0.201 0.154 0.104 0.141 0.264 0.279 Ours-Semantic-only 0.263 0.191 0.145 0.098 0.138
2770165365	On the Automatic Generation of Medical Imaging Reports	2302086703	al network to learn the alignment between visual and semantic features, and then produce the descriptions for given images. Recently, attention mechanisms have shown to be useful for image captioning [20,21]. Xu et al. [20] introduce a spatial-visual attention mechanism over image features extracted from intermediate layers of CNN. You et al. [21] propose a semantic attention mechanism over tags of given
2770165365	On the Automatic Generation of Medical Imaging Reports	1895577753	anually generated and Medical Text Indexer (MTI) generated. 2http://peir.path.uab.edu/library/ 4.3. Baselines We ﬁrst compare our method with several state-of-the-art image captioning models: CNN-RNN [19], LRCN [5], Soft ATT [20], ATT-RK [21]. We re-implemented all of these models and adopt VGG-19 [17] as CNN encoder. However, these models are built for single sentence captions. To better show the eff
2770165365	On the Automatic Generation of Medical Imaging Reports	1686810756	Baselines We ﬁrst compare our method with several state-of-the-art image captioning models: CNN-RNN [19], LRCN [5], Soft ATT [20], ATT-RK [21]. We re-implemented all of these models and adopt VGG-19 [17] as CNN encoder. However, these models are built for single sentence captions. To better show the effectiveness of hierarchical LSTM for paragraph generation, we also implement a hierarchical model wi
2770165365	On the Automatic Generation of Medical Imaging Reports	2254252455	better leverage both of the visual features and semantic tags, we propose a co-attention mechanism for report generation. Instead of only generating one sentence caption for an image, Johnson et al. [8] introduce the dense captioning task which requires the model to generate a description for each of the detected image regions. Krause et al. [11] and Liang et al. [12] generate paragraph captions for
2770165365	On the Automatic Generation of Medical Imaging Reports	1686810756	enote the presence and absence of the i-th tag respectively, and MLC i means the i-th output of MLC network. For simplicity, we extract visual features from the last convolutional layer of the VGG-19 [17] and adopt the last two fully connected layers of VGG-19 as MLC. Finally, the embedding of the top Mtags fa mgM m=1 2 RE are used as semantic features for topic generation. 3.3. Co-Attention Previous
2770165365	On the Automatic Generation of Medical Imaging Reports	1947481528	erated and Medical Text Indexer (MTI) generated. 2http://peir.path.uab.edu/library/ 4.3. Baselines We ﬁrst compare our method with several state-of-the-art image captioning models: CNN-RNN [19], LRCN [5], Soft ATT [20], ATT-RK [21]. We re-implemented all of these models and adopt VGG-19 [17] as CNN encoder. However, these models are built for single sentence captions. To better show the effectiveness
2770165365	On the Automatic Generation of Medical Imaging Reports	2302086703	exer (MTI) generated. 2http://peir.path.uab.edu/library/ 4.3. Baselines We ﬁrst compare our method with several state-of-the-art image captioning models: CNN-RNN [19], LRCN [5], Soft ATT [20], ATT-RK [21]. We re-implemented all of these models and adopt VGG-19 [17] as CNN encoder. However, these models are built for single sentence captions. To better show the effectiveness of hierarchical LSTM for pa
2770165365	On the Automatic Generation of Medical Imaging Reports	2302086703	generate single sentence report. For PEIR Gross dataset, we apply the same preprocessing as IU X-Ray which yields 4,452 unique words for the dataset and 12.0 words for each image. Besides, similar to [21], we treat words with top 1000 occurrences as tags. 4.2. Implementation details We set the dimensions of all hidden states and embeddings as 512. For words and tags, we use different embedding matrice
2770165365	On the Automatic Generation of Medical Imaging Reports	1889624880	generates topic vector t 2RK for word LSTM through topic generator and determines whether to continue or stop generating captions by stop control component. Topic Generator We use a deep output layer [15] to strengthen the context information in topic vector t(s). The deep output layer takes as input the hidden state h(s) sent and the joint context vector ctx(s) of current step: t(s) = tanh(W t;h sent
2770165365	On the Automatic Generation of Medical Imaging Reports	1686810756	hlights key concepts in the report. We treat tag prediction as a multi-label classiﬁcation task. To evaluate the prediction performance of multi-task learning, we compare our model and vanilla VGG-19 [17] on recall@5, racall@10, recall@20. Note that the loss function of both models are softmax cross-entropy loss [7]. Dataset Methods R@5 R@10 R@20 IU X-Ray VGG-19 [17] 0.643 0.719 0.793 Ours-CoAttention
2770165365	On the Automatic Generation of Medical Imaging Reports	1947481528	indicates the effectiveness of the proposed co-attention mechanism. Dataset Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDER IU X-Ray CNN-RNN [19] 0.316 0.211 0.140 0.095 0.159 0.267 0.111 LRCN[5] 0.369 0.229 0.149 0.099 0.155 0.278 0.190 Soft ATT[20] 0.399 0.251 0.168 0.118 0.167 0.323 0.302 ATT-RK[21] 0.369 0.226 0.151 0.108 0.171 0.323 0.155 Ours-no-Attention 0.505 0.383 0.290 0.224 0.200 0
2770165365	On the Automatic Generation of Medical Imaging Reports	2605045867	ion for an image, Johnson et al. [8] introduce the dense captioning task which requires the model to generate a description for each of the detected image regions. Krause et al. [11] and Liang et al. [12] generate paragraph captions for images through hierachical LSTM. Our method also adopts a hierarchical LSTM for paragraph generation, whereas different from [11], we use a co-attention network to gen
2770165365	On the Automatic Generation of Medical Imaging Reports	2302086703	LEU-4 METEOR ROUGE CIDER IU X-Ray CNN-RNN [19] 0.316 0.211 0.140 0.095 0.159 0.267 0.111 LRCN[5] 0.369 0.229 0.149 0.099 0.155 0.278 0.190 Soft ATT[20] 0.399 0.251 0.168 0.118 0.167 0.323 0.302 ATT-RK[21] 0.369 0.226 0.151 0.108 0.171 0.323 0.155 Ours-no-Attention 0.505 0.383 0.290 0.224 0.200 0.420 0.259 Ours-Semantic-only 0.504 0.371 0.291 0.230 0.207 0.418 0.286 Ours-Visual-only 0.507 0.373 0.297 0
2770165365	On the Automatic Generation of Medical Imaging Reports	1895577753	model without any attention: Ours-no-Attention. The input of Oursno-Attention is the overall image feature of VGG-19 network, which has a dimension of 4096. Ours-no-Attention can be viewed as CNN-RNN [19] equipped with hierarchical LSTM decoder. Besides, to show the effectiveness of the proposed co-attention mechanism, we also implement two ablated versions of our model: Ours-Semantic-only and Ours-Vi
2770165365	On the Automatic Generation of Medical Imaging Reports	1895577753	n details We set the dimensions of all hidden states and embeddings as 512. For words and tags, we use different embedding matrices since an tag might contain multiple words. We use full VGG-19 model [19] for tag prediction. We use the embedding of top 10 tags with highest probabilities to be the semantic feature vectors fa m gM=10 =1 . We extract the visual features from the last convolutional layer
2770165365	On the Automatic Generation of Medical Imaging Reports	1895577753,1905882502,1931639407,2254252455,2302086703,2549599535	orks. Image captioning with deep learning Image captioning aims at automatically generating text descriptions for given images. Most of recent image captioning models are based on a CNN-RNN framework [19,6,9,20,21,8,11]. Vinyals et al. [19] feed the image features extracted from the last hidden layer of CNN into an LSTM to generate captions. Fang et al. [6] ﬁrst use CNN to detect the concepts in the image, and then
2770165365	On the Automatic Generation of Medical Imaging Reports	1956340063	he paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table1) results using the following captioning evaluation tools: BLEU [14], METEOR [4], ROUGE [13] and CIDER [18]. For paragraph generation, as shown in the upper part of Table1, it is clear that models with single LSTM decoder perform much worse than hierarchical LSTM decoder. Note that the only difference betw
2770165365	On the Automatic Generation of Medical Imaging Reports	1514027499	rediction performance of multi-task learning, we compare our model and vanilla VGG-19 [17] on recall@5, racall@10, recall@20. Note that the loss function of both models are softmax cross-entropy loss [7]. Dataset Methods R@5 R@10 R@20 IU X-Ray VGG-19 [17] 0.643 0.719 0.793 Ours-CoAttention 0.644 0.716 0.792 PEIR Gross VGG-19 [17] 0.392 0.506 0.595 Ours-CoAttention 0.398 0.494 0.596 Table 3. Tag predi
2770165365	On the Automatic Generation of Medical Imaging Reports	1686810756	s-CoAttention 0.398 0.494 0.596 Table 3. Tag prediction on IU X-Ray dataset (upper part), and PEIR Gross dataset (lower part). R denotes recall. Results in Table3show that Ours-CoAttention and VGG-19 [17] network performs very similar for tag prediction. Even though no improvement has been made by multitask learning, our model is an end-to-end model which avoids managing a complex sequential pipeline.
2770165365	On the Automatic Generation of Medical Imaging Reports	2549599535	will also stop producing words. 3.5. Word LSTM The words of each sentence are generated by a word LSTM, which is a single layer LSTM initialized by topic vector tproduced by sentence LSTM. Following [11], when generating words, topic vector t and the special START token are used as the ﬁrst and the second input of the word LSTM, and the subsequent inputs are the word sequence. The hidden state h word
2770165365	On the Automatic Generation of Medical Imaging Reports	2549599535	ting one sentence caption for an image, Johnson et al. [8] introduce the dense captioning task which requires the model to generate a description for each of the detected image regions. Krause et al. [11] and Liang et al. [12] generate paragraph captions for images through hierachical LSTM. Our method also adopts a hierarchical LSTM for paragraph generation, whereas different from [11], we use a co-at
2770165365	On the Automatic Generation of Medical Imaging Reports	1895577753	own in the upper part of Table1, it is clear that models with single LSTM decoder perform much worse than hierarchical LSTM decoder. Note that the only difference between Ours-NoAttention and CNN-RNN [19] in Table1is that Ours-NoAttention adopts a hierarchical LSTM decoder while CNNRNN [19] only adopts a single-layer LSTM. The comparison between these two models directly demonstrates the effectiveness
2770317517	Learning Document Embeddings With CNNs	1486649854	ed NLP tasks such as sentiment classiﬁcation. We compare our approach against the leading unsupervised embedding models including doc2vec (Le &amp; Mikolov, 2014), doc2vecC (Chen,2017), skip-thought (Kiros et al., 2015) and SIF (Arora et al.,2017); all described in Section1. For each baseline we use the code from the respective authors, and extensively tune the model using parameter sweeps. Skip-thought code provide
2770317517	Learning Document Embeddings With CNNs	2153579005	ing it impossible to model long range semantic dependencies. Recently, signiﬁcant attention has been devoted to embedding approaches that use distributed representations of words (Bengio et al. ,2003;Mikolov et al. 2013). Models within this category are trained to produce document embeddings from word representations, and either jointly learn word representations during training or use a pretrained word model. The ma
2770317517	Learning Document Embeddings With CNNs	2120615054	with the same length as the number of convolutional kernels in layer l. A further generalization of this procedure involves storing multiple values from each row of hlusing an operator such as max k(Kalchbrenner et al., 2014) which outputs top-k values instead of top-1. The order in which the kmaximum values occur is preserved in this operation, allowing the fully connected layers to capture additional temporal informatio
2770317517	Learning Document Embeddings With CNNs	2567070169	sequences within each document. 2.1. Model Architecture CNN models have recently been shown to perform well on supervised NLP tasks with distributed representations (Kim,2014;Kalchbrenner et al.,2014;Dauphin et al., 2017;Conneau et al.,2017), and are considerably more efﬁcient than RNNs. Inspired by these results, we propose a CNN model for f. Given an input matrix ˚(D), we apply multiple layers of convolutions to it
2770317517	Learning Document Embeddings With CNNs	2153579005	updated with binary cross-entropy objective. As word sequences can’t be used as input directly, a common approach is to ﬁrst transform them into numeric format. Recently, distributed representation (Mikolov et al., 2013;Pennington et al.,2014) has become increasingly more popular as it allows us to preserve temporal information, and doesn’t suffer from sparsity or dimensionality explosion problems. Given a dictionar
2770564921	FAST READING COMPREHENSION WITH CONVNETS	2567070169	ecture to avoid recurrent units in text precessing. More speciﬁcally, we use a combination of residual networks (He et al., 2016), dilated convolutions (Yu &amp; Koltun, 2016) and gated linear units (Dauphin et al., 2017). 1.1 READING COMPREHENSION TASKS Reading comprehension tasks focus on one’s ability to read a piece of text and subsequently answer questions about it (see TriviaQA examples in Figure 1). We follow t
2770564921	FAST READING COMPREHENSION WITH CONVNETS	2626778328	g (for example as part of search engines or mobile assistants), as it requires the user to wait patiently for the answer. Recent development of “attention only” deep text models Parikh et al. (2016); Vaswani et al. (2017) in various tasks allows modeling of long range dependencies without regard to their distance. By parallelization within one instance, these models can have much better inference time than those which
2770564921	FAST READING COMPREHENSION WITH CONVNETS	2613904329	lying ConvNet architectures to reduce the sequential computation in sequence to sequence models such as Extended Neural GPU(Kaiser &amp; Bengio, 2016), ByteNet (Kalchbrenner et al., 2016) and ConvS2S Gehring et al. (2017). In these models, the number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions, linearly for ConvS2S and logarithmically
2770564921	FAST READING COMPREHENSION WITH CONVNETS	2567070169	here the meaning of large phrases are composed of the meaning of their sub-phrases. These constraints lead us to choose dilated convolutional networks (Yu &amp; Koltun, 2016) with gated linear units (Dauphin et al., 2017). By increasing the receptive ﬁeld in our convolutional units, dilation can help to model arbitrarily long-distance dependencies. Unfortunately, the receptive region is pre-determined, which prevents
2770564921	FAST READING COMPREHENSION WITH CONVNETS	2567070169	sing dilation will miss this. In practice, “Socrates” is combined with its context to give a ﬁxed size representation for a long context. We alleviate some of this effect by using Gated Linear Units (Dauphin et al., 2017) in our convolutions. These units allow us to selectively retain (and compute gradients for) important features of low-level words and phrases, even at convolutions with larger dilations. Dilated Conv
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2622000134	. (2017) similarly use NMT to generate paraphrase corpora. Other work has used neural MT architectures and training settings to obtain better word embeddings (Hill et al., 2014a,b). This work extends Wieting et al. (2017), scaling it up to a larger corpus of machine translations and producing state-of-the-art paraphrastic sentence embeddings. 3 Neural Machine Translation To create the data used for training our models
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2250539671	(300d, addition) 79.6 Trigram-Word (600d, concatenation) 79.9 Trigram-Word-LSTM (600d, concatenation) 79.2 Unsupervised (Related Work) (Conneau et al., 2017) (AllSNLI) 70.6 (Pham et al., 2015) 63.9 (Pennington et al., 2014) 40.6 (Mikolov et al., 2013) 56.5 (Pagliardini et al., 2017) 75.5 Supervised (Tai et al., 2015) LSTM 70.5 (Tai et al., 2015) BiLSTM 71.1 (Tai et al., 2015) Dep. Tree LSTM 71.2 (Tai et al., 2015) Const
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1486649854	(Conneau et al., 2017) 4096 57.1 50.4 66.2 65.2 63.5 FastSent (Hill et al., 2016) 100 - - 63 - - DictRep (Hill et al., 2016) 500 - - 67 - - CaptionRep (Hill et al., 2016) 500 - - 46 - - SkipThought (Kiros et al., 2015) 4800 - - 29 - - CPHRASE (Pham et al., 2015) - - - 65 - - ParagraphVec (Le and Mikolov, 2014) 100 - - 42 - - CBOW (from (Hill et al., 2016)) 500 - - 64 - - Skipgram (from (Hill et al., 2016)) - - - 62
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1486649854	. to sentence embeddings. Most of this work is centered on general-purpose embeddings that can be used for any task, when they are used as features for a linear classiﬁer or a shallow neural network (Kiros et al., 2015; Conneau et al., 2017) Our recent work has focused on paraphrastic sentence representations where sentences that have the same meaning lie close together in a vector space (Wieting et al., 2015, 2016
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251047310	5.2 63.5 FastSent (Hill et al., 2016) 100 - - 63 - - DictRep (Hill et al., 2016) 500 - - 67 - - CaptionRep (Hill et al., 2016) 500 - - 46 - - SkipThought (Kiros et al., 2015) 4800 - - 29 - - CPHRASE (Pham et al., 2015) - - - 65 - - ParagraphVec (Le and Mikolov, 2014) 100 - - 42 - - CBOW (from (Hill et al., 2016)) 500 - - 64 - - Skipgram (from (Hill et al., 2016)) - - - 62 - - Table 7: Results on the STS tasks of ou
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1486649854	7/80.7 - - Unsupervised (Ordered Sentences) FastSent (Hill et al., 2016) 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - FastSent+AE (Hill et al., 2016) 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - SkipThought (Kiros et al., 2015) 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 Unsupervised (Structured Resources) CaptionRep (Hill et al., 2016) 61.9 69.3 77.4 70.8 - 72.2 - - - DictRep (Hill et al., 2016) 76.7 78.7 90.7 87.2
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1681397005	75.1/82.3 0.885 86.3 BiLSTM-Max (on AllNLI) (Conneau et al., 2017)y 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 Supervised (Direct) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - AdaSent (Zhao et al., 2015) 83.1 86.3 95.5 93.3 - 92.4 - - - TF-KLD (Ji and Eisenstein, 2013) - - - - - - 80.4/85.9 - - Illinois-LH (Lai and Hockenmaier, 2014) - - - - - - - - 84.5 Dependency Tree-LSTM (Tai et al., 2015) - - -
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2611248707	ain all models for 5 epochs. For optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 0.001. For the recurrent neural networks, we set the scrambling rate to 0.3. We found, like in (Wieting and Gimpel, 2017), that scrambling signiﬁcantly improves the results even though we are using far Filtering Method Para. Sim. Trigram Ovl. Trans. Score LSTM Avg. 83.8 83.4 83.4 Table 4: Best result on each fold of the
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251861449	al. (2016b). We use the paraphrases as training data to create paraphrastic sentence embeddings, then evaluate the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017). Lastly, we evaluate on the STS Benchmark (Cer et al., 2017). The STS Benchmark was designed to be a standard for comparing sentence representation across a
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2123442489	or each data and trained 3 different models, Word Averaging, Trigram Averaging, 5We computed the inverse-document-frequencies from Wikipedia. 6We computed these parse trees using the Stanford Parser (Manning et al., 2014) Data Word Avg. Trigram Avg. LSTM Avg. Common-crawl 80.9 80.2 79.1 Czeng1.6 83.6 81.5 82.5 Europarl 78.9 78.0 80.4 News 80.2 78.2 80.5 Table 3: Results of STS2017 for the data resources. The table sho
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	174630521	discovery. Many researchers have developed methods for generating or ﬁnding naturally-occurring sentential paraphrase pairs (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015; Lan et al., 2017). The most relevant work uses bilingual corpora, e.g., Bannard and Callison-Burch (2005), culminating in the Paraphrase Database (PP
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2175723921	e structure, and we release it to the community. and release it to the community. To show its utility, we use it to train paraphrastic sentence embeddings using only minor changes to the framework of Wieting et al. (2016b). The resulting embeddings outperform all supervised systems on every SemEval semantic textual similarity (STS) competition, and are a signiﬁcant improvement in capturing paraphrastic similarity ove
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1814992895	eau et al., 2017). However, we found that for a given ﬁxed output dimension, this wasn’t as effective as just averaging the hidden states. 4.2 Training We follow the training procedure of prior work (Wieting et al., 2015, 2016b), with one modiﬁcation. We introduce “mega-batching” for selecting negative examples which results in signiﬁ- cant performance gains. The training data is a set Sof paraphrastic pairs hs 1;s 2
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2175723921	the embeddings x t of all words in s. The only parameters learned in this model are those in the word embeddings themselves, which are stored in the word embedding matrix W w. This model was found by Wieting et al. (2016b) to per2We also experimented with using our highest performing model from Section 6. This is similar to self-training in which a model is trained (typically with a small amount of supervision) and t
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2622000134	ent work has focused on paraphrastic sentence representations where sentences that have the same meaning lie close together in a vector space (Wieting et al., 2015, 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017) In this paper, we make two contributions. First, we scale up the work of Wieting et al. (2017). We use neural machine translation (NMT) to generate an automatic paraphrase corpus of over 51 million s
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1486649854	er this setting as well (Arora et al., 2017). Other work in learning general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Conneau et al., 2017). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1814992895	etwork (Kiros et al., 2015; Conneau et al., 2017) Our recent work has focused on paraphrastic sentence representations where sentences that have the same meaning lie close together in a vector space (Wieting et al., 2015, 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017) In this paper, we make two contributions. First, we scale up the work of Wieting et al. (2017). We use neural machine translation (NMT) to ge
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2622000134	impel2 1Carnegie Mellon University, Pittsburgh, PA, 15213, USA 2Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA jwieting@cs.cmu.edu, kgimpel@ttic.edu Abstract We extend the work of Wieting et al. (2017), back-translating a large parallel corpus to produce a dataset of more than 51 million English-English sentential paraphrase pairs in a dataset we call PARANMT-50M. We ﬁnd this corpus to be cover man
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2143927888	Keown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015; Lan et al., 2017). The most relevant work uses bilingual corpora, e.g., Bannard and Callison-Burch (2005), culminating in the Paraphrase Database (PPDB; Ganitkevitch et al., 2013). Our goals are similar to those of PPDB, which has likewise been generated for many languages (Ganitkevitch and CallisonBurch
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1814992895	any languages (Ganitkevitch and CallisonBurch, 2014) since it only needs parallel text. Prior work has shown that PPDB can be used for learning embeddings for words and phrases (Faruqui et al., 2015; Wieting et al., 2015). However, when learning sentence embeddings, Wieting and Gimpel (2017) showed that PPDB is not as effective as sentential paraphrases, especially for recurrent networks. These results are intuitive b
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251047310	is likely more important when training the model on sentences since they have so much more space to have diverse meaning unlike prior work on learning on text snippets (Wieting et al., 2015, 2016b,a; Pham et al., 2015). 6.5 Simple Models We ﬁrst investigate how well simple models (Word Avg., Trigram Avg., and LSTM Avg.) do on their own. Table 7 shows the results on the STS tasks from 2012-2016. Table 9 shows result
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251047310	n) 79.9 Trigram-Word-LSTM (300d, addition) 79.6 Trigram-Word (600d, concatenation) 79.9 Trigram-Word-LSTM (600d, concatenation) 79.2 Unsupervised (Related Work) (Conneau et al., 2017) (AllSNLI) 70.6 (Pham et al., 2015) 63.9 (Pennington et al., 2014) 40.6 (Mikolov et al., 2013) 56.5 (Pagliardini et al., 2017) 75.5 Supervised (Tai et al., 2015) LSTM 70.5 (Tai et al., 2015) BiLSTM 71.1 (Tai et al., 2015) Dep. Tree LST
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2622000134	nce similarity scores in semantic evaluations. They used pairs of NMT systems, one to translate an English sentence into multiple foreign translations and the other to then translate back to English. Wieting et al. (2017) only uses the NMT system to generate training data for training sentence embeddings, rather than using it in a similarity model itself. This permitted us to decouple decisions made in designing the N
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2136189984	ng of 250 examples. form very strongly for semantic similarity tasks. The second model is similar to word averaging, but instead of averaging word embeddings, we average character trigram embeddings (Huang et al., 2013). Wieting et al. (2016a) found this to perform strongly for sentence embeddings compared to other n-gram orders and compared to wordaveraging. The third model are various LSTM architectures(Hochreiter
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2175723921	orm very strongly for semantic similarity tasks. The second model is similar to word averaging, but instead of averaging word embeddings, we average character trigram embeddings (Huang et al., 2013). Wieting et al. (2016a) found this to perform strongly for sentence embeddings compared to other n-gram orders and compared to wordaveraging. The third model are various LSTM architectures(Hochreiter and Schmidhuber, 1997
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	131533222	Paraphrase generation and discovery. Many researchers have developed methods for generating or ﬁnding naturally-occurring sentential paraphrase pairs (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015; Lan et al., 2017). The most relevant work uses bilingual corpora, e.g., Bannard and Callison-Burch (2005), culminating in the Par
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251047310	ral purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Conneau et al., 2017). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest conﬁgurations from their
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2175723921	raphrases, and ﬁnally prior work in leveraging neural machine translation for embedding learning. Paraphrasticsentenceembeddings. Our learning and evaluation setting is the same as that considered by Wieting et al. (2016b) and Wieting et al. (2016a), in which the goal is to learn paraphrastic sentence embeddings that can be used for downstream tasks. They trained models on PPDB and evaluated them using a suite of sem
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1980776243	from their results. Paraphrase generation and discovery. Many researchers have developed methods for generating or ﬁnding naturally-occurring sentential paraphrase pairs (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015; Lan et al., 2017). The most relevant work uses bilingual corpora, e.g., Bannard and Callison-Burch (200
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2251919380	rvised (Direct) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - AdaSent (Zhao et al., 2015) 83.1 86.3 95.5 93.3 - 92.4 - - - TF-KLD (Ji and Eisenstein, 2013) - - - - - - 80.4/85.9 - - Illinois-LH (Lai and Hockenmaier, 2014) - - - - - - - - 84.5 Dependency Tree-LSTM (Tai et al., 2015) - - - - - - - 0.868 - Table 8: General sentence representations tasks for unsupervsied, learning, unsupervised learning using structured r
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1814992895	in Section 5. Second, we show the utility of the corpus by using it to train state-of-the-art paraphrastic sentence embeddings. We perform a thorough search of the space, motivated by our prior work (Wieting et al., 2015, 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017) and ﬁnd that many neural architectures can be used to create strong embeddings. Moreover, given the large size of the paraphrase corpus, we f
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2107130271	the strongest conﬁgurations from their results. Paraphrase generation and discovery. Many researchers have developed methods for generating or ﬁnding naturally-occurring sentential paraphrase pairs (Barzilay and McKeown, 2001; Dolan et al., 2004; Dolan and Brockett, 2005; Quirk et al., 2004; Coster and Kauchak, 2011; Xu et al., 2014, 2015; Lan et al., 2017). The most relevant work uses bilingual corpora, e.g., Bannard and
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1486649854	to use our generated paraphrase data for training paraphrastic sentence embeddings as well as general-purpose sentence embeddings to be used as features for a wide range of tasks as has been done in (Kiros et al., 2015; Conneau et al., 2017). 6.1 Evaluation 6.1.1 Paraphrastic Sentence Embedding Evaluation We evaluate the quality of a paraphrase dataset by using the experimental setting of Wieting et al. (2016b). We
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2175723921	utoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Conneau et al., 2017). Wieting et al. (2016b) and Hill et al. (2016) provide many empirical comparisons to this prior work. For conciseness, we compare only to the strongest conﬁgurations from their results. Paraphrase generation and discovery
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2153579005	Word (600d, concatenation) 79.9 Trigram-Word-LSTM (600d, concatenation) 79.2 Unsupervised (Related Work) (Conneau et al., 2017) (AllSNLI) 70.6 (Pham et al., 2015) 63.9 (Pennington et al., 2014) 40.6 (Mikolov et al., 2013) 56.5 (Pagliardini et al., 2017) 75.5 Supervised (Tai et al., 2015) LSTM 70.5 (Tai et al., 2015) BiLSTM 71.1 (Tai et al., 2015) Dep. Tree LSTM 71.2 (Tai et al., 2015) Const. Tree LSTM 71.9 (Shao, 2017
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2611248707	of wordaveraging and BiLSTM models with different sizes of mega-batches. more sentence level data than was used in those previous experiments. We found a smaller value than the 0.5 primarily used in (Wieting and Gimpel, 2017) worked better most likely due to the larger amount of training data. 6.3 Filtering Data In order to ﬁnd the optimal training data to use for training our models, we ﬁrst ranked all 51M+ generated par
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2130942839	xt simpliﬁcation by Coster and Kauchak (2011). It was created by aligning sentences from Simple English and standard English Wikipedia. Neural machine translation for paraphrastic embedding learning. Sutskever et al. (2014) trained NMT systems and visualized part of the space of the source language encoder for their English!French system. Hill et al. (2016) evaluated the encoders of English-to-X NMT systems as sentence
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2103305545	y (STS) tasks and supervised semantic tasks. Others have begun to consider this setting as well (Arora et al., 2017). Other work in learning general purpose sentence embeddings has used autoencoders (Socher et al., 2011; Hill et al., 2016), encoder-decoder architectures (Kiros et al., 2015), or other learning frameworks (Le and Mikolov, 2014; Pham et al., 2015; Conneau et al., 2017). Wieting et al. (2016b) and Hill
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	2622000134	y using it to train state-of-the-art paraphrastic sentence embeddings. We perform a thorough search of the space, motivated by our prior work (Wieting et al., 2015, 2016b,a; Wieting and Gimpel, 2017; Wieting et al., 2017) and ﬁnd that many neural architectures can be used to create strong embeddings. Moreover, given the large size of the paraphrase corpus, we found beneﬁt from making a small change to the training pro
2770626128	ParaNMT: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations	1814992895	ﬁrst used the translation scores from decoding. Secondly, we used trigram overlap ﬁltering as was done by Wieting et al. (2017). Lastly, we experimented with using the PARAGRAMPHRASE embeddings from Wieting et al. (2015), with higher scoring pairs indicating a stronger paraphrase relationship between the sentences.2 We then proceeded to ﬁnd the quality training data. To do this, we ranked all 51M+ paraphrase pairs in
2770646692	Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent	2112177991	. Grounding language in an interactive environment is an active area of research, however a number of recent works employ synthetic, templated language only (Sukhbaatar et al., 2015; Yu et al., 2017; Bordes et al., 2010; Hermann et al., 2017; Mikolov et al., 2015; Chaplot et al., 2017). Some works that do utilize real natural language and interaction include Wang et al. (2016), where language is learnt to solve bloc
2770646692	Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent	2251512949	ta, notably the ESP game (Von Ahn &amp; Dabbish, 2004), which is an image annotation tool where users are paired and have to “read each others mind” to agree on the contents of an image. ReferItGame (Kazemzadeh et al., 2014) and Peekaboom (Von Ahn et al., 2006) have similar ideas but for localizing objects. In a completely different ﬁeld, Foldit is an online game where players compete to manipulate proteins (Eiben et al.
2770646692	Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent	2151023586	he Turker acts as the model’s teacher they are essentially deﬁning the curriculum as teachers do for students. Choosing the best examples for the model to see next is also related to active learning (Cohn et al., 1994) except in our case this is chosen on the teacher’s, rather than the learner’s side. MTD is not easily exploitable/gameable Mechanical Turk data collection is notorious for providing poor results unl
2770653590	Visual Question Answering as a Meta Learning Task	2496096353	ably, performance improves as additional and more relevant examples are included. a question and image to scores over a predetermined, ﬁxed vocabulary of possible answers using the maximum likelihood [39]. This approach has inherent scalability issues, as it attempts to represent all world knowledge within the ﬁnite set of parameters of a model such as deep neural network. Consequently, a trained VQA
2770653590	Visual Question Answering as a Meta Learning Task	2250539671	anisms or other computer vision techniques [3,37,39]. We follow the implementation of [34] for the embedding part. For concreteness, let us mention that the question embedding uses GloVe word vectors [28] and a Recurrent Gated Unit (GRU [10]). The image embedding uses features from 1The separation of the network into an embedding and a classiﬁer parts is conceptual. The division is arbitrarily placed
2770653590	Visual Question Answering as a Meta Learning Task	1933349210	re answers, and a better sample efﬁciency than existing models. 2. Related Work Visual question answering Visual question answering has gathered signiﬁcant interest from the computer vision community [6], as it constitutes a practical setting to evaluate deep visual understanding. In addition to visual parsing, VQA requires the comprehension of a text question, and combined reasoning over vision and
2770653590	Visual Question Answering as a Meta Learning Task	1933349210	always approached in a supervised setting, using large datasets [6,15,22,44] of human-proposed questions with their correct answers to train a machine learning model. The VQA-real and VQA v2 datasets [6,15] have served as popular benchmarks by which to evaluate and compare methods. Despite the large scale of those datasets, e.g. more than 650,000 questions in VQA v2, several limitations have been recogn
2770653590	Visual Question Answering as a Meta Learning Task	2137825550,2427497464	broadly refers to methods that learn to learn, i.e. that train models to make better use of training data. It applies to approaches including the learning of gradient descent-like algorithms such as [5,13,17,30] for faster training or ﬁne-tuning of neural networks, and the learning of models that can be directly fed training examples at test time [7,33,36]. The method we propose falls into the latter categor
2770653590	Visual Question Answering as a Meta Learning Task	2583010282	cedures that are only suitable for a small number of classes. Our model uses a set of memories within a neural network to store the activations computed over the support set. Similarly, Kaiser et al. [19] store past activations to remember “rare events”, which was notably evaluated on machine translation. Our model also uses network layers parametrized by dynamic weights, also known as fast weights. T
2770653590	Visual Question Answering as a Meta Learning Task	2496096353	dress only the second part. Our contributions are orthogonal to developments on the embedding part, which could also beneﬁt e.g. from advanced attention mechanisms or other computer vision techniques [3,37,39]. We follow the implementation of [34] for the embedding part. For concreteness, let us mention that the question embedding uses GloVe word vectors [28] and a Recurrent Gated Unit (GRU [10]). The imag
2770653590	Visual Question Answering as a Meta Learning Task	2273038706	e baseline is most effective with frequent answers, the proposed model fares better (mostly positive values) in the long tail of rare answers. This corroborates previous discussions on dataset biases [15,18,43] which classical models are prone to overﬁt to. The proposed model is inherently more robust to such behaviour. 6. Conclusions and Future Work We have devised a new approach to VQA through framing it
2770653590	Visual Question Answering as a Meta Learning Task	2273038706	e complicated questions. The aggregate accuracy metric used to compare methods is thus a poor indication of method capabilities for visual understanding. Improvements to datasets have been introduced [1,15,43], including the VQA v2, but they only partially solve the evaluation problems. An increased interest has appeared in the handling of rare words and answers [29,35]. The model proposed in this paper is
2770653590	Visual Question Answering as a Meta Learning Task	2496096353	g. In addition to visual parsing, VQA requires the comprehension of a text question, and combined reasoning over vision and language, sometimes on the basis of external or common-sense knowledge. See [39] for a recent survey of methods and datasets. VQA is always approached in a supervised setting, using large datasets [6,15,22,44] of human-proposed questions with their correct answers to train a mach
2770653590	Visual Question Answering as a Meta Learning Task	2157331557	iques [3,37,39]. We follow the implementation of [34] for the embedding part. For concreteness, let us mention that the question embedding uses GloVe word vectors [28] and a Recurrent Gated Unit (GRU [10]). The image embedding uses features from 1The separation of the network into an embedding and a classiﬁer parts is conceptual. The division is arbitrarily placed after the fusion of the question and
2770653590	Visual Question Answering as a Meta Learning Task	2496096353	in a Meta Learning Setting The traditional approach to VQA is in a supervised setting described as follows. A model is trained to map an input question Q and image I to scores over candidate answers [39]. The model is trained to maximize the likelihood of correct answers over a training set Tof triplets (Q;I;^s), where ^s 2[0;1]A represents the vector of ground truth scores of the predeﬁned set of Ap
2770653590	Visual Question Answering as a Meta Learning Task	2136462581	nd language, sometimes on the basis of external or common-sense knowledge. See [39] for a recent survey of methods and datasets. VQA is always approached in a supervised setting, using large datasets [6,15,22,44] of human-proposed questions with their correct answers to train a machine learning model. The VQA-real and VQA v2 datasets [6,15] have served as popular benchmarks by which to evaluate and compare me
2770653590	Visual Question Answering as a Meta Learning Task	1933349210	orm, long-tailed distribution of answers) and the question-conditioned bias (making answers easy to guess given a question without the image). For example, the answer Yes is particularly prominent in [6] compared to no, and questions starting with How many can be answered correctly with the answer two more than 30% of the time [15]. These issues plague development in the ﬁeld by encouraging methods w
2770653590	Visual Question Answering as a Meta Learning Task	2171810632,2496096353	he proposed model (Fig.3) is a deep neural network that extends the state-of-art VQA system of Teney et al. [34]. Their system implements the joint embedding approach common to most modern VQA models [39,41,18, 20], which is followed by a multi-label classiﬁer over candidate answers. Conceptually, we separate the architecture into (1) the embedding part that encodes the input question and image, and (2) the cla
2770653590	Visual Question Answering as a Meta Learning Task	2175714310	rmined at test time depending on the actual input to the network. Dynamic parameters have a long history in neural networks [32] and have been used previously for few-shot recognition [7] and for VQA [27]. One of the memories within our network stores the gradient of the loss with respect to static weights of the network, which is similar to the Meta Networks model proposed by Munkhdalai et al. [24].
2770653590	Visual Question Answering as a Meta Learning Task	2038276547,2583010282	scale, in particular for handling the memory of dynamic weights that currently grows linearly with the support set. Clustering schemes could be envisioned to reduce its size [33] and hashing methods [4,19] could improve the efﬁciency of the content-based retrieval. Generally, the handling of additional data at test time opens the door to VQA systems that interact with other sources of information. Whil
2770970123	FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension	2521709538	els have been proposed for this challenge and they generally frame this problem as a machine reading comprehension (MRC) task (Hochreiter &amp; Schmidhuber, 1997; Wang et al., 2017; Seo et al., 2017; Shen et al., 2017; Xiong et al., 2017; Weissenborn et al., 2017; Chen et al., 2017a). The key innovation in recent models lies in how to ingest information in the question and characterize it in the context, in order
2770970123	FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension	2250539671	for FusionNet is shown in Figure 4. It consists of the following components. Input Vectors. First, each word in C and Q is transformed into an input vector w. We utilize the 300-dim GloVe embedding (Pennington et al., 2014) and 600-dim contextualized vector (McCann et al., 2017). In the SQuAD task, we also include 12-dim POS embedding, 8-dim NER embedding and a normalized term frequency for context C as suggested in (Ch
2770970123	FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension	2521709538	LSTM (Wang &amp; Jiang, 2016) 64.7 / 73.7 BiDAF (Seo et al., 2017) 68.0 / 77.3 SEDT (Liu et al., 2017) 68.2 / 77.5 RaSoR (Lee et al., 2016) 70.8 / 78.7 DrQA (Chen et al., 2017a) 70.7 / 79.4 ReasoNet (Shen et al., 2017) 70.6 / 79.4 R. Mnemonic Reader (Hu et al., 2017) 73.2 / 81.8 DCN+ 74.9 / 82.8 R-nety 75.7 / 83.5 FusionNet 76.0 / 83.9 Ensemble Model ReasoNet (Shen et al., 2017) 75.0 / 82.3 MEMEN (Pan et al., 2017)
2770970123	FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension	2626778328	ntion (MLP) (Bahdanau et al., 2015): sT tanh(W 1x+W 2y): 2.Multiplicative attention: xT UT Vy: 3.Scaled multiplicative attention: p1 k xT UT Vy;where kis the attention hidden size. It is proposed in (Vaswani et al., 2017). 4.Scaled multiplicative with nonlinearity: p1 k f(Ux)T f(Vy). 5.Our proposed symmetric form: xT UT DUy;where Dis diagonal. 6.Proposed symmetric form with nonlinearity: f(Ux)T Df(Uy). We consider the
2771039833	CROWDSOURCING QUESTION-ANSWER MEANING REPRESENTATIONS	1584130740	, how often, how old, how big, etc. (see example 10a). Outside of our sample, we also found examples of verbs being used to paraphrase the relationships in noun compounds, similar to those proposed byNakov (2008). For example, the question Who conducted the poll? for the phrase Gallup poll, and Who received the bailouts? for the phrase bank bailouts. There is room for improvement here: many noun modiﬁers are
2771039833	CROWDSOURCING QUESTION-ANSWER MEANING REPRESENTATIONS	2252123671	lations identiﬁed in step 3 of our structure induction algorithm) is an interesting avenue of future work. 5 Related Work In addition to the semantic formalisms (Palmer et al.,2005;Meyers et al.,2004;Banarescu et al., 2013;He et al.,2015) we have already discussed, FrameNet (Baker et al.,1998) also focuses predicate-argument structure, but has more ﬁnegrained argument types.Gerber and Chai(2010) target implicit nominal
2771039833	CROWDSOURCING QUESTION-ANSWER MEANING REPRESENTATIONS	2125436846	losely related to the approach of Nakov(2008), which used crowdsourcing to paraphrase the semantic relations in noun compounds. Question-answering tasks such as SQuAD (Rajpurkar et al.,2016), MCTest (Richardson et al., 2013), and VQA (Antol et al.,2015) also use crowdsourcing for cheap and scalable collection of question-answer pairs, albeit for very different end purposes. 6 Conclusion and Future Work QAMR provides a ne
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	147290778	be difﬁcult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al.,2010; Liang et al.,2011;Krishnamurthy and Mitchell, 2012;Kwiatkowski et al.,2013;Berant et al., 2013;Cai and Yates,2013;Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the resul
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	2252136820	with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al.,2010; Liang et al.,2011;Krishnamurthy and Mitchell, 2012;Kwiatkowski et al.,2013;Berant et al., 2013;Cai and Yates,2013;Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environ
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	2161002933	om denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade. Early work focused on traditional loglinear models (Clarke et al.,2010;Liang et al., 2011;Kwiatkowski et al.,2013;Berant et al., 2013), but recently denotations have been used to train neural semantic parsers as well (Liang et al.,2017;Krishnamurthy et al.,2017;Rabinovich et al.,2017;Chen
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	1933349210	rain neural semantic parsers (Liang et al.,2017;Krishnamurthy et al., 2017;Rabinovich et al.,2017;Cheng et al.,2017). Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al.,2017a). The advantage of CNLVR is that language utterances are both natural and compositional. Treating visual reasoning as an end-to-end semantic parsing problem has been p
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	2189089430	s has led to a body of work on weaklysupervised semantic parsing (Clarke et al.,2010; Liang et al.,2011;Krishnamurthy and Mitchell, 2012;Kwiatkowski et al.,2013;Berant et al., 2013;Cai and Yates,2013;Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig.1). Naturally, collecting denotati
2771052642	Weakly Supervised Semantic Parsing with Abstract Examples	2152263452	semantically coherent, we can generate correct examples from abstract examples by sampling pairs of utteranceprogram tokens for each cluster. This is equivalent to a synchronous context-free grammar (Chiang, 2005) that has a rule for generating each manuallyannotated abstract utterance-program pair, and rules for generating synchronously utterance and program tokens from the seven clusters. We generated 4,962
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2125436846	2018) and becomes the new state of the art6. 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xi
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2411480514	chardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model ﬁrst maps the symbolic representation of the documents and questions into a neural spa
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2411480514	ches the question and document only once and produce the ﬁnal answers. It is simple yet efﬁcient and can be trained using the classical back-propagation algorithm, thus it is adopted by most systems (Chen et al., 2016; Seo et al., 2016; Wang et al., 2017; Liu et al., 2017b; Chen et al., 2017; Weissenborn et al., 2017; 5It is the same model structure as (Liu et al., 2018) by using softmax over all candidate passage
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2250539671	is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Qand P. Following Chen et al. (2017), we use three additional types of linguistic features for each token p i in the passage P: 9-dimensional POS tagging embedding for total 56 differen
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2427527485	customer service support. It has been hypothesized that difﬁcult MRC problems require some form of multi-step synthesis and reasoning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) illustrates the need for synthesis of information across sentences and multiple steps of reasoning: Q: What collection does the V&amp;A Theator &amp; Performance galleries hold? P: The V&amp;A Theato
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2131494463	e complexity of the questions/documents), it is natural to devise an iterative way to ﬁnd answers as multi-step reasoning. Pioneered by (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015), who used a predetermined ﬁxed number of reasoning steps, Shen et al (2016; 2017) showed that multi-step reasoning outperforms single-step ones and dynamic multi-step reasoning further outperforms th
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2101105183	e extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer.
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2427527485	eve that while training time per epoch is similar between SAN and other models, it is recommended to train SAN for more epochs in order to achieve gains in EM/F1. Single model: AddSent AddOneSent LR (Rajpurkar et al., 2016) 23.2 30.3 SEDT (Liu et al., 2017a) 33.9 44.8 BiDAF (Seo et al., 2016) 34.3 45.7 jNet (Zhang et al., 2017) 37.9 47.0 ReasoNet(Shen et al., 2017) 39.4 50.3 RaSoR(Lee et al., 2016) 39.5 49.5 Mnemonic(Hu
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2427527485	exclusively on a particular step to generate correct output. We used a dropout rate of 0.4 in experiments. 3 Experiment Setup Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). This contains about 23K passages and 100K questions. The passages come from approximately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The crowdsourced workers
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2131494463	gh the same text and integrating intermediate information in the process. The ﬁrst models employed a predetermined ﬁxed number of steps (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015). Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al. (2017) empirically showed t
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2133564696	Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model ﬁrst maps the symbolic representation of the documents and question
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2738015883	ome approximate hyper-parameter tuning for the number of steps, but it is not necessary to ﬁnd the exact optimal value. Finally, we test SAN on two Adversarial SQuAD datasets, AddSent and AddOneSent (Jia and Liang, 2017), where the passages contain (a) EM comparison on different systems. (b) F1 score comparison on different systems. Figure 3: K-Best Oracle results auto-generated adversarial distracting sentences to f
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2154652894	s. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of
2771857412	Stochastic Answer Networks for Machine Reading Comprehension	2427527485	how that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art6. 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types
2771873754	Neural Cross-lingual Entity Linking	2157191138	.1 This work 92.1 This work+CtxLSTMs 93.0 This work+CtxLSTMs+LDC 93.4 This work+CtxLSTMs+LDC+MPCM 94.0 (a) CoNLL2003 Systems In-KB acc. % TAC Rank 1 79.2 TAC Rank 2 71.6 Sil &amp; Florian (2016) 78.6 He et al. (2013) 81.0 Chisholm &amp; Hachey (2015) 80.7 Sun et al. (2015) 83.9 Yamada et al. (2016) 85.2 Globerson et al. (2016) 87.2 This work 85.0 This work+CtxLSTMs 86.3 This work+CtxLSTMs+LDC 86.9 This work+CtxLS
2771873754	Neural Cross-lingual Entity Linking	2295227292	82.5 Gupta et al. (2017) 82.9 He et al. (2013) 85.6 Francis-Landau et al. (2016) 85.5 Sil &amp; Florian (2016) 86.2 Lazic et al. (2015) 86.4 Chisholm &amp; Hachey (2015) 88.7 Ganea et al. (2015) 87.6 Pershina et al. (2015) 91.8 Globerson et al. (2016) 92.7 Yamada et al. (2016) 93.1 This work 92.1 This work+CtxLSTMs 93.0 This work+CtxLSTMs+LDC 93.4 This work+CtxLSTMs+LDC+MPCM 94.0 (a) CoNLL2003 Systems In-KB acc. % TAC
2771873754	Neural Cross-lingual Entity Linking	2336089273	ablation study, we notice that adding the LDC layer provides a boost to our model in both the datasets, Systems In-KB acc. % Hoffart et al. (2011) 82.5 Gupta et al. (2017) 82.9 He et al. (2013) 85.6 Francis-Landau et al. (2016) 85.5 Sil &amp; Florian (2016) 86.2 Lazic et al. (2015) 86.4 Chisholm &amp; Hachey (2015) 88.7 Ganea et al. (2015) 87.6 Pershina et al. (2015) 91.8 Globerson et al. (2016) 92.7 Yamada et al. (2016) 93
2771873754	Neural Cross-lingual Entity Linking	2517161177	al. 2016), and Neural Tensor Networks (Socher et al. 2013a; 2013c) to encode this information and ultimately perform EL. The TAC community is also interested in cross-lingual EL (Tsai and Roth 2016; Sil and Florian 2016): given a mention in a foreign language document e.g. Spanish or Chinese, one has to ﬁnd its corresponding link in the English Wikipedia. The main motivation of the task is to do Information Extractio
2771873754	Neural Cross-lingual Entity Linking	2151048449	on algorithms (Globerson et al. 2016; Milne and Witten 2008; Cheng and Roth 2013; Sil and Yates 2013) but are more expensive since they capture coherence among titles in the given document. However, (Ratinov et al. 2011) argue that global systems provide a minor improvement over local systems. Our proposed EL system is a local system which comprises of a deep neural network architecture with various layers computing
2771873754	Neural Cross-lingual Entity Linking	2131340601	andau, Durrett, and Klein 2016), which chose to use the entire document for modeling. Hence, following similar ideas in (Barrena et al. 2014; Lee et al. 2012), we run a coreference resolution system (Luo et al. 2004) and assume a “one link per entity” paradigm (similar to one sense per document (Gale, Church, and Yarowsky 1992; Yarowsky 1993)). We then use these to build a sentence-based context representation of
2771873754	Neural Cross-lingual Entity Linking	2517161177	beddings. For the MPBL node, the number of dimensions is 100. Comparison with the SOTA The current SOTA for English EL are (Globerson et al. 2016) and (Yamada et al. 2016). We also compare with LIEL (Sil and Florian 2016) which is a language-independent EL system and has been a top performer in the TAC annual evaluations. For cross-lingual EL, our major competitor is (Tsai and Roth 2016) who uses multi-lingual embeddi
2771873754	Neural Cross-lingual Entity Linking	2151048449	e base or KB, and is one of the major tasks in the Knowledge-Base Population (KBP) track at the Text Analysis Conference (TAC) (Ji et al. 2014; 2015). Most of the previous EL research (Cucerzan 2007; Ratinov et al. 2011; Sil and Yates 2013) have used Wikipedia as the target catalog of entities, because of its coverage and frequent updates made by the community of users. Some ambiguous cases for entity linking requir
2771873754	Neural Cross-lingual Entity Linking	2260776682	el obtains 7.77% (on CoNLL) and 8.75% (on TAC) points better than (Sil and Florian 2016), which is a SOTA multi-lingual system. Another interesting fact we observe is that our full model outperforms (Sun et al. 2015) by 3.5% points, where they employ NTNs to model the semantic interactions between the context and the mention. Our model uses NTNs to model the left and right contexts from the full entity coreferenc
2771873754	Neural Cross-lingual Entity Linking	86887328	lled a knowledge base or KB, and is one of the major tasks in the Knowledge-Base Population (KBP) track at the Text Analysis Conference (TAC) (Ji et al. 2014; 2015). Most of the previous EL research (Cucerzan 2007; Ratinov et al. 2011; Sil and Yates 2013) have used Wikipedia as the target catalog of entities, because of its coverage and frequent updates made by the community of users. Some ambiguous cases for
2771873754	Neural Cross-lingual Entity Linking	1548663377	A methods. In Spanish, LS and CCA are tied but in Chinese, CCA performs better than LS. Note that “this work” in Table 2 indicates our full model with LDC and MPCM. Related Work Previous works in EL (Bunescu and Pasca 2006; Mihalcea and Csomai 2007) involved ﬁnding the similarity of the context in the source document and the context of the candidate Wikipedia titles. Recent research on EL has focused on sophisticated g
2771873754	Neural Cross-lingual Entity Linking	2124033848,2127426251,2251939518	Ms (Hochreiter and Schmidhuber 1997), Lexical Composition and Decomposition (Wang, Mi, and Ittycheriah 2016), Multi-Perspective Context Matching (MPCM) (Wang et al. 2016), and Neural Tensor Networks (Socher et al. 2013a; 2013c) to encode this information and ultimately perform EL. The TAC community is also interested in cross-lingual EL (Tsai and Roth 2016; Sil and Florian 2016): given a mention in a foreign langua
2771873754	Neural Cross-lingual Entity Linking	2150295085	multi-lingual embeddings. However, their model needs to be re-trained for every new language and hence is not entirely suitable/convenient for the TAC task. We propose a zero shot learning technique (Palatucci et al. 2009; Socher et al. 2013b) for our neural EL model: once trained in English, it is applied for cross-lingual EL without the need for re-training. We also compare three popular multilingual embeddings stra
2771873754	Neural Cross-lingual Entity Linking	2252213141	n the surrounding sentences of min D, in contrast to (He et al. 2013; Francis-Landau, Durrett, and Klein 2016), which chose to use the entire document for modeling. Hence, following similar ideas in (Barrena et al. 2014; Lee et al. 2012), we run a coreference resolution system (Luo et al. 2004) and assume a “one link per entity” paradigm (similar to one sense per document (Gale, Church, and Yarowsky 1992; Yarowsky 1
2771873754	Neural Cross-lingual Entity Linking	2157191138	network (Figure 2). Noting that the entire document Dmight not be useful 3 for disambiguating m, we choose to represent the mention m based only on the surrounding sentences of min D, in contrast to (He et al. 2013; Francis-Landau, Durrett, and Klein 2016), which chose to use the entire document for modeling. Hence, following similar ideas in (Barrena et al. 2014; Lee et al. 2012), we run a coreference resoluti
2771873754	Neural Cross-lingual Entity Linking	2157191138	does not. While doing ablation study, we notice that adding the LDC layer provides a boost to our model in both the datasets, Systems In-KB acc. % Hoffart et al. (2011) 82.5 Gupta et al. (2017) 82.9 He et al. (2013) 85.6 Francis-Landau et al. (2016) 85.5 Sil &amp; Florian (2016) 86.2 Lazic et al. (2015) 86.4 Chisholm &amp; Hachey (2015) 88.7 Ganea et al. (2015) 87.6 Pershina et al. (2015) 91.8 Globerson et al. (
2771873754	Neural Cross-lingual Entity Linking	2517161177	ntext described in the Embeddings section, Equation (4) and the embedding of the candidate page link. 4. Within-language Features: We also feed in all the local features described in the LIEL system (Sil and Florian 2016). LIEL uses several features such as “how many words overlap between the mention and Wikipedia title match?” or “how many outlink names of the candidate Wikipedia title appear in the query document?”
2771873754	Neural Cross-lingual Entity Linking	2517161177	oth the top rankers from this challenging annual evaluation by 8% absolute percentage points. Note that in both the datasets, our model obtains 7.77% (on CoNLL) and 8.75% (on TAC) points better than (Sil and Florian 2016), which is a SOTA multi-lingual system. Another interesting fact we observe is that our full model outperforms (Sun et al. 2015) by 3.5% points, where they employ NTNs to model the semantic interactio
2771873754	Neural Cross-lingual Entity Linking	2131357087	S and CCA are tied but in Chinese, CCA performs better than LS. Note that “this work” in Table 2 indicates our full model with LDC and MPCM. Related Work Previous works in EL (Bunescu and Pasca 2006; Mihalcea and Csomai 2007) involved ﬁnding the similarity of the context in the source document and the context of the candidate Wikipedia titles. Recent research on EL has focused on sophisticated global disambiguation algori
2771873754	Neural Cross-lingual Entity Linking	2124033848,2127426251,2251939518	s. However, their model needs to be re-trained for every new language and hence is not entirely suitable/convenient for the TAC task. We propose a zero shot learning technique (Palatucci et al. 2009; Socher et al. 2013b) for our neural EL model: once trained in English, it is applied for cross-lingual EL without the need for re-training. We also compare three popular multilingual embeddings strategies and perform e
2771873754	Neural Cross-lingual Entity Linking	2150295085	s-Lingual Neural Entity Linking Neural Model Architecture The general architecture of our neural EL model is described in Figure 2. Our target is to perform “zero shot learning” (Socher et al. 2013b; Palatucci et al. 2009) for cross-lingual EL. Hence, we want to train a model on English data and use it to decode in any other language, provided we have access to multi-lingual embeddings from English and the target langu
2771873754	Neural Cross-lingual Entity Linking	2260776682	STMs+LDC 93.4 This work+CtxLSTMs+LDC+MPCM 94.0 (a) CoNLL2003 Systems In-KB acc. % TAC Rank 1 79.2 TAC Rank 2 71.6 Sil &amp; Florian (2016) 78.6 He et al. (2013) 81.0 Chisholm &amp; Hachey (2015) 80.7 Sun et al. (2015) 83.9 Yamada et al. (2016) 85.2 Globerson et al. (2016) 87.2 This work 85.0 This work+CtxLSTMs 86.3 This work+CtxLSTMs+LDC 86.9 This work+CtxLSTMs+LDC+MPCM 87.4 (b) TAC2010 Table 1: Performancecompari
2771873754	Neural Cross-lingual Entity Linking	2096335387	tences of min D, in contrast to (He et al. 2013; Francis-Landau, Durrett, and Klein 2016), which chose to use the entire document for modeling. Hence, following similar ideas in (Barrena et al. 2014; Lee et al. 2012), we run a coreference resolution system (Luo et al. 2004) and assume a “one link per entity” paradigm (similar to one sense per document (Gale, Church, and Yarowsky 1992; Yarowsky 1993)). We then use
2771873754	Neural Cross-lingual Entity Linking	2124033848,2127426251,2251939518	TN(l;r;W) 2Rk 4. Cross-Lingual Neural Entity Linking Neural Model Architecture The general architecture of our neural EL model is described in Figure 2. Our target is to perform “zero shot learning” (Socher et al. 2013b; Palatucci et al. 2009) for cross-lingual EL. Hence, we want to train a model on English data and use it to decode in any other language, provided we have access to multi-lingual embeddings from Eng
2771873754	Neural Cross-lingual Entity Linking	2260776682	ty of the source documents and the potential entity link candidates modeled using techniques like neural tensor network, multi-perspective cosine similarity and lexical composition and decomposition. Sun et al. (2015) used neural tensor networks for entity linking, between mention and the surrounding context. But this did not give good results in our case. Instead, the best results were obtained by composing the l
2771873754	Neural Cross-lingual Entity Linking	2122865749	usly used by (Pershina, He, and Grishman 2015; Globerson et al. 2016; Yamada et al. 2016). We also use the mappings provided by (Hoffart et al. 2011) obtained by extending the “means” tables of YAGO (Hoffart et al. 2013). Hyperparameters We tune all our hyper-parameters on the development data. We run CNNs on the sentences and the Wikipedia embeddings with ﬁlter size of 300 and width 2. The non-linearity used is tanh
2771873754	Neural Cross-lingual Entity Linking	342285082	the widely used CBOW word2vec model (Mikolov et al. 2013) to generate English mono-lingual word embeddings. Multi-lingual Embeddings Canonical Correlation Analysis (CCA): This technique is based on (Faruqui and Dyer 2014) who learn vectors by ﬁrst performing SVD on text in different languages, then applying CCA on pairs of vectors for the words that align in parallel corpora. For cross-lingual EL, we use the embedding
2771873754	Neural Cross-lingual Entity Linking	2517161177	This work+CtxLSTMs+LDC+MPCM 87.4 (b) TAC2010 Table 1: PerformancecomparisonontheCoNLL2003testbandTAC2010datasets.Our system outperforms all EL systems, including the only other multi-lingual system, (Sil and Florian 2016). and the multi-perspective context matching (MPCM) layer provides an additional 0.5% (average) points improvement. We see that adding in the context LSTM based layer (ﬁnegrained context) adds almost
2771873754	Neural Cross-lingual Entity Linking	2260776682	this work, we also introduced state-of-the-art similarity models like MPCM and LDC for entity linking. Combination of all these components helps our model score 3.5 absolute accuracy improvement over Sun et al. (2015). The cross-lingual evaluation at TAC KBP EL Track that started in 2011 (Ji, Grishman, and Dang 2011; Ji et al. 2015) has Spanish and Chinese as the target foreign languages. One of the top performers
2772136736	CONTEXTUALIZED WORD REPRESENTATIONS FOR READING COMPREHENSION	2738015883	(Seo et al.,2016;Xiong et al.,2017b; Huang et al.,2017;Wang et al.,2017). Analysis of current RC models has shown that models tend to react to simple word-matching between the question and document (Jia and Liang, 2017), as well as beneﬁt from explicitly providing matching information in model inputs (Hu et al.,2017;Chen et al.,2017;Weissenborn et al., 2017). In this work, we hypothesize that the stillrelatively-sma
2772136736	CONTEXTUALIZED WORD REPRESENTATIONS FOR READING COMPREHENSION	2566011400	+ TR [3] 44.5 53.9 MPCM [4] 40.3 50.0 RaSoR (base model) [5] 39.5 49.5 ReasoNet [6] 39.4 50.3 jNet [7] 37.9 47.0 Table 3: Single-model F1 on adversarial SQuAD. [1,3] This work. [2]Hu et al.(2017) [4]Wang et al. (2016) [5]Lee et al.(2016) [6]Shen et al.(2017) [7] Zhang et al.(2017) 5 Experimental setup We use pre-trained GloVe embeddings (Pennington et al.,2014) of dimension d w = 300 and produce character-based wo
2772136736	CONTEXTUALIZED WORD REPRESENTATIONS FOR READING COMPREHENSION	2557764419	ument and answering questions about its content. RC has attracted substantial attention over the last few years with the advent of large annotated datasets (Hermann et al., 2015;Rajpurkar et al.,2016;Trischler et al., 2016;Nguyen et al.,2016;Joshi et al.,2017), computing resources, and neural network models and optimization procedures (Weston et al.,2015; Sukhbaatar et al.,2015;Kumar et al.,2015). Reading comprehension
2772474126	MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	2099471712	been considered as one of the most exciting tasks in the ﬁeld of AI. Recent years have seen major progress in generating images, videos and text, notably using generative adversarial networks (GANs) (Goodfellow et al. 2014; Radford, Metz, and Chintala 2016; Vondrick, Pirsiavash, and Torralba 2016; Saito, Matsumoto, and Saito 2017; Yu et al. 2017). Similar attempts have also been made to generate symbolic music, but the
2772474126	MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	2559110679	d tags) with an additional monophonic drums track by using hierarchical RNNs to coordinate the three tracks. Some recent works have also started to explore using GANs for generating music. C-RNN-GAN (Mogren 2016) generated polyphonic music as a series of note events10 by introducing some ordering of notes and using RNNs in both the generator and the discriminator. SeqGAN (Yu et al. 2017) combined GANs and rei
2772474126	MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	1556219185	dataset the Lakh Pianoroll Dataset (LPD). We also present the subset LPD-matched, which is derived from the LMDmatched, a subset of 45,129 MIDIs matched to entries in the Million Song Dataset (MSD) (Bertin-Mahieux et al. 2011). Both datasets, along with the metadata and the conversion utilities, can be found on the project website.1 Data Preprocessing As these MIDI ﬁles are scraped from the web and mostly user-generated (R
2772474126	MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	2343635552	es, and Pachet 2017), a surging number of models have been proposed lately for symbolic music generation. Many of them used RNNs to generate music of different formats, including monophonic melodies (Sturm et al. 2016) and four-voice chorales (Hadjeres, Pachet, and Nielsen 2017). Notably, RNN-RBM (Boulanger-Lewandowski, Bengio, and Vincent 2012), a generalization of the recurrent temporal restricted Boltzmann machi
2772474126	MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment	2099471712	es can be found on our project website.1 Generative Adversarial Networks The core concept of GANs is to achieve adversarial learning by constructing two networks: the generator and the discriminator (Goodfellow et al. 2014). The generator maps a random noise zsampled from a prior distribution to the data space. The discriminator is trained to distinguish real data from those generated by the generator, whereas the gener
2772950382	Deep Semantic Role Labeling with Self-Attention	2613904329	016). Compared with the standard convolutional neural network, GLU is much easier to learn and achieves impressive results on both language modeling and machine translation task (Dauphin et al. 2016; Gehring et al. 2017). Given two ﬁlters W 2Rk d and V 2Rk d, the output activations of GLU are computed as follows: GLU(X) = (XW)˙(XV) (8) The ﬁlter width kis set to 3 in all our experiments. Feed-forward Sub-Layer The f
2772950382	Deep Semantic Role Labeling with Self-Attention	2151170651	2 with AM-DIR, AM-LOC and AM-MNR, but to a lesser extent. This indicates that our model has some advantages on such difﬁcult adjunct distinction (Kingsbury, Palmer, and Marcus 2002). Related work SRL Gildea and Jurafsky (2002) developed the ﬁrst automatic semantic role labeling system based on FrameNet. Since then the task has received a tremendous amount of attention. The focus of traditional approaches is devising approp
2772950382	Deep Semantic Role Labeling with Self-Attention	2183341477	ability of 0.8. Dropout is also applied before the attention softmax layer and the feed-froward ReLU hidden layer, and the keep probabilities are set to 0.9. We also employ label smoothing technique (Szegedy et al. 2016) with a smoothing value of 0.1 during training. Learning Parameter optimization is performed using stochastic gradient descent. We adopt Adadelta (Zeiler 2012) (= 106 and ˆ= 0:95) as the optimizer. T
2772950382	Deep Semantic Role Labeling with Self-Attention	2138437366	applications, such as Information Extraction (Bastianelli et al. 2013), Question Answering (Surdeanu et al. 2003; Moschitti, Morarescu, and Harabagiu 2003; Dan and Lapata 2007), Machine Translation (Knight and Luk 1994; Uefﬁng, Haffari, and Sarkar 2007; Wu and Fung 2009) and Multi-document Abstractive Summarization (Genest and Lapalme 2011). Semantic roles are closely related to syntax. Therefore, traditional SRL a
2772950382	Deep Semantic Role Labeling with Self-Attention	2103076621	r capturing the overall sentence structure. Combination of different syntactic parsers was also proposed to avoid prediction risk which was introduced by Surdeanu et al. (2003); Koomen et al. (2005); Pradhan et al. (2013). Beyond these traditional methods above, Collobert et al. (2011) proposed a convolutional neural network for SRL to reduce the feature engineering. The pioneering work on building an end-to-end syste
2772950382	Deep Semantic Role Labeling with Self-Attention	2567070169	to combine two representations: ! h t= LSTM(x ; 1) (5) h t = LSTM(x t; h t+1) (6) y t= ! h + h (7) Convolutional Sub-Layer For convolutional sub-layer, we use the Gated Linear Unit (GLU) proposed by Dauphin et al. (2016). Compared with the standard convolutional neural network, GLU is much easier to learn and achieves impressive results on both language modeling and machine translation task (Dauphin et al. 2016; Gehr
2772950382	Deep Semantic Role Labeling with Self-Attention	2787443966	ent properties and relations among relevant entities in the sentence and provide an intermediate level of semantic representation thus beneﬁting many NLP applications, such as Information Extraction (Bastianelli et al. 2013), Question Answering (Surdeanu et al. 2003; Moschitti, Morarescu, and Harabagiu 2003; Dan and Lapata 2007), Machine Translation (Knight and Luk 1994; Uefﬁng, Haffari, and Sarkar 2007; Wu and Fung 2009
2772950382	Deep Semantic Role Labeling with Self-Attention	2158899491	erent syntactic parsers was also proposed to avoid prediction risk which was introduced by Surdeanu et al. (2003); Koomen et al. (2005); Pradhan et al. (2013). Beyond these traditional methods above, Collobert et al. (2011) proposed a convolutional neural network for SRL to reduce the feature engineering. The pioneering work on building an end-to-end system was proposed by Zhou and Xu (2015), who applied an 8 layered LS
2772950382	Deep Semantic Role Labeling with Self-Attention	151579683	hitti, Morarescu, and Harabagiu 2003; Dan and Lapata 2007), Machine Translation (Knight and Luk 1994; Uefﬁng, Haffari, and Sarkar 2007; Wu and Fung 2009) and Multi-document Abstractive Summarization (Genest and Lapalme 2011). Semantic roles are closely related to syntax. Therefore, traditional SRL approaches rely heavily on the syntactic structure of a sentence, which brings intrinsic complexity and restrains these syste
2772950382	Deep Semantic Role Labeling with Self-Attention	2434901392	lli et al. 2013), Question Answering (Surdeanu et al. 2003; Moschitti, Morarescu, and Harabagiu 2003; Dan and Lapata 2007), Machine Translation (Knight and Luk 1994; Uefﬁng, Haffari, and Sarkar 2007; Wu and Fung 2009) and Multi-document Abstractive Summarization (Genest and Lapalme 2011). Semantic roles are closely related to syntax. Therefore, traditional SRL approaches rely heavily on the syntactic structure of
2772950382	Deep Semantic Role Labeling with Self-Attention	2153813398	meNet. Since then the task has received a tremendous amount of attention. The focus of traditional approaches is devising appropriate feature templates to describe the latent structure of utterances. Pradhan et al. (2005); Surdeanu et al. (2003); Palmer, Gildea, and Xue (2010) explored the syntactic features for capturing the overall sentence structure. Combination of different syntactic parsers was also proposed to a
2772950382	Deep Semantic Role Labeling with Self-Attention	11155487	ntic representation thus beneﬁting many NLP applications, such as Information Extraction (Bastianelli et al. 2013), Question Answering (Surdeanu et al. 2003; Moschitti, Morarescu, and Harabagiu 2003; Dan and Lapata 2007), Machine Translation (Knight and Luk 1994; Uefﬁng, Haffari, and Sarkar 2007; Wu and Fung 2009) and Multi-document Abstractive Summarization (Genest and Lapalme 2011). Semantic roles are closely relat
2772950382	Deep Semantic Role Labeling with Self-Attention	2194775991	nts. Deep Topology Previous works pointed out that deep topology is essential to achieve good performance (Zhou and Xu 2015; He et al. 2017). In this work, we use the residual connections proposed by He et al. (2016) to ease the training of our deep attentional neural network. Speciﬁcally, the output Y of each sub-layer is computed by the following equation: Y = X+Sub-Layer(X) (10) We then apply layer normalizati
2772950382	Deep Semantic Role Labeling with Self-Attention	2158899491	parsing speed are slower as a result of larger parameter counts. Word Embedding Previous works found that the performance can be improved by pre-training the word embeddings on large unlabeled data (Collobert et al. 2011; Zhou and Xu 2015). We use the GloVe (Pennington, Socher, and Manning 2014) embeddings pre-trained on Wikipedia and Gigaword. The embeddings are used to initialize our networks, but are not ﬁxed duri
2772950382	Deep Semantic Role Labeling with Self-Attention	2091896764	syntactic features for capturing the overall sentence structure. Combination of different syntactic parsers was also proposed to avoid prediction risk which was introduced by Surdeanu et al. (2003); Koomen et al. (2005); Pradhan et al. (2013). Beyond these traditional methods above, Collobert et al. (2011) proposed a convolutional neural network for SRL to reduce the feature engineering. The pioneering work on build
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	2064675550	, with 3 frames to the left (for US English) and 7 frames for multi-dialect, and downsampled to a 30ms frame rate. The encoder network architecture consists of 5 unidirectional long short-term memory [21] (LSTM) layers, with the size speciﬁed in the results section. Additive attention is used for all experiments [22]. The decoder network is a 2 layer LSTM with 1,024 hidden units per layer. The graphem
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	1974974326,2050526637	e of phonemes produced by the acoustic model to words. Finally, the LM assigns probabilities to word sequences. There have been many attempts in the community to fold the AM and PM into one component [1, 2]. This is particularly helpful when training multi-lingual systems [3], as a single AM+PM can be potentially used for all languages. A recent popular approach to jointly learn the AM+PM is to have a m
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	2765157699	end-to-end model must then be combined with a separate PM and LM to decode the best hypotheses from the model. End-to-end phoneme models have been explored for a small-footprint keyword spotting task [12], where the authors found that models trained to predict phonemes were better than graphemes. However, this system produced a small number of keyword outputs, thus requiring a simple lexicon and no la
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	1522301498	ish, and a uniﬁed set of 50 CIPs for multi-dialect. All neural networks are trained with the cross-entropy criterion, using asynchronous stochastic gradient descent (ASGD) optimization [23] with Adam [24] and are trained using TensorFlow [25]. 4. RESULTS 4.1. Tuning CIP Models Our ﬁrst set of experiments explore what parameters are important for decoding an end-to-end model trained with CIP. 4.1.1. Tu
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	2117541506	all languages. A recent popular approach to jointly learn the AM+PM is to have a model directly predict graphemes. However, to date, most grapheme-based systems do not outperform phone-based systems [4, 5, 6]. More recently, there has been a growing popularity in end-to-end systems, which attempt to learn the AM, PM and LM together in one system. Most work to date has explored end-to-end models which pred
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	2765157699	obability, thus de-emphazising the score from the language model component. 4.1.2. Incorporating End-of-Word Symbol Next, we explore different ways of using the &lt;eow&gt; symbol during decoding. In [12], the &lt;eow&gt; symbol was required during decoding and was shown to help in identifying the spacing between words. In particular, since models were decoded without a separate LM, requiring an &lt;e
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	1489125746	We refer the reader to [18] for more details about the experimental setup. All English experiments use 80-dimensional log-mel features, computed with a 25-ms window and shifted every 10ms. Similar to [19, 20], at the current frame, t, these features are stacked, with 3 frames to the left (for US English) and 7 frames for multi-dialect, and downsampled to a 30ms frame rate. The encoder network architecture
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	854541894,2782005958	rowing popularity in end-to-end systems, which attempt to learn the AM, PM and LM together in one system. Most work to date has explored end-to-end models which predict either graphemes or wordpieces [7, 8, 9, 10], which removes the need for a hand-designed lexicon as well. These end-to-end systems outperform systems which learn only an AM+PM jointly [11], though to date many of these systems still do not outp
2773134482	No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models	2765157699	stem. In this work, we explore having the model predict context-independent phonemes (CIP), thus removing the need for a Ctransducer. Following the small-footprint keyword spotting end-to-end work in [12], we train our model to predict a set of 44 CI phonemes, as well as an extra &lt;eow&gt; token, specifying the end of a word, analogous to the &lt;space&gt; token in graphemes (e.g., the cat !d ax &lt
2773795499	A user-study on online adaptation of neural machine translation to human post-edits	2251912507	oard strokes and mouse clicks that have been used in computer-assisted translation. We also attempt to quantify improvements in translation quality by measuring improvements in sentence-level BLEU+1 (Nakov et al, 2012) and TER between the iteratively rened NMT outputs and static human reference translations. We nd signicant improvements with respect to both metrics, showing a domain adaptation eect due to online
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2001496424	“man” - “woman” + “queen” ˇ “king”) when trained on a sufﬁciently large corpus. These learned relationships and word similarities are superior to those derived with traditional minimum edit distance [11], and can be exploited to achieve much deeper inference, as in the astonishing example “Russian” + “river” ˇ“Volga river” [10]. The continuous encoding proposed by Mikolov et al. is obtained by means
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2153579005	6], [7], [8], nor can they easily capture hidden semantic relationships needed for more challenging tasks, such as information retrieval, summarization and question answering [3], [9]. Mikolov et al. [10] introduce a vector encoding for text words that outperforms traditional NLP techniques based on arXiv:1712.01476v1 [cs.CL] 5 Dec 2017 2 co-occurrence counts and n-gram statistics. The encoding effect
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2171928131	atistics that were very powerful at the time are no longer the most appropriate model to manage the massive amount of text generated daily on the web (e.g., Google corpus, Wikipedia corpus) [6], [7], [8], nor can they easily capture hidden semantic relationships needed for more challenging tasks, such as information retrieval, summarization and question answering [3], [9]. Mikolov et al. [10] introdu
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2250539671	de by considering that the latter is better suited for learning representations of infrequent words in the corpus. In their article, “Glove: Global Vectors for Word Representation”, Pennington et al. [13] discuss the properties that an encoding algorithm must obey for word vectors to display semantic relationships similar to those introduced by Mikolov et al. The authors propose a slightly different o
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2254252455	Mostly convolutional, these networks compose input features (or words) into high-level signals (or sentences), and can be exhaustively trained for solving challenging tasks, such as dense captioning [22]. Recurrent neural network (RNN) architectures have been consistently investigated in deep NLP because of their resemblance to linguistic grammars [23], [15], [24] and superior classiﬁcation accuracy
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	196214544	ging tasks, such as dense captioning [22]. Recurrent neural network (RNN) architectures have been consistently investigated in deep NLP because of their resemblance to linguistic grammars [23], [15], [24] and superior classiﬁcation accuracy in language-related tasks. RNN architectures with memory cells have been developed that are capable of retaining long sequence patterns. As the most prominent memb
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	1983364832	izing context information. Deep network architectures have been successfully applied to different areas of computer vision, such as face detection and recognition [17], [18], human action recognition [19], and handwriting recognition [20], [21]. Mostly convolutional, these networks compose input features (or words) into high-level signals (or sentences), and can be exhaustively trained for solving cha
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2064675550	t are capable of retaining long sequence patterns. As the most prominent member of this category, long short-term memory (LSTM) networks are suitable for text classiﬁcation and language comprehension [25]. Recently, deep neural networks have gained attention in Fig. 1. NPT duration (red) and number of reports (green) for the top 10 most inactive wells in the ﬁeld. NLP because of their state-of-the-art
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2153579005	ties are superior to those derived with traditional minimum edit distance [11], and can be exploited to achieve much deeper inference, as in the astonishing example “Russian” + “river” ˇ“Volga river” [10]. The continuous encoding proposed by Mikolov et al. is obtained by means of a variation of cross-entropy minimization. To efﬁciently obtain an optimal language model, the authors introduce a proxy lo
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2122585011	twork architectures have been successfully applied to different areas of computer vision, such as face detection and recognition [17], [18], human action recognition [19], and handwriting recognition [20], [21]. Mostly convolutional, these networks compose input features (or words) into high-level signals (or sentences), and can be exhaustively trained for solving challenging tasks, such as dense capt
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2144354855	ure and its capability of memorizing context information. Deep network architectures have been successfully applied to different areas of computer vision, such as face detection and recognition [17], [18], human action recognition [19], and handwriting recognition [20], [21]. Mostly convolutional, these networks compose input features (or words) into high-level signals (or sentences), and can be exhau
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	581956982	ving challenging tasks, such as dense captioning [22]. Recurrent neural network (RNN) architectures have been consistently investigated in deep NLP because of their resemblance to linguistic grammars [23], [15], [24] and superior classiﬁcation accuracy in language-related tasks. RNN architectures with memory cells have been developed that are capable of retaining long sequence patterns. As the most pr
2775476020	Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing	2077428231	Wikipedia corpus) [6], [7], [8], nor can they easily capture hidden semantic relationships needed for more challenging tasks, such as information retrieval, summarization and question answering [3], [9]. Mikolov et al. [10] introduce a vector encoding for text words that outperforms traditional NLP techniques based on arXiv:1712.01476v1 [cs.CL] 5 Dec 2017 2 co-occurrence counts and n-gram statistics
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2065157922	) baselines, which select from all the possible senses for a given term t: 1.the most frequent sense in WordNet, where frequencies of senses are observed on a manually annotated semantic concordance (Miller et al. 1993). 2.the most frequent sense in BabelNet. Since BabelNet combines WordNet and Wikipedia, this amounts to: i) the WordNet MFS for senses originally found in WordNet, and ii) the most cited (i.e., intern
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2135254172	) and then outputting all their English lexicalizations. The WN+XWN system is the top-ranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau 2007). It alleviates sparsity by combining WordNet with the eXtended WordNet (Mihalcea and Moldovan 2001). The latter Distributional Semantics for Enriching Lexical Semantic Resources 31 Model Sense Repres
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2151386575	007; Carlson et al. 2010; Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Ponzetto and Strube 2011; Nastase and Strube 2012), or complemented with manually assembled knowledge sources (Suchanek, Kasneci, and Weikum 2008; Navigli and Ponzetto 2012a; Gurevych et al. 2012; Hoart et al. 2013). As a r
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2141599568,2153579005	15).3 There are many possible ways to compute a graph of semantically similar words, including count-based approaches, such as (Lin 1998; Curran 2002) or predictionbased approaches, such as word2vec (Mikolov et al. 2013), GloVe (Pennington, Socher, and Manning 2014), and word2vecf (Levy and Goldberg 2014). Here, we opt for a count-based approach to distributional semantics based on LMI based on two considerations, na
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1981938229	able to expand WordNet-based sense representations with many relevant terms from the related terms of our sense-disambiguated PCZs. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau 2008), BabelNet, WN+XWN (Cuadros and Rigau 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best conguration reported in the origina
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1802860984	advantage of using dependency-based context representations over the bag-of-words representations. This is in line with prior studies on semantic similarity (Pado and Lapata 2007; Van de Cruys 2010; Panchenko and Morozova 2012). For this reason, we use syntactic features in our experiments but would like to stress that the overall framework also allows simpler context representations, giving rise to its application to resou
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2249610072	arch the automatic acquisition of machine-readable knowledge on a large scale by mining large repositories of textual data (Banko et al. 2007; Carlson et al. 2010; Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Ponzetto and Strube 2011; Nastase and Strube 2012), or complemented with manually ass
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2153225416	ata (Banko et al. 2007; Carlson et al. 2010; Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Ponzetto and Strube 2011; Nastase and Strube 2012), or complemented with manually assembled knowledge sources (Suchanek, Kasneci, and Weikum 2008; Navigli and Ponzetto 2012a; Gurevych et al. 2012; H
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1512387364	bility across methods { nevertheless, they are commonly used in the eld of knowledge acquisition to estimate the quality of knowledge resources (Banko et al. 2007; Suchanek, Kasneci, and Weikum 2008; Carlson et al. 2010; Velardi, Faralli, and Navigli 2013) (inter alia). We performed manual validation as follows. We rst collected all disambiguated entries of the wiki-p1.6 model (cf. Table 1), where these 17 target wo
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2107598941	C focus on exploiting other KBs (Wang et al. 2012; Bryl and Bizer 2014) for acquiring additional knowledge, or rely on text corpora { either based on distant supervision (Snow, Jurafsky, and Ng 2006; Mintz et al. 2009; Aprosio, Giuliano, and Lavelli 2013) or by rephrasing KB relations as queries to a search engine (West et al. 2014) that returns results from the web as corpus. Alternative methods primarily rely on
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2168820179	Cartney, and Manning 2006) obtained with the PCFG model of the Stanford parser (Klein and Manning 2003). Features of each word are weighted and ranked using the Local Mutual Information (LMI) metric (Evert 2005). Subsequently, these word representations are pruned keeping 1,000 most salient features per word and 1,000 most salient words per feature. The pruning reduces computational complexity and noise (Rie
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2153267064	cleaning and unsupervised, end-to-end taxonomy learning. We believe that the hybrid lexical resources developed in our work will benet high-end applications, e.g. ranging from entity-centric search (Lin et al. 2012; Schuhmacher, Dietz, and Ponzetto 2015) all the way through fulledged document understanding (Rospocher et al. 2016). 40 Biemann et al. Downloads. We release all resources produced in this work under
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1010415138	comparable to the state of the art. Option 3: Inducing PCZ using word sense embeddings. Finally, a PCZ can be also induced using sparse (Reisinger and Mooney 2010) and dense (Neelakantan et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016) multi-prototype vector space models (the latter are also known as word sense embeddings). These models directly induce sense vectors from a text corpus, not requiring the word
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2436001372	ctively. Also, we consistently outperform the strong performance exhibited by the MFS baselines, which, in line with previous ndings on similar tasks (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Navigli 2009) provide a hard-to-beat competitor. Thanks to our method, in fact, we are able to achieve an accuracy improvement over the MFS baseline ranging from 1.4% to 14.3% on WordNet mappings, and from 24.4% t
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2164973920	d for unsupervised word sense disambiguation, yielding results comparable to the state of the art. Option 3: Inducing PCZ using word sense embeddings. Finally, a PCZ can be also induced using sparse (Reisinger and Mooney 2010) and dense (Neelakantan et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016) multi-prototype vector space models (the latter are also known as word sense embeddings). These models directly induce
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1513718494	e of the art. Option 3: Inducing PCZ using word sense embeddings. Finally, a PCZ can be also induced using sparse (Reisinger and Mooney 2010) and dense (Neelakantan et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016) multi-prototype vector space models (the latter are also known as word sense embeddings). These models directly induce sense vectors from a text corpus, not requiring the word sense induction step of
2776198661	A framework for enriching lexical semantic resources with distributional semantics	205765513	e.g., those from our PCZs (Figure 1) { which can be obtained, for instance, by exploiting lexical-syntactic paths (Hearst 1992; Snow, Jurafsky, and Ng 2004), distributional representations of words (Baroni et al. 2012; Roller, Erk, and Boleda 2014) or a combination of both (Shwartz, Goldberg, and Dagan 2016). Due to the automatic acquisition process, such lists typically contain noisy, inconsistent relations { e.g
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1519776800	e title of the word sense, e.g.mouse (device) or mouse (animal). Finally, to further improve the interpretability of the induced senses, we add images to our sense clusters as follows. Previous work (Faralli and Navigli 2012) showed that Web search engines can be used to bootstrap sense-related information. Consequently, we assign an image to each word in the cluster querying the Bing image search API5 using the query com
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2068737686	ecomparative evaluation. 4.3 Labeling Induced Senses with Hypernyms and Images more detail and to improve its interpretability. First, we extract hypernyms from the input corpus. Here, we rely on the Hearst (1992) patterns, yet the approach we use can benet also from more advanced methods for extraction of hypernyms, e.g. HypeNet (Shwartz, Goldberg, and Dagan 2016) or the Dual Tensor Model (Glavas and Ponzet
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2141599568,2153579005	et al. with distributional information, where words are usually represented as dense numerical vectors, a.k.a. embeddings. Examples of such approaches include methods that modify the Skip-gram model (Mikolov et al. 2013) to learn sense embeddings (Chen, Liu, and Sun 2014) based on the sense inventory of WordNet, methods that learn embeddings of synsets as given in a lexical resource (Rothe and Schutze 2015) or metho
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1940888119	evel abstract concept { i.e., any of entity, object, etc. and more in general nodes that correspond to abstract concepts that we can expect to be part of a core ontology such as, for instance, DOLCE (Gangemi et al. 2002); ii) a leaf terminological node (i.e., instances such as Pet Shop Boys); or iii) a middle-level concept (e.g., celebrity), namely concepts not tting into any of the previous classes. We compute stand
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1750338658	f distributionally induced word senses (cf. Section 4.5). Since we are working with a PCZ on the source side, as opposed to using Wikipedia, we cannot rely on graph-based algorithms such as PageRank (Niemann and Gurevych 2011) ‘out of the box’: while PCZs can be viewed as graphs, these are inherently noisy and require cleaning techniques in order to remove cycles and wrong relations (cf. Section 7 where we accordingly addr
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2032327522	Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Ponzetto and Strube 2011; Nastase and Strube 2012), or complemented with manually assembled knowledge sources (Suchanek, Kasneci, and Weikum 2008; Navigli and Ponzetto 2012a; Gurevych et al. 2012; Hoart et al. 2013). As a result of this, recent year
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2141599568,2153579005	g of syntactic dependencies Distributional Semantics for Enriching Lexical Semantic Resources 9 Method high low Lin’s similarity (Lin 1998) 0.2872 0.2291 t-test (Curran 2002) 0.2589 0.2067 Skip-gram (Mikolov et al. 2013) 0.2548 0.2068 Skip-gram with dependency features (Levy and Goldberg 2014) 0.2632 0.1992 LMI with trigram features (Riedl and Biemann 2013) 0.2621 0.2003 LMI with dependency features (Riedl and Bieman
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2166776180	a Graph of Semantically Related Words The goal of this rst stage is to build a graph of semantically related words, with edges such as (mouse, keyboard, 0.78), i.e., a distributional thesaurus (DT) (Lin 1998). To induce such graph in an unsupervised way we, rely on a count-based approach to distributional semantics based on the JoBimText framework (Biemann and Riedl 2013). Each word is represented by a ba
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2150034910	ic Resources with Distributional Semantics The construction of our hybrid aligned resource (HAR) builds upon methods used to link various manually constructed lexical resources to construct BabelNet (Ponzetto and Navigli 2010) and UBY (Gurevych et al. 2012), among others. In our method, however, linking is performed between two networks that are structurally similar, but have been constructed in two completely dierent way
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2164946598	induced from text corpora tend to be coarse and corpus-specic. Consequently, the low coverage comes from the fact that we connect a coarse and a ne-grained sense inventory { cf. also previous work (Faralli and Navigli 2013) showing comparable proportions between coverage and extracoverage of automatically acquired knowledge (i.e., glosses) from corpora. Finally, our results indicate dierences between the order of magni
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2068737686	as input a list of subsumption relations between terms or, optionally, word senses { e.g., those from our PCZs (Figure 1) { which can be obtained, for instance, by exploiting lexical-syntactic paths (Hearst 1992; Snow, Jurafsky, and Ng 2004), distributional representations of words (Baroni et al. 2012; Roller, Erk, and Boleda 2014) or a combination of both (Shwartz, Goldberg, and Dagan 2016). Due to the auto
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2097606805	larity function. such as conj and(rat, ) or prep of(click,), extracted from the Stanford Dependencies (De Marnee, MacCartney, and Manning 2006) obtained with the PCFG model of the Stanford parser (Klein and Manning 2003). Features of each word are weighted and ranked using the Local Mutual Information (LMI) metric (Evert 2005). Subsequently, these word representations are pruned keeping 1,000 most salient features pe
2776198661	A framework for enriching lexical semantic resources with distributional semantics	38703128	m and the hypernym. In these systems, the harvested hypernymy relations are then arranged into a taxonomy structure, e.g., using cycle pruning and ‘longest path’ heuristics to induce a DAG structure (Kozareva and Hovy 2010) or by relying on a variant of Chu-Liu Edmonds’ optimal branching algorithm (Velardi, Faralli, and Navigli 2013). In general, all such lexical-based approaches suer from the limitation of not being s
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2166776180	mework, Riedl is able to compare a wide range of dierent approaches for the construction of a weighted similarity graph, including state-of-the-art approaches based on sparse vector representations (Lin 1998; Cur3 Word and sense representations used in our experiments can be inspected by selecing the \Stanford (English)&quot; model in the JoBimViz demo at http://jobimtext.org/ jobimviz/. 10 Biemann et al
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2250404399	mon features for two words. This is, again, followed by a pruning step in which for every word, only the 200 most similar terms are kept. The resulting graph of word similarities is browsable online (Ruppert et al. 2015).3 There are many possible ways to compute a graph of semantically similar words, including count-based approaches, such as (Lin 1998; Curran 2002) or predictionbased approaches, such as word2vec (Mik
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2128407051	n text corpora { either based on distant supervision (Snow, Jurafsky, and Ng 2006; Mintz et al. 2009; Aprosio, Giuliano, and Lavelli 2013) or by rephrasing KB relations as queries to a search engine (West et al. 2014) that returns results from the web as corpus. Alternative methods primarily rely on existing information in the KB itself (Bordes et al. 2011; Jenatton et al. 2012; Socher et al. 2013; Klein, Ponzetto
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1529533208	nowledge sources with distributional semantic information, thus extending them with text-based contextual information. Another line of research has looked at the problem of Knowledge Base Completion (Nickel et al. 2016) (KBC). Many approaches to KBC focus on exploiting other KBs (Wang et al. 2012; Bryl and Bizer 2014) for acquiring additional knowledge, or rely on text corpora { either based on distant supervision (
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2135254172	ntations with many relevant terms from the related terms of our sense-disambiguated PCZs. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau 2008), BabelNet, WN+XWN (Cuadros and Rigau 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best conguration reported in the original paper (KnowNet-20), which extends each se
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2127978399	ntic Resources In the past decade, large eorts have been undertaken to research the automatic acquisition of machine-readable knowledge on a large scale by mining large repositories of textual data (Banko et al. 2007; Carlson et al. 2010; Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Pon
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2250930514	ntics with Lexical Resources Several prior approaches combined distributional information extracted from text with information available in lexical resources like e.g. WordNet. This includes a model (Yu and Dredze 2014) to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch, Van Durme, and CallisonBurch 2013). The objective function of this model combines that of the skip-gr
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1512387364	he past decade, large eorts have been undertaken to research the automatic acquisition of machine-readable knowledge on a large scale by mining large repositories of textual data (Banko et al. 2007; Carlson et al. 2010; Fader, Soderland, and Etzioni 2011; Faruqui and Kumar 2015). At this, collaboratively constructed resources have been exploited, used either in isolation (Bizer et al. 2009; Ponzetto and Strube 2011
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2013093146	if rank(c t) th then 15: M step = M step [f(j i;c t)g 16: M = M [M step 17: for all (j i;R ji ;H ji ) 2T:senses=M:senses do 18: M = M [f(j i;j i)g 19: return M algorithms for knowledge base linking (Pavel and Euzenat 2013): here, we build upon simple, yet high-performing previous approaches to linking LRs that achieved state-of-the-art performance. These rely at their core on computing the overlap between the bags of w
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2247119764	r by rephrasing KB relations as queries to a search engine (West et al. 2014) that returns results from the web as corpus. Alternative methods primarily rely on existing information in the KB itself (Bordes et al. 2011; Jenatton et al. 2012; Socher et al. 2013; Klein, Ponzetto, and Glavas 2017) to simultaneously learn continuous representations of KB concepts and KB relations by exploiting the KB structure as the
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1662133657	resources is an important problem (Biemann 2005; Jurgens and Pilehvar 2016) as it attempts to alleviate the extremely costly process of manual lexical resource construction. Distributional semantics (Turney and Pantel 2010; Baroni, Dinu, and Kruszewski 2014; Clark 2015) provides an alternative automatic meaning representation framework that has been shown to benet many text-understanding applications. Recent years hav
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2068737686	s are arranged together within a proper global taxonomic structure (cf. previous Section 7.1). Related work. Existing approaches like (Kozareva and Hovy 2010) (among others) use Hearst-like patterns (Hearst 1992) to bootstrap the extraction of terminological sister terms and hypernyms. Instead, in (Velardi, Faralli, and Navigli 2013) the extraction of hypernymy relations is performed with a classier, which i
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2127426251	a search engine (West et al. 2014) that returns results from the web as corpus. Alternative methods primarily rely on existing information in the KB itself (Bordes et al. 2011; Jenatton et al. 2012; Socher et al. 2013; Klein, Ponzetto, and Glavas 2017) to simultaneously learn continuous representations of KB concepts and KB relations by exploiting the KB structure as the ground truth for supervision, inferring ad
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2141599568,2153579005	sense representations that are human readable and easy to interpret, in contrast to dense vector representations, a.k.a. word embeddings like GloVe (Pennington, Socher, and Manning 2014) or word2vec (Mikolov et al. 2013). As a result of this, we are able to provide, to the best of our knowledge, the rst hybrid knowledge resource that is fully integrated and embedded within the Semantic Web ecosystem provided by the L
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2436001372	senses discovered from text. In these contributions, the benets of such hybrid knowledge sources for tasks in computational semantics such as semantic similarity and Word Sense Disambiguation (WSD) (Navigli 2009) have been extensively demonstrated. However, none of the existing approaches, to date, have been designed to use distributional information for the enrichment of lexical semantic resources, i.e. addi
2776198661	A framework for enriching lexical semantic resources with distributional semantics	38703128	structure of a taxonomy. Therefore, the task of taxonomy construction focuses on bringing order among these extractions, and on removing noise by organizing them into a directed acyclic graph (DAG) (Kozareva and Hovy 2010). Related work. State-of-the-art algorithms dier by the amount of human supervision required and their ability to respect some topological properties while pruning the noise. Approaches like those of
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2436001372	t;. This SemEval task is meant to provide an evaluation benchmark to assess wide-coverage lexical resources on the basis of a traditional lexical understanding task, namely Word Sense Disambiguation (Navigli 2009). The evaluation framework consists of two main phases: 1. Generation of sense representations. From each lexical resource, sense representations, also known as \topic signatures&quot;, are generated,
2776198661	A framework for enriching lexical semantic resources with distributional semantics	38703128	by a taxonomy construction phase in which local semantic relations are arranged together within a proper global taxonomic structure (cf. previous Section 7.1). Related work. Existing approaches like (Kozareva and Hovy 2010) (among others) use Hearst-like patterns (Hearst 1992) to bootstrap the extraction of terminological sister terms and hypernyms. Instead, in (Velardi, Faralli, and Navigli 2013) the extraction of hype
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2004858782	th text-based contextual information. Another line of research has looked at the problem of Knowledge Base Completion (Nickel et al. 2016) (KBC). Many approaches to KBC focus on exploiting other KBs (Wang et al. 2012; Bryl and Bizer 2014) for acquiring additional knowledge, or rely on text corpora { either based on distant supervision (Snow, Jurafsky, and Ng 2006; Mintz et al. 2009; Aprosio, Giuliano, and Lavelli
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2127978399	time-consuming, do not scale and hinder direct comparability across methods { nevertheless, they are commonly used in the eld of knowledge acquisition to estimate the quality of knowledge resources (Banko et al. 2007; Suchanek, Kasneci, and Weikum 2008; Carlson et al. 2010; Velardi, Faralli, and Navigli 2013) (inter alia). We performed manual validation as follows. We rst collected all disambiguated entries of th
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2166776180	ulting graph of word similarities is browsable online (Ruppert et al. 2015).3 There are many possible ways to compute a graph of semantically similar words, including count-based approaches, such as (Lin 1998; Curran 2002) or predictionbased approaches, such as word2vec (Mikolov et al. 2013), GloVe (Pennington, Socher, and Manning 2014), and word2vecf (Levy and Goldberg 2014). Here, we opt for a count-bas
2776198661	A framework for enriching lexical semantic resources with distributional semantics	1513718494	utional models we show that such unsupervised knowledge-free disambiguation models yield state-of-the-art results as compared to the unsupervised systems participated in SemEval 2013 and the AdaGram (Bartunov et al. 2016) sense embeddings model. An interactive demo that demonstrates our model developed in these experiments is described in Figure 2 and in (Panchenko et al. 2017). 7 Applications Linked distributional di
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2250383887	wer-law distribution of sense cluster sizes. Decreasing connectivity of the ego-network via the nparameter leads to more ne-grained inventories (cf. Table 6). Finally, we use the method described in (Riedl and Biemann 2015) to compute a dataset that includes automatically extracted multiword terms using the Wikipedia corpus (wiki-p1.6-mwe). Since most of the multiwords are monosemous, average polysemy of this dataset de
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2141599568,2153579005	word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch, Van Durme, and CallisonBurch 2013). The objective function of this model combines that of the skip-gram model (Mikolov et al. 2013) with a term that takes into account lexical relations of target words. In work aimed at retrotting word vectors (Faruqui et al. 2015), a related approach was proposed that performs post-processing o
2776198661	A framework for enriching lexical semantic resources with distributional semantics	2138605095	y, the bridge between wide-coverage conceptual knowledge and its instantiation within natural language texts. While there are large-scale lexical resources derived from large corpora such as ProBase (Wu et al. 2012), these are usually not sense-aware but con ate the notions of term and concept. With this work, we provide a framework that aims at augmenting any of these wide-coverage knowledge sources with distri
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2510828927	(Hosseini et al., 2014). It achieved a cross validation accuracy of 77:19%, which is competitive with the state of the art accuracy of 78% achieved with the same level of supervision. The system of (Mitra and Baral, 2016) achieved 86:07% accuracy on this dataset, but requires rich annotations for formulas and alignment of numbers to formulas. 6.2 New Dataset Creation In order to remove the aforementioned biases from t
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251349042	(one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2613312549	al works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017). There has also been work on generating rationale for word problem solving (Ling et al., 2017). More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics.
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2105717194	construction or creation, like “build”, “ﬁll”, etc., while DESTROY verbs indicate destruction related verbs like “destroy”, “eat”, “use”, etc. This verb classiﬁcation is largely based on the work of (Hosseini et al., 2014). Finally, the declarative rules for this concept have the following form: [Verb1 2 HAVE] ^[Verb2 2 GIVE] ^ [Coref(Subj1;IObj2)] )Addition where Coref(A;B) is true when Aand Brepresent the same entity
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2119295783	dicates of our declarative knowledge. More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and tau Yih, 2004; wei Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Ans
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2105717194	e choose the most similar verb v0from the seed lists according to Glove vector (Pennington et al., 2014) based similarity . We assign vthe category of v0. This can be replaced by a learned component (Hosseini et al., 2014). However we found seed list based categorization to work well in most cases. For explicit math, we check for a small list of patterns to detect and categorize math terms. Note that for both the cases
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	1547374399	edge in the form of constraints or expectations from the output (Roth and tau Yih, 2004; wei Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness of the appr
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2161002933	equires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Similarly, we learn to
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2250861254	erb of both numbers are identical. If they are, we assign ko to be Part-Whole relationship, otherwise, we assign it to be Transfer. We extract the dependent verb using the Stanford dependency parser (Chen and Manning, 2014). The annotations obtained via these rules are of course not perfect. We could not detect certain uncommon rate patterns like “dividing the cost 4 ways”, and “I read same number of books 4 days runnin
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2510828927	are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) as well as this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction. Some of these approaches also try to incorporate some form of declarative or domain knowledge. Hosseini et al. (2014) incorporate
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	1894835849	to incorporate knowledge in the form of constraints or expectations from the output (Roth and tau Yih, 2004; wei Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assur
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2105717194	incorporates the transfer phenomenon by classifying verbs; Mitra and Baral (2016) maps problems to a set of formulas. Both require extensive annotations for intermediate steps (verb classiﬁcation for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Baral (2016), etc). In contrast, our method can handle a more general class of problems, while training only requires problem-equation pairs coupled wi
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2105717194	lication and division), and are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) as well as this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction. Some of these approaches also try to incorporate some form of declarative or domain knowledge. Hossein
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251935656	lot of these quantitative reasoning problems. Consequently, there has been a growing interest in developing automated methods to solve math word problems (Kushman et al., 2014; Hosseini et al., 2014; Roy and Roth, 2015). Arithmetic Word Problem Mrs. Hilt baked pies last weekend for a holiday dinner. She baked 16 pecan pies and 14 apple pies. If she wants to arrange all of the pies in rows of 5 pies each, how many ro
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251935656	ly. We compare to the top performing systems for arithmetic word problems. They are as follows: 1.TEMPLATE : Template based algebra word problem solver of (Kushman et al., 2014). 2.LCA++ : System of (Roy and Roth, 2015) based on lowest common ancestors of math expression trees. 3.UNITDEP: Unit dependency graph based solver of (Roy and Roth, 2017). We refer to our approach as KNOWLEDGE. For all solvers, we use the sy
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251935656	nal operation. Finally, to detect irrelevant numbers (numbers which are not used in the solution), we use a set of rules based on the units of numbers. Again, this can be replaced by a learned model (Roy and Roth, 2015). 6 Experiments 6.1 Results on Existing Dataset We ﬁrst evaluate our approach on the existing datasets of AllArith, AllArithLex, and AllArithTmpl (Roy and Roth, 2017). AllArithLex and AllArithTmpl are
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2475046758	New Dataset Finally, we evaluate the systems on the Aggregate dataset. Following previous work (Roy and Roth, 2017), we compute two subsets of Aggregate comprising 756 problems each, using the MAWPS (Koncel-Kedziorski et al., 2016) system. The ﬁrst, called AggregateLex, is one with low lexical repetitions, and the second called AggregateTmpl is one with low repetitions of equation forms. We System AllArith AllArith Lex AllArith
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2738015883	note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness of the approach or lack of (Khashabi et al., 2016; Jia and Liang, 2017). We make used of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6). 3 Knowledge Representation We introduce here our representation of domain knowledge. We organ
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2276364082	oblems. Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and severa
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251349042	ocabulary, and new equation forms respectively. We compare to the top performing systems for arithmetic word problems. They are as follows: 1.TEMPLATE : Template based algebra word problem solver of (Kushman et al., 2014). 2.LCA++ : System of (Roy and Roth, 2015) based on lowest common ancestors of math expression trees. 3.UNITDEP: Unit dependency graph based solver of (Roy and Roth, 2017). We refer to our approach as
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251349042	ord problems form a natural abstraction to a lot of these quantitative reasoning problems. Consequently, there has been a growing interest in developing automated methods to solve math word problems (Kushman et al., 2014; Hosseini et al., 2014; Roy and Roth, 2015). Arithmetic Word Problem Mrs. Hilt baked pies last weekend for a holiday dinner. She baked 16 pecan pies and 14 apple pies. If she wants to arrange all of
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251935656	r almost all the problems in our dataset; only missing 4 problems involving application of area formulas. We also checked earlier arithmetic problem datasets from the works of (Hosseini et al., 2014; Roy and Roth, 2015), and found that the math concepts and declarative rules introduced in this paper cover all their problems. A major challenge in applying these concepts and rules to algebra word problems is the use o
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2137807494	recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and tau Yih, 2004; wei Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et al., 2010; Bisk and Hockenmaier, 2012; Gimpel and Bansal, 2014). Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	1559723967	t al., 2017), which requires handling a wider range of problems and often more complex semantics. 2.2 Semantic Parsing Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011). The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base. Si
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2250539671	th numbers are identical. If they are, we assign ko to be Part-Whole relationship, otherwise, we assign it to be Transfer. We extract the dependent verb using the Stanford dependency parser (Chen and Manning, 2014). The annotations obtained via these rules are of course not perfect. We could not detect certain uncommon rate patterns like “dividing the cost 4 ways”, and “I read same number of books 4 days runnin
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2510828927	tra and Baral (2016) maps problems to a set of formulas. Both require extensive annotations for intermediate steps (verb classiﬁcation for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Baral (2016), etc). In contrast, our method can handle a more general class of problems, while training only requires problem-equation pairs coupled with rate component annotations. Roy and Roth (2017) focuses on
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2105717194	tural abstraction to a lot of these quantitative reasoning problems. Consequently, there has been a growing interest in developing automated methods to solve math word problems (Kushman et al., 2014; Hosseini et al., 2014; Roy and Roth, 2015). Arithmetic Word Problem Mrs. Hilt baked pies last weekend for a holiday dinner. She baked 16 pecan pies and 14 apple pies. If she wants to arrange all of the pies in rows of 5 p
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2251935656	Word Problems Arithmetic problems involve combining numbers with basic operations (addition, subtraction, multiplication and division), and are generally directed towards elementary school students. Roy and Roth (2015), Roy and Roth (2017) as well as this work focus on this class of word problems. The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and
2777048118	Mapping to Declarative Knowledge for Word Problem Solving	2250769864	y the use of (one or more) variables in contructing (one or more) equations. These are typically middle or high school problems. Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems. Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approa
2779681640	The NarrativeQA Reading Comprehension Challenge	2427527485	0 sentence passages from 108 children’s books Cloze-form, from the 21st sentence multiple choice BookTest (Bajgar et al., 2016) 14.2M, similar to CBT Cloze-form, similar to CBT multiple choice SQuAD (Rajpurkar et al., 2016) 23K paragraphs from 536 Wikipedia articles 108K human generated, based on the paragraphs spans NewsQA (Trischler et al., 2016) 13K news articles from the CNN dataset 120K human generated, based on he
2779681640	The NarrativeQA Reading Comprehension Challenge	2557764419	14.2M, similar to CBT Cloze-form, similar to CBT multiple choice SQuAD (Rajpurkar et al., 2016) 23K paragraphs from 536 Wikipedia articles 108K human generated, based on the paragraphs spans NewsQA (Trischler et al., 2016) 13K news articles from the CNN dataset 120K human generated, based on headline, highlights spans MS MARCO (Nguyen et al., 2016) 1M passages from 200K+ documents retrieved using the queries 100K searc
2779681640	The NarrativeQA Reading Comprehension Challenge	2126209950	2640 human generated, based on the document multiple choice CNN/Daily Mail (Hermann et al., 2015) 93K+220K news articles 387K+997K Cloze-form, based on highlights entities Children’s Book Test (CBT) (Hill et al., 2016) 687K of 20 sentence passages from 108 children’s books Cloze-form, from the 21st sentence multiple choice BookTest (Bajgar et al., 2016) 14.2M, similar to CBT Cloze-form, similar to CBT multiple choi
2779681640	The NarrativeQA Reading Comprehension Challenge	2427527485	alistic (bAbI; Weston et al. (2015)). Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering (Chen et al., 2016; Rajpurkar et al., 2016). This, we suspect, is largely an artifact of the question generation methodology, in which annotators have created questions from a context document, or where context documents that explicitly answer
2779681640	The NarrativeQA Reading Comprehension Challenge	2558203065	articles 108K human generated, based on the paragraphs spans NewsQA (Trischler et al., 2016) 13K news articles from the CNN dataset 120K human generated, based on headline, highlights spans MS MARCO (Nguyen et al., 2016) 1M passages from 200K+ documents retrieved using the queries 100K search queries human generated, based on the passages SearchQA (Dunn et al., 2017) 6.9m passages retrieved from a search engine using
2779681640	The NarrativeQA Reading Comprehension Challenge	1544827683	cannot be asked Dataset Documents Questions Answers MCTest (Richardson et al., 2013) 660 short stories, grade school level 2640 human generated, based on the document multiple choice CNN/Daily Mail (Hermann et al., 2015) 93K+220K news articles 387K+997K Cloze-form, based on highlights entities Children’s Book Test (CBT) (Hill et al., 2016) 687K of 20 sentence passages from 108 children’s books Cloze-form, from the 21
2779681640	The NarrativeQA Reading Comprehension Challenge	2125436846	us to consider answer selection (from among the 30) as a simpler version of the QA than answer generation from scratch. Answer selection (Hewlett et al., 2016) and multiple-choice question answering (Richardson et al., 2013; Hill et al., 2016) are frequently used. We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable, given the summary, and p
2779681640	The NarrativeQA Reading Comprehension Challenge	2558203065	d, or several discontinuous spans are needed to generate a correct answer. This restricts the scalability and applicability of models doing well on SQuAD or NewsQA to more complex problems. MS MARCO (Nguyen et al., 2016) presents a bolder challenge: questions are paired with sets of snippets (“context passages”) that contain the information necessary to answer the question, and answers are free-form human generated t
2779681640	The NarrativeQA Reading Comprehension Challenge	2557764419	e CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of a questions and answers are provided for a set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words fro
2779681640	The NarrativeQA Reading Comprehension Challenge	2116492146	e further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and Rouge-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric.
2779681640	The NarrativeQA Reading Comprehension Challenge	2125436846	e summarize the key features of a collection of popular recent datasets in Table 1. In this section, we brieﬂy discuss the nature and limitations of these datasets and their associated tasks. MCTest (Richardson et al., 2013) is a collection of short stories, each with multiple questions. Each such question has set of possible answers, one of which is labelled as correct. While this could be used as a QA task, the MCTest
2779681640	The NarrativeQA Reading Comprehension Challenge	2126209950	ection (from among the 30) as a simpler version of the QA than answer generation from scratch. Answer selection (Hewlett et al., 2016) and multiple-choice question answering (Richardson et al., 2013; Hill et al., 2016) are frequently used. We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable, given the summary, and provide an answer if
2779681640	The NarrativeQA Reading Comprehension Challenge	1525961042	hrases. Summary of Limitations. We see several limitations of the scope and depth of the RC problems in existing datasets. First, several datasets are small (MCTest) or not overly naturalistic (bAbI; Weston et al. (2015)). Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering (Chen et al., 2016; Rajpurkar et al., 2016). This, we s
2779681640	The NarrativeQA Reading Comprehension Challenge	2154652894	ible approaches in the following sections. We require the use of metrics for generated text. We evaluate using Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and Rouge-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evalua
2779681640	The NarrativeQA Reading Comprehension Challenge	2190067570	ls, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulﬁlls a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts. 8 Conclusion We have i
2779681640	The NarrativeQA Reading Comprehension Challenge	1544827683	main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size. In contrast, CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) each provide large amounts of question–answer pairs. Questions are Cloze-form (predict the missing word) and are pr
2779681640	The NarrativeQA Reading Comprehension Challenge	2295759824	modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more ﬁne grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017),
2779681640	The NarrativeQA Reading Comprehension Challenge	2510759893	As we have multiple questions per summary/story, this allows us to consider answer selection (from among the 30) as a simpler version of the QA than answer generation from scratch. Answer selection (Hewlett et al., 2016) and multiple-choice question answering (Richardson et al., 2013; Hill et al., 2016) are frequently used. We additionally collected a second reference answer for each question by asking annotators to
2779681640	The NarrativeQA Reading Comprehension Challenge	2130942839	ndidate span using pre-trained GloVe word embeddings (Pennington et al., 2014). 4.2 Neural Benchmarks As a ﬁrst benchmark we consider a simple bidirectional LSTM sequence to sequence (Seq2Seq) model (Sutskever et al., 2014) predicting the answer directly from the query. Importantly, we provide no context information from either summary or story. Such a model might classify the question and predict an answer of similar t
2779681640	The NarrativeQA Reading Comprehension Challenge	2101105183	neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and Rouge-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluat
2779681640	The NarrativeQA Reading Comprehension Challenge	2126209950	ore as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size. In contrast, CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) each provide large amounts of question–answer pairs. Questions are Cloze-form (predict the missing word) and are produced from either short abstractive summaries (
2779681640	The NarrativeQA Reading Comprehension Challenge	2411480514	or not overly naturalistic (bAbI; Weston et al. (2015)). Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering (Chen et al., 2016; Rajpurkar et al., 2016). This, we suspect, is largely an artifact of the question generation methodology, in which annotators have created questions from a context document, or where context documen
2779681640	The NarrativeQA Reading Comprehension Challenge	2125436846	quences of words from the document. Although the answers are not just single word/entity answers, many plausible questions for assessing RC cannot be asked Dataset Documents Questions Answers MCTest (Richardson et al., 2013) 660 short stories, grade school level 2640 human generated, based on the document multiple choice CNN/Daily Mail (Hermann et al., 2015) 93K+220K news articles 387K+997K Cloze-form, based on highlight
2779681640	The NarrativeQA Reading Comprehension Challenge	1544827683	on the summary-reading task as well as the full story task. 5.1 Data Preparation The provided narratives contain a large number of named entities (such as names of characters or places). Inspired by Hermann et al. (2015), we replace such entities with markers, such as @entity42. These markers are permuted during training and testing so that none of their embeddings learn a speciﬁc entity’s representation. This allows
2779681640	The NarrativeQA Reading Comprehension Challenge	2427527485	t is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of a questions and answers are provided for a set of documents, where the answers are spans of the context document, i.
2779681640	The NarrativeQA Reading Comprehension Challenge	2158794898	yer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulﬁlls a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of
2780116402	Analogy Mining for Specific Design Needs	2145454741	bjects, or even that they are composed of materials that respond to shear forces in a similar way to cardboard. This insight is consistent with classic cognitive models of analogy (cited above, e.g., [8]), which retain key properties of objects during analogical mapping that are essential to the core relational structure: for example, in the atom/solar-system analogy, the absolute size of the sun/pla
2780116402	Analogy Mining for Specific Design Needs	2250539671	diverse matches for each scenario. The purpose of comparing our system to this method is to determine to what extent we have advanced the state-of-the-art in analogy-ﬁnding. 2. OverallGloVe baseline. [23] This method approximates the status quo for information-retrieval systems, which tend to operate on the whole document. Each document in the corpus is represented by the average of GloVe vectors for
2780116402	Analogy Mining for Specific Design Needs	1994764129	documents in the corpus). Some research has explored how to enable people to construct problem representations that abstract away the surface details of the problem. For example, the WordTree method [18] has people use the WordNet [21] lexical ontology to systematically “walk up” levels of abstraction for describing the core desired functionality, leading to the possiblity of discovering analogous fu
2780116402	Analogy Mining for Specific Design Needs	2081580037	ent of our system, we faced and dealt with several design challenges:  Choosing an appropriate knowledge base. To ﬁnd abstractions to show the designers, we explored several knowledge bases. Wordnet [21] is a large English lexical database, including relations like synonym, hypernym and hyponym. Wordnet is lexical and not focused on facts about the world, rendering it less useful for our purpose. In
2780116402	Analogy Mining for Specific Design Needs	2147152072,2250539671	r generating creative ideas [6,12,28]. However, ﬁnding useful distant analogies in large databases of textual documents remains challenging for existing machine learning models of document similarity [7,4,20,23], which are largely dependent on surface features like word overlap. An additional challenge is that in real world contexts with complex problems, designers are often interested in exploring and abstr
2780116402	Analogy Mining for Specific Design Needs	2147152072	ions of words based on the way that words are statistically distributed across word contexts in a large corpus of documents; notable examples include vector-space models like Latent Semantic Indexing [7], probabilistic topic modeling approaches like Latent Dirichlet Allocation [4], and word embedding models like Word2Vec [20] and GloVe [23]. The semantic representations produced by these methods are
2780116402	Analogy Mining for Specific Design Needs	2250539671	les include vector-space models like Latent Semantic Indexing [7], probabilistic topic modeling approaches like Latent Dirichlet Allocation [4], and word embedding models like Word2Vec [20] and GloVe [23]. The semantic representations produced by these methods are quite useful for ﬁnding very speciﬁcally relevant documents/results for a query, but are limited in their ability to ﬁnd matches that are a
2780116402	Analogy Mining for Specific Design Needs	2081580037	research has explored how to enable people to construct problem representations that abstract away the surface details of the problem. For example, the WordTree method [18] has people use the WordNet [21] lexical ontology to systematically “walk up” levels of abstraction for describing the core desired functionality, leading to the possiblity of discovering analogous functions in other domains. For ex
2780116402	Analogy Mining for Specific Design Needs	2107658650	t supports is very small. Another alternative we considered is Conceptnet [25]. Conceptnet includes knowledge from crowdsourcing and other resources, rendering it very noisy. We ended up choosing Cyc [16] [17] as our main Knowledge Base. Cyc is a very large, logic-based knowledge base representing commonsense knowledge. Cyc contains over ﬁve hundred thousand terms, seventeen thousand types of relation
2780116402	Analogy Mining for Specific Design Needs	2250539671	verall representation of a product’s purpose (what it is good for) and mechanism (how it works). It uses annotators to mark words related to the purpose/mechanism of the product, and weighs the Glove [23] values of those words to assign an overall purpose/mechanism representation for each document. It then uses a an artiﬁcial neural network model (speciﬁcally a bidirectional recurrent neural network,
2780116402	Analogy Mining for Specific Design Needs	2145454741	work focused on devising algorithms for reasoning about analogies between manually created knowledge representations that were rich in relational structure (e.g., predicate calculus representations) [8,10]. While these algorithms achieved impressive human-like accuracy for analogical reasoning, their reliance on well-crafted representations critically limited their applicability to mining analogies amo
2780155557	Hierarchical Text Generation and Planning for Strategic Dialogue	1486649854	higher performance on a strategic dialogue task. Other work has explored generating sentence embeddings for open domain text—for example, based on maximizing the likelihood of surrounding sentences (Kiros et al., 2015), supervised entailment data (Conneau et al.,2017), and auto-encoders (Bowman et al.,2015). 12. Conclusion We have introduced a novel approach to creating sentence representations, within the context
2780155557	Hierarchical Text Generation and Planning for Strategic Dialogue	2625113742	ng of sentences leads to agents that use language more ﬂuently and intelligently to achieve their goals. 2. Background 2.1. Natural Language Negotiations We focus on the negotiation task introduced byLewis et al. (2017), as it possess both linguistic and reasoning challenges. Lewis et al. collected a corpus of human dialogues on a multi-issue bargaining task, where the agents must divide a collection of items of 3 d
2780155557	Hierarchical Text Generation and Planning for Strategic Dialogue	2251235149	t). Therefore, a tis a distribution over what deal would be agreed if the dialogue stopped after message x t. This action can be thought of as latent proxy for a traditional annotated dialogue state (Williams et al., 2013). When predicting x t+1 and a t, the model only has access to latent variables z 0:t, so z tmust contain useful information about the meaning of x t. We employ a hierarchical RNN, in which message e t
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2141599568	ages X and Y, and a dictionary of (source;target) word pairs with embeddings xs and yt, respectively, a transformation matrix T, such that yt = Txs, can be estimated with various degrees of accuracy (Mikolov et al., 2013a). Large, accurate dictionaries result in better transformations, but a good fit can also be obtained using a few thousand word pairs even in the presence of noise (see section 4.3.4 for an empirical
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2250976127	hang et al., 2017), encouraging results were reported using a carefully-tuned adversarial auto-encoder that achieves an accuracy comparable to supervised linear projection with hundreds of seeds. In (Kiela et al., 2015), bilingual lexicon induction is achieved by matching visual features extracted from images that correspond to each word using a convolutional neural network. The image-based approach performs particu
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2141599568	of this hypothesis is most evident in the performance of distributed vector representations of words, i.e word embeddings, that are automatically induced from large text corpora (Bengio et al., 2003; Mikolov et al., 2013b). These word embeddings have been used to complement or substitute for linguistic feature extraction, which helps improve the performance and generalization power of natural language classifiers. Th
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2141599568	larity in word cooccurrence patterns within unrelated German and English texts is correlated with the number of corresponding word positions in the monolingual co-occurrence matrices. More recently, (Mikolov et al., 2013a) showed that a linear projection can be learned to transform word embeddings from one language into the vector space of another using a medium-size seed dictionary, which demonstrates that the multl
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2123261262	and mapped to one other, such as the alignment of isomorphic protein structures (Wang and Mahadevan, 2009) and cross-lingual document alignment with unsupervised topic models (Diaz and Metzler, 2007; Wang and Mahadevan, 2008). 2 Background 2.1 Skip-gram Word Embeddings with Subword Features In the skip-gram model presented in (Mikolov et al., 2013b), a feed-forward neural network is trained to maximize the probability of
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2141599568	ment alignment with unsupervised topic models (Diaz and Metzler, 2007; Wang and Mahadevan, 2008). 2 Background 2.1 Skip-gram Word Embeddings with Subword Features In the skip-gram model presented in (Mikolov et al., 2013b), a feed-forward neural network is trained to maximize the probability of all words within a fixed window around a given word. Formally, given a word w in a vocabulary W, the objective of the skip-g
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	1889220380	nd step (section 3.2), we iteratively refine the correspondences using a greedy optimization algorithm, which we call Iterative Mapping (IM for short). IM is a variation on the word mapping model in (Diab and Finch, 2000). The model does not make language-specific assumptions, making it suitable for learning cross-lingual correspondences. We then use these correspondences to learn a linear transformation between the s
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2123442489	re parallel datasets, and the remaining are non-parallel. fr-en-d is extracted from separate time periods to ensure that there is no overlap in content. and French datasets using the CoreNLP toolkit (Manning et al., 2014). We also converted all characters to lower case and normalized numeric sequences to a single token. Arabic text was tokenized using the Madamira toolkit (Pasha et al., 2014). We used the D3 tokenizat
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	77273554	ries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have bee
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	1828724394	ross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable di
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2257408573	sets with different levels of similarity to test the proposed unsupervised word mapping approach. We used the following data sources: WMT’14 the Workshop on Machine Translation French-English corpus (Bojar et al., 2014). This is a parallel corpus, but we don’t use the sentence alignments. AFP Agence France Presse corpora from Gigaword datasets for English (Parker et al., 2011b), French (Mendonça et al., 2009), and A
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	1542713999	t resulted in the smallest loss. For the final linear transformation T, we used the most frequent 50K words in both source and target languages, and we used the hubness reduction method described in (Dinu et al., 2015) with c=5000. We extracted dictionary pairs from the Multilingual WordNet (Miller, 1995) where the source words are within the top 15K words in all datasets. From these pairs, we extracted a random sa
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2141599568	tances between words and clusters seem to reflect semantic or syntactic relationships, which makes it possible to perform arithmetic on word vectors for analogical reasoning and semantic composition (Mikolov et al., 2013b). For example, in a vector space V where f = V(‘‘france”), p = V(‘‘paris”), and g = V(‘‘germany”), the distance f p reflects the county-capital relationship, and g+f p results in a vector closest to
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	1889220380	tures such as the normalized edit distance between source and target words can be used to extract a seed lexicon for bootstrapping the bilingual dictionary induction process (Hauer et al., 2017). In (Diab and Finch, 2000), an unsupervised model of learning a mapping that preserves the pairwise distances between word representations in the original and the mapped space was proposed. The model was only evaluated monolin
2780578493	Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings	2081580037	uent 50K words in both source and target languages, and we used the hubness reduction method described in (Dinu et al., 2015) with c=5000. We extracted dictionary pairs from the Multilingual WordNet (Miller, 1995) where the source words are within the top 15K words in all datasets. From these pairs, we extracted a random sample of 2K unique (source;target) pairs for training the supervised method, and the rema
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2194775991	5,000 images for validation, and 5,000 images for testing. We use the same vocabulary as that employed in [10], which consists of 8,791 words. 5.2 Evaluation For the CNN of Fig. 1, we used ResNet-152 [12], pretrained on the ImageNet dataset. The feature vector v has 2048 dimensions. Word embedding vectors in W eare downloaded from the web [25]. The model is implemented in TensorFlow [1] with the defau
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	1971844566,2131494463	7, 4, 8, 14, 15, 16]. Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks [31, 18, 17, 2, 34, 19, 30, 26]. Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is ﬁxed, but is designed to support the learning of distributed representati
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2558834163	e also re-implemented the model using the latest ResNet features and report the results in the second line of Table 1. Our re-implementation of the CNN-LSTM method matches the performance reported in [10], showing that the baseline is a state-of-the-art implementation. As shown in Table 1, compared to the CNN-LSTM baseline, the proposed TPGN signiﬁcantly outperforms the benchmark schemes in all metric
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2123301721	ile S^ tis 25 25); the vocabulary size V = 8;791; the dimension of u tand f tis d2 = 625. The main evaluation results on the MS COCO dataset are reported in Table 1. The widely-used BLEU [24], METEOR [3], and CIDEr [32] metrics are reported in our quantitative evaluation of the performance of the proposed model. In evaluation, our baseline is the widely used CNN-LSTM captioning method originally prop
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2064675550	all the image-speciﬁc information for producing the caption. (We will call a caption a “sentence” even though it may in fact be just a noun phrase.) The two subnets Sand Uare mutually-connected LSTMs [13]: see Fig. 2. The internal hidden state of U, p t, is sent as input to S; Ualso produces output, the unbinding vector u t. The internal hidden state of S, S t, is sent as input to U, and also produced
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2558834163	are omitted from all equations in this paper. The initial state ^S 0 is initialized by: ^S 0 = C s(v v) (9) where v 2R2048 is the vector of visual features extracted from the current image by ResNet [10] and v is the mean of all such vectors; C s2R d 2048. On the output side, x t2RV is a 1-hot vector with dimension equal to the size of the caption vocabulary, V, and W e2Rd V is a word embedding matr
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	1905882502,2558834163	performance of our proposed model, we use the COCO dataset [6]. The COCO dataset contains 123,287 images, each of which is annotated with at least 5 captions. We use the same pre-deﬁned splits as in [14, 10]: 113,287 images for training, 5,000 images for validation, and 5,000 images for testing. We use the same vocabulary as that employed in [10], which consists of 8,791 words. 5.2 Evaluation For the CNN
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	2101105183	s 625 625 (while S^ tis 25 25); the vocabulary size V = 8;791; the dimension of u tand f tis d2 = 625. The main evaluation results on the MS COCO dataset are reported in Table 1. The widely-used BLEU [24], METEOR [3], and CIDEr [32] metrics are reported in our quantitative evaluation of the performance of the proposed model. In evaluation, our baseline is the widely used CNN-LSTM captioning method ori
2781148822	TENSOR PRODUCT GENERATION NETWORKS FOR DEEP NLP MODELING	1905882502,1947481528,2171361956	sentation that is then used to drive a natural-language generation process, typi12 Figure 7: Unbinding vectors of 169 prepositions in red and 831 words of other types of POS in grey. cally using RNNs [21, 33, 7, 4, 8, 14, 15, 16]. Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks [31, 18, 17, 2, 34,
2781526798	An Attentive Sequence Model for Adverse Drug Event Extraction from Biomedical Text.	2153579005	by creating PoS embeddings and label embeddings. Both of these embedding matrices are randomly initialized. 1) Word Embedding: We use two instances of pretrained word embeddings from PubMed word2vec [16]. We disable gradient updates on the ﬁrst instance - Fixed Embedding (emb f) while we enable gradient updates on the second instance - Variable Embedding (emb v). Fixed embeddings provide good, stable
2781526798	An Attentive Sequence Model for Adverse Drug Event Extraction from Biomedical Text.	2064675550	from large data. The NLP community has adopted Recurrent Neural Networks (RNN) for processing large, unstructured, variable length text sequences. Miwa et al., [11] used Long Short Term Memory (LSTM) [20] based RNNs for relation classiﬁcation. Li et al., [13] used a feed forward neural network to jointly extracting drug-disease entity mentions and their ADE relations. Li et al.’s work [3], which appea
2781526798	An Attentive Sequence Model for Adverse Drug Event Extraction from Biomedical Text.	2138768461	of models used to solve this task. The ﬁrst category uses traditional pipeline method which consists of two steps - Named Entity Recognition (NER) followed by relation classiﬁcation. Most early works [9], [10] in this area, use pipeline models for ADE task. Pipeline models heavily rely on manual feature engineering. The second category of models are largely based on End-to-End Deep Neural Networks (D
2781589167	Shielding Google's language toxicity model against adversarial attacks.	2744770762	ord- or character-level n-grams machine learning models [17] coupled with ﬁlters of obfuscation–prone customised vocabulary [14]. Lastly, exploring feature spaces derived from conversational networks [13] may improve the eﬀectiveness of our methods. As a ﬁnal remark, we believe that ensuring aggression–free, respectful and opinionated online discussions would require a pipeline of text processors work
2781589167	Shielding Google's language toxicity model against adversarial attacks.	2001496424	some of the visual deception factor still pose alluring challenges (e.g., fuck→ fukc, SHIT→ S|-|IT). Besides, computational speedups in the execution of the ﬁlters are also of practical interest (see [10] or more recently, [11]), considering that language is an entity in constant evolution[5] whose toxic vocabulary evolves likewise. Moreover, it would be feasible to extend our adversarial toxic commen
2781846447	Slugbot: An Application of a Novel and Scalable Open Domain Socialbot Framework.	2163074454	ence resolution by mapping the coreference tags returned by CoreNLP to the data stored within our system. Finally, we use a homegrown named entity recognizer and topic classiﬁer in addition to the NPS[6] dialogue act classiﬁer to make our internal representation as robust as possible. 2.2 Data Management Our system uses the human mind as a model for managing memory. To our knowledge, we are the ﬁrst
2781846447	Slugbot: An Application of a Novel and Scalable Open Domain Socialbot Framework.	2123442489	noise in their input. After preprocessing the data we use our NLU engine to create a deep structure representation of the user’s utterance. Our ﬁrst layer of NLU relies on the Stanford CoreNLP Toolkit[10]. Our internal representation is based on the dependency parse of the respective utterance which is consolidated into a concise tree using the dependency relations. The part-of-speech (POS) and sentim
2781846447	Slugbot: An Application of a Novel and Scalable Open Domain Socialbot Framework.	2160458012	Sample question answering Retrieval: There are several existing retrieval based chatbots which operate on large existing corpora such as Twitter[11, 8], the Open Subtitles corpus[5], or movie scripts[3,2]. While this approach works well for speciﬁc user utterances, it generally performs worse than other dialogue management methods[8]. Therefore our system uses retrieved responses sparingly. Our retrie
2782054271	Social Media Analysis based on Semanticity of Streaming and Batch Data.	1662133657	1 &lt;i&gt;n) and(1 &lt;j&gt;m) (2.1) Step 2 - Under lying semantic information and relation between words can be obtained using latent vector by nding Eigen vectors of VVT which is column space of V [24]. Thus the computed latent vectors spans the word space by satisfying the following condition, Ax= xA= VVT ; (2.2) where x, is Eigen vector and Eigen value of A. This literally means that projecting
2782054271	Social Media Analysis based on Semanticity of Streaming and Batch Data.	1533057952	esults with supervised algorithm. Ituses classical namedentity tagssuch asPerson, LocationandOrganization and evaluated the corpus with mentioned tags and other named entity tags also like car brands [37]. A paper titled Focused Named Entity Recognition using Machine Learning by Li Zhang, Yue Pan and Tong Zhang described about focused named entities which is useful for document summarization. This use
2782054271	Social Media Analysis based on Semanticity of Streaming and Batch Data.	1593502373	They mainly concentrated on Punjabi language and NER system developed on Punjabi language [34]. NER was also implemented in other languages such as Arabic using integrated machine learning techniques [43]. NER system is also developed for Manipuri language which is a less computerized Indian language [64]. This was performed by Thoudam Doren et al using Support Vector Machines [64]. This resulted in a
2782054271	Social Media Analysis based on Semanticity of Streaming and Batch Data.	2004763266	oimbatore’ is a LOCATION entity and ’7th August 2014’ is DATE entity. NER is in need of external knowledge and non-local features in order to nd the multiple occurrences of named entities in the text [30]. Many fundamental decisions such as identifying the sequential named entities and noun chunks are made [30]. Mostly the sequential named entities are chunks. NER plays a key role in NLP applications
2782054271	Social Media Analysis based on Semanticity of Streaming and Batch Data.	1451039621	tor Machines [64]. This resulted in accuracy of 94.59% as F-Score. Asif Ekbal and Sivaji Bandyopadhyay also developed a NER system for Bengali using SVM that makes use of diﬀerent context information [45]. A domain focused application on nested NER was done by Vijayakrishna and S. L. Devi in Tamil language using Conditional Random Fields [32]. An automated system for NER was also developed in Tamil La
2782124856	Towards Understanding and Answering Multi-Sentence Recommendation Questions on Tourism.	2250225488	dge Type Answer Type Related Work Structured (eg. DBPedia, Freebase) Entity (Lukovnikov et al.,2017;Bordes et al., 2014b,2015) Single Sentence Structured (Open IE style KBs) Entity (Fader et al.,2014;Berant and Liang, 2014) Structured + Unstructured (Open IE style KBs with supporting text passages on entities ) Entity (Das et al.,2017) Structured (Databases) Tables/ Table rows (Saha et al.,2016;Pazos R. et al.,2013) Uns
2782124856	Towards Understanding and Answering Multi-Sentence Recommendation Questions on Tourism.	2250225488	many tourism questions, as our experiments show. The more traditional solutions (e.g., semantic parsing) that parse the questions deeply can process only single-sentence questions (Fader et al.,2014;Berant and Liang, 2014;Fader et al.,2013;Kwiatkowski et al., 2013). Finally, systems such as QANTA (Iyyer et al., 2014) also answer complex multi-sentence questions but their methods can only select answers from a small li
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2250333922	) that denote an idea, event, or an object and typically have a set of properties. 2 ing, existing methods can be divided into two categories. First, methods that learn embeddings of KB concepts only [4, 5, 6]. Second, methods that jointly learn embeddings of words and concepts in the same semantic space [7, 8, 9, 10]. In this paper, we introduce an eective approach for jointly learning word and concept v
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2250539671	ere Simis a similarity function7. A good performance on this task indicates the model’s ability to learn semantic and syntactic patterns as linear relationships between vectors in the embedding space [3]. 3.1.1. Dataset We use the word analogies dataset [1]. The dataset contains 19,544 questions divided into semantic analogies (8,869), and syntactic analogies (10,675). The semantic analogies are ques
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2250333922	exts from this corpus even if the corpus has no link structure (e.g., news stories, scientic publications, medical guidelines...etc). Our proposed model is computationally less costly than Hu et al. [4] and Yamada et al. [8] models as it requires few hours rather than days to train on similar computing resources. 5. Conclusion &amp; Discussion Concepts are lexical expressions (single or multiwords)
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	1491611863	ft’s Bing4 repository using syntactic patterns. The concept KB was then leveraged for text conceptualization to support text understanding tasks such as clustering of Twitter messages and News titles [11, 12], search query understanding [13], short text segmentation [14], and term similarity [15]. Probase has a dierent modality than Wikipedia because the knowledge is organized as a graph whose nodes are
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2158899491	ign concept instances to their correct categories. 3.2.3. Compared Systems We compare our model to various word, entity, and category embedding methods including: 1. Word embeddings: Collobert et al. [19] model (WE Senna) trained on Wikipedia. Here vectors of multiword concepts are obtained by averaging their individual word vectors. We also create a baseline model (WE b) by training the skip-gram mod
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2128870637	ing a threshold and bootstrapping using vectors of N instances if their similarity scores exceed that threshold. 3.2.2. Datasets As in Li et al. [5], we utilize two benchmark datasets: 1) Battig test [18], which contains 83 single word concepts (e.g., cat, tuna, spoon..etc) belonging to 10 categories (e.g., mammal, sh, kitchenware..etc), and 2) DOTA which was created by Li et al. [5] from Wikipedia ar
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2250333922	ional meaning of their individual words. More robust entity embeddings can be learned from the entity’s corresponding article and/or from the structure of the employed KB (e.g., its link graph) as in [4, 5, 8, 10] who all utilize the skip-gram model, but dier in how they dene the context of the target concept. However, all these methods utilize one KB only (Wikipedia), to learn entity representations. Our ap
2782221572	Beyond word embeddings: learning entity and concept representations from large scale knowledge bases	2250539671	ip-gram [1], b) Word2Vec sgb, a baseline model we created by training the skip-gram model on the same Wikipedia dump we used for our CME model, and c) GloVe, word embedding model trained on Wikipedia [3]. 2. Entity mention embeddings: MPME, a recent model proposed by Cao et al. [7]. The model jointly learn embeddings of words and entity mentions by training the skip-gram on Wikipedia and utilizing an
2782590789	MIZAN: A Large Persian-English Parallel Corpus.	2026306693	ed SMT. We use KenLM (Heaﬁeld, 2011) to build Persian and English language model with order of ﬁve. For Persian language model, in addition to Persian side of MIZAN corpus, we used Hamshahri corpus¯ (AleAhmad et al., 2009) which is a monolingual resource with 10M terms. We evaluate the SMT performance using 1,000 held-out and 900 out of domain sentences from an English in Travel for Persians (EiT) book. We also build S
2782590789	MIZAN: A Large Persian-English Parallel Corpus.	2135161317	ian side was manually typewritten from the corresponding translations. Transcription process takes about 3 years employing multiple typists. 2.1 Reﬁnement Reﬁnement is a common preprocessing for SMT (Habash and Sadat, 2006). Persian texts suffers vast amount of computational issues from choosing correct character set and encoding to morphological and orthographical ambiguities (Kasheﬁ et al., 2010; Rasooli et al., 2013)
2782590789	MIZAN: A Large Persian-English Parallel Corpus.	2006969979	statistical machine translation (SMT), that is based on using somehow language independent statistical methods trained by large parallel corpora containing foreign and target language sentence pairs (Brown et al., 1993; Koehn et al., 2003). There exist some multilingual parallel corpora for resource-rich languages such as Europarl (Koehn, 2005) and JRC-Acquis (Steinberger et al., 2006). In addition, there are many
2782590789	MIZAN: A Large Persian-English Parallel Corpus.	1539361473	t al., 2000). Mousavi (2009) proposed a proprietary corpus containing 100K sentence pairs. TEP is a publicly available corpus containing about 550K sentence pairs with 8M terms from movies subtitles (Pilevar et al., 2011). This corpus is built from colloquial Persian that in some cases differs from formal Persian in terms of both morphology and syntax. Apparently, researchers have attempted to build Persian-English pa
2782590789	MIZAN: A Large Persian-English Parallel Corpus.	2153653739	translation (SMT), that is based on using somehow language independent statistical methods trained by large parallel corpora containing foreign and target language sentence pairs (Brown et al., 1993; Koehn et al., 2003). There exist some multilingual parallel corpora for resource-rich languages such as Europarl (Koehn, 2005) and JRC-Acquis (Steinberger et al., 2006). In addition, there are many bilingual corpora, wi
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2147152072	..$15.00 https://doi.org/10.1145/3159652.3159703 closer (e.g. red and blue are closer than red and squirrel). Classic word embedding techniques started in the 90s and relied on statistical approaches [9, 20]. Later, neural network approaches [5], as well as recent advances such as word2vec [24, 25] and GloVE [27] have greatly improved the performance of word representation learning. However, these techni
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	132022748,250892164	]. Temporalwordembeddingsandevaluations: WhileNLPtools have been used frequently to discover emergingword meanings and societal trends, many of them rely on changes in the co-occurrence or PMI matrix [11, 13, 22, 26, 37], changes in parts of speech, [23] or other statistical methods [4, 15, 35]. A few works use lowdimensional word embeddings, but either do no smoothing [31], or use two-step methods [12, 14, 15]. Sema
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1570098300	)Ub(t)T =U(t)RRTU(t)T =U(t)U(t)T . For this reason, it is important to enforce alignment; if word w did not semantically shift from t to t +1, then we additionally require uw(t)≈uw(t +1). To do this, [12, 15] propose two-step procedures; first, they factorize eachY(t)separately, and afterwards enforce alignment using local linear mapping [15] or solving an orthogonal procrustes problem [12]. Note that in
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2250539671	actorization of PMI(D,L)1. Our approach is primarily motivated by this observation. We note that though the PMI matrices are of size V ×V, in real-world datasets it is typically sparse as observed in [27], for which efficient factorization methods exist [39]. 2.2 Temporal word embeddings A natural extension of the static word embedding intuition is to use this matrix factorization technique on each ti
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2250539671	alignment quality is provided in Section 6. 5.3 Popularity determination It has often been observed that word embeddings computed by factorizing PMI matrices have norms that grow with word frequency [2, 27]. These word vector norms across time can be viewed as a Figure 2: Norm (top) and relative frequency (bottom) throughoutyears1990-2016.WeselectthenamesofU.Spresidents within this time frame - clinton,
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2250539671	Classic word embedding techniques started in the 90s and relied on statistical approaches [9, 20]. Later, neural network approaches [5], as well as recent advances such as word2vec [24, 25] and GloVE [27] have greatly improved the performance of word representation learning. However, these techniques usually do not consider temporal factors, and assume that the word is static across time. In this pape
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1570098300	on to d = 50. We have the following baselines: •Static-Word2Vec (SW2V): the standard word2vec embeddings [25], trained on the entire corpus and ignoring time information. •Transformed-Word2Vec (TW2V) [15]: the embeddings U(t) are first trained separately by factorizing PPMI matrix for each year t, and then transformed by optimizing a linear transformation matrix which minimizes the distance between uw
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1570098300,1984567138	discover emergingword meanings and societal trends, many of them rely on changes in the co-occurrence or PMI matrix [11, 13, 22, 26, 37], changes in parts of speech, [23] or other statistical methods [4, 15, 35]. A few works use lowdimensional word embeddings, but either do no smoothing [31], or use two-step methods [12, 14, 15]. Semantic shift and emergence are also evaluated in many different ways. In [31]
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2117130368	e idea of word embeddings has existed at least since the 90s, with vectors computed as rows of the co-occurrence [20], through matrix factorization [9], and most famously through deep neural networks [5, 8]. They have recently been repopularized with the success of low-dimensional embeddings like GloVE [27] and word2vec [24, 25], which have been shown to greatly improve the performance in key NLP tasks,
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2148763605	een a word and its neighbors. One of the several tests in [15] create synthetic data with injected semantic shifts, and quantifies the accuracy of capturing them using various time series metrics. In [23], the authors show the semantic meaningfulness of key lexical features by using them to predict the time-stamp of a particular phrase. And, [26] makes the connection that emergent meanings usually coe
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1614298861,2250539671	emantically similar words often have similar neighboring words in a corpus [10]. This is the idea behind learning dense low-dimensional word representations both traditionally [5, 9, 20] and recently [24, 27]. In several of these methods, the neighboring structure is captured by the frequencies by which pairs of words co-occur within a small local window. We compute the V ×V pointwise mutual information (
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2147152072	incidents of influenza. Word embedding learning: The idea of word embeddings has existed at least since the 90s, with vectors computed as rows of the co-occurrence [20], through matrix factorization [9], and most famously through deep neural networks [5, 8]. They have recently been repopularized with the success of low-dimensional embeddings like GloVE [27] and word2vec [24, 25], which have been sho
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2147152072	ing literature is that semantically similar words often have similar neighboring words in a corpus [10]. This is the idea behind learning dense low-dimensional word representations both traditionally [5, 9, 20] and recently [24, 27]. In several of these methods, the neighboring structure is captured by the frequencies by which pairs of words co-occur within a small local window. We compute the V ×V pointwis
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2072644219,2100597616,2171343266	l effects in natural language processing:There are several studies that investigate the temporal features of natural language.Someareusingtopicmodelingonnewscorpus[1]ortimestamped scientific journals [6, 33, 36] to find spikes and emergences of themes and viewpoints. Simpler word count features are used in [7, 13, 22] to find hotly discussed concepts and cultural phenomena, in [21, 32, 34] to analyze teen be
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1570098300	ment problem, which is an issue in general if embeddings are learned independently for each time slice. Unlike traditional methods, literature on learning temporal word embedding is relatively short: [12, 15, 19, 40]. In general, the approaches in these works follow a similar two-step pattern: first compute static word embeddings in each time slice separately, then find a way to align the word embeddings across t
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2020098476	minimizes with respect to a single block (U(t)or W(t)) at a time, and the block size can be made even smaller (a few rows of U(t)orW(t)) to maintain scalability. The main appeal of BCD is scalability [39]; however, a main drawback is lack of convergence guarantees, even in the case of convex optimization [29]. 3nnz(·)is the number of nonzeros in the matrix. (a) apple (b) amazon (c) obama (d) trump Fig
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1984567138	namic embeddings to discover and identify multisenses, evaluated against WordNet. Primarily, temporal word embeddings are evaluated against human-created databases of known semantically shifted words [12, 15, 35] which is our approach as well. 8 CONCLUSION We studied the evolution of word semantics as a dynamic word embedding learning problem. We proposed a model to learn timeaware word embeddings and used it
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	132022748	natural language.Someareusingtopicmodelingonnewscorpus[1]ortimestamped scientific journals [6, 33, 36] to find spikes and emergences of themes and viewpoints. Simpler word count features are used in [7, 13, 22] to find hotly discussed concepts and cultural phenomena, in [21, 32, 34] to analyze teen behavior in chatrooms, and in [13] to discover incidents of influenza. Word embedding learning: The idea of wo
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1614298861	the number of occurrences of words w and c in D. |D|is total number of word tokens in the corpus. L is typically around 5 to 10; we set L = 5 throughout this paper. The key idea behind both word2vec [24] and GloVE [27] is to find embedding vectors uw and uc such that for any w,c combination, uT wu c≈PMI(D,L), , (2) where each uw has length d ≪V. While both [24] and [27] offer highly scalable algorith
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2094661073	o hold steady. 7 RELATED WORK Temporal effects in natural language processing:There are several studies that investigate the temporal features of natural language.Someareusingtopicmodelingonnewscorpus[1]ortimestamped scientific journals [6, 33, 36] to find spikes and emergences of themes and viewpoints. Simpler word count features are used in [7, 13, 22] to find hotly discussed concepts and cultural
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2250539671	occurrences of words w and c in D. |D|is total number of word tokens in the corpus. L is typically around 5 to 10; we set L = 5 throughout this paper. The key idea behind both word2vec [24] and GloVE [27] is to find embedding vectors uw and uc such that for any w,c combination, uT wu c≈PMI(D,L), , (2) where each uw has length d ≪V. While both [24] and [27] offer highly scalable algorithms such as nega
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2148763605	ools have been used frequently to discover emergingword meanings and societal trends, many of them rely on changes in the co-occurrence or PMI matrix [11, 13, 22, 26, 37], changes in parts of speech, [23] or other statistical methods [4, 15, 35]. A few works use lowdimensional word embeddings, but either do no smoothing [31], or use two-step methods [12, 14, 15]. Semantic shift and emergence are also
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2020098476	otivated by this observation. We note that though the PMI matrices are of size V ×V, in real-world datasets it is typically sparse as observed in [27], for which efficient factorization methods exist [39]. 2.2 Temporal word embeddings A natural extension of the static word embedding intuition is to use this matrix factorization technique on each time slice Dt separately. Specifically, for each time sl
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1614298861,2153579005	ough matrix factorization [9], and most famously through deep neural networks [5, 8]. They have recently been repopularized with the success of low-dimensional embeddings like GloVE [27] and word2vec [24, 25], which have been shown to greatly improve the performance in key NLP tasks, like document clustering [16], LDA [28], and word similarity [3, 18]. There is a close connection between these recent meth
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1570098300	The parameter τ controls how fast we allow the embeddings to change; τ = 0 enforces no alignment, and picking τ →∞converges to a static embedding with U(1)= U(2)= ... = U(T). Note that the methods of [12, 15] can be viewed as suboptimal solutions of (5), in that they optimize for each term separately. For one, while the strategies in [15] and [12] enforce alignment pairwise, we enforce alignment across al
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1614298861,2153579005	red and squirrel). Classic word embedding techniques started in the 90s and relied on statistical approaches [9, 20]. Later, neural network approaches [5], as well as recent advances such as word2vec [24, 25] and GloVE [27] have greatly improved the performance of word representation learning. However, these techniques usually do not consider temporal factors, and assume that the word is static across tim
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2023867700	scientific journals [6, 33, 36] to find spikes and emergences of themes and viewpoints. Simpler word count features are used in [7, 13, 22] to find hotly discussed concepts and cultural phenomena, in [21, 32, 34] to analyze teen behavior in chatrooms, and in [13] to discover incidents of influenza. Word embedding learning: The idea of word embeddings has existed at least since the 90s, with vectors computed a
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2020098476	is smaller than that of BCD; however, SGD comes with the well-documented issues of slow progress and hard-to-tune step sizes, and in practice, can be much slower for matrix factorization applications [30, 39]. However, we point out that the choice of the optimization method is agnostic to our model; anything that successfully solves (5) should lead to an equally successful embedding. 4 EXPERIMENTAL DATASE
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	1615991656,2251803266	w-dimensional embeddings like GloVE [27] and word2vec [24, 25], which have been shown to greatly improve the performance in key NLP tasks, like document clustering [16], LDA [28], and word similarity [3, 18]. There is a close connection between these recent methods and our proposed method, in that both word2vec and GloVE have been shown to be equivalent to matrix factorization of a shifted PMI matrix [17
2782822144	Dynamic Word Embeddings for Evolving Semantic Discovery	2153579005	W2V) against other temporal word embedding methods.8 In all cases, we set the embedding dimension to d = 50. We have the following baselines: •Static-Word2Vec (SW2V): the standard word2vec embeddings [25], trained on the entire corpus and ignoring time information. •Transformed-Word2Vec (TW2V) [15]: the embeddings U(t) are first trained separately by factorizing PPMI matrix for each year t, and then t
2782940392	Topic-based Evaluation for Conversational Bots.	2250539671	ball , Sports_Football and Sports_Scores were merged into Sports ). Accuracy reaches over 80% for the latter case. Method Accuracy 55 topics 26 merged topics Random initialization 71.2%  Fixed GloVe [18] initialization 75.6% 77.1% GloVe [18] + ne tune 77.3% 82.4% Table 3: DAN model topic classication accuracy on the internal Question test data We also evaluate DAN on the Alexa knowledge-query data wh
2782940392	Topic-based Evaluation for Conversational Bots.	2120615054	d a profound inuence on natural language and dialog research, and researchers have experimented with Recurrent Neural Networks (RNNs)[ 11 ], [20 ], [13 ] and Convolutional Neural Networks (CNNs) [9], [4], [8] for text classication and summarization tasks. Recent work indicates that simpler bag-of-words neural models, like Deep Averaging Networks (DAN) [ 6] or FastText [ 7], can achieve state of the a
2782940392	Topic-based Evaluation for Conversational Bots.	2153579005	mension. S is passed through a series of fully connected layers and fed through a softmax. The word embedding matrix can be initialized with any pre-trained embeddings such as GloVe [18] and Word2Vec [15, 16], while the topic-word attention matrix is initialized randomly and is learnt from data. We use ReLUs as the nonlinearity. During testing, the learnt topic-word weights can be examined for each test u
2782940392	Topic-based Evaluation for Conversational Bots.	2250539671	s the embedding dimension. S is passed through a series of fully connected layers and fed through a softmax. The word embedding matrix can be initialized with any pre-trained embeddings such as GloVe [18] and Word2Vec [15, 16], while the topic-word attention matrix is initialized randomly and is learnt from data. We use ReLUs as the nonlinearity. During testing, the learnt topic-word weights can be ex
2783586329	Algebraic Specifications of Wayfinding Using Cognitive Map	2071434200	sense, speciﬁcations for a software system can be tested before committing to build the system. Algebraic speciﬁcations written in an executable programming language can be tested as a prototype [4], [44]. C. Functional Programming There is a possibility to write algebraic speciﬁcations as an executable code by using functional programming languages. Functional programming language is a formal languag
2783586329	Algebraic Specifications of Wayfinding Using Cognitive Map	2071434200	ssuch as state machine models and axiomatic descriptionsalgebraic deﬁnitions have proven to be good candidates for specifying data abstractions for spatial and temporal domains [5], [14], [29], [36], [44]. B. Algebra and The Algebraic Speciﬁcations The purpose of speciﬁcations is the mathematical description of concepts. Speciﬁcations can be informal-e.g., expressed in a natural language-or formal. Fo
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2250539671	give better results, however this heuristic already proved beneﬁcial compared to the original network. 4.4 Seq2Seq The input sequence x is encoded by applying he t= LSTM enc(x | het−1). We use GloVe (Pennington et al., 2014) for our word embeddings. The ﬁnal hidden state, he t, is fed into the decoder LSTM dec as the initial state hd 0. For each time step t, the decoder then produces the probability of a word j occurring
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	1793121960	dge in the domain being discussed, and a persistent personality during discussions. A promising direction, that is still in its infancy, to ﬁx this issue is to use a memory-augmented network instead (Sukhbaatar et al., 2015; Dodge et al., 2015) and either provide or learn appropriate external memories. A related class of neural methods is to use similarly architectures, but to retrieve and rank candidates similarly to t
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2399880602	ie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b). Naively training on these datasets leads to models with the lack of a consistent personality as they will learn a model averaged over many different speakers. Moreover, the data does little to enc
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2399880602	rowing recent interest. A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al.,2015;Li et al.,2016b;Serban et al., 2017b). Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free approach means they lack long-term coherence and a persistent personality, as di
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2399880602	s have continued to be used in applications to this day. For example, modern solutions that build an open-ended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements (Serban et al., 2017a). Amongst the simplest of statistical systems that can be used in this domain, that are based on data rather than hand-coding, are information retrieval models (Sordoni et al., 2015), which retrieve
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2139575250	t (Dunbar et al., 1997). For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called “Meformers” (Naaman et al., 2010). In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a conﬁgurable, but persistent persona, encoded by multiple sentences of textual description, termed
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	1793121960	can try to learn to mimic. Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks (Sukhbaatar et al., 2015) as well as other standard retrieval baselines. We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the ne
2784400615	Personalizing Dialogue Agents: I have a dog, do you have pets too?	2130942839	in this work. End-to-end neural approaches are a class of models which have seen growing recent interest. A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al.,2015;Li et al.,2016b;Serban et al., 2017b). Rooted in language modeling, they are able to produce syntactically coherent novel responses, but their memory-free a
2784439163	A Universal Semantic Space.	2118090838	ingual embedding spaces and subsequently uses a transformation to create a uniﬁed space.Mikolov et al.(2013b) ﬁnd the transformation by minimizing the Euclidean distance between word pairs. Similarly,Zou et al. (2013),Xiao and Guo(2014) andFaruqui and Dyer(2014) use different data sources for identifying word pairs and creating the transformation (e.g., by CCA).Duong et al.(2017) is also similar. These approaches
2784439163	A Universal Semantic Space.	2126530744	nd target in the same space. Similarly, in transfer learning, models trained in one language on multilingual embeddings can be deployed in other languages (Zeman and Resnik, 2008;McDonald et al.,2011;Tsvetkov et al., 2014). Automatically learned embeddings have the added advantage of requiring fewer resources for training (Klementiev et al.,2012;Hermann and Blunsom,2014b;Guo et al.,2016). Thus, massively multilingual w
2784439163	A Universal Semantic Space.	2156985047	tion. We only used a member of the IBM class of models (Dyer et al.,2013), but presumably we could improve results by using either higher performing albeit slower aligners or non-IBM aligners (e.g., (Och and Ney, 2003;Tiedemann,2003;Melamed,1997)). Other alignment algorithms include 2D linking (Kobdani et al.,2009), sampling based methods (e.g.,Vulic and Moens(2012)) and EFMARAL (Ostling and¨ Tiedemann,2016). EFMA
2784713665	SentiPers: A Sentiment Analysis Corpus for Persian.	38037257	a dataset has been generated composed of 511 positive and 509 negative online customer reviews in Persian from some brands of cell phone products. Two annotators labeled these reviews manually [19], [20]. Another dataset has been created named BS Data containing user reviews from a Persian website, mobile.ir. This dataset is composed of a total number of 263 positive and negative reviews [21]. In ano
2784713665	SentiPers: A Sentiment Analysis Corpus for Persian.	1579082069	ned multilingual sentiment corpus that includes both German and English. This resource contains the annotation of the product reviews selected from the Amazon with both aspects and subjective phrases [17]. Unlike the significant number of rich corpora developed for English, there has not been much sentiment corpora developed for Persian. In addition, most of these Persian corpora have been labeled onl
2784713665	SentiPers: A Sentiment Analysis Corpus for Persian.	2153804780	rget and opinion words here are not of nominal type and there is no fixed number of categories in each sentence for these tags while kappa measures are more suitable for nominal or categorical values [29], [30]. Consequently, for measuring the agreement for the identified target and opinion words by the annotators we used the same method that has been used for measuring agreement for text anchors in [
2784713665	SentiPers: A Sentiment Analysis Corpus for Persian.	2147687659	ributes including an opinion about the hotel, its date and writer as well [22]. There are also some other collections of Persian reviews that has been used in studies on sentiment analysis in Persian [23], [24]. 3. Corpus data The first and one of the most important steps in developing a corpus is selecting the appropriate data source. The data used in construction of SentiPers is • extracted from a w
2784713665	SentiPers: A Sentiment Analysis Corpus for Persian.	2088879970	y [19], [20]. Another dataset has been created named BS Data containing user reviews from a Persian website, mobile.ir. This dataset is composed of a total number of 263 positive and negative reviews [21]. In another study, a dataset is collected from a Persian website named hellokish on hotel domain. This dataset contains 1805 negative and 4630 positive reviews. Each review has some attributes includ
2784760603	What did you Mention? A Large Scale Mention Detection Benchmark for Spoken and Written Text.	2151048449	datasets, among them the above mentioned AIDA and AQUAINT. Most of those benchmarks focus on named entities probably as those entities are well deﬁned. Few benchmarks such as (Milne and Witten, 2008; Ratinov et al., 2011) cover also general entities, but they are relatively small, containing only few hundred Mentions. Moreover, all existing benchmarks refer to written text only. In contrast, the benchmark data describ
2784760603	What did you Mention? A Large Scale Mention Detection Benchmark for Spoken and Written Text.	2161066414	e TAC-KBP (Text Analytic Conference - Knowledge Base Population (McNamee and Dang, 2009))3, the Microspots NEEL (Named Entity rEcognition and Linking)4 and ERD (Entity Recognition and Disambiguation (Carmel et al., 2014)). In addition there are benchmarks published by speciﬁc research groups such as AIDA (Hoffart et al., 2011), AQUAINT (Milne and Witten, 2008), MSNBC (Ratinov et al., 2011) and more. The Gerbil projec
2784760603	What did you Mention? A Large Scale Mention Detection Benchmark for Spoken and Written Text.	1713614699	ofﬁce. Text 1 Most of the existing Mention Detection tools focus on extracting named entities, probably since this task is more easily deﬁned. The task of linking all types of entities is more vague (Ling et al., 2015) and requires clear guidelines on what to annotate, how to deal with nested terms, and how to resolve speciﬁcity of entities. However, extracting all types of entities, not only named entities, is cru
2784760603	What did you Mention? A Large Scale Mention Detection Benchmark for Spoken and Written Text.	2151048449	ognition and Disambiguation (Carmel et al., 2014)). In addition there are benchmarks published by speciﬁc research groups such as AIDA (Hoffart et al., 2011), AQUAINT (Milne and Witten, 2008), MSNBC (Ratinov et al., 2011) and more. The Gerbil project (Usbeck et al., 2015) is a framework for the evaluation of Mention Detection tools. It deﬁnes formats and APIs for adding new benchmark data and new Mention Detection too
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2303217074	, they combined sentence-level features with dictionary-based features and achieved significant improvements in cyberhate polarity prediction by 4.3% compared to the baselines. Using pattern analysis [42] detected cyberhate using sentence structure - specifically patterns starting with the word ’I’. They assume that the word ’I’ means that the user is talking about the emotions that he or she is feeli
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2340954483,2473555522	apture consecutive words of varying sizes (from 1...n) and have been used to improve the performance of hate speech classification by capturing context within a sentence that is lost in the BoW model [6, 11, 20, 35, 48]. [48] found that character n-grams have been shown to be appropriate for abusive language tasks due to their ability to capture variations of words associated with hate. In addition, [48], [32] found
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2613977835	cyberhate classification models by around 3% to 4% in F1 score. However, they limited their study by comparing the classification results with TF-BoW and TF-IDF-BoW for the same comments. Similarly, [4] compared the classification accuracy of the combination of different baselines and classifiers (Char n-gram, TF-IDF, BoW and LSTM) and found that learning embedding with gradientboosted decision tree
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2153579005	Doc2Vec model, paragraphs are represented as low-dimensional vectors and are jointly learned with distributed vector representations of tokens using a distributed memory model (for further detail see [34]). Every sentence is mapped to a unique vector, and every word included in the sentence is also mapped to a unique vector. In our context this means each tweet is fed into the embedding learning metho
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2340954483	e results as a comparison of F-measure for each model. At an individual type level, the baseline results for religion were 0.77, 0.83 and 0.85 for [10] (Baseline model 1), [15] (Baseline model 2) and [35] (Baseline model 3) respectively. Using the othering lexicon alone we were able to show an improvement to 0.90. This was mainly due to a large reduction in false negatives (missed instances of cyberha
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1071251684	eature lexicon. The candidate methods included Support Vector Machines (SVM), Decision Trees (DT), Naive Bayes (NB) and Random Forests (RF), as used in [9][10] and Logistic Regression (LR) as used by [15]. The SVM parameters were set to normalize data, use a gamma of 0.1 and C of 1.0 and we employed radial basis function (RBF) kernel. Decision tree classifiers (DT) iteratively identify the feature fro
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1871142974	are enabling distributed societies to be connected, one unanticipated disadvantage of the technology is the ability for hateful and antagonistic content, or cyberhate, to be published and propagated [9, 51]. Several studies have shown how individuals with biased or negative views towards a range of minority groups are taking to the Web to spread such hateful messages [28, 38]. Instances of cyberhate and
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2340954483	g. send them home) . While previous studies highlight the utility of methods capable of measuring semantic distances between words, such as embedding learning using individual words [15], and n-grams [35], this example requires an additional layer of qualitative context that sits above combinations of individual words. Recent studies have begun to interpret the effective features for machine classific
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2156580264	hod suffers from a high rate of false positives, since the presence of hateful words can lead to the misclassification of tweets being hateful when they are used in a different context (e.g. ’black’) [22]. For instance, [14] demonstrated how non-hateful content might be misclassified due to the fact that it contains words used in racist text. In contrast, they also showed that hateful instances were m
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2153579005	ic learning when word vectors are mapped into a vector space, such that distributed representations of sentences and documents with semantically similar words have similar vector representations [33] [34]. Based on the distributional representation of the text, many methods of deriving word representations that are related to cyberhate and offensive language detection are explored in the following wor
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2540646130	ion in classification error [7]. Random Forest Decision Tree algorithm was trained with 100 trees. In addition to the previous classifiers, we also examined the Multilayer Perceptron (MLP) classifier [57]. MLP is a feed-forward artificial neural network model which maps input data sets on an appropriate set of outputs. MLP consists of multiple layers of nodes in a directed graph, with each layer being
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2340954483	nd 200 iterations. 1https://radimrehurek.com/gensim/models/Doc2Vec.html 5 RESULTS AND DISCUSSION The baseline results for this work stem from the previous state-of-the-art, produced in [10], [15] and [35]. The previous studies use different feature sets for cyberhate classification, part of which we applied in our study. [10] used a Bag of Words (BoW), n-gram, and Typed Dependency features. [15] used
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1071251684	ne methods on this basis, especially given the MLP model appears to find the best balance between false positive (FP) and false negative (FN) classifier outputs. The Logistic Regression model used by [15] comes close to MLP, and performs best in the disability class - but even then there is a disproportionate balance of FP and FN (0 FP and 11 FN), whereas MLP has a lower F-measure in this case but a b
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2250473257	network model which maps input data sets on an appropriate set of outputs. MLP consists of multiple layers of nodes in a directed graph, with each layer being fully connected to the next layer [19]. [24] demonstrate that multilayer feed-forward networks can provide competitive results on sentiment classification and factoid question answering. We examined MLP with two hidden layer, five hidden connec
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2199529914	neural network model which maps input data sets on an appropriate set of outputs. MLP consists of multiple layers of nodes in a directed graph, with each layer being fully connected to the next layer [19]. [24] demonstrate that multilayer feed-forward networks can provide competitive results on sentiment classification and factoid question answering. We examined MLP with two hidden layer, five hidden
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1871142974	o the size of the sample. However, these are random instances of the full datasets for each event and they are considered representative of the overall levels of cyberhate within the corpus of tweets [9]. We evaluate the relative improvement in classification performance using this dataset, which we refer to as Dataset 2. 3.3 Summary Statistics for Othering Language in the Datasets To provide an init
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2160660844	parse tree) may improve classification results for contents on social media. Typed dependencies have been widely used for extracting the functional role of context words for sentiment classification [23, 25] and document polarity [45]. [9, 10] demonstrated the effectiveness of applying typed dependencies for classifying cyberhate. Their study showed that typed dependencies consistently improved the perfo
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1071251684	plications were shown to be capable of capturing specific semantic features from complex natural language (e.g. location [37], entity [41] and images feature [2]). For hate speech detection purposes, [15] solved the problem of high dimensionality and sparsity by applying sentence embedding (paragraph2vec). In their study, paragraph2vec, which is an extended version of Word2Vec for sentences, has been
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1871142974	on results for contents on social media. Typed dependencies have been widely used for extracting the functional role of context words for sentiment classification [23, 25] and document polarity [45]. [9, 10] demonstrated the effectiveness of applying typed dependencies for classifying cyberhate. Their study showed that typed dependencies consistently improved the performance of machine classification for
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2340954483	the semantic relations, their works targeted other areas of research, not cyberhate. One study that has combined typed dependencies with embedding learning in the context of cyberhate was reported by [35]. They developed a machine learning approach to cyberhate based on different syntactic features as well as different types of embedding features, and reported its effectiveness when combined with some
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2153579005	sentations of the sentence (tweet) [27]. In the Distributed Memory component (PV-DM), the sentence acts as a memory that remembers the missed word in the current context of the sentence. According to [34], the distributed memory model is consistently better than PVDBOW. To find the best implementation for our data, we experimented with both and found that distributed memory performed better in learnin
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	2022204871	sentiment expressions using semantic and subjectivity features with an orientation towards hate speech, and then used these features to create a classifier for cyberhate detection. Work conducted by [53] proposed a method for automatic cyberhate detection using two steps: first, they used word features (tokens), sentence/structure features (dependency relations) and document features (document topic)
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1871142974	specifically focused on othering features. As shown at the bottom of Figure4- prior to implementing the embedding phase we introduced Dataset 2, which includes the labelled data from previous studies [9][10] for evaluation purposes. Dataset 2 was parsed using the Stanford Typed Dependency parser. At this point the Typed Dependency features from the hateful samples in Dataset 2 were concatenated with
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1071251684	tion accuracy, (e.g. send them home) . While previous studies highlight the utility of methods capable of measuring semantic distances between words, such as embedding learning using individual words [15], and n-grams [35], this example requires an additional layer of qualitative context that sits above combinations of individual words. Recent studies have begun to interpret the effective features for
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1071251684	units, and 200 iterations. 1https://radimrehurek.com/gensim/models/Doc2Vec.html 5 RESULTS AND DISCUSSION The baseline results for this work stem from the previous state-of-the-art, produced in [10], [15] and [35]. The previous studies use different feature sets for cyberhate classification, part of which we applied in our study. [10] used a Bag of Words (BoW), n-gram, and Typed Dependency features. [
2784778675	Cyber Hate Classification: 'Othering' Language And Paragraph Embedding	1871142974,2085582472	W) feature extraction approach. BoW has been successfully applied as a feature extraction method for automated detection of hate speech, relying largely on keywords relating to offence and antagonism [9, 36, 50]. However, the method suffers from a high rate of false positives, since the presence of hateful words can lead to the misclassification of tweets being hateful when they are used in a different conte
2784808670	A Deep Reinforcement Learning Chatbot (Short Version).	2121863487	ribes the ﬁve approaches we have investigated to learn the model selection policy. The approaches are evaluated with real-world users in the next section. We use the reinforcement learning framework [Sutton and Barto, 1998]. The dialogue manager is an agent, which takes actions in an environment in order to maximize rewards. For each time step t=1;:::;T, the agent observes the dialogue history h tand must choose one of
2784808670	A Deep Reinforcement Learning Chatbot (Short Version).	2171278097	systems consist of many independent sub-models combined intelligently together. Examples of such ensemblesystemsincludethewinneroftheNetﬂixPrize[Korenetal.,2009],theIBMWatsonquestionanswering system [Ferrucci et al., 2010] and Google’s machine translation system [Wu et al., 2016]. Our system consists of an ensemble of response models (see Figure 1). Each response model takes as input a dialogue history and outputs a r
2784928177	HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments.	2097726431	f happiness relating to products and services from comments on social media. Viewed in that perspective, analyzing happy moments can also be seen as a reﬁned analysis of sentiments (e.g., (Liu, 2012; Pang and Lee, 2008)). HappyDB is a collection of sentences in which crowdworkers answered the question: what made you happy in the past 24 hours (or alternatively, the past 3 months). Naturally, the descriptions of happ
2784928177	HappyDB: A Corpus of 100, 000 Crowdsourced Happy Moments.	630883834	lasses (e.g., stative, cognition, communication, social, motion etc.) and 26 noun classes (e.g., person, artifact, cognition, food etc.). We trained a supersense tagger with the SemEval-2016 dataset (Schneider and Smith, 2015) using CRF (Okazaki, 2007). The supersense annotated HappyDB is also provided as part of HappyDB. Table 3 shows the proportion of sentences for the top seven supersense labels in HappyDB. It also disp
2784966376	Context Models for OOV Word Translation in Low-Resource Languages.	2626778328	. sentence-level context on OOV disambiguation. 4.3 Baseline MT Systems Baseline MT systems were developed for these tasks using phrase-based MT and attentionbased neural MT (the Transformer model of Vaswani et al. (2017))3. The PBMT system was trained using Moses (Koehn et al. (2007)) and uses a ﬂat phrase-based model with a maximum phrase length of 7, a backoff 4-gram language model trained on the target side of the
2784966376	Context Models for OOV Word Translation in Low-Resource Languages.	2117198860	Callison-Burch (2013) new translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features. Saluja et al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rul
2784966376	Context Models for OOV Word Translation in Low-Resource Languages.	2577255746	hidden layer size of 48 and also utilize a context size of 4 sentences. Word embedding vectors in both types of language models are initialized randomly. Both LSTMs and DCLMs were trained with DyNet (Neubig et al. (2017)).5 The vocabulary for the language models consists of the OOV translation candidates and the words from the one-best MT hypotheses. A comparison between sentence-level and document-level model perple
2784966376	Context Models for OOV Word Translation in Low-Resource Languages.	2625092622	are shown in Table 2. Scoring was done in a case-insensitive fashion against a single reference translation. Previous studies of neural sequence-to-sequence models for resource-poor scenarios (e.g., Koehn and Knowles (2017)) have found that PBMT models performed signiﬁcantly better on low-resource languages unless the NMT models were enriched with additional components, such as a lexical memory (Nguyen and Chiang (2017)
2784966376	Context Models for OOV Word Translation in Low-Resource Languages.	2293547632	w translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features. Saluja et al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rules from monolingual dat
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2516196286	al. (2015) created a large cloze-style dataset using CNN and Daily Mail news articles. Several models (Hermann et al. 2015; Chen, Bolton, and Manning 2016; Kadlec et al. 2016; Kobayashi et al. 2016; Cui et al. 2017; Dhingra et al. 2017) based on neural attentional and pointer networks (Vinyals, Fortunato, and Jaitly 2015) have been proposed since then. Rajpurkar et al. (2016) released the SQuAD dataset where th
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2516930406	awled from the Jeopardy archive and are augmented with text snippets retrieved from Google search. Recently, many neural models have been proposed (Wang et al. 2016; Pan et al. 2017; Seo et al. 2017; Wang and Jiang 2017; Weissenborn, Wiese, and Seiffe 2017; Xiong, Zhong, and Socher 2017; Yang et al. 2017), which mostly focus on passage-question interaction to capture the context similarity for extracting a text span
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2550837020	e search. Recently, many neural models have been proposed (Wang et al. 2016; Pan et al. 2017; Seo et al. 2017; Wang and Jiang 2017; Weissenborn, Wiese, and Seiffe 2017; Xiong, Zhong, and Socher 2017; Yang et al. 2017), which mostly focus on passage-question interaction to capture the context similarity for extracting a text span as the answer. However, most of the models do not focus on synthesizing evidence from
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2415755012	ed a large cloze-style dataset using CNN and Daily Mail news articles. Several models (Hermann et al. 2015; Chen, Bolton, and Manning 2016; Kadlec et al. 2016; Kobayashi et al. 2016; Cui et al. 2017; Dhingra et al. 2017) based on neural attentional and pointer networks (Vinyals, Fortunato, and Jaitly 2015) have been proposed since then. Rajpurkar et al. (2016) released the SQuAD dataset where the answers are freeform
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2064675550	g to incorporate contextual information. Let ep t and e q t be the tth embedding vectors of the passage and the question respectively. The embedding vectors are fed to a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber 1997). Considering that the outputs of the BiLSTMs are unfolded across time, we represent the outputs as P 2RT H and Q 2RU H for passage and question respectively. H is the number of hidden units for the B
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2516930406	on the idea of chunking and ranking include Yu et al. (2016) and Lee et al. (2016). Yang et al. (2017) used a ﬁne-grained gating mechanism to capture the correlation between a passage and a question. Wang and Jiang (2017) used a Match-LSTM to encode the question and passage together and a boundary model determined the beginning and ending boundary of an answer. Trischler et al. (2016) reimplemented Match-LSTM for the
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2475151947	ing dataset. Hermann et al. (2015) created a large cloze-style dataset using CNN and Daily Mail news articles. Several models (Hermann et al. 2015; Chen, Bolton, and Manning 2016; Kadlec et al. 2016; Kobayashi et al. 2016; Cui et al. 2017; Dhingra et al. 2017) based on neural attentional and pointer networks (Vinyals, Fortunato, and Jaitly 2015) have been proposed since then. Rajpurkar et al. (2016) released the SQuAD
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2427527485	Kadlec et al. 2016; Kobayashi et al. 2016; Cui et al. 2017; Dhingra et al. 2017) based on neural attentional and pointer networks (Vinyals, Fortunato, and Jaitly 2015) have been proposed since then. Rajpurkar et al. (2016) released the SQuAD dataset where the answers are freeform unlike in the previous MC datasets. Most of the previously released datasets are closed-world, i.e., the questions and answers are formulated
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	1522301498	layer. For multi-factor attentive encoding, we choose 4 factors (m) based on our experimental ﬁndings (refer to Table 7). During training, the minibatch size is ﬁxed at 60. We use the Adam optimizer (Kingma and Ba 2015) with learning rate of 0.001 and clipnorm of 5. During testing, we enforce the constraint that the ending pointer will always be equal to or greater than the beginning pointer. We use exact match (EM)
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	2550837020	resolution, etc. Related Work Recently, several neural network-based models have been proposed for QA. Models based on the idea of chunking and ranking include Yu et al. (2016) and Lee et al. (2016). Yang et al. (2017) used a ﬁne-grained gating mechanism to capture the correlation between a passage and a question. Wang and Jiang (2017) used a Match-LSTM to encode the question and passage together and a boundary mod
2785176118	A Question-Focused Multi-Factor Attention Network for Question Answering	1544827683	wer for a given question by understanding texts. In recent years, several MC datasets have been released. Richardson, Burges, and Renshaw (2013) released a multiple-choice question answering dataset. Hermann et al. (2015) created a large cloze-style dataset using CNN and Daily Mail news articles. Several models (Hermann et al. 2015; Chen, Bolton, and Manning 2016; Kadlec et al. 2016; Kobayashi et al. 2016; Cui et al.
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2340254858	, development of a dataset annotated for veracity is very challenging, as judgments from professionals are needed to carefully verify and subsequently annotated stories. As shown by previous research [10], average users struggle to distinguigh true and false stories, and it is therefore not a suitable task to be performed through crowdsourcing, requiring professional input instead. As a result, few da
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2014658588	. I. INTRODUCTION S OCIAL media platforms such as Twitter and Facebook are increasingly being used by the general public to follow the latest news [20], [11] and by journalists for newsgathering [6], [28]. The fact that anyone can post and share content in social media without moderation enables decentralised production of citizen journalism with an unprecedented detail of report [4]. However, the unm
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2051405935	bjective here instead is to aim for early classiﬁcation of stories, with the ultimate goal of detecting hoaxes early on. Research looking into either real-time or early detection of hoaxes is scarce. [13] use a set of features including user metadata and propagation structure to verify stories within hours of being posted for the ﬁrst time. They show competitive performance with the use of both featur
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2128634885	build class-speciﬁc language models [3]. Brown clusters have been successfully used by researchers for training word representations [24], natural language processing tasks such as dependency parsing [9] or for building class-speciﬁc language models [2], among others. As a stateof-the-art approach for semantic word representation, here we make use of word embeddings [16]. We propose to train and leve
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2020360881	dex Terms—social media, hoax detection, breaking news. I. INTRODUCTION S OCIAL media platforms such as Twitter and Facebook are increasingly being used by the general public to follow the latest news [20], [11] and by journalists for newsgathering [6], [28]. The fact that anyone can post and share content in social media without moderation enables decentralised production of citizen journalism with an
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2337464997	enables decentralised production of citizen journalism with an unprecedented detail of report [4]. However, the unmoderated nature of social media also leads to the production and diffusion of hoaxes [15], [1], which exacerbates the credibility of social media as a source for news consumption. Research in automated detection of misinformation in social media has indeed increased in recent years [21],
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2340254858	indeed increased in recent years [21], [27]. Researchers have assessed the capacity of average people to identify reports that are inaccurate, ﬁnding that their performance leaves much to be desired [10]. This reinforces the need to develop automated systems for hoax detection, however existing work has largely limited to post-hoc classiﬁcation of reports as true or false, which means that reports ca
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2142869398	k on veracity classiﬁcation has used different social media platforms including Twitter [23] and Sina Weibo 1http://www.snopes.com/tag/celebrity-death-hoaxes/ arXiv:1801.07311v1 [cs.CL] 22 Jan 2018 2 [26]. However, most of this work has performed post-hoc classiﬁcation of reports as true or false [8], [14], [22], which means that they need to observe the entire development of a story before classifyin
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2115893922	news. I. INTRODUCTION S OCIAL media platforms such as Twitter and Facebook are increasingly being used by the general public to follow the latest news [20], [11] and by journalists for newsgathering [6], [28]. The fact that anyone can post and share content in social media without moderation enables decentralised production of citizen journalism with an unprecedented detail of report [4]. However, t
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2051405935	nstead. As a result, few datasets have been produced, and most of these datasets are created by ﬁrst collecting false stories, and then completing the datasets with randomly picked true stories [12], [13]. The use of different methodologies for collecting false and true stories is however not ideal as it will inevitably differ from a real scenario. In this work, we describe a novel approach for semiau
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2742330194	in order to aggregate the different stances, which may impede early determination of report veracity. Research in veracity classiﬁcation has been largely limited by the dearth of proper datasets. As [21] stated, development of a dataset annotated for veracity is very challenging, as judgments from professionals are needed to carefully verify and subsequently annotated stories. As shown by previous re
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2153579005	on patterns. Please see the appendix for more details of these features.  Textual features using word embeddings (w2v): As a state-of-the-art word representation approach, we use Word2Vec embeddings [16] to represent the content of the tweets associated with a report. The model we use for the embeddings was trained from the entire collection of tweets in the training set, i.e. all the 2012 and 2013 t
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2121227244	Representations Class-speciﬁc word representations have been found to be useful for different classiﬁcation tasks, as is the case with the use of Brown clusters to build class-speciﬁc language models [3]. Brown clusters have been successfully used by researchers for training word representations [24], natural language processing tasks such as dependency parsing [9] or for building class-speciﬁc langu
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2101196063	rms—social media, hoax detection, breaking news. I. INTRODUCTION S OCIAL media platforms such as Twitter and Facebook are increasingly being used by the general public to follow the latest news [20], [11] and by journalists for newsgathering [6], [28]. The fact that anyone can post and share content in social media without moderation enables decentralised production of citizen journalism with an unpre
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2742330194	s [15], [1], which exacerbates the credibility of social media as a source for news consumption. Research in automated detection of misinformation in social media has indeed increased in recent years [21], [27]. Researchers have assessed the capacity of average people to identify reports that are inaccurate, ﬁnding that their performance leaves much to be desired [10]. This reinforces the need to deve
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2153579005	sks such as dependency parsing [9] or for building class-speciﬁc language models [2], among others. As a stateof-the-art approach for semantic word representation, here we make use of word embeddings [16]. We propose to train and leverage class-speciﬁc word embeddings to learn the patterns of each class in the training data. The difﬁculty to achieve this generally lies in the necessity for large-scale
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2158139315	ssiﬁcation tasks, as is the case with the use of Brown clusters to build class-speciﬁc language models [3]. Brown clusters have been successfully used by researchers for training word representations [24], natural language processing tasks such as dependency parsing [9] or for building class-speciﬁc language models [2], among others. As a stateof-the-art approach for semantic word representation, here
2785200186	Learning Class-specific Word Representations for Early Detection of Hoaxes in Social Media.	2159981908	thwhile considering, neither of their systems was publicly released and the features they use are hardly reproducible for the reader. Others have taken a different approach by using stance classiﬁers [17]. Instead of using a classiﬁer that directly outputs one of true or false given a report as input, they try to determine the stance that each social media post expresses with respect to a report, such
2785237045	Reasoning about multiple aspects in DLs: Semantics and Closure Construction.	342819117,1509410808,2045427198,2161178987,2175390224	ge without consideringall the alternative consistent bases. 6 Related Work A lot of work has been done in orderto extend the basic formalism of Description Logics(DLs)withnonmonotonicreasoningfeatures[36,1,15,16,20,23,29,7,5,11,34,31,30,10,3]. The purpose of these extensions is to allow reasoning about prototypical properties of individualsor classes ofindividuals.A detailed descriptionsof these formalismsandof their relations to our appr
2785237045	Reasoning about multiple aspects in DLs: Semantics and Closure Construction.	342819117,1509410808,2045427198,2161178987,2175390224	iﬁcial Intelligence and in Description Logics (DLs). In particular, a lot of work has beendevotedto extendingDLs with non-monotonigformalismsto allow reasoning aboutprototypicalpropertiesofindividuals[36,1,15,16,20,23,24,29,7,21,5,11,34,31,30,10,3]. In this paperwe proposean extensionof rational closure [32] for dealingwith multiplepreferences.Oneofthemaindifﬁcultiesofrationalclosuretodealwithinheritance of defeasible properties of concepts is
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2251818205	379 G,H1,F5I.E5F-379 Figure 2: Performance comparisons of the simple average model. 9https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip 10https://radimrehurek.com/gensim/ 8 where WikiQA (Yang et al., 2015) is an open-domain question answering dataset from Microsoft research. The results on the enhanced vectors are better on the above three datasets. This indicates that enhanced vectors may fuse the dom
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2250539671	algorithm 1 to combine pre-trained word vectors with those word2vec (Mikolov et al., 2013) generated on the training set. Here the pre-trainined word vectors can be from known methods such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013) and FastText (Bojanowski et al., 2016). Algorithm 1: Combine pre-trained word embedding with those generated on training set. Input : Pre-trained word embedding set f
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2891416139	ave shown that the proposed method has signiﬁcantly improved the performance of original ESIM model and obtained state-ofthe-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus (Wu et al., 2017). On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on R10@1 is 3.8% and our ensemble model on R10@1 is 75.9%. On Douban Conversation Corpus, the improvement
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	1938755728	b-search ranking. Bojanowski et al. (2016) represented a word by the sum of the vector representation of character n-gram. Santos et al (Santos &amp; Zadrozny, 2014; Santos &amp; Guimaraes, 2015) and Kim et al. (2016) used convolution neural network to generate character-level representation (embedding) of a word. The former combined both word-level and character-level representation for part-of-speech and name en
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2891416139	e dataset. On both validation and test sets, each context contains one positive response and 9 negative responses. Some statistics of this corpus are presented in Table 1. Douban conversation corpus (Wu et al., 2017) which are constructed from Douban group 3(a popular social networking service in China) is also used in experiments. Response candidates on the test set are collected by Lucene retrieval model, other
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2153579005	ectors on general large text-corpus are available. For domainspeciﬁc tasks, out-of-vocabulary may become an issue. Here we propose algorithm 1 to combine pre-trained word vectors with those word2vec (Mikolov et al., 2013) generated on the training set. Here the pre-trainined word vectors can be from known methods such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013) and FastText (Bojanowski et al.,
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2891416139	es after bi-directional LSTM layer. Zhou et al. (2016) treated each turn 2 in multi-turn context as an unit and joined word sequence view and utterance sequence view together by deep neural networks. Wu et al. (2017) explicitly used multi-turn structural info on Ubuntu dialogue corpus to propose a sequential matching method: match each utterance and response ﬁrst on both word and sub-sequence levels and then aggr
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2153579005	ic similarity (e.g. ‘car’ and ‘bmw’) and it cannot be applied to Asian languages (e.g. Chinese characters). In this paper, we generate word embedding vectors on the training corpus based on word2vec (Mikolov et al., 2013). Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation mainta
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2123442489	ing and character embedding matrix were ﬁxed during the training. After algorithm 1 was applied, the remaining out-of-vocabulary words were initialized as zero vectors. We used Stanford PTBTokenizer (Manning et al., 2014) on the Ubuntu corpus. The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus. For the ensemble model, we use the average prediction output of models with
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2131876387	ize our ﬁndings and outline future research directions. 2 RELATED WORK Character-level representation has been widely used in information retrieval, tagging, language modeling and question answering. Shen et al. (2014) represented a word based on character trigram in convolution neural network for web-search ranking. Bojanowski et al. (2016) represented a word by the sum of the vector representation of character n-
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	1938755728	n the pre-built GloVe word vectors1. Although character-level representation which models sub-word morphologies can alleviate this problem to some extent (Huang et al., 2013; Bojanowski et al., 2016; Kim et al., 2016), character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. ‘car’ and ‘bmw’) and it cannot be applied to Asian l
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2153579005	orithm 1. WP1Used the ﬁxed pre-trained GloVe vectors 7. WP2Word embedding were initialized by GloVe vectors and then updated during the training. WP3Generated word2vec embeddings on the training set (Mikolov et al., 2013) and updated them during the training (dropout). WP4Used the pre-built ConceptNet NumberBatch (Speer et al., 2017) 8. 7glove.42B.300d.zip in https://nlp.stanford.edu/projects/glove/ 8numberbatch-en-17
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2891416139	tes per context 2 2 10 #positive candidates per context 1 1 1.18 Avg. # turns per context 6.69 6.75 6.45 Avg. #words per utterance 18.56 18.50 20.74 Table 2: Statistics of Douban Conversation Corpus (Wu et al., 2017). 4.2 IMPLEMENTATION DETAILS Our model was implemented based on Tensorﬂow (Abadi et al., 2016). ADAM optimization algorithm (Kingma &amp; Ba, 2014) was used for training. The initial learning rate was
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2891416139	uban dialogue with additional keywords extracted from the context on the test set was used as query to retrieve 10 response candidates from the Lucene index set (Details are referred to section 4 in (Wu et al., 2017)). For the 2https://github.com/rkadlec/ubuntu-ranking-dataset-creator 3https://www.douban.com/group 5 Train Validation Test #positive pairs 499,873 19,560 18,920 #negative pairs 500,127 176,040 170,28
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2136189984	unique tokens whereas only 22% tokens occur in the pre-built GloVe word vectors1. Although character-level representation which models sub-word morphologies can alleviate this problem to some extent (Huang et al., 2013; Bojanowski et al., 2016; Kim et al., 2016), character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. ‘car’ an
2785414597	Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus	2250539671	yer, the number of hidden units with ReLu activation was set to 256. We did not use dropout and regularization. Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors 5 (Pennington et al., 2014). For character-level embedding, we used one hot encoding with 69 characters (68 ASCII characters plus one unknown character). Both word embedding and character embedding matrix were ﬁxed during the t
2785591836	Recent Advances in Neural Program Synthesis.	2553303224	application is optimizing the learning process over the commonalities of multiple tasks, as seen in [25]. This form will lead to quick adaptation to any particular task. Another application, seen in [32], is extending learning to not only change parameter values but also hyperparameters, including the model architecture itself. 5.3 Conclusion Neural program synthesis is a ﬁeld with very lofty goals a
2785591836	Recent Advances in Neural Program Synthesis.	2511174833	ble to perform well in online coding challenges, and the neural network is used to deduce patterns and insights from the problem speciﬁcation. There is also the idea of the Differentiable Interpreter [13], which learns programs by correcting itself when the instructions it chooses are executed at inference time. The differentiable interpreter framework, however, is demonstrated as merely a starting po
2785591836	Recent Advances in Neural Program Synthesis.	2604763608	ch in metaoptimization. This term refers to the notion of optimizing an optimization procedure. One application is optimizing the learning process over the commonalities of multiple tasks, as seen in [25]. This form will lead to quick adaptation to any particular task. Another application, seen in [32], is extending learning to not only change parameter values but also hyperparameters, including the m
2785591836	Recent Advances in Neural Program Synthesis.	2108598243	does not make for a cohesive, comprehensive and rigorous dataset. Until the deep learning and/or program synthesis communities launch a large scale effort to create large-scale datasets like ImageNet [2], this class of problems will remain very challenging. 4 Strategies to Reshape Program Induction It may be surprising to know that even for a ﬁeld as new as neural program induction, fundamental model
2785591836	Recent Advances in Neural Program Synthesis.	2550120381	of other columns. String transformations can be summarized with a fairly simple DSL, consisting of concatenations and slicing based on conditions. &quot;Neuro-Symbolic Program Synthesis&quot; (NSPS) [28] and &quot;Robustﬁll&quot; [24] are two works that use deep learning methods to also tackle the FlashFill problem. NSPS proposes a new architecture called the Recursive Reverse-Recursive Neural Networ
2785591836	Recent Advances in Neural Program Synthesis.	2214429195	ent programs required to answer its questions. Hence, the solutions may be compositional, but require fewer steps than there are tokens in the questions. Figure 6: Schematic for the Neural Programmer [17]. The input is a sequence of tokens and so takes multiple time steps to run. On the other hand, Neural RAM tries to specialize in creating highly compositional programs. Since the model’s 14 pre-deﬁne
2785591836	Recent Advances in Neural Program Synthesis.	2586050494	ertain aspects of program induction. 4.1 Structured Attention While attention has proven to be an important tool for deep learning, the approach has more promise yet. In Structured Attention Networks [26], the researchers attempt to dismantle some assumed limitations of attention mechanisms. The paper makes the key observation that soft attention mechanisms are designed to attend over individual compo
2785591836	Recent Advances in Neural Program Synthesis.	1945616565	es. But while CNNs are powerful in their ability to &quot;perceive&quot; and &quot;intuit&quot; the correct answers, they also fall prey to tricky &quot;edge cases&quot; known as adversarial examples [8]. We should be wary of any program induction black box since establishing guarantees on correctness is made far more difﬁcult. We should ask ourselves, what sort of black box would the human program s
2785591836	Recent Advances in Neural Program Synthesis.	2601273560	formations can be summarized with a fairly simple DSL, consisting of concatenations and slicing based on conditions. &quot;Neuro-Symbolic Program Synthesis&quot; (NSPS) [28] and &quot;Robustﬁll&quot; [24] are two works that use deep learning methods to also tackle the FlashFill problem. NSPS proposes a new architecture called the Recursive Reverse-Recursive Neural Network (R3NN), which can be thought
2785591836	Recent Advances in Neural Program Synthesis.	2607264901	h large problem sizes of the Traveling Salesman Problem, the network is liable to produce invalid results with signiﬁcant probabilities. In &quot;Learning Combinatorial Optimization Over Graphs&quot; [23] the researchers demonstrate high quality results when the neural programming model is constrained to produce greedy algorithms. In particular, reinforcement learning was a potent paradigm for this ta
2785591836	Recent Advances in Neural Program Synthesis.	2251957808	hat are applicable for the tables is even harder. Oftentimes, the only realistic solution is to crowd-source this data online from sources like Amazon Mechanical Turk and have an active learning loop [10] that can make training more efﬁcient. Figure 9: Seq2SQL [31] uses a pointer network to select question words and column names in order to construct an SQL query. Program induction/synthesis tasks tha
2785591836	Recent Advances in Neural Program Synthesis.	2610002206	ion to construct a program, but make modiﬁcations to the DSL so as to increase the vocabulary by making compositional programs into literal ones. In a third work, &quot;Abstract Syntax Networks&quot; [29], the researchers also built AST representations of programs but instead for general purpose programming languages. A notable facet of the architecture design is that the decoder which generates the A
2785591836	Recent Advances in Neural Program Synthesis.	2133564696	mechanism known as attention. Attention is valuable because it lets a decoder access the information of each encoder state without producing a bottleneck. Neural machine translation techniques as in [7] use this to consider each input token individually when formulating the output. More broadly, however, attention distributions can be thought of as simply generating a non-negative distribution that
2785591836	Recent Advances in Neural Program Synthesis.	2250539671	ng tasks. A major practical breakthrough for RNNs was the creation of gated recurrence which is nowadays most often implemented with the Long Short-Term Memory (LSTM) cell. Given data embeddings e.g. [5], LSTM networks are effective at classiﬁcation and regression tasks such as image captioning. These networks often exploit an encoder-decoder framework, in which an LSTM network is ﬁrst tasked with it
2785591836	Recent Advances in Neural Program Synthesis.	2173051530	nsors of order greater than 1 because the convolution operation picks up information in spatial neighborhoods of the data. One particularly successful neural program induction model is the Neural GPU [15], which is recurrent, but each &quot;timestep&quot; involves a gated convolutional operation. Input data is ﬁrst embedded into a 3D tensor called the &quot;internal state&quot; of the model which is t
2785591836	Recent Advances in Neural Program Synthesis.	2610002206	o create a constructor. This is, in some ways, similar to the sub-function calls used in the Neural Programmer-Interpreter. Figure 12: A program can be suitably represented in an abstract syntax tree [29]. This, however, requires attention and function hierarchy to be fully effective. These papers showcase applications of (hierarchical) attention, program embeddings, management of function hierarchy,
2785591836	Recent Advances in Neural Program Synthesis.	2214429195	becomes prohibitively hard to utilize. 2.5 Memory and Pre-Deﬁned Primitives All the previous models have been purely connectionist in their design. Now, we will explore two models, Neural Programmer [17] and Neural RAM [16] that can only apply explicitly deﬁned transformations on data. In particular, the controllers do not issue direct read/write instructions to memory, and instead, perform one of se
2785591836	Recent Advances in Neural Program Synthesis.	2167839676	size regime within which they are trained, it is nearly impossible to guarantee performance on greater problem sizes and verify corner cases. 2.4 Attention with Memory The Neural Turing Machine (NTM) [4] introduced the notion of augmenting a neural network with external memory. In effect, the neural network now acts as a controller, issuing read and write commands to the memory, rather than having to
2785591836	Recent Advances in Neural Program Synthesis.	1933349210	uction/synthesis tasks that require query parsing are by far the hardest to succeed at. It isn’t that language is hard to parse, since there are numerous papers which involve question answering (e.g. [6]). Rather, there is simply the additional challenge of gathering enough high quality data for the program synthesis tasks. It is expensive to curate pairs of (natural language, program language) or (p
2785591836	Recent Advances in Neural Program Synthesis.	2550492213	yles. This also relates to the NLP problem, since naming conventions could fall under this category. An example of favoring human source code bias using traditional program synthesis techniques is in [12]. 6. Building more complete solutions will no doubt require greater automation in learning. Current methods rely heavily on hand-crafted curricula to incrementally challenge a model and improve genera
2785591836	Recent Advances in Neural Program Synthesis.	2550100435	ynthesis domain. Other fascinating works in neural program synthesis are less connected and show the diversity of the challenges the ﬁeld is tackling. For example, the &quot;Deepcoder&quot; framework [21] uses the neural network component to augment and improve the perfomance of traditional program synthesis techniques like SMT solvers. Their objective is to be able to perform well in online coding ch
2785629297	Tools and resources for Romanian text-to-speech and speech-to-text applications.	2336737996	o help linguists in their study of spoken language. It enables one to query our oral corpus using combinations of wordforms, lemmas and partof-speech tags. It is currently part of the COROLA project (Tuﬁs et al., 2015) and it is hosted in the RACAI cloud4, but, if desired we are willing to provide access to our code-base and help deploy the platform on-site. 3In our experiments we set the threshold to 5 for frequen
2785638793	A Neurobiologically Motivated Analysis of Distributional Semantic Models.	1822239915	isingresult was that emotionalattributes were not predictedas accuratelyas social and cognitiveones, although a large number of NLP studies have demonstrated successful results of sentiment analysis (Taboada, 2016). From a cognitive science (or embodied cognition) perspective, however,this result suggests that emotional information is more likely to be acquired from direct emotional experiences than from lingui
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2133564696,2172140247	al-valued vector representation and another layer for decoding that representation into the output of the target language. Various variants of that 4 model have been proposed in the recent literature [17,18] with the same general idea of compressing a sentence into a representation that captures all necessary aspects of the input to enable proper translation in the decoder. An important requirement is th
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2251803266	applied to a number of practical tasks. Predictive models based on neural network classiers and neural language models [8,9] have superseded models that are purely based on co-occurrence counts (see [10] for a comparison of common approaches). Semantic vector spaces show even interesting algebraic properties that re ect semantic compositionality, support vector-based reasoning and can be mapped acros
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2222949842	that idea. Various multilingual extensions of NMT have already been proposed in the literature. The authors of [19,20] apply multitask learning to train models for multiple languages. Zoph and Knight [21] propose a multi-source model and [22] introduces a character-level encoder that is shared across several source languages. In our setup, we will follow the main idea proposed by Johnson et al. [23].
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2133564696	lustration in Figure 1 includes an important addition to the model, a so-called attention mechanism. Attention makes it possible to focus on particular regions from the encoded sentence when decoding [18] and, with this, the representation becomes much more exible and dynamic and greatly improves the translation of sentences with variable lengths. meaning Learning Algorithm neural network language dat
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2141599568	rison of common approaches). Semantic vector spaces show even interesting algebraic properties that re ect semantic compositionality, support vector-based reasoning and can be mapped across languages [11,12]. Multilingual models have been proposed as well [13,14]. Neural language models are capable of integrating multiple languages [15], which makes it possible to discover relations between them based on
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2172589779,2251743902	and strong abstractions in C can be expected. Figure 2 illustrates the intuition behind that idea. Various multilingual extensions of NMT have already been proposed in the literature. The authors of [19,20] apply multitask learning to train models for multiple languages. Zoph and Knight [21] propose a multi-source model and [22] introduces a character-level encoder that is shared across several source l
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2550821151	t [21] propose a multi-source model and [22] introduces a character-level encoder that is shared across several source languages. In our setup, we will follow the main idea proposed by Johnson et al. [23]. The authors of that paper suggest a simple addition by means of a language ag on the source language side (see Figure 2) to indicate the target language that needs to be produced by the decoder. Thi
2785847164	Emerging Language Spaces Learned From Massively Multilingual Corpora.	2155734786	tudies. Dyvik [1] proposes to use word translations to discover lexical semantic elds, Carpuat et al. [2] discuss the use of parallel corpora for word sense disambiguation, van der Plas and Tiedemann [3] present work on the extraction of synonyms and Villada and Tiedemann [4] explore multilingual word alignments to identify idiomatic expressions. The idea of cross-lingual disambiguation is simple. Th
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	2508664155	/facts). Thus, we further enrich Ku with external KB triples14. Given a relation rand an observed triple (s , t) in training or testing, the pair (s, ) is regarded as a +ve instance for r. Following (Wang et al., 2016a), for each +ve instance (s, t), we generatetwonegative ones, one byrandomly corrupting the source s, and the other by corrupting the target t. Note that, the test triples are not in KB or Ku and non
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	2128407051	Even if some existing systems can use very large knowledge bases either harvested from a large data source such as the Web or built manually, these KBs still miss a large number of facts (knowledge) (West et al., 2014). It is thus important for a chatbot to continuously learn new knowledge in the conversation process to expand its KB and to improve its conversation ability. In recent years, researchers have studied
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	277886906	hod cannot handle unknown relations. Thus, these methods are not suitable for our open-world setting. None of the existing KB inference methods perform interactive knowledge learning like LiLi. NELL (Mitchell et al., 2015) continuously updates its KB using facts extracted from the Web. Ourtask is very different as wedo not do Webfact extraction (which isalso useful). Wefocus onuser interactions in this paper. Our work
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	2337601653	modeling synthetic tasks. LiLi formulates query-speciﬁc inference strategies which embed interaction behaviors. Also, no existing dialogue systems (Vinyals and Le, 2015; Li et al., 2016a; Bordes and Weston, 2016; Weston,2016;Zhang et al.,2017)employ lifelong learning to train prediction models by using information/knowledge retained in the past. Our work is related to general lifelong learning in (Chen and L
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	2508664155	relations linking them. In our work, we adopt the latest PR method, C-PR (Mazumder and Liu, 3We choose path-ranking (PR) due to its high interpretability and better performance than latent features (Wang et al., 2016a; Toutanova, 2015) User: (Obama, CitizenOf, USA)?“Is Obama a citizen of USA?” [Query] LiLi: I do not know what “CitizenOf” mean? Can you provide me an example? [Ask for Clue] User: (David Cameron, Ci
2785917544	Towards an Engine for Lifelong Interactive Knowledge Learning in Human-Machine Conversations	2508664155	from the Web. Ourtask is very different as wedo not do Webfact extraction (which isalso useful). Wefocus onuser interactions in this paper. Our work is related to interactive language learning (ILL) (Wang et al., 2016b, 2017), but these are not about KB completion. The work in (Li et al., 2016b) allows a learner to ask questions in dialogue. However, this work used RL to learn about whether to ask the user or not.
2785931413	The Reification of an Incorrect and Inappropriate Spreadsheet Model.	1559906461	and the above review. Although recollection of the meeting is not exact, it is certain that the quality of the model and the data upon which it relied were positively asserted with undue confidence [Panko, 2003]. No mention was made of any testing [Pryor, 2004; Panko, 2000 &amp; 2006] that had taken place and no documentation [Pryor, 2006] was produced. A copy of the financial model and the presentation mad
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2107019937	acquisition can go beyond mapping language as input patterns to output labels for merely obtaining high rewards or accomplishing tasks. We take a step further to require the language to be grounded (Harnad, 1990). Speciﬁcally, we consult the paradigm of procedural semantics (Woods, 2007) which posits that words, as abstract procedures, should be able to pick out referents. We will attempt to explicitly link w
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2189070436	ant to robotics navigation under language commands (Chen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA) (Antol et al., 2015; Gao et al., 2015; Ren et al., 2015; Lu et al., 2016; Yang et al., 2016; Anderson et al., 2017; de Vries et al., 2017). The interactive setting of learning to accomplish tasks is similar to that of learning to play vi
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2611884151	of which is believed to be the basis of meaning (Kiela et al., 2016). Contemporary to our work, several end-to-end systems also address language grounding problems in a simulated world with deep RL. Misra et al. (2017) maps instructions and visual observations to actions of manipulating blocks on a 2D plane. Hermann et al. (2017); Chaplot et al. (2018) learn to navigate in 3D under instructions, and both evaluate Z
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	1575833922	did, we train a CNN from scratch to accommodate to XWORLD. The CNN is conﬁgured as the one employed by our model. The rest of our model is unchanged. VIS-LSTM [VL] An adaptation of a model devised by Ren et al. (2015) which was originally proposed for VQA. We ﬂatten hand project it to the word embedding space RD. Then it is appended to the input sentence sas the ﬁrst word. The augmented sentence goes through an LS
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2236233024	ding language in images and videos (Yu &amp; Siskind, 2013; Gao et al., 2016; Rohrbach et al., 2016). The NAV task is relevant to robotics navigation under language commands (Chen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA) (Antol et al., 2015; Gao et al., 2015; Ren et al., 2015; Lu et al., 2016; Yang et al., 2016; Anderson et al., 2017; d
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2620290674	e a much larger space of zero-shot sentences and additionally require ZS2 generalization, which is in fact a transfer learning (Pan &amp; Yang, 2010) problem. Other recent work (Andreas et al., 2017; Oh et al., 2017) on zero-shot multitask learning treats language tokens as (parsed) labels for identifying skills. In these papers, the zero-shot settings are not intended for language understanding. Our learning pro
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2553882142	e other worlds, we have a much larger space of zero-shot sentences and additionally require ZS2 generalization, which is in fact a transfer learning (Pan &amp; Yang, 2010) problem. Other recent work (Andreas et al., 2017; Oh et al., 2017) on zero-shot multitask learning treats language tokens as (parsed) labels for identifying skills. In these papers, the zero-shot settings are not intended for language understanding
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2463565445	guage commands (Chen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA) (Antol et al., 2015; Gao et al., 2015; Ren et al., 2015; Lu et al., 2016; Yang et al., 2016; Anderson et al., 2017; de Vries et al., 2017). The interactive setting of learning to accomplish tasks is similar to that of learning to play video games from pixels (Mnih et al.,
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2171810632	hen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA) (Antol et al., 2015; Gao et al., 2015; Ren et al., 2015; Lu et al., 2016; Yang et al., 2016; Anderson et al., 2017; de Vries et al., 2017). The interactive setting of learning to accomplish tasks is similar to that of learning to play video games from pixels (Mnih et al., 2015). 4 EXPERIMEN
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2611884151	ly, let the multimodal module be M, the action module be A, and the prediction module be P, this idea can be formulated as: NAV: A MpRNNpsq;CNNpeqq QA: P MpRNNpsq;CNNpeqq : (1) Hermann et al. (2017); Misra et al. (2017); Chaplot et al. (2018) all employ the above paradigm. In their implementations, M is either vector concatenation or element-wise product. For any particular word in the sentence, fusion with the imag
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2463565445	nism, is different from the existing visual attention models. Some attention models such as Xu et al. (2015); de Vries et al. (2017) violate deﬁnitions III and IV. Some work (Andreas et al., 2016a;b; Lu et al., 2016) violates deﬁnition IV in a way that language is fused with vision by a multilayer perceptron (MLP) after image attention. Anderson et al. (2017) proposes a pipeline similar to ours but violates deﬁni
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2521274174	ormation for performing the tasks. 4 Published as a conference paper at ICLR 2018 The beneﬁts of this framework are two-fold. First, the explicit grounding strategy provides a conceptual abstraction (Garnelo et al., 2016) that maps high-dimensional linguistic input to a lowerdimensional conceptual state space and abstracts away irrelevant input signals. This improves the generalization for similar linguistic inputs. G
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2611884151	his projected and concatenated with the embedding. The concatenated vector is used for both NAV and QA (Figure 18 Appendix D). This concatenation mechanism is also employed by Hermann et al. (2017); Misra et al. (2017). NavAlone [NAVA] An ablation of our model that does not have the QA pipeline (M P and P) and is trained only on the NAV tasks. The rest of the model is the same. In the following experiments, we trai
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2247513039	ro-shot settings are not intended for language understanding. Our learning problem is related to some recent work on grounding language in images and videos (Yu &amp; Siskind, 2013; Gao et al., 2016; Rohrbach et al., 2016). The NAV task is relevant to robotics navigation under language commands (Chen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2171810632	tention map x loc. The ﬁlter covers the 3 3 neighborhood of each feature vector in the spatial domain. The rest of the model is unchanged. StackedAttentionNet [SAN]An adaptation of a model devised by Yang et al. (2016) which was originally proposed for VQA. We replace our interpreter with their stacked attention model to compute the attention map x loc. Instead of employing a pretrained CNN as they did, we train a
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	2730230212	tern of object pairs in the image without having to recognize the objects. This suggests that neural networks tend to exploit data in an unexpected way to achieve tasks if no constraints are imposed (Kottur et al., 2017). To sum up, our model exhibits a strong generalization ability on both ZS1 and ZS2 sentences, the latter of which pose a great challenge for traditional language grounding models. Although we use a p
2785955089	Interactive Grounded Language Acquisition and Generalization in a 2D World	1575833922	vigation under language commands (Chen &amp; Mooney, 2011; Tellex et al., 2011; Barrett et al., 2015). The QA task is relevant to image question answering (VQA) (Antol et al., 2015; Gao et al., 2015; Ren et al., 2015; Lu et al., 2016; Yang et al., 2016; Anderson et al., 2017; de Vries et al., 2017). The interactive setting of learning to accomplish tasks is similar to that of learning to play video games from pix
2785972710	Syntax and Semantics of Italian Poetry in the First Half of the 20th Century	2280580494	ce NCS) may also depend on its frequency of use in given contexts. Italian noncanonical structures are relatively highly represented, as the following table shows, as extracted from VIT, our treebank [9], where they have been explicitly marked with the labels indicated below: NCS/Types LDC S_DIS S_TOP S_FOC DiscMods Total % Non Project. % NCS/ TSSen Counts 251 1037 2165 266 12,437 16,156 7% 84.59% Ta
2785972710	Syntax and Semantics of Italian Poetry in the First Half of the 20th Century	2280580494	e taken from the Anthology of Poetry [20] published by Einaudi in 1994 and republished by L’Espresso collected from texts ranging from 1980 to 2000 of contemporary written and spoken Italian(see [7]; [9]; [13]; [14]) . The comparison will allow to distinguish uncommon non-canonical syntactic structures in poetry from common ones, that can also be found in texts taken from daily newspapers and politic
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	1983440957	[cs.CL] 2 Feb 2018 2 (00:08.33) (00:16.67) (00:25.00) (00:33.33) (00:41.67) Entire Video A. B. Fig. 1. Emoji prediction used for video summarization and query-by-emoji, adapted from our previous work [8]. A. The emoji summarization of the entire video presents a more complete representation of the video’s contents than a single screenshot might. B. Emoji can be used as a language-agnostic query langu
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	1982029265	c understanding of the accompanying conversation transcript. Like emoji, new modalities are sometimes the result of a newly developed technology, as with 3D models [15] or the growth of microblogging [2]. Though ideograms are ancient, emoji is a modern technological evolution of that ancient idea. The march of technology sometimes facilitates new looks at old problems, such as the use of infrared ima
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	1971709890	e problem of emoji prediction [8], and approached from a zero-shot perspective due to a lack of established dataset. Following on from the work, a query-by-emoji video search engine was also proposed [9]. These works reported quantitative results only on related tasks in other modalities, and presented only qualitative results for the emoji modality. We instead present results on a large scale, real-
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2048261712	egarding the emoji and must then try to determine its relevancy to images or text. This task shares some resemblance to that of zero-shot image classiﬁcation [3], [31] or zero example video retrieval [10], [18]. Generally, in zero-shot classiﬁcation the model has a disjoint set of seen and unseen classes, and attempts to leverage the knowledge of seen classes as well as external information to classif
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2736116184	eral user-deﬁned databases of emoji meaning. Their later work then uses this data to learn a model for sentiment analysis which performs comparably to models trained directly on real world usage data [45]. This kind of structured, pre-deﬁned understanding of emoji is similar to the no-example approach explored in our previous work [8] and further explored in this work. This work, however, targets emoj
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2535739320	m of understanding emoji usage through building models on top of real world usage data, there has also been work on trying to build an emoji understanding in a more hand-crafted fashion. For example, [44] acquires a structured understanding of emoji usage through combining several user-deﬁned databases of emoji meaning. Their later work then uses this data to learn a model for sentiment analysis which
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2161956125	nd their relationship with already established information channels. One way in which to do this is to explore the cross-modal relationships between the modality and other modalities. When Lee et al. [19] identiﬁed nonverbal head nods as an information-rich and overlooked modality, they sought to provide understanding through prediction of them based on semantic understanding of the accompanying conve
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2013075750	ng the emoji and must then try to determine its relevancy to images or text. This task shares some resemblance to that of zero-shot image classiﬁcation [3], [31] or zero example video retrieval [10], [18]. Generally, in zero-shot classiﬁcation the model has a disjoint set of seen and unseen classes, and attempts to leverage the knowledge of seen classes as well as external information to classify the
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2106947945	nological evolution of that ancient idea. The march of technology sometimes facilitates new looks at old problems, such as the use of infrared imagery for facial recognition instead of natural images [43]. Often, the presentation of new tasks as research challenges can accelerate research progress, as it did with acoustic scenes [39] and video concepts [38]. We look to this history of multimedia chall
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2064675550	nsists of a bidirectional LSTM, which processes the text both in standard order and reverse order, on top of a word embedding layer [25]. LSTMs use their memory to help emphasize relevant information [17], but there is still a degradation of information propagation. The bi-directional nature of the LSTM helps to combat this effect and ensure that information from the beginning of the sentence isn’t lo
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	1983440957	o training data available. Query-by-Emoji seeks to retrieve relevant multi-modal documents using queries composed with emoji. Our previous work was the ﬁrst to look at the problem of emoji prediction [8], and approached from a zero-shot perspective due to a lack of established dataset. Following on from the work, a query-by-emoji video search engine was also proposed [9]. These works reported quantit
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2153579005	To place an emoji within this embedding space without the need for training examples, a short textual description of the emoji can be used as its representation. We utilize a word2vec representation [26] that is pre-trained on a corpus of millions of lines of text accompanying Flickr photos [41]. Input modalities are then embedded in this shared space, where relationships between items are evaluated
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	1983440957	rms comparably to models trained directly on real world usage data [45]. This kind of structured, pre-deﬁned understanding of emoji is similar to the no-example approach explored in our previous work [8] and further explored in this work. This work, however, targets emoji as a rich, informative modality rather than only a means to perform sentiment analysis. [22] is an early investigation into the co
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2736116184	s to make emoji a challenging modality for algorithmic understanding, worthy of pursuing and with a high ceiling for perfection. The relationship among emoji themselves has been studied in [6], [33], [45]. The work of [33] gives a thorough analysis of emoji usage, and proposes a model for analyzing the relatedness of pairs of emoji. Similarly, [6] looks at the problem of trying to identify text tokens
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2740582239	ted only qualitative results for the emoji modality. We instead present results on a large scale, real-world emoji dataset, with proposed tasks and state-of-the-art supervised baselines. Felbo et al. [14] learn a model to predict emoji based on input text. Rather than using the model directly for the task of emoji prediction, they use this model as a form of pre-training for learning a sentiment predi
2786142957	New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval	2097117768	can also train a model for image-to-emoji prediction using our data. We use a CNN to represent images accompanying tweets. It is a GoogLeNet architecture trained to predict 13k ImageNet classes [24], [40]. We use the representation yielded at the penultimate layer for our image input. We train a single soft-max layer on top of this representation with emoji prediction as the objective, with the weight
2786145718	Natural Language Inference over Interaction Space: ICLR 2018 Reproducibility Report.	1840435438	tages of training (to stay consistent with the paper). We tried another run with the additional dropout layers. 3 RESULTS We have trained the network on Stanford’s Natural Language Inference dataset (Bowman et al., 2015). The paper also reports results for MultiNLI (Williams et al., 2017) and Quora Question Pairs datasets, but we didn’t have time to run the models on those. We also didn’t try ensembling. 3https://git
2786167132	A Formal Definition of Importance for Summarization.	1882070667	gated background knowledge for update summarization with Bayesian surprise. Several techniques have replaced distributions over ngrams by distributions over topics (Celikyilmaz and Hakkani-Tur¨ ,2011;Delort and Alfonseca, 2012;Hennig,2009). Divergences have also been used to devise evaluation metrics:Lin et al.(2006) proposed to measure the similarity between system summaries and reference summaries based on JS divergence.
2786198934	Online Learning for Effort Reduction in Interactive Neural Machine Translation.	1514971736	d, aiming to increase the productivity of this process, thereby diminishing the human eort required. Among them, interactive machine translation (IMT) (Barrachina et al.,2009;Casacuberta et al.,2009;Foster et al., 1997) is one of the most attractive strategies. The IMT framework introduces the human corrector into the editing process: the system reacts to each human action. As the user makes a correction, the system
2786198934	Online Learning for Effort Reduction in Interactive Neural Machine Translation.	2625092622	En!De and En!Fr. But in this case, their static systems were much worse than ours (13:1/21:2, for En!De/En!Fr). NMT 15 systems perform better than PB-SMT if they have a large amount of training data (Koehn and Knowles, 2017), as in this case. Even though, OL was able to rene a strong NMT system trained with a large corpus. 7.1.2. Interactive machine translation with online learning Table 6: KSMR [%] eort required for a
2786198934	Online Learning for Effort Reduction in Interactive Neural Machine Translation.	2594990650	ented using either Tensor ow (Abadi et al.,2016) or Theano (Theano Development Team,2016). In our experiments, used the latter framework. Taking advantage of the extensive experimentation conducted byBritz et al. (2017), we set the dimensions of encoder, decoder, attention model and word embedding to 512. Due to hardware restrictions, we used a single layer for encoder and decoder. The encoder was a LSTM network, wh
2786198934	Online Learning for Effort Reduction in Interactive Neural Machine Translation.	2064675550	m the second LSTM block at the previous time-step (st t1), obtaining the intermediate representation s0: s0 t= LSTM 1(E(y );s ) (2) The LSTM1 function is dened according to the following equations1 (Hochreiter and Schmidhuber, 1997;Gers et al.,2000): s0 t= o 0 tc 0 t c0 t= f 0 tc 0 t 1 + i 0 tc~ 0 t ~c 0 t= tanh(WcE(y 1) + Ucs ) f0 t= ˙(W0 fE(y 1) + U 0 fst 1) i0 t= ˙(W0 tE(y 1) + U 0 ts 1) o0 t= ˙(W0 oE(y 1) + U 0 ost 1) wh
2786198934	Online Learning for Effort Reduction in Interactive Neural Machine Translation.	2118434577	uences. This attentional NMT system was the basis of many works, which aimed to tackle its main issues, namely the management of large vocabularies and the out-of-vocabulary problem (Jean et al.,2015;Luong et al., 2015). The most satisfactory solution, was the use of subword sequences instead of words (Luong and Manning,2016;Sennrich et al.,2016). This has become a de facto standard in NMT. NMT systems are typically
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	1816313093	ation in the sentence. In all experiments the NMT vocabulary is limited to the most common 30K words in both languages and we preprocess source and target language data with Byte-pair encoding (BPE) (Sennrich et al., 2016) using 30K merge operations. We also use a Phrase-based translation system similar to Moses (Koehn et al., 2007) as baseline to explore PBMT performance for idiom translation. 4. Idiom Translation Eva
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	1902237438	and an attention mechanism, generates the target translation (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014). We use a 4-layer attention-based encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension size of 1,000, and batch size of 80 for 20 epochs. WMT test sets 2008-2016 Idiom test set Model BLEU BLEU Unigram Precision Word-level Accuracy PBMT Baseline 20.2 19.7 5
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2101105183	no cost and have the advantage of replicability. We use the following metrics to evaluate the translation quality with a speciﬁc focus on idiom translation accuracy: BLEU The traditional BLEU score (Papineni et al., 2002) is a good measure to determine the overall quality of the translations. However this measure considers the precision of all n-grams in a sentence and by itself does not focus on the translation quali
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2133564696	e the problem of idiom translation we also provide the output of three NMT systems for this sentence: GoogleNMT (Wu et al., 2016), DeepL1, and the OpenNMT implementation (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). All systems fail to generate the proper translation of the idiom expression. This problem is particularly pronounced when the source idiom is very different from its equivale
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2133564696	ence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous LSTM hidden states and an attention mechanism, generates the target translation (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014). We use a 4-layer attention-based encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension size of 1,000, and batch size of
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2130942839	an encoder builds up a representation of the source sentence and a decoder, using the previous LSTM hidden states and an attention mechanism, generates the target translation (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014). We use a 4-layer attention-based encoder-decoder model as described in (Luong et al., 2015) trained with hidden dimension size of 1,000, and batch size of 80 for 20 epochs. WMT te
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2148708890	he idiom translation in the sentence, we look at the word-level alignments between the idiom expression in the source sentence and the generated translation in the target sentence. We use fast-align (Dyer et al., 2013) to extract word alignments. Since idiomatic phrases and the respective translations are not contiguous in many cases we only compare the unigrams of the two phrases. Note that for this metric we have
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2133564696	nary NMT experiments as the ﬁrst step towards better idiom translation. Keywords:multiword expression, idioms, bilingual corpora, machine translation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance,
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	1902237438	slation we also provide the output of three NMT systems for this sentence: GoogleNMT (Wu et al., 2016), DeepL1, and the OpenNMT implementation (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). All systems fail to generate the proper translation of the idiom expression. This problem is particularly pronounced when the source idiom is very different from its equivalent in the target languag
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	110339475	uzny and Zettlemoyer, 2013; Markantonatou et al., 2017), there is limited work on building a parallel corpus annotated with idioms, which is necessary to investigate this problem more systematically. Salton et al. (2014) selected a small subset of 17 English idioms, collected 10 sentence examples for each idiom from the internet, and manually translated them into Brazilian-Portuguese to use for the translation task.
2786226395	Examining the Tip of the Iceberg: A Data Set for Idiom Translation.	2130942839	the ﬁrst step towards better idiom translation. Keywords:multiword expression, idioms, bilingual corpora, machine translation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance, subject-verb agreement,
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2513276162	the context of how the importance of a word is deﬁned, this task has found use in varieties of applications such as text summarization (Hong and Nenkova, 2014; Yih et al., 2007), text classiﬁcation (Sheikh et al., 2016), or speech synthesis (Mishra et al., 2007). Our laboratory is currently designing a system to beneﬁt people who are deaf or hard-of-hearing (DHH) who are engaged in a live meeting with hearing collea
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	1884372994	erall meaning of the text. Depending on the context of how the importance of a word is deﬁned, this task has found use in varieties of applications such as text summarization (Hong and Nenkova, 2014; Yih et al., 2007), text classiﬁcation (Sheikh et al., 2016), or speech synthesis (Mishra et al., 2007). Our laboratory is currently designing a system to beneﬁt people who are deaf or hard-of-hearing (DHH) who are eng
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2168119002	iticisms of WER (McCowan et al., 2004; Morris et al., 2004), and several researchers have recommended alternative measures to better predict human task-performance in applications that depend on ASR (Garofolo et al., 2000; Mishra et al., 2011; Kaﬂe and Huenerfauth, 2016). Among these newly proposed metrics, a common theme has been: rather than simply counting the number of errors, it would be better to consider the im
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2064418625	stimation (Matsuo and Ishizuka, 2004) – as well as supervised methods that leverage various linguistic features from text to achieve strong predictive performance (Liu et al., 2011; Liu et al., 2004; Hulth, 2003; Sheeba and Vivekanandan, 2012). While this conceptualization of word importance as a keyword-extraction problem has led to positive results in arXiv:1801.09746v2 [cs.CL] 9 Feb 2018 the ﬁeld of text
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2158018156	ted over the years, including unsupervised methods using, e.g. Term Frequency x Inverse Document Frequency (TF-IDF) weighting (HaCohen-Kerner et al., 2005), word co-occurrence probability estimation (Matsuo and Ishizuka, 2004) – as well as supervised methods that leverage various linguistic features from text to achieve strong predictive performance (Liu et al., 2011; Liu et al., 2004; Hulth, 2003; Sheeba and Vivekanandan,
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2765816858	users, we have found that an evaluation metric designed for predicting the usability of an ASR-generated transcription as a caption text for these users could beneﬁt from word importance information (Kaﬂe and Huenerfauth, 2017). However, estimating the importance of a word has been challenging for our team thus far, because we have lacked corpora of conversational dialogue with word-importance annotation, for training a wor
2786331481	A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts.	2043004216	vekanandan, 2012). While this conceptualization of word importance as a keyword-extraction problem has led to positive results in arXiv:1801.09746v2 [cs.CL] 9 Feb 2018 the ﬁeld of text summarization (Litvak and Last, 2008; Wan et al., 2007; Hong and Nenkova, 2014), this approach may not generalize to other applications. For instance, given the sometimes meandering nature of topic transition in spontaneous speech dialo
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2342096063	, a set of 22,033 question-answer pairs (such as “Greece held its last Summer Olympics during which year?”) that can be answered using these tables. Demonstrating the ability for collection at scale, Sun et al. (2016) extract a total of 104 million tables from Wikipedia and the web, and develop a model that constructs relational chains between table rows using a deep-learning framework.1 Using their system and ta1
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2125444198	ating 4 separate pieces of knowledge to explainably answer, with some questions requiring much longer explanations. Though few QA solvers explicitly report the aggregation limits of their algorithms, Fried et al. (2015), Khabashi et al. (2016) and Jansen et al. (2017) appear to show limits or substantial decreases in performance after aggregating two pieces of knowledge. To the best of our knowledge, of systems that
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2471366537	of automatic generation, though relations are often represented as &lt; subject;relation;argument &gt; triples, Yin et al. (2015) create a large table containing 120M n-tuple relations using OpenIE (Etzioni et al., 2011), arguing that the extra expressivity afforded by these more detailed relations allows their system to answer more complex questions. Yin et al. use this to successfully reason over the WebQuestions d
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2250322043	e a standardized comparison of modern inference techniques against human performance, with individual QA solvers generally answering between 40% to 50% of multiple choice science questions correctly (Khot et al., 2015; Clark et al., 2016; Khashabi et al., 2016; Khot et al., 2017; Jansen et al., 2017, inter alia), and top-performing ensemble models nearly reaching a passing grade of 60% on middle school (8th grade)
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2125444198	e above systems share the commonality that they work to connect (or aggregate) multiple pieces of knowledge that, through a variety of inference methods, move towards the goal of answering questions. Fried et al. (2015) report that information aggregation for QA is the web introduce more noise into the inference process than the high-quality tables from Wikipedia currently very challenging, with few methods able to
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2338747497	erforming ensemble models nearly reaching a passing grade of 60% on middle school (8th grade) science exams during a recent worldwide competition of 780 teams sponsored by the Allen Institute for AI (Schoenick et al., 2017). One of the central shortcomings of question answering models is that while solvers are steadily increasing the proportion of questions they answer correctly, most solvers generally lack the capacity
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2101964891	hile the tables themselves are generally either collected from the web, automatically generated by extracting relations from free text, or manually constructed. At the collection end of the spectrum, Pasupat and Liang (2015) extract 2,108 HTML tables from Wikipedia, and propose a method of answering these questions by reasoning over the tables using formal logic. They also introduce the WikiTableQuestions dataset, a set
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2608309533	inst human performance, with individual QA solvers generally answering between 40% to 50% of multiple choice science questions correctly (Khot et al., 2015; Clark et al., 2016; Khashabi et al., 2016; Khot et al., 2017; Jansen et al., 2017, inter alia), and top-performing ensemble models nearly reaching a passing grade of 60% on middle school (8th grade) science exams during a recent worldwide competition of 780 te
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2252136820	ir system and ta1Sun et al. (2016) note that the 99 million tables extracted from ble store, Sun et al. demonstrate state-of-the-art performance on several benchmark datasets, including WebQuestions (Berant et al., 2013), a set of popular questions asked from the web designed to be answerable using the large structured knowledge graph Freebase (e.g. “What movies does Morgan Freeman star in?”). In terms of automatic g
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2342096063	ledge formalism for question answering that balances the cost of manually crafting highly-structured knowledge bases with the difﬁculties in acquiring this knowledge from free text (Yin et al., 2015; Sun et al., 2016; Jauhar et al., 2016). The methods for question answering over tables generally take the form of constructing chains of multiple table rows that lead from terms in the question to terms in the answer
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2161951443	red correctly, depending on the questions, the domain, and the knowledge and inference requirements. Standardized science exams have recently been proposed as a challenge task for question answering (Clark, 2015), as these questions have very challenging knowledge and inference requirements (Clark et al., 2013; Jansen et al., 2016), but are expressed in simple-enough language that the linguistic challenges ar
2786395785	WorldTree: A Corpus of Explanation Graphs for Elementary Science Questions supporting Multi-hop Inference.	2125444198	ser to 6), but inference methods tend to have difﬁculty aggregating more than 2 pieces of knowledge from free-text together due to the semantic or contextual “drift” associated with this aggregation (Fried et al., 2015). Because of the difﬁculty in assembling training data for the information aggregation task, some have arXiv:1802.03052v1 [cs.CL] 8 Feb 2018 Figure 1: An example multiple choice science question, the
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	836999996	; Bordes et al. 2015; Rajpurkar et al. 2016; Nguyen et al .2016; Onishi et al 2016; Richardson, Burges, and Renshaw 2013; Berant et al. 2014) and Conversation Systems (Ritter, Cherry, and Dolan 2010; Lowe et al. 2015; Banchs 2012; Bordes and Weston 2016) have received a lot of attention in the recent past, we would like to focus on such real life settings encountered by chatbots which involve a combination of QA
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2252016937	; Wang, Smith, and Mitamura 2007; Yang, Yih, and Meek 2015; Berant et al. 2013; Bordes et al. 2015; Rajpurkar et al. 2016; Nguyen et al .2016; Onishi et al 2016; Richardson, Burges, and Renshaw 2013; Berant et al. 2014) and Conversation Systems (Ritter, Cherry, and Dolan 2010; Lowe et al. 2015; Banchs 2012; Bordes and Weston 2016) have received a lot of attention in the recent past, we would like to focus on such re
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2250895046	15; Rajpurkar et al. 2016; Nguyen et al .2016; Onishi et al 2016; Richardson, Burges, and Renshaw 2013; Berant et al. 2014) and Conversation Systems (Ritter, Cherry, and Dolan 2010; Lowe et al. 2015; Banchs 2012; Bordes and Weston 2016) have received a lot of attention in the recent past, we would like to focus on such real life settings encountered by chatbots which involve a combination of QA and dialog. S
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2409591106	is then a concatenation of the Glove embedding (if available, 0s otherwise) and the KG embedding (if available, 0s otherwise). 3. Candidate generation: State of the art memory network based methods (Miller et al. 2016) learn to compute an attention function over the tuples in the KG based on the given question (or dialog context in our case). For large sized KGs, it is infeasible to compute the attention over the e
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2399880602	CSQA and there is no explicit Knowledge Graph associated with the conversations. Here again, neural network based (hierarchical) sequence to sequence methods (Luong et al. 2015; Serban et al. 2016a; Serban et al. 2017) have become the de facto choice. Recently, (Bordes and Weston 2016) proposed a dataset which contains knowledge graph driven goal oriented dialogs for the task of restaurant reservation. However, the
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2252016937	ing comprehension style QA (Rajpurkar et al. 2016; Nguyen et al .2016), (iv) cloze style QA (Mostafazadeh et al 2016; Onishi et al. 2016) (v) multiple choice QA (Richardson, Burges, and Renshaw 2013; Berant et al. 2014) Of the above QA tasks, factoid QA is most relevant to us as the questions in our CSQA dataset are factoid questions. Existing factoid QA datasets contain Simple Questions which can be answered from a
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2409591106	lues of each hyperparameter considered. On average, we found that the candidate generation step produces 10K candidate tuples, hence we kept upto 10K key value pairs in the memory network. Following (Miller et al. 2016), we set H= 2. We used Precision and Recall as the evaluation metrics which capture the percentage of entities in the ﬁnal decoder output that were correct and the percentage of actual entities that w
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2028175314	n speciﬁc domains such as education, entertainment, sports, etc. where it is often required to answer factual questions while being aware of the context of the conversation. While Question Answering (Voorhees and Tice 2000; Wang, Smith, and Mitamura 2007; Yang, Yih, and Meek 2015; Berant et al. 2013; Bordes et al. 2015; Rajpurkar et al. 2016; Nguyen et al .2016; Onishi et al 2016; Richardson, Burges, and Renshaw 2013;
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2409591106	nd QA, we propose a model which is a cross between (i) the HRED model (Serban et al. 2016a) which is one of the state of the art models for dialog systems and (ii) the key value memory network model (Miller et al. 2016) which is a state of the art QA system. Our model has the following components: 1. Hierarchical Encoder: The model contains a lower level RNN encoder which goes over the words in an utterance and comp
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2409591106	ni 2014) to embedding based methods (Bordes, Weston, and Usunier 2014; Bordes, Chopra, and Weston 2014; Yang et al. 2014) and state of the art Memory Networks based architectures (Bordes et al. 2015; Miller et al. 2016; Kumar et al. 2016). In this work, we experiment with Memory Network based architectures and make a case for the need of better architectures when going beyond simple questions. Since we are interest
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	836999996	ntial Question Answering (CSQA). Needless to say, CSQA is very different from the kind of conversations found in existing dialog datasets such as the Twitter (Ritter, Cherry, and Dolan 2010), Ubuntu (Lowe et al. 2015) and Movie Subtitles (Banchs 2012) datasets. Table 1 shows an example of one such conversation from our dataset containing a series of questions. Note that to answer the question in Turn 11, the bot n
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2252136820	oorhees and Tice 2000; Wang, Smith, and Mitamura 2007; Yang, Yih, and Meek 2015) where the aim is to answer a question from a collection of documents (ii) factoid QA over structured knowledge graphs (Berant et al. 2013; Bordes et al .2015; Serban et al 2016b) (iii) reading comprehension style QA (Rajpurkar et al. 2016; Nguyen et al .2016), (iv) cloze style QA (Mostafazadeh et al 2016; Onishi et al. 2016) (v) multip
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2557851318	QA and recommendation but unlike our dataset, their dataset does not contain coherently linked question answer pairs. Further, the KG is again much smaller (75K entities and 11 relations). Recently (Neelakantan et al. 2016) have explored complex question answering over around 18.5K queries from the WikiTableQuestions dataset, but their tables have less than 100 rows and a handful of columns whereas our complex QAs are g
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2341790067	QA over structured knowledge graphs (Berant et al. 2013; Bordes et al .2015; Serban et al 2016b) (iii) reading comprehension style QA (Rajpurkar et al. 2016; Nguyen et al .2016), (iv) cloze style QA (Mostafazadeh et al 2016; Onishi et al. 2016) (v) multiple choice QA (Richardson, Burges, and Renshaw 2013; Berant et al. 2014) Of the above QA tasks, factoid QA is most relevant to us as the questions in our CSQA dataset ar
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2175256910	r quantitative operations on this subgraph. Also the Knowledge Graph used in our work is orders of magnitude larger than those used in some existing works (Bordes and Weston 2016; Bordes et al. 2015; Dodge et al. 2015; Fader, Soderland, and Etzioni 2011) which lie at the intersection of QA and dialog. Having motivated the task of CSQA and highlighted its differences from existing work on dialog and QA, we now brie
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2251289180	range from semantic parsing based methods (Berant and Liang 2014; Fader, Zettlemoyer, and Etzioni 2014) to embedding based methods (Bordes, Weston, and Usunier 2014; Bordes, Chopra, and Weston 2014; Yang et al. 2014) and state of the art Memory Networks based architectures (Bordes et al. 2015; Miller et al. 2016; Kumar et al. 2016). In this work, we experiment with Memory Network based architectures and make a ca
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2028175314	Related Work Our work lies at the intersection of Question Answering and Dialog Systems. Question Answering has always been of interest to the research community starting from early TREC evaluations (Voorhees and Tice 2000) . Over the years various datasets and tasks have been introduced to advance the state of the art in QA. These datasets can be divided into 5 main types (i) TREC style Open Domain QA (Voorhees and Tic
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2252136820	required to answer factual questions while being aware of the context of the conversation. While Question Answering (Voorhees and Tice 2000; Wang, Smith, and Mitamura 2007; Yang, Yih, and Meek 2015; Berant et al. 2013; Bordes et al. 2015; Rajpurkar et al. 2016; Nguyen et al .2016; Onishi et al 2016; Richardson, Burges, and Renshaw 2013; Berant et al. 2014) and Conversation Systems (Ritter, Cherry, and Dolan 2010;
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2250225488	requiring logical, quantitative and comparative reasoning involving larger subgraphs of the KG as opposed to a single tuple. Solutions to the simple QA task range from semantic parsing based methods (Berant and Liang 2014; Fader, Zettlemoyer, and Etzioni 2014) to embedding based methods (Bordes, Weston, and Usunier 2014; Bordes, Chopra, and Weston 2014; Yang et al. 2014) and state of the art Memory Networks based arch
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2250895046	rk on dialog systems. Over the past few years three large scale dialog datasets, viz., Twitter-Conversations (Ritter, Cherry, and Dolan 2010), Ubuntu Dialogue (Lowe et al. 2017) and Movie-Dic Corpus (Banchs 2012) have become very popular. However, none of these datasets have the ﬂavor of CSQA and there is no explicit Knowledge Graph associated with the conversations. Here again, neural network based (hierarch
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2409591106	the second objective, we propose a model for CSQA which is a cross between a state of the art hierarchical conversation model (Serban et al. 2016a) and a key value based memory network model for QA (Miller et al. 2016). Through our experiments, we demonstrate the inadequacy of these models and highlight speciﬁc challenges that need to be addressed. It is also worth mentioning that the unambiguous (context independe
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2250895046	ss to say, CSQA is very different from the kind of conversations found in existing dialog datasets such as the Twitter (Ritter, Cherry, and Dolan 2010), Ubuntu (Lowe et al. 2015) and Movie Subtitles (Banchs 2012) datasets. Table 1 shows an example of one such conversation from our dataset containing a series of questions. Note that to answer the question in Turn 11, the bot needs to remember that the question
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2252136820	Table 1 already highlights some of the challenges involved in CSQA, we now discuss an orthogonal set of challenges which arise from the complexity of the questions. Existing datasets for Factual QA (Berant et al. 2013; Bordes et al. 2015; Yang, Yih, and Meek 2015) deal with Simple Questions, each of which can be answered from arXiv:1801.10314v2 [cs.CL] 4 Oct 2018 Turn State Utterance T1 Simple Q. USER : Can you te
2786472750	Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph	2175256910	ven goal oriented dialogs for the task of restaurant reservation. However, the size of the KG here is very small (&lt;10 cuisines, locations, ambience, etc.) and the dialog contains very few states. (Dodge et al. 2015) also uses a dataset for QA and recommendation but unlike our dataset, their dataset does not contain coherently linked question answer pairs. Further, the KG is again much smaller (75K entities and 1
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	2127411301	. The fast pace and interconnected nature of online communication is particularly conducive to new words, and social media provides a “birds-eye view” on the process of change (Androutsopoulos, 2011; Danescu-Niculescu-Mizil et al., 2013; Kershaw et al., 2016; Tsur and Rappoport, 2015). We use Reddit as an example online community to track the growth and decline of nonstandard words. Social dissemination Language changes as a result
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	2101761627	e heads (Kroch, 1989; Ito and Tagliamonte, 2003) or across nouns of different semantic classes (D’Arcy and Tagliamonte, 2015). However, the poor performance of automatic parsers on social media data (Eisenstein, 2013; Blodgett et al., 2016) and the limits of manual annotation may render this typical analysis difﬁcult or impossible. Future work should also investigate the possibility of more semantically-aware deﬁ
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	2252735083	were identiﬁed by langid.py (Lui and Baldwin,2012) as English. 4We restricted the vocabulary because of the qualitative analysis required to identify nonstandard words. dard words (Grieve et al.,2016;Kershaw et al., 2016). The ﬁrst set of words is ﬁltered by a Spearman correlation coefﬁcient above the 85th percentile (N = 15;017). From this set of words, one of the authors manually identiﬁed 1,120 words in set G(“grow
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	1490782231	ments generated by known bots and spam users2 and ﬁlter all comments created in well1Data downloaded from http://files.pushshift. io/reddit/comments/ (Accessed 1 October 2016). 2The same list used in Tan and Lee (2015) available online: https://chenhaot.com/data/multi-community/ README.txt (Accessed 1 October 2016). Total Monthly mean Comments 1,625,271,269 45,146,424 Tokens 56,674,728,199 1,574,298,006 Subreddits
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	2252735083	ords to potential adopters. Limitations One limitation in the study was the exclusion of orthographic and morphological features such as afﬁxation, which has been noted as a predictor of word growth (Kershaw et al., 2016). Future work should incorporate these features as additional predictors. Our study also omitted borrowings, unlike prior work in word adoption that has focused on borrowings (Chesley and Baayen,2010;
2786577295	Making “fetch” happen: The influence of social and linguistic context on nonstandard word growth and decline	2141141510	of words across forum users and threads to predict the words’ change in frequency in Usenet, ﬁnding a positive correlation between frequency change and both kinds of social dissemination. In contrast,Garley and Hockenmaier (2012) use the same metric to predict the growth of English loanwords on German hip-hop forums, and ﬁnd that social dissemination has less predictive power than expected. We seek to replicate these prior ﬁn
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2084168100,2127563440	[33] when using cosine similarity to compare proteins. It is traditionally challenging to evaluate semantic similarity measures, and their performances differ between biological problems and datasets [23,24,31,32]. The main advantage of Onto2Vec representations is their ability to be used in trainable similarity measures, i.e., problem- and dataset-specific similarity measures generated in a supervised way fro
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2158698691	+FN against the false-positive rate (FPR or 1 specificity) defined as FPR= FP FP+TN , where TPis the number of true positives, FPis the number of false positives and TNis the number of true negatives [11]. We used ROC curves to evaluate protein-protein interaction prediction of our method as well as baseline methods, and we reported the area under the ROC curve (ROCAUC) as a quantitative measure of cl
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2084168100	are annotated. We used cosine similarity to determine the similarity between vectors. To compare sets of vectors (representing GO classes) to each other, we used the Best Match Average (BMA) approach [32], where pairs of vectors are compared using cosine similarity. We term the approach in which we compared vectors generated from adding proteins to our knowledge base Onto2Vec; Onto AddVec when using c
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2084168100,2127563440	atures may be more or less relevant to define the notion of similarity. It has previously been observed that different similarity measures perform well on some datasets and tasks, and worse on others [23,24,31,32], without any measure showing clear superiority across multiple tasks. One possible way to define a common similarity measure that performs equally well on multiple tasks may be to establish a way to
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1614298861,2153579005	ch word that occurs in the corpus. The representation of a word in the vocabulary (and therefore of a class or property in O) is a vector that is predictive of words occurring within a context window [26,27](More technical details on the representation learning can be found in Section 5.2). Onto2Vec can also be used to learn vector-based representations of biological entities that use ontologies for anno
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2136961946	in clustering [28]. However, while some parts of ontologies, such as their underlying taxonomy or partonomy, can naturally be expressed as graphs in which edges represent well-defined axiom patterns [18,37], it is challenging to represent the full semantic content of ontologies in such a way [35]. It is possible to materialize the implicit, inferred content of formally represented knowledge bases throug
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2139315672	e, and the set of axiom in O(and O‘) a corpus of sentences. The vocabulary of this corpus consists of the classes and relations that occur in Oas well as the keywords used to formulate the OWL axioms [15,43]. Onto2Vec then uses a skip-gram model to learn a representation of each word that occurs in the corpus. The representation of a word in the vocabulary (and therefore of a class or property in O) is a
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1659833910	ec. The cosine similarity, cos sim, between two vectors Aand Bis calculated as cos sim(A;B) = AB jjAjjjjBjj ; (2) where ABis the dot product of Aand B. 11 We used Resnik’s semantic similarity measure [33] as the baseline for comparison. Resnik’s semantic similarity measure is widely used in biology [32]. It is based on the notion of information content which quantifies the specificity of a given class
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2084168100	ed into different types depending on how annotations (or instances) of ontology classes are incorporated or weighted, and the type of information from an ontology that is used to determine similarity [16,32]. Most similarity measures treat ontologies as graphs in which nodes represent classes and edges an axiom involved the connected classes [16,32]. However, not all the axioms in an ontology can natural
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1969271108	f a biological entity (or a class of biological entities) and one or more classes from an ontology, usually together with meta-data about the source and evidence for the association, the author, etc. [17]. Due to the wide-spread use of ontologies, several methods have been developed to utilize the information in ontologies for data analysis [20]. In particular, a wide range of semantic similarity meas
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1821527815	hether or not an entity is associated with a particular class, and the semantic content in ontologies (i.e., the subclass hierarchy) can be used to generate \semantically closed&quot; feature vectors [39]. Alternatively, the output of semantic similarity measures is widely used as features for machine learning applications, for example in drug repurposing systems [14] or identification of causative ge
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1529533208	hose defined by Linked Data [9]. These methods can be applied to predict new relations between entities in a knowledge graph, perform similarity-based predictions, reason by analogy, or in clustering [28]. However, while some parts of ontologies, such as their underlying taxonomy or partonomy, can naturally be expressed as graphs in which edges represent well-defined axiom patterns [18,37], it is chal
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1614298861,2153579005	hrough HermiT. 10 5.3 Representation learning using Word2Vec We treated an ontology as a set of axioms, each of which constitutes a sentence. To process the axioms syntactically, we used the Word2Vec [26,27] methods. Word2Vec is a set of neural-network based tools which generate vector representations of words from large corpora. The vector representations are obtained in such a way that words with simil
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1974896839	ide the means to formally structure the classes and relations within a domain, and are now employed by a wide range of biological databases, webservices, and file formats to provide semantic metadata [20]. Notably, ontologies are used for the annotation of biological entities such as genomic variants, genes and gene products, or chemicals, to classify their biological activities and associations [38].
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2154851992	lated efforts that use unsupervised learning to generate dense feature vectors for structured and semantically represented data. Notably, there is a large amount of work on knowledge graph embeddings [7,29,30], i.e., a set of feature learning methods applicable to nodes in heterogeneous graphs, such as those defined by Linked Data [9]. These methods can be applied to predict new relations between entities
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1974896839	ource and evidence for the association, the author, etc. [17]. Due to the wide-spread use of ontologies, several methods have been developed to utilize the information in ontologies for data analysis [20]. In particular, a wide range of semantic similarity measures has been developed [32] and applied to the similarity-based analysis of ontologies and entities annotated with them. Semantic similarity i
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2136961946	reat ontologies as graphs in which nodes represent classes and edges an axiom involved the connected classes [16,32]. However, not all the axioms in an ontology can naturally be represented as graphs [18,35,37]. A possible alternative may be to consider all axioms in an ontology when computing semantic; the challenge is to determine how each axiom should contribute to determine similarity beyond merely cons
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1821527815	tablished baseline methods, we further applied the Resnik’s semantic similarity measure [33] with the BMA approach, and we generated sparse binary vector representations from proteins’ GO annotations [39] and compared them using cosine similarity (termed Binary GO) (more technical details on the similarity measures used in Section 5.4). Furthermore, to evaluate the contribution of using an automated r
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	1614298861,2153579005	ts each context-target as a new observation, which works better for larger datasets. The skip-gram model has the added advantage of producing higher quality representation of rare words in the corpus [26,27]. Here, we chose the skip-gram architecture since it meets our need to produce high quality representations of all biological entities occurring in our large corpus, including infrequent ones. Formall
2786757148	Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations	2084168100	use of ontologies, several methods have been developed to utilize the information in ontologies for data analysis [20]. In particular, a wide range of semantic similarity measures has been developed [32] and applied to the similarity-based analysis of ontologies and entities annotated with them. Semantic similarity is a measure defined over an ontology, and can be used to measure the similarity betwe
2786790428	Universal Neural Machine Translation for Extremely Low Resource Languages	2550821151	ages can be trained equally. Multi-lingual NMT is quite appealing for low-resource languages; a number of papers highlighted the characteristic that make it a good ﬁt for that such asLee et al.(2016),Johnson et al. (2016),Zoph et al. ) andFirat et al. (2016). Multi-lingual NMT utilizes the training examples of multiple languages to regularize the models to avoid over-ﬁtting to the limited data of the smaller languages
2786790428	Universal Neural Machine Translation for Extremely Low Resource Languages	2133564696	Translation (NMT) (Bahdanau et al.,2015;Sutskever et al.,2014) is based on Sequence-to-Sequence encoder-decoder model along with an attention mechanism to enable better handling of longer sentences (Bahdanau et al., 2015). Attentional sequence-to-sequence models are modeling the log conditional probability of the arXiv:1802.05368v2 [cs.CL] 17 Apr 2018 Figure 1: BLEU scores reported on the test set for RoEn. The amount
2786790428	Universal Neural Machine Translation for Extremely Low Resource Languages	1816313093	ze of 128. The dropout rates for both the encoder and the decoder is set to 0.4. We have open-sourced an implementation of the proposed model. 6 4.2 Back-Translation We utilize back-translation (BT) (Sennrich et al., 2016a) to encourage the model to use more information of the zero-resource languages. More concretely, we build the synthetic parallel corpus 6https://github.com/MultiPath/NANMT/tree/universal_translation
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2582956876	ations h = h 1; ;h T0 x , where each annotation h i is a concatenation of the corresponding forward and backward ~ states: h i = (h~ i h i) 2R2m, with m the encoder cell size. This model differs from [2], which did not use convolutions, but time pooling between each LSTM layer, resulting in a shorter sequence (pyramidal encoder). 3.2. Character-level decoder We use a character-level decoder composed
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2582956876	ermore, we double the training size by concatenating the aligned references with the Google Translate references. We also mirror our experiments on the BTEC synthetic speech corpus, as a follow-up to [2]. 3. END-TO-END MODELS For the three tasks, we use encoder-decoder models with attention [9, 10, 11, 2, 3]. Because we want to share some parts of the model between tasks (multi-task training), the AS
2786891429	End-to-End Automatic Speech Translation of Audiobooks	1494198834	larger than the existing corpora described above. We started from the LibriSpeech corpus used for Automatic Speech Recognition (ASR), which has 1000 hours of speech aligned with their transcriptions [4]. The read audiobook recordings derive from a project based on a collaborative effort: LibriVox. The speech recordings are based on public domain books available on Gutenberg Project2 which are distri
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2594229957	nvolutions, but time pooling between each LSTM layer, resulting in a shorter sequence (pyramidal encoder). 3.2. Character-level decoder We use a character-level decoder composed of a conditional LSTM [12], followed by a dense layer. s t;o = update1(s0 1;E(y )) (1) c t = look(o t;h) (2) s0 t;o 0 t= update 2(s 1;c ) (3) y t = generate(o0t c t E(y t 1)) (4) where update1 and update2 are two LSTMs with ce
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2582956876	per is organized as follows: after presenting our corpus in section 2, we present our end-to-end models in section 3. Section 4 describes our evaluation on two datasets: the synthetic dataset used in [2] and the audiobook dataset described in section 2. Finally, section 5 concludes this work. 2. AUDIOBOOK CORPUS FOR END-TO-END SPEECH TRANSLATION 2.1. Augmented LibriSpeech Large quantities of parallel
2786891429	End-to-End Automatic Speech Translation of Audiobooks	1494198834	osed and evaluated on a real speech corpus by [3]. This paper is a follow-up of our previous work [2]. We now investigate end-to-end speech-to-text translation on a corpus of audiobooks - LibriSpeech [4] - speciﬁcally augmented to perform end-to-end speech translation [5]. While previous works [2, 3] investigated the extreme case where source language transcription is not available during learning no
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2582956876	re, and the AST and MT models use the same decoder architecture. 3.1. Speech Encoder The speech encoder is a mix between the convolutional encoder presented in [3] and our previously proposed encoder [2]. It takes as input a sequence of audio features: x = (x 1;:::;x T x ) 2RT x n. Like [2], these features are given as input to two non-linear (tanh) layers, which output new features of size n0. Like
2786891429	End-to-End Automatic Speech Translation of Audiobooks	1522301498	Speech decoders use an output layer size of l = 512. For BTEC, we do not use any non-linear output layer, as we found that this led to overﬁtting. 4.2. Training settings We train our models with Adam [16], with a learning rate of 0:001, and a mini-batch size of 64 for BTEC, and 32 for LibriSpeech (because of memory constraints). We use variational dropout [17], i.e., the same dropout mask is applied t
2786891429	End-to-End Automatic Speech Translation of Audiobooks	2582956876	translation without proposing a complete end-to-end translation system. The ﬁrst attempt to build an end-to-end speech-to-text translation system (which does not use source language) is our own work [2] but it was applied to a synthetic (TTS) speech corpus. A similar approach was then proposed and evaluated on a real speech corpus by [3]. This paper is a follow-up of our previous work [2]. We now in
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2341401723	2014; Bahdanau et al., 2015; Cho et al., 2015). For example, such models have been applied to speech recognition (Chorowski et al., 2015), video captioning (Yao et al., 2015), and text summarization (Nallapati et al., 2016). Typically, two approaches have been used for image-caption generation. The ﬁrst approach retrieves and ranks existing captions (Kuznetsova et al., 2012, 2014; Vendrov et al., 2016; Yagcioglu et al.,
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	1923211482	ation. In particular, encoder-decoder (enc-dec) models, e.g., sequenceto-sequence models, have been studied (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2013, 2014; Bahdanau et al., 2015; Cho et al., 2015). For example, such models have been applied to speech recognition (Chorowski et al., 2015), video captioning (Yao et al., 2015), and text summarization (Nallapati et al., 2016). Typically, two approa
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2130167591	e the semantic representation of what a human recalls using the fMRI data of brain activity evoked by visual stimuli, such as natural movies and images (Mitchell et al., 2008; Nishimoto et al., 2011; Pereiraa et al., 2013; Huth et al., 2012; Stansbury et al., 2013; Horikawa et al., 2013). Stansbury et al. (2013) employed latent Dirichlet allocation (Blei et al., 2003) to assign semantic labels to still pictures using
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	1923211482	ections 3.1 and 3.2). Figure 1 presents an overview of the proposed method. 3.1 Image !caption model (A) We employed an image-captioning model (A) based on a DNN framework, i.e., the enc-dec network (Cho et al., 2015; Vinyals et al., 2015), as the main component of the proposed model. In the enc-dec framework, by combining two DNN models functioning as an encoder and a decoder, the model encodes input information
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2110798204	hree-layer neural network model, and a ﬁve-layer DNN model, to determine which machine learning method is suitable for this model. The ﬁve-layer DNN model was pre-trained using stacking autoencoders (Bengio et al., 2006) to avoid delay and overﬁtting in training due to the lack of brain data. We used pairs of fMRI brain activity data and the images a subject observed as training data; however, we did not use natural
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2064675550	method by building an enc-dec network employing GoogLeNet (Ioffe and Szegedy, 2015), which can extract image features effectively, as an encoder and a Long Short-Term Memory Language Model (LSTM-LM) (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which is a deep neural language model, as an decoder. In this study, we build and train an enc-dec network based on those prior studies Vinyals et al. (2015); Xu et al. (201
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2064675550	he model. Similar to such previous models, we constructed an image !image feature !caption model (A) employing VGGNet (Simonyan and Zisserman, 2015) as an encoder and a two-layer LSTM language model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) as a decoder. We used pairs of image and caption data as training data. 3.2 Brain activity data !image feature model (B) To apply the above image-captioning process to handle
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2081580037	ubject pays attention to objects in a video. Huth et al. (2012, 2016a,b) revealed the corresponding relationships between brain activities and visual stimuli using the semantic categories of WordNet (Miller, 1995). They used the categories to construct a map for semantic representation in the cerebral cortex and showed that semantic information is represented in various patterns over broad areas of the cortex.
2786981933	Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli	2147152072	wed that semantic information is represented in various patterns over broad areas of the cortex. Nishida et al. (2015) showed that, compared to other language models such as Latent Semantic Indexing (Deerwester et al., 1990) and latent Dirichlet allocation, the word2vec (skip-gram) model by Mikolov et al. (2013) gives better accuracy in modeling the semantic representation of human brain activity. Furthermore, they showe
2787425834	TextZoo, a New Benchmark for Reconsidering Text Classification.	2097117768	Padding input embedding for a ﬁxed size and concat the feature maps after convolution. Multi-layer CNN. CNN with multi layers for high-level modelling. CNNwith Inception. CNNwith Inception mechanism (Szegedy et al., 2015). Capsules. CNN with Capsules Networks (Sabour et al., 2017) . CNN inspired by Quantum. Neural representation inspired by Quantum Theory (Zhang et al., 2018; Niu et al., 2017). RCNN (Lai et al., 2015)
2787425834	TextZoo, a New Benchmark for Reconsidering Text Classification.	2131744502	r relatively short sentence, Question answering, machine comprehension and dialogue system), which there are little word-level overlaps in bag-of-word vector space. Distributed representation (Le and Mikolov, 2014) in a ﬁxed low-dimensional space trained from large-scale ∗†means equal contribution. Need to be peer-reviewed corpus have been proposed to enhance the features of text, then break through the perform
2787425834	TextZoo, a New Benchmark for Reconsidering Text Classification.	1966443646	SST 3 Trec IMDB 2 25000 25000 SST-1 5 8544 2210 SST-2 2 6920 1821 SUBJ 2 9000 1000 Table 1: Font guide. Basic CNN. Convolution over the input embedding (Kalchbrenner et al., 2014). Multi-window CNN (Severyn and Moschitti, 2015). Padding input embedding for a ﬁxed size and concat the feature maps after convolution. Multi-layer CNN. CNN with multi layers for high-level modelling. CNNwith Inception. CNNwith Inception mechanism
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2610245951	17): We leverage the best 3-way model proposed in (Nakayama and Nishida 2017) as our baseline, which adopts end to end training strategy and trains the decoder with image and description. 4 TS model (Chen et al. 2017): This method is originally designed for leveraging a third language as a pivot to enable zero-resource NMT. We follow the teacher-student framework and replace the pivot language with image. The teac
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2176263492	6a), we adopt beam search for gradient estimation. Compared with random sampling, beam search can help to avoid the very large variance and sometimes unreasonable results brought by image captioning (Ranzato et al. 2015). Speciﬁcally, we run beam search with the captioner A 1 to generate top-Khigh-probability middle outputs in the source language, and use the averaged value on the middle outputs to approximate the tr
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2613904329	dels the translation process in an end-to-end way, has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and German-English (Johnson et al. 2016; Gehring et al. 2017; Vaswani et al. 2017). The success is mainly attributed to the quality and scale of available parallel corpora to train NMT systems. However, preparing such parallel corpora has remained a big proble
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	1514535095	e res4fx (end of Block-4) layer after ReLU. For some baseline methods that do not support attention mechanism, we extract 2048- dimension feature after the pool5 layer. We follow the architecture in (Xu et al. 2015) for image captioning and standard RNNSearch architecture (Bahdanau, Cho, and Bengio 2015) for translation. We leverage dl4mt3 and arctic-captions4 for all our experiments. The beam search size is 2 i
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2546938941	EnglishGerman parallel corpus. Table 1 and 2 summarizes data statistics. Experimental Setup To extract image features, we follow the suggestion of (Caglayan et al. 2016) and adopt ResNet-50 network (He et al. 2016b) pre-trained on ImageNet without ﬁnetuning. We use the (14,14,1024) feature map of the res4fx (end of Block-4) layer after ReLU. For some baseline methods that do not support attention mechanism, we
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2546938941	ged source sentence between agents. The goal of training is to ﬁnd the parameters of the agents that maximize the expected reward: E( z!x; x!y) = E P(x midjz; z!x)[r(y;x mid; x!y)]: (3) We follow He et al. (2016a) and deﬁne the reward as the log probability of agent A 2 generates y from x mid: r(y;x mid; x!y) = logP(yjx mid; x!y): (4) As a result, the expected reward in the multi-agent communication game c
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	1514535095	igure 1: Zero-resource neural machine translation through a two-agent communication game within a multimodal environment. Agent A 1 is an image captioning model implemented with CNN-RNN architecture (Xu et al. 2015); Agent A 2 is a neural machine translation model implemented with RNNSearch (Bahdanau, Cho, and Bengio 2015). Taking an image as input, agent A 1 sends a message in the source language to agent A 2,
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2621379712	k in improving translation performance (Bahdanau, Cho, and Bengio 2015). In this work, we introduce a multi-agent communication game within a multimodal environment (Lazaridou, Pham, and Baroni 2016; Havrylov and Titov 2017) to achieve direct modeling of zero-resource source-to-target NMT. We have two agents in the game: a captioner which describes an image in the source language and a translator which translates a sourc
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	1514535095	l languages. One way to ground a natural language to a visual image is through image captioning, which annotates a description for an input image with natural language through a CNN-RNN architecture (Xu et al. 2015; Karpathy and Fei-Fei 2015). Below, we call a pair of a text description and its counterpart image a “document” and use z to denote an image. Given documents in the target language D z;y = fhzn;ynigN
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2101105183	le image-to-source caption generation. During validation and testing, we set the beam search size to be 5 for both the captioner and the translator. All models are quantitatively evaluated with BLEU (Papineni et al. 2002). For the Multi30K dataset, each image is paired with 5 English descriptions and 5 German descriptions in the test set. We follow the setting in (Caglayan et al. 2016) and let the NMT generate a targe
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	1753482797	mental results on the IAPR-TC12 and Multi30K datasets show that the proposed learning mechanism signiﬁcantly improves over the state-of-the-art methods. Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom 2013; Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015), which directly models the translation process in an end-to-end way, has achieved state-of-theart translation performance on resource
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2626778328	process in an end-to-end way, has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and German-English (Johnson et al. 2016; Gehring et al. 2017; Vaswani et al. 2017). The success is mainly attributed to the quality and scale of available parallel corpora to train NMT systems. However, preparing such parallel corpora has remained a big problem in some speciﬁc doma
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2610245951	scenarios to naturally enable zero-resource translation. In addition to the above multilingual methods, several authors propose to train the zeroresource source-to-target translation model directly. Chen et al. (2017) propose a teacher-student framework under the assumption that parallel sentences have close probabilities of generating a sentence in a third language. Zheng et al. (2017) maximize the expected likel
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2610245951	translation without direct source-target parallel corpora has attracted increasing attention in the community recently. These methods utilize a third language (Firat et al. 2016; Johnson et al. 2016; Chen et al. 2017; Zheng, Cheng, and Liu 2017; Cheng et al. 2017) or modality (Nakayama and Nishida 2017) as a pivot to enable zero-resource sourceto-target translation. Although promising results have been obtained,
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2337363174	ty and scale of available parallel corpora to train NMT systems. However, preparing such parallel corpora has remained a big problem in some speciﬁc domains or between resource-scarce language pairs. Zoph et al. (2016) indicate that NMT trends to obtain much worse translation quality than statistical machine translation (SMT) under small-data conditions. As a result, developing methods to achieve neural machine tra
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	1905882502	way to ground a natural language to a visual image is through image captioning, which annotates a description for an input image with natural language through a CNN-RNN architecture (Xu et al. 2015; Karpathy and Fei-Fei 2015). Below, we call a pair of a text description and its counterpart image a “document” and use z to denote an image. Given documents in the target language D z;y = fhzn;ynigN n=1 , an image caption mode
2787442509	Zero-Resource Neural Machine Translation with Multi-Agent Communication Game	2546938941	x!y)]; (8) in which the expectation is taken over x mid and r = log[P(yjx mid; x!y)]. Unfortunately, Eqn. 7 and 8 are intractable to calculate due to the exponential search space of X(z). Following (He et al. 2016a), we adopt beam search for gradient estimation. Compared with random sampling, beam search can help to avoid the very large variance and sometimes unreasonable results brought by image captioning (R
2787481916	A Survey of Word Embeddings Evaluation Methods.	2167609405	2003]. 7. JAIR (acronym for Journal of Artiﬁcial Intelligence Research), 430 questions divided into 20 semantic classes. Notably, dataset contains not only words but collocations (like solar system) [Turney, 2008]. 4.1.3. Thematic ﬁt method evaluates the ability of a model to separate different thematic roles of arguments of a predicate (also called selectional preference [Baroni et al., 2014]). The idea is t
2787481916	A Survey of Word Embeddings Evaluation Methods.	2120779048	al thesaurus of WordNet super-senses). The most extensive one is Wikipedia, which is used for document vectorization with a similar thesaurus vector-based technique called Explicit Semantic Analysis [Gabrilovich and Markovitch, 2007]. It is usually applied in cross-language information retrieval. 4.3.2. Dictionary deﬁnition graph evaluation method is based on the idea that co-occurrences of words in dictionary deﬁnitions could c
2787481916	A Survey of Word Embeddings Evaluation Methods.	2135964261	for Almuhareb and Poesio), 402 words divided into 21 categories [Almuhareb, 2006]. 3. BLESS (acronym for Baroni and Lenci Evaluation of Semantic Spaces), 200 words divided into 27 semantics classes [Baroni and Lenci, 2011]. Despite the fact that BLESS was designed for another type for evaluation, it is also possible to use this dataset in a word categorization task, as in [Jastrzebski et al., 2017]. 4. ESSLLI-2008 (ac
2787481916	A Survey of Word Embeddings Evaluation Methods.	2005181355	ations (F-Loc) [Ferretti et al., 2001]. 4. P07 (acronym for Pado), 414 verb-object-subject pairs [Padó, 2007]. 5. UP (acronym for Ulrike and Pado), 211 verb-noun pairs, the set of roles is unlimited [Padó and Lapata, 2007]. 6. MT98 (acronym for McRae and Tanenhaus), a subset of 200 verbs from MSTNN where each verb has two nouns, one is a good subject, but a bad object, and one which is a good object, but a bad subject
2787481916	A Survey of Word Embeddings Evaluation Methods.	2135964261	bases is a previously mentioned BLESS dataset which contains 200 pairs of words (for example, for the [motorcycle,moped]word pair these are the two sets of attributes: [large,small] and [fast,slow]) [Baroni and Lenci, 2011]. Another example is Feature Norms Dataset containing 24 963 pairs of words, for which a least one pair of distinctive features is selected (for example, for the pair [airplane,helicopter]the existen
2787481916	A Survey of Word Embeddings Evaluation Methods.	131533222	a binary mark reporting the existence of a similarity [Baumel et al., 2016,Bakarov and Gureenkova, 2017]. There are several datasets for this task, including the Microsoft Research Paraphrase Corpus [Dolan and Brockett, 2005] and the Quora Question Pairs Dataset4. 11. Textual Entailment Detection (also called natural language inference task). The task is in a some way similar to the previously mentioned paraphrase detect
2787481916	A Survey of Word Embeddings Evaluation Methods.	2164973920	dings, one word corresponds to multiple vectors, depending on the number of its senses. First attempts to investigate possible methods of evaluating such models are already made [Borbély et al., 2016,Reisinger and Mooney, 2010,Upadhyay et al., 2016], but I argue that mainstream approaches to evaluation like word similarity datasets would be even less applicable to such embeddings than to the “classic” monolanguage and mono
2787481916	A Survey of Word Embeddings Evaluation Methods.	2250550379	g. noun + adjective) should correlate with the frequency of this group in a corpus (bi-gram co-occurrence frequency). In other words, bi-gram co-occurrence frequency could be used as a gold standard [Kornai and Kracht, 2015]. I think that any representative corpus or dictionary of n-gram co-occurrence frequency dataset like the Google 1T Frequency Dataset6 can be used for evaluation. 5 Future challenges In this survey I
2787481916	A Survey of Word Embeddings Evaluation Methods.	25483125	ich one word is a target word, and 4 words are potential synonyms where the only one is a correct answer. 1. RDWP (acronym for Reader’s Digest Word Power Game), 300 synonym questions (5-word tuples) [Jarmasz and Szpakowicz, 2004]. 2. TOEFL (acronym for Test of English as a Foreign Language), 80 questions [Landauer and Dumais, 1997]. 3. ESL (acronym for English as a Second Language), 50 questions [Turney, 2001]. 4.1.6. Outlie
2787481916	A Survey of Word Embeddings Evaluation Methods.	2135964261	n task could also be used to compile a word analogy set. In this case, it also worth looking at the Lexical Relation set which is a compilation of diﬀerent semantic relation datasets including BLESS [Baroni and Lenci, 2011] (12 458 word pairs with a relation comprising 15 relation types) [Vylomova et al., 2015] and the Semantic Neighbors set (14 682 word pairs with a relation comprising 2 relation types, meaningful and
2787481916	A Survey of Word Embeddings Evaluation Methods.	2128870637	rds n (noun) and v (verb) by the most frequent role in which n used with v (e.g., pair people,eat would be classiﬁed as the subject since it is more common to use people as a subject with that verb) [Baroni and Lenci, 2010]. In my opinion, the main problem of this method lies in two of its features. First, it needs a corpus annotated with thematic roles. Second, it is unclear which method of obtaining an embedding for
2787481916	A Survey of Word Embeddings Evaluation Methods.	2132631284	s assessed by semantic similarity with a scale from 0 to 4 [Baker et al., 2014]. 12. YP-130 (acronym for Yang and Powers), 130 pairs of verbs assessed by semantic similarity with a scale from 0 to 4 [Yang and Powers, 2006]. 13. RG-65 (acronym for Rubenstein and Goodenough), 65 pairs assessed by semantic similarity with a scale from 0 to 4 [Rubenstein and Goodenough, 1965]. 14. MC-30 (acronym for Miller and Charles), 3
2787481916	A Survey of Word Embeddings Evaluation Methods.	2128870637	thod of vectorization of “slots” for certain thematic roles, which are obtained by averaging several most frequent nouns encountered in a given role – but applicability of such method is not obvious [Baroni and Lenci, 2010]. The following datasets could be used for evaluation with the thematic ﬁt task: 1. MSTNN (abbreviation mentioned in [Sayeed et al., 2016]), 1 444 verbobject-subject pairs [McRae et al., 1997]. 2. GD
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1686810756	(words) according to its captions. Then we formulate word prediction as a multi-label classiﬁcation task as [28]. Figure4summarizes our word prediction network. Different from [28], which uses VggNet [23] as the initialization of the CNN, we adopt the more powerful ResNet-152 [11] pre-trained on ImageNet [5]. After initialization, the CNN is ﬁne-tuned on our image-words dataset by minimizing the eleme
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2463565445	42 54.06 79.01 35.55 36.80 Concepts [28] 57.46 79.77 36.79 43.10 57.62 79.72 36.04 43.44 ACK [29] 59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 SAN [33] 58.70 79.30 36.60 46.10 58.90 - - - HieCoAtt [16] 61.80 79.70 38.70 51.70 62.10 - - - MCB [8] 60.80 81.20 35.10 49.30 - - - - MCB-ensemble [8] 66.70 83.40 39.80 58.50 66.50 83.20 39.50 58.00 Human [2] - - - - 83.30 95.77 83.39 72.67 Word-based VQA 5
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1931639407	6 words, which cover over 90% of the word occurrences in the dataset. These words can be any part of speech, including nouns (object names), verbs (actions) or adjectives (properties). In contrast to [7], our words are not tense or plurality sensitive, for example, “horse” and “horses” are considered as the same word. This signiﬁcantly decreases the size of our word list. Given the word list, every i
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1836465849	a batch size of 128. The initial learning rate is 0.01 and is dropped to 0.001 after ﬁrst 10 epochs. The training is stopped after another 10 epochs. In addition, dropout [24] and batch normalization [13] are used in the training procedure. 4.2. Word-based VQA An important characteristics of our framework is that the quality of explanations can inﬂuence the ﬁnal VQA performance. In this section, we an
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1933349210	ccuracy. The current system achieves comparable performance to the state-of-the-art and can naturally improve with explanation quality. Extensive experiments are conducted on the popular VQA dataset [2]. We dissect all results according to the measurements of the quality of explanations to present a thorough analysis of the strength and weakness of our framework. 2. Related Work There is a growing r
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1895577753	is computed as: r = qT p jjqjjjjpjj (3) 3.2. Sentence Generation This section we talk about generating sentence-level explanations for images by using a pre-trained image captioning model. Similar to [26], we train an image captioning model by maximizing the probability of the correct caption given an image. Suppose we have an image Ito be described by a caption S= fs 1;s 2;:::;s Lg;s t 2V, where V is
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2176212817	ectors and image captioning model, the VQA performance can be further improved. Compared with the state-of-the-art, our framework achieves better performance than LSTM Q+I [2], Concepts [28], and ACK [29], which use CNN features, high-level concepts, and external knowledge, respectively. Both SAN [33] and HieCoAtt [16] use attention mechanism in images or questions and yield comparable performance wit
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2463565445	our framework achieves better performance than LSTM Q+I [2], Concepts [28], and ACK [29], which use CNN features, high-level concepts, and external knowledge, respectively. Both SAN [33] and HieCoAtt [16] use attention mechanism in images or questions and yield comparable performance with ours. MCB-ensemble [8] achieves signiﬁcantly better performance than ours and other top methods, but it suffers fr
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2189070436	the image as the ﬁrst token and feed it into RNN along with the question to predict an answer. Inspired by [6], [17] pass the image into RNN at each time step, instead of only seeing the image once. [9] adapt m-RNN models [18] to handle the VQA task in the multi-lingual setting. Despite these methods show promising results, they fail to recognize novel instances in images and are highly relied on qu
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1947481528	ing models, recurrent neural networks (RNNs) in VQA are used to encode questions. [21] treat the image as the ﬁrst token and feed it into RNN along with the question to predict an answer. Inspired by [6], [17] pass the image into RNN at each time step, instead of only seeing the image once. [9] adapt m-RNN models [18] to handle the VQA task in the multi-lingual setting. Despite these methods show pro
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1931639407,2404394533	ions should be attended to. High-level Concepts. In the scenario of vision-tolanguage, high-level concepts exhibit superior performance than the low-level or middle-level visual features of the image [7,28,29]. Each concept corresponds to a word mined from the training image captions and represents the objects and attributes presented in the image. [7] ﬁrst learn independent detectors for visual words base
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2179022885	lation task [3] and then is brought into the vision-to-language tasks [32,34,31,33,16,12,19,35]. The visual attention in the vision-to-language tasks is used to address the problem of “where to look” [22]. In VQA, the question is used as a query to search for the relevant regions in the image. [33] propose a stacked attention model which queries the image for multiple times to infer the answer progres
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2404394533	list, y i = [y i1;y i2;:::;y iV ];y ij 2f0;1gis the label vector of the ith image, p i = [p i1;p i2;:::;p iV ] is the probability vector. In the testing phase, instead of using region proposals like [28], we directly feed the whole image into the word prediction CNN in order to keep simple and efﬁcient. As a result, each image is encoded into a ﬁxed-length vector, where each dimension represents the
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2176212817	nce. Method test-dev test-standard All Y/N Num Others All Y/N Num Others LSTM Q+I [2] 53.74 78.94 35.24 36.42 54.06 79.01 35.55 36.80 Concepts [28] 57.46 79.77 36.79 43.10 57.62 79.72 36.04 43.44 ACK [29] 59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 SAN [33] 58.70 79.30 36.60 46.10 58.90 - - - HieCoAtt [16] 61.80 79.70 38.70 51.70 62.10 - - - MCB [8] 60.80 81.20 35.10 49.30 - - - - MCB-ensemble [8]
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1575833922,2142192571,2189070436	nt advances from two directions. CNN-RNN. Inspired by the signiﬁcant progress of image captioning achieved by combining CNN and RNN, the paradigm of CNN-RNN has become the most common practice in VQA [9,17,21]. Visual features of images are extracted via pre-trained convolutional neural networks (CNNs). Different from image captioning models, recurrent neural networks (RNNs) in VQA are used to encode quest
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2171810632,2255577267,2302086703,2463565445,2546696630	questions while neglecting the image content [1,14]. Attention in VQA. The attention mechanism is ﬁrstly used in the machine translation task [3] and then is brought into the vision-to-language tasks [32,34,31,33,16,12,19,35]. The visual attention in the vision-to-language tasks is used to address the problem of “where to look” [22]. In VQA, the question is used as a query to search for the relevant regions in the image.
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1933349210	rates external training data, which is an extra advantage over other methods. Humanis the human performance for reference. Method test-dev test-standard All Y/N Num Others All Y/N Num Others LSTM Q+I [2] 53.74 78.94 35.24 36.42 54.06 79.01 35.55 36.80 Concepts [28] 57.46 79.77 36.79 43.10 57.62 79.72 36.04 43.44 ACK [29] 59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 SAN [33] 58.70 79.30 36.60 46.10
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1889081078	the readability and understandability of attributes also makes them an intuitive way to explain what the model learns from images. Similar to [7], we ﬁrst build a word list based on MS COCO Captions [4]. We extract the most N frequent words in all captions and ﬁlter them by lemmatization and removing stop words to determine a list of 256 words, which cover over 90% of the word occurrences in the dat
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2171810632,2175714310,2293453011,2404394533	ring a machine to be equipped with cross-modality understanding across language and vision [2,36,10]. Signiﬁcant progress has been made on VQA in recent 1 arXiv:1801.09041v1 [cs.CV] 27 Jan 2018 years [28,33,20,30]. A widely used pipeline is to ﬁrst encode an image with Convolutional Neural Networks (CNNs) and represent associated questions with Recurrent Neural Networks (RNNs), and then formulate the vision-to
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2108598243	s [28]. Figure4summarizes our word prediction network. Different from [28], which uses VggNet [23] as the initialization of the CNN, we adopt the more powerful ResNet-152 [11] pre-trained on ImageNet [5]. After initialization, the CNN is ﬁne-tuned on our image-words dataset by minimizing the element-wise sigmoid cross entropy loss: J = 1 N XN i=1 XV j=1 y ijlogp (1 y )log(1 p ) (1) where N is batch s
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2404394533	the size of our word list. Given the word list, every image is paired with multiple labels (words) according to its captions. Then we formulate word prediction as a multi-label classiﬁcation task as [28]. Figure4summarizes our word prediction network. Different from [28], which uses VggNet [23] as the initialization of the CNN, we adopt the more powerful ResNet-152 [11] pre-trained on ImageNet [5]. A
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1933349210	ss as: J(I;Q;A) = J(W;S;Q;A) = logp(A) (8) where p(A) denotes the probability of the ground truth A. 4. Experiments and Analysis 4.1. Experiment Setting Dataset. We evaluate our framework on VQA-real [2] dataset. For each image in VQA-real, 3 questions are annotated by different workers and each question has 10 answers from different annotators. We follow the ofﬁcial split and report our results on t
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2171810632	state-of-the-art, our framework achieves better performance than LSTM Q+I [2], Concepts [28], and ACK [29], which use CNN features, high-level concepts, and external knowledge, respectively. Both SAN [33] and HieCoAtt [16] use attention mechanism in images or questions and yield comparable performance with ours. MCB-ensemble [8] achieves signiﬁcantly better performance than ours and other top methods,
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1931639407,2302086703,2404394533	ted in Figure3, the framework consists of three modules: word prediction, sentence generation, and answer reasoning. Next, we describe the three modules in details. 3.1. Word Prediction From the work [7,28,34], we have learned that explicit high-level attributes can beneﬁt vision-to-language tasks. In fact, besides performance gain, the readability and understandability of attributes also makes them an int
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1933349210,2136462581,2560730294	text-based applications (e.g., sentence generation [25] and text QA [27]), VQA takes one step further, requiring a machine to be equipped with cross-modality understanding across language and vision [2,36,10]. Signiﬁcant progress has been made on VQA in recent 1 arXiv:1801.09041v1 [cs.CV] 27 Jan 2018 years [28,33,20,30]. A widely used pipeline is to ﬁrst encode an image with Convolutional Neural Networks
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2194775991	ti-label classiﬁcation task as [28]. Figure4summarizes our word prediction network. Different from [28], which uses VggNet [23] as the initialization of the CNN, we adopt the more powerful ResNet-152 [11] pre-trained on ImageNet [5]. After initialization, the CNN is ﬁne-tuned on our image-words dataset by minimizing the element-wise sigmoid cross entropy loss: J = 1 N XN i=1 XV j=1 y ijlogp (1 y )log(
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1811254738	token and feed it into RNN along with the question to predict an answer. Inspired by [6], [17] pass the image into RNN at each time step, instead of only seeing the image once. [9] adapt m-RNN models [18] to handle the VQA task in the multi-lingual setting. Despite these methods show promising results, they fail to recognize novel instances in images and are highly relied on questions while neglecting
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	196214544	from a training set bias or from the image content. processing communities. This task is considered as the milestone of “AI-complete.” Compared with text-based applications (e.g., sentence generation [25] and text QA [27]), VQA takes one step further, requiring a machine to be equipped with cross-modality understanding across language and vision [2,36,10]. Signiﬁcant progress has been made on VQA in r
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1933349210	use the feature concatenation of words, sentence, and question in Eq.6. Model Conﬁguration. We explain our model setting and training details here. We build the vocabulary from questions in VQA-real [2]. We convert all sentences to lower case and tokenize them by NLTK tool and discard words which occur less than ﬁve times, resulting in the ﬁnal vocabulary with 6,148 unique words. For the question an
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2546696630	wer progressively. Beyond the visual attention, Lu et al. exploit a hierarchical question-image co-attention strategy to attend to both related regions in the image and crucial words in the question. [19] propose the dual attention network, which reﬁnes the visual and textual attention via multiple reasoning steps. [8] incorporate a powerful feature fusion method into visual attention and obtain impre
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1889081078	y used metrics: BLEU@N, METEOR, ROUGE-L and CIDEr-D, which try to consider the accuracy of the generated sentence from different perspectives. Detailed explanations about these metrics can be seen in [4]. Note that we normalize all the metrics into [0;1] before fusion. The latter metric is to measure the relevance between the generated sentence and the question. The binary TF weights are calculated o
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	2171810632	Y/N Num Others LSTM Q+I [2] 53.74 78.94 35.24 36.42 54.06 79.01 35.55 36.80 Concepts [28] 57.46 79.77 36.79 43.10 57.62 79.72 36.04 43.44 ACK [29] 59.17 81.01 38.42 45.23 59.44 81.07 37.12 45.83 SAN [33] 58.70 79.30 36.60 46.10 58.90 - - - HieCoAtt [16] 61.80 79.70 38.70 51.70 62.10 - - - MCB [8] 60.80 81.20 35.10 49.30 - - - - MCB-ensemble [8] 66.70 83.40 39.80 58.50 66.50 83.20 39.50 58.00 Human [2
2787553714	Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions	1522301498	yer LSTMs with 512-dimensional hidden states and word embedding size is 512. We select the most frequent 3000 answers in training set as the list of candidate answers. In training, we use Adam solver [15] with a batch size of 128. The initial learning rate is 0.01 and is dropped to 0.001 after ﬁrst 10 epochs. The training is stopped after another 10 epochs. In addition, dropout [24] and batch normaliz
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	1026270304	ension projections and a residual connection from the ﬁrst to second layer. The context insensitive type representation uses 2048 character n-gram convolutional ﬁlters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely c
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2493916176	ent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g.,Wieting et al.,2016;Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g.,Neelakantan et al.,2014). Our approach also beneﬁts from subword units through the use of character convolutions, and we seamlessly incorporate
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2153579005	that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013;Pennington et al.,2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2610748790	lar toCollobert et al. (2011). As shown in Table1, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over ﬁve runs. The key difference between our system and the previous state of the art fromPeters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereasPeters et al.(2017) only use the top biLM layer. As shown in Sec.5.1, using all layers instead of just the las
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2250539671	problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010;Mikolov et al.,2013;Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al.,2017), textual entailment (Chen et al.,2017) and semantic role labeling (He et al.,201
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2158139315	rovide similar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010;Mikolov et al.,2013;Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al.,2017), textual entailment (Chen et al.,20
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2158899491	s et al.,2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random ﬁeld (CRF) loss (Lafferty et al.,2001), similar toCollobert et al. (2011). As shown in Table1, our ELMo enhanced biLSTM-CRF achieves 92.22% F 1 averaged over ﬁve runs. The key difference between our system and the previous state of the art fromPeters et al. (2017) is that
2787560479	DEEP CONTEXTUALIZED WORD REPRESENTATIONS	2427527485	section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. Question answering The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph. Our baseline model (Clark and Gardner,2017) is an improved version of the Bidirectional At
2787568264	Quantitative fine-grained human evaluation of machine translation systems: a case study on English to Croatian	2133564696	(surface form and morphosyntactic description). This system is described in detail by Sanchez-Cartagena et al (2016). The NMT system is based on the sequence-to-sequence architecture with attention (Bahdanau et al, 2015) and it was built with Nematus (Sennrich et al, 2017). We applied sub-word segmentation with byte pair encoding (Sennrich et al, 2016) jointly on the source and target languages. We performed 85000 jo
2787568264	Quantitative fine-grained human evaluation of machine translation systems: a case study on English to Croatian	2133564696	dels human evaluation error annotation multidimensional quality metrics (MQM) 1 Introduction A machine translation (MT) paradigm based on deep neural networks, usually referred to as neural MT (NMT) (Bahdanau et al, 2015), has emerged in the past few years. This has disrupted the MT eld since NMT, despite its infancy, has already surpassed the performance of phrase-based MT (PBMT) (Koehn et al, 2003), the mainstream a
2787653206	Biomedical term normalization of EHRs with UMLS.	1981625432	, we consider 303 common Spanish words except “no”, “sin” and “con” (no, without, and with, respectively) because they alter the polarity of expressions, which is essential to capture in this domain (Ceusters et al., 2007). 2LOINC R descriptors look typically like “especie de Thrichomonas:numero areico:punto en el tiempo:sedimento uri-´ nario:cuantitativo:microscopia.de luz.campo de gran aumento”, so they are not suite
2787653206	Biomedical term normalization of EHRs with UMLS.	1557664247,2105769172	. As for Spanish, there have been a few attempts to process clinical free text in this language. Below, we present some of these attempts that are relevant to the work presented in this paper. GALEN (Carrero et al., 2008a; Carrero et al., 2008b) proposed a “Spanish MetaMap” that combines machine translation techniques with the use of MM. Unfortunately, they did not apply this system to any task, so performance scores
2787653206	Biomedical term normalization of EHRs with UMLS.	1550258693	8 # words 26,490 23,374 23,311 21,093 Table 2: Evaluation corpora We test the two methods boundary detection — ngram and phrase—, the three scoring functions —Lucene, L, Castro et al. (2010), C, and (Aronson, 2001), A—, and UKB against a random disambiguation baseline, rand. The thresholds have been set empirically and are indicated between parenthesis in the results’ table (Table 3). Figure 3: Results page of
2787653206	Biomedical term normalization of EHRs with UMLS.	2412562470	Aronson, 2006) maps biomedical text to the UMLS Metathesaurus. It is “knowledge intensive” as it relies heavily on the SPECIALIST Lexicon, a large syntactic lexicon of biomedical and general English. Meystre and Haug (2005) evaluated MM with 160 clinical documents of diverse nature (radiology reports, exam reports, and so on). MM’s results were compared to 1https://www.nlm.nih.gov/research/umls/ annotations by 8 physici
2787653206	Biomedical term normalization of EHRs with UMLS.	1550258693	e applications that are being effectively exploited for different purposes and by different organizations as of today. In what follows, we present some of the better-known applications. MetaMap (MM) (Aronson, 2001; Aronson, 2006) maps biomedical text to the UMLS Metathesaurus. It is “knowledge intensive” as it relies heavily on the SPECIALIST Lexicon, a large syntactic lexicon of biomedical and general English
2787653206	Biomedical term normalization of EHRs with UMLS.	1672743	g assessment, they report an average precision of 39% and a recall of 0.65%. Partial matching increases precision to 71%, but recall is still 0.75%. FreelarXiv:1802.02870v1 [cs.CL] 8 Feb 2018 ingMed (Oronoz et al., 2013) use the Freeling analyzer (Carreras et al., 2004) and extend its linguistic data with various knowledge sources: SNOMED CT R , a list of medical abbreviations (Yetano, 2003), Bot PLUS, and ICD-9. The
2787653206	Biomedical term normalization of EHRs with UMLS.	2055518963	purpose. We propose a sequential pipeline that retrieves mapping candidates from an indexed UMLS Metathesaurus, and uses the IXA pipeline (Agerri et al., 2014) for basic language processing and UKB (Agirre and Soroa, 2009) for word sense disambiguation (WSD). In addition to the pipeline itself, this paper also presents a demonstration interface for the tool that will be available on-line. 2. Related Work Biomedical ter
2787653206	Biomedical term normalization of EHRs with UMLS.	1550258693	ssigns new scores to the candidates using a function other than that of Lucene’s. We explore two such functions: the one by Castro et al. (2010), Figure 1: Architecture of the pipeline and the one by Aronson (2001) implemented in MM. Furthermore, a threshold can be applied to discard candidates with low scores. Matching, reranking and thresholding are not done with all the spans detected; the mapping candidate
2787653206	Biomedical term normalization of EHRs with UMLS.	2106797966	tates textual data with ontology terms from the UMLS and BioPortal ontologies. The details of how MGREP —the concept recognition tool— works are limited to the conference poster by Dai et al. (2008). Shah et al. (2009) experimented with the task of large-scale indexing of online biomedical resources: MM recognized more concepts but with a lower precision than MGREP, and MGREP turned to be faster than MM. cTakes (Sa
2787653206	Biomedical term normalization of EHRs with UMLS.	2170344111	me tied in ﬁrst position; the system needs to carry out a disambiguation process in order to choose the correct mapping. It resorts to UKB to do so. The algorithm behind UKB is Personalized PageRank (Haveliwala, 2002). Agirre et al. (2010) and Stevenson et al. (2012) prove that it can be used for WSD with the UMLS. Here we implement a little variation of their approach. The context to initialize the Knowledge Grap
2787739989	A Short Survey on Sense-Annotated Corpora for Diverse Languages and Resources.	2518202280	ave been proposed (Agirre et al.,2014;Moro et al.,2014;Camacho-Collados et al.,2016b; Butnaru et al.,2017;Chaplot and Salakhutdinov,2018), supervised approaches (Zhong and Ng,2010;Melamud et al.,2016;Iacobacci et al., 2016;K˚ageb ack and Salomonsson¨ ,2016) have been more effective in terms of performance (Raganato et al.,2017a), on those languages where sense-annotated datasets are available. Unfortunately, obtaining
2787739989	A Short Survey on Sense-Annotated Corpora for Diverse Languages and Resources.	2065157922	data tend to be available for English only. This produces the so-called knowledge-acquisition bottleneck (Gale et al.,1992). The ﬁrst main approach towards building senseannotated corpora was SemCor (Miller et al., 1993), providing annotations for the WordNet sense inventory (Fellbaum,1998). Since then, several semi-automatic and automatic approaches have also been proposed. These automatic efforts tend to produce no
2787739989	A Short Survey on Sense-Annotated Corpora for Diverse Languages and Resources.	2518202280	e parallel corpus (Eisele and Chen,2010, MultiUN corpus). OMSTI3, coupled with SemCor, has already been successfully leveraged as training data for training supervised systems (Taghipour and Ng,2015a;Iacobacci et al., 2016;Raganato et al.,2017a). 2.2 Wikipedia Wikipedia is a collaboratively-constructed encyclopedic resource consisting of concepts and entities and their corresponding pages. In addition to a large covera
2787739989	A Short Survey on Sense-Annotated Corpora for Diverse Languages and Resources.	2101293500	s and 226,040 sense annotations. SemCor has been the largest manuallyannotated corpus for many years, and is the main corpus used in the literature to train supervised WSD systems (Agirre et al.,2009;Zhong and Ng, 2010;Raganato et al.,2017b). SemEval evaluation datasets. SemEval datasets provide reliable benchmarks for testing WSD systems. The main datasets from Senseval and SemEval competitions have been compiled
2787770455	Inducing Grammars with and for Neural Machine Translation	2514713644	re is only one attention layer from the target to the source S = fs ign 1. In all the models, we share the weights of target word embeddings and the output layer as suggested byInan et al.(2017) andPress and Wolf (2017). 5.3 Hyper-parameters and Training For all the models, we set the word embedding size to 1024, the number of LSTM layers to 2, and the dropout rate to 0.3. Parameters are initialized uniformly in ( 0
2787770455	Inducing Grammars with and for Neural Machine Translation	2557321428	linguistic structure with hard attention indicates that models can capture interesting structures beyond semantic cooccurrence via discrete actions. This corroborates previous work (Choi et al.,2017;Yogatama et al., 2017) which has shown that non-trivial structures are learned by using REINFORCE (Williams, 1992) or the Gumbel-softmax trick (Jang et al., 2016) to backprop through discrete units. Our approach also outpe
2787770455	Inducing Grammars with and for Neural Machine Translation	2586050494	t differs in two important ways. First we model non-projective dependency trees. Second, we utilize the Kirchhoff’s Matrix-Tree Theorem (Tutte,1984) instead of the sum-product algorithm presented in (Kim et al., 2017) for fast evaluation of the attention probabilities. We note that (Liu and Lapata,2018) were ﬁrst to propose using the Matrix-Tree Theorem for evaluating the marginals in end to end training of neural
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2756778986	able to encode some regularities in the speech signal in order to decode predictable sequences of characters in a target language. Secondly, an attention-based Neural Machine Translation (NMT) model [36] was trained between phones in Mboshi and text in French, while soft-alignment probability matrices generated by the attention mechanism, were extracted. These alignments were post-processed to segmen
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2079460648	c units of the low-resource language from the raw speech data, while assuming no other information about the language is available, and using these to build ASR systems (zero resource approach; e.g., [2, 3, 4, 5, 6]). Another strand of research focuses on building ASR systems using speech data from multiple languages, thus trying to create universal or crosslingual ASR systems [7, 8, 9, 10]. Children though, whe
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2582956876	ents, and is efﬁcient even with long utterances. 4. END-TO-END TASKS 4.1. Speech-to-translation End-to-End speech translation, i.e., translation from raw speech without any intermediate transcription [34, 35], is attractive for language documentation, which often uses corpora made of audio recordings aligned with their translation in another language (no transcript in the source language) [1, 14]. Here, X
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	1833498382	h two major modiﬁcations. First, the truncated Dirichlet process of [3] was replaced by a symmetric Dirichlet distribution, which provides a good and yet simple approximation of the Dirichlet Process [27]. Second, to cope with larger databases, the Variational Bayes Inference algorithm originally used in [3] was replaced with the faster Stochastic Variational Bayes Inference algorithm. Experiments sho
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2251025892	ifferent types of acoustic features; [22, 23]) for the evaluation of the discovered units, and quantitative measures, such as BLEU score, error rates, and word discovery metrics (see for more details [24, 25]). 2.3. XNMT Toolkit The end-to-end systems used during the project were built using the neural machine translation toolkit XNMT [26], which was greatly improved during the course of this project. XNM
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2586148577	ightly improved word segmentation compared to (Mboshi-French). Implementation of a bilingual loss is probably an interesting future work. 4.2. Speech-to-Image Speech-to-image is a relatively new task [11, 12, 13]. A speech-to-image system learns to map images and speech to the same embedding space, and retrieves an image using spoken captions. While doing so, it uses multi-modal input to discover speech units
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2586148577	imarily in the visual modality. This has led to a new strand of research which uses visual information, from images, to discover word-like units from the speech signal using speech-image associations [11, 12, 13]. The “Speaking Rosetta” project at the 2017 Frederick Jelinek Memorial Summer Workshop, which took place at Carnegie Mellon University, Pittsburgh, pushed this idea further by using multi-modal datas
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2137010615	kR corpus contains 5 different natural language text captions (obtained using Amazon Mechanical Turk; AMT) for each of 8000 images captured from the FlickR photo sharing website. AMT was also used by [16] to obtain 40K spoken versions of the captions. We augmented this corpus by adding Japanese translations 1The dataset will be made available for free by ELRA; its current version is online at: https:/
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	1861492603	is online at: https://github.com/besacier/mboshi-french-parallel-corpus (Google MT) for all 40K captions, as well as Japanese tokenization. SPEECH-COCO-synthetic [17, 18] is an augmentation of MSCOCO [19] which consists of 123,287 images with ﬁve different descriptions per image. We generated speech captions using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) pai
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	1520968739	down into short utterances of about 8-10 seconds each. Transcriptions consist of summaries of what was spoken. The cleanest 45 hours out of the 480 hours were used. From the Spoken Dutch Corpus (CGN, [20]), 64 hours of read speech were used. 2.2. Evaluation The two types of task, i.e., linguistic unit discovery and endto-end, were evaluated using a battery of tests which include qualitative measures,
2787779284	Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop	2137010615	st language. Our speechto-image system (based on the implementation of [13]) was implemented using XNMT. Four types of acoustic features were compared: Mel-frequency Filterbanks (baseline, similar to [16] but with added speaker-dependent mean-variance normalization on the features before zero-padding/truncation), the pseudo-phones generated by the AUD system [3] (which were downsampled by a factor of
2787783427	Question-Answer Selection in User to User Marketplace Conversations.	2064675550	candidate answers A is an ordered sequence of sentences coming from the product description. Therefore, an LSTM could help add sequential information as context to the answer sentence representations [3]. We speciﬁcally use a bi-directional LSTM and set the initial hidden state to be y(q). This allows the question embedding to inﬂuence the answer sentence representations. Conversational Context Quest
2787783427	Question-Answer Selection in User to User Marketplace Conversations.	2626778328	e use an LSTM to model the embedded messages, taking the ﬁnal LSTM hidden state as the question encoding [3]. Attention Layer We use the feed-forward self-attention layer implemented in Tensor2Tensor [9] to further enrich the representations of the candidate answers. 3.3 Conversational Pre-training The full corpus of 36M conversations, containing 400M messages, was used to pretrain the model on the r
2787783427	Question-Answer Selection in User to User Marketplace Conversations.	1793121960	present a generative approach that conditions a wordlevel recurrent neural network (RNN) on facts retrieved from a knowledge base [13]. Others model facts using memory modules, which RNNs attend over [8, 12]. Extractive techniques extract an answer sentence or phrase from a given context. Many recent phrase-level extractive models are built and evaluated on the Stanford Question Answering Dataset (SQuAD)
2787783427	Question-Answer Selection in User to User Marketplace Conversations.	2064675550	textual information obtained from the messages before the question, M = fm 1; m 2; :::; m Hg. We use an LSTM to model the embedded messages, taking the ﬁnal LSTM hidden state as the question encoding [3]. Attention Layer We use the feed-forward self-attention layer implemented in Tensor2Tensor [9] to further enrich the representations of the candidate answers. 3.3 Conversational Pre-training The full
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2250539671	., 2003; Collobert et al., 2011; Huang et al., 2012). Recent research has shown that inter-word distances in word embeddings correlate with human ratings of semantic similarity (Mikolov et al., 2013; Pennington et al., 2014). Furthermore, these distances are geometrically consistent across different word pairs that share a common semantic relation. For instance, the location of !&quot;# relative to !&quot;#$% is similar
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2140480387	, but they fare relatively poorly in approximating detailed perceptual properties in comparison to abstract (e.g., encyclopedic or functional) knowledge (Baroni and Lenci, 2008; Andrews et al., 2009; Baroni et al., 2010; Riordan and Jones, 2011; Hill et al., 2016). The current results suggest a more nuanced view (see Figure 2): first, knowledge about some perceptual features (e.g., size) was successfully predicted f
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2140480387	-occurrences with more elaborate information such as dependency parses or supervised identification of particular linguistic patterns (Poesio and Almuhareb, 2005; Barbu, 2008; Baroni and Lenci, 2009; Baroni et al., 2010; Kelly et al., 2014; Rubinstein et al., 2015). Our results therefore challenge these approaches by suggesting that the distributional, sub-symbolic representational format of word embeddings can supp
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2078894097	, US states, and first names. These categories have been used in feature-elicitation studies, with varying degrees of prevalence (Paivio et al., 1968; Battig and Montague, 1969; Cree and McRae, 2003; McRae et al., 2005; Pereira et al., 2013; Binder et al., 2016): the first four have been frequently used; the next two—less frequently; and the last three have been rarely used but, unlike the other categories, consist
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2158899491	“word embedding” or a “distributional semantic model” (for reviews, see Lenci, 2008; Turney and Pantel, 2010; Erk, 2012; Pereira et al., 2013; Baroni et al., 2014; Clark, 2015) (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012). Recent research has shown that inter-word distances in word embeddings correlate with human ratings of semantic similarity (Mikolov et al., 2013; Pennington et al., 2014). Furth
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	1997161938	9 items. In order to ensure that items were representative of their respective categories, we chose within each category the 50 most frequent nouns according to the SubtlexUS Word Frequency Database (Brysbaert and New, 2009) (candidate items consisted of all the relevant nouns from Mahowald et al., 2014). We discarded multi-word expressions (except for US States, e.g., “North-Dakota”) and words that did not appear in the
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2078894097	alth, gender, danger, age, religiosity, political leaning, cost and arousal. These features have been produced in feature-elicitation studies with varying degrees of prevalence (Cree and McRae, 2003; McRae et al., 2005; Binder et al., 2016): the first four—almost invariably; the next three— frequently; the next six—less frequently; and the last four—very rarely. Words describing possible values of all features (e.g
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	1608322251	continues the tradition of applying simple linear algebraic operations to perform useful semantic comparisons in word embeddings (e.g., vector subtraction, cosine similarities, matrix multiplication; Baroni and Zamparelli, 2010; Mikolov et al., 2013). Moreover, compared to prior attempts at extracting semantic knowledge from patterns of natural language use, this method requires significantly less human supervision and/or c
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2251803266	the corresponding words. The resulting space is called a “word embedding” or a “distributional semantic model” (for reviews, see Lenci, 2008; Turney and Pantel, 2010; Erk, 2012; Pereira et al., 2013; Baroni et al., 2014; Clark, 2015) (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012). Recent research has shown that inter-word distances in word embeddings correlate with human ratings of semantic simila
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	1987181863	e elaborate information such as dependency parses or supervised identification of particular linguistic patterns (Poesio and Almuhareb, 2005; Barbu, 2008; Baroni and Lenci, 2009; Baroni et al., 2010; Kelly et al., 2014; Rubinstein et al., 2015). Our results therefore challenge these approaches by suggesting that the distributional, sub-symbolic representational format of word embeddings can support the flexible re-
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2250539671	gory items along each feature in two ways: by collecting such ratings from humans, and via semantic projection in a word embedding that was created with Global Vectors for Word Representation (GloVe; Pennington et al., 2014). Our test then compared these two sets of feature-specific ratings. 2.1. Semantic categories and features We created a dataset of categories and features that met four criteria: (i) each category con
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2153579005	k, 2015) (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012). Recent research has shown that inter-word distances in word embeddings correlate with human ratings of semantic similarity (Mikolov et al., 2013; Pennington et al., 2014). Furthermore, these distances are geometrically consistent across different word pairs that share a common semantic relation. For instance, the location of !&quot;# relative
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2079580818	from natural corpora have had to augment the tracking of word co-occurrences with more elaborate information such as dependency parses or supervised identification of particular linguistic patterns (Poesio and Almuhareb, 2005; Barbu, 2008; Baroni and Lenci, 2009; Baroni et al., 2010; Kelly et al., 2014; Rubinstein et al., 2015). Our results therefore challenge these approaches by suggesting that the distributional, sub-sy
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2153579005	plying simple linear algebraic operations to perform useful semantic comparisons in word embeddings (e.g., vector subtraction, cosine similarities, matrix multiplication; Baroni and Zamparelli, 2010; Mikolov et al., 2013). Moreover, compared to prior attempts at extracting semantic knowledge from patterns of natural language use, this method requires significantly less human supervision and/or corpora annotation. Most
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2130167591	rrence probability of the corresponding words. The resulting space is called a “word embedding” or a “distributional semantic model” (for reviews, see Lenci, 2008; Turney and Pantel, 2010; Erk, 2012; Pereira et al., 2013; Baroni et al., 2014; Clark, 2015) (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012). Recent research has shown that inter-word distances in word embeddings correlate with human ratin
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2036931463	rse and Connell, 2011), but they fare relatively poorly in approximating detailed perceptual properties in comparison to abstract (e.g., encyclopedic or functional) knowledge (Baroni and Lenci, 2008; Andrews et al., 2009; Baroni et al., 2010; Riordan and Jones, 2011; Hill et al., 2016). The current results suggest a more nuanced view (see Figure 2): first, knowledge about some perceptual features (e.g., size) was suc
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2130167591	st names. These categories have been used in feature-elicitation studies, with varying degrees of prevalence (Paivio et al., 1968; Battig and Montague, 1969; Cree and McRae, 2003; McRae et al., 2005; Pereira et al., 2013; Binder et al., 2016): the first four have been frequently used; the next two—less frequently; and the last three have been rarely used but, unlike the other categories, consisted of proper nouns and
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2169678114	the tracking of word co-occurrences with more elaborate information such as dependency parses or supervised identification of particular linguistic patterns (Poesio and Almuhareb, 2005; Barbu, 2008; Baroni and Lenci, 2009; Baroni et al., 2010; Kelly et al., 2014; Rubinstein et al., 2015). Our results therefore challenge these approaches by suggesting that the distributional, sub-symbolic representational format of wor
2787788033	Semantic projection: recovering human knowledge of multiple, distinct object features from word embeddings.	2250539671	val of 4 category/feature pairs for which such knowledge was extremely noisy). 2.2. Semantic projection 2.2.1. The GloVe word embedding We chose to conduct our experiment in the GloVe word embedding (Pennington et al., 2014), because it outperforms several other word embeddings in predicting word similarity judgments (Pereira et al., 2016). We used 300-dimensional GloVe vectors derived from the Common Crawl corpus (http:
2787948962	Towards End-to-end Spoken Language Understanding	2130942839	which are directly related with the task. It is thus desirable to train an SLU system in an end-to-end fashion. End-to-end learning has been widely used in several areas, such as machine translation [9, 10, 11] and imagecaptioning[12]. It has also been investigated for speech synthesis [13] and ASR tasks [14, 15, 16]. For example, the CTC loss function is used to train an ASR system to map the feature seque
2787948962	Towards End-to-end Spoken Language Understanding	2400801499	networks 1. INTRODUCTION With the growing demand of voice interfaces for mobile and virtual reality (VR) devices, spoken language understanding (SLU) has received many researchers’ attention recently [1, 2, 3, 4, 5, 6, 7]. Given a spoken utterance, a typical SLU system performs three main tasks: domain classiﬁcation, intent detection and slot ﬁlling [1]. Standard SLU systems are usually designed as a pipeline structur
2787948962	Towards End-to-end Spoken Language Understanding	1836465849	rd network (hidden size 1024) to produce either the domain or intent class. The network was optimized with Adam [26] algorithm until convergence on the validation set. We used the batch normalization [27] for every feed-forward connection to speed up the convergence of this network. The baseline NLU model for our experiments was a recurrent network similar to our model with an exception that we did no
2787948962	Towards End-to-end Spoken Language Understanding	2293634267	rule: W = argmax W p(WjX) = argmax W p(XjW)p(W): Therefore, the ASR system is usually divided into two models: an acoustic model p(XjW)and a language model p(W) (AM and LM respectively). CD-HMM-LSTM [20] is widely used as an AM, in which the feature vector sequence is converted to the likelihood vectors of context-dependent HMM states for each acoustic frame. Together with the LM (p(W) is usually a s
2787948962	Towards End-to-end Spoken Language Understanding	2293634267	shows the upper bound for our models and corresponds to the perfect speech recognition. The later regime was transcribed by a speech recognizer: the core part of AM is a four-layer CD-HMM-LSTM network[20] with 800 memory cells in each layer, trained on the same 320 hours training data with a cross-entropy criterion to predict posterior probabilities of 6,133 clustered content-dependent states; the voc
2787948962	Towards End-to-end Spoken Language Understanding	2130942839	stinct intents. Roughly 11,000 utterances, totaling 10 hours audio, are used as the evaluation set. For all our experiments we used log-Mel ﬁlterbank features. We used an encoder-decoder architecture [9]. The encoder is a 4 layer bidirectional GRU network with 256 units every layer. The output was sub-sampled with a stride of 2 at every layer to reduce the length of the representation. The decoder ne
2787948962	Towards End-to-end Spoken Language Understanding	2473965551	R tasks, including large vocabulary ASR. End-of-end learning of memory networks is also used arXiv:1802.08395v1 [cs.CL] 23 Feb 2018 for knowledge carryover in multi-turn spoken language understanding [19]. Inspired by these success, we explore the possibility to extend the end-to-end ASR learning to include NLU component and optimize the whole system for SLU purpose. As the ﬁrst step towards an end-to
2787976184	JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts.	2402354285	considered sentiment analysis of tweets in Indian languages. The systems that performed the best was based on machine learning algorithms (Sarkar 2018; Sarkar and Bhowmik, 2017; Patra et. al., 2015; Sarkar and Chakraborty, 2015). This paper is organized as follows; a description of the training data was given in Section 2. The system description is given in section 3. Section 4 represents the experimental result and analysis
2787976184	JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts.	2160660844	e English SentiWordNet does not contain any neutral words. Though the BengaliSentiWordnet is developed by us, the English SentiWordnet is created by collecting the positive and negative words used by Hu and Liu (2004) for mining and summarizing customer reviews. Bengali SentiWordnet was initially created in UTF 8 format using Bengali font. But this type of data is not suitable for sentiment analysis in code mixed
2787976184	JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts.	2112422413	lexicon, a collection of known and precompiled sentiment terms. It is mainly divided into two approaches: Dictionary-based approaches and Corpus-based approaches. The work in (Minging and Bing, 2004; Kim and Hovy, 2004) represent the main strategy of Dictionary-based approaches. One of the Corpus-based approaches presented in (Hatzivassiloglou and McKeown, 1997) uses a list of seed opinion adjectives and imposed a s
2787976184	JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts.	2015186536	munities interested in analyzing its content. For example, some research works highlighted how social media content can be used to predict real-world outcomes, such as box-office revenues for movies (Asur and Huberman, 2010) or topic-based twitter sentiment analysis for stock prediction (Si et. al., ,2013) Among the applications, sentiment analysis (Agarwal et. al., 2011; Mohammad et. al, 2013) is one of the typical tech
2788097004	The JHU Speech LOREHLT 2017 System: Cross-Language Transfer for Situation-Frame Detection.	2156985047	able employed in the MT system that was developed for a separate LoReHLT MT evaluation. The translation table is derived from the parallel training data with words aligned automatically by the GIZA++ [21] and Berkeley aligner [22]. In addition to using the training data provided for the evaluation, native informants were also consulted (independently under the MT effort) to produce hundreds of paralle
2788097004	The JHU Speech LOREHLT 2017 System: Cross-Language Transfer for Situation-Frame Detection.	2286443923	n. 3.1.1. Universal Phone Set ASR We refer to the transfer of acoustic models trained on many languages sharing a common phonemic representation as universal phone set ASR. Our approach is similar to [8]. We use a selection of 10 BABEL languages for training, 7 of which were chosen as in [8], with 3 more chosen arbitrarily (Guarani, Mongolian, Dholuo). Diphthongs and triphtongs are split into their c
2788097004	The JHU Speech LOREHLT 2017 System: Cross-Language Transfer for Situation-Frame Detection.	2169724380	stem that was developed for a separate LoReHLT MT evaluation. The translation table is derived from the parallel training data with words aligned automatically by the GIZA++ [21] and Berkeley aligner [22]. In addition to using the training data provided for the evaluation, native informants were also consulted (independently under the MT effort) to produce hundreds of parallel sentences and word trans
2788097004	The JHU Speech LOREHLT 2017 System: Cross-Language Transfer for Situation-Frame Detection.	2286443923	v:1802.08731v1 [cs.CL] 23 Feb 2018 Fig. 2. Using the English SF-Type classiﬁer to obtain adaptation/training data on one or more higher-resource (potentially related) language(s). Previous approaches [5, 6, 7, 8] have explored similar cross-language ASR transfer assuming shared phonemic representations, generally using the GlobalPhone corpus [9], while [10] examines multilingual training of a deep neural netw
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2224454470	100 unique Bash utilities. We also present a set of experiments to demonstrate that NL2Bash is a challenging task which is worthy of future study. We build on recent work in neural semantic parsing (Dong and Lapata, 2016; Ling et al., 2016), by evaluating the standard Seq2seq model (Sutskever et al., 2014) and the CopyNet model (Gu et al., 2016). We also include a recently proposed stage-wise neural semantic parsing
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2611818442	ally verify the equivalence of different regular expressions by converting them to minimal deterministic ﬁnite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash commands is undecidable. Alternatively, one can check c
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2212703438	bjective with mini-batched Adam (Kingma and Ba, 2014), using the default momentum hyperparameters. Our initial learning rate is 0.0001 and the mini-batch size is 128. We used variational RNN dropout (Gal and Ghahramani, 2016) with 0.4 dropout rate. For decoding we set the beam size to 100. The hyperparameters were set based on the model’s performance on a development dataset (x3.4.). Our baseline system implementation is
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2130942839	ce levels for future work, we evaluated two neural machine translation models that have demonstrated strong performance in both NL-to-NL translation and NL-to-code translation tasks, namely, Seq2Seq (Sutskever et al., 2014; Dong and Lapata, 2016) and CopyNet (Gu et al., 2016). We also evaluated a stage-wise natural language programing model, Tellina (Lin et al., 2017), which includes manually-designed heuristics for ar
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2605887895	command options using pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difﬁcult to apply. 6. Baseline System Performance To establish performance levels for future work, we evaluated two neural machine translation models that have demonstrat
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2131774270	cs. 6.1. Implementation Details We used the Seq2Seq formulation as speciﬁed in (Sutskever et al., 2014). We used the gated recurrent unit (GRU) (Chung et al., 2014) RNN cells and a bidirectional RNN (Schuster and Paliwal, 1997) encoder. We used the copying mechanism proposed by (Gu et al., 2016). The rest of the model architecture is the same as the Seq2Seq model. We evaluated both Seq2Seq and CopyNet at three levels of tok
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2133564696	e the NL description speciﬁes a complex task and would be better broken into separate sentences (Table 10). When the task gets complicated, the NL description gets verbose. As noted in previous work (Bahdanau et al., 2014), the performance of RNNs decreases for longer sequences. Giving high-quality NL description for complex tasks are also more difﬁcult for the users in practice — multi-turn interaction is probably nec
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2610403318	et al., 2016) formally verify the equivalence of different regular expressions by converting them to minimal deterministic ﬁnite automaton (DFAs). Others (Kwiatkowski et al., 2013; Long et al., 2016; Guu et al., 2017; Iyer et al., 2017; Zhong et al., 2017) evaluate the generated code through execution. As Bash is a Turingcomplete language, verifying the equivalence of two Bash commands is undecidable. Alternative
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2304113845	f future study. We build on recent work in neural semantic parsing (Dong and Lapata, 2016; Ling et al., 2016), by evaluating the standard Seq2seq model (Sutskever et al., 2014) and the CopyNet model (Gu et al., 2016). We also include a recently proposed stage-wise neural semantic parsing model, Tellina, which uses manually deﬁned heuristics for better detecting and translating the command arguments (Lin et al., 2
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2511149293	GEO880 DSL 880 284 60= 7.6 19.1 (Zelle and Mooney, 1996) Freebase917 DSL 917 – – – – (Cai and Yates, 2013) ATISH DSL 5,410 936 176= 11.1 28.1 (Dahl et al., 1994) WebQSP DSL 4,737 – – – – search log (Yih et al., 2016) NL2RX-KB13 Regex 824 715 85Y= 7.1 19.0Y turker written (Kushman and Barzilay, 2013) DjangoK Python 18,805 – – 14.3 – expert written scraped (Oda et al., 2015) NL2Bash Bash 9,305 7,790 6,234 11.7 7.7
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2130942839	gument slots in the code template with the extracted constants using a learned alignment model and reformatting heuristics. 6.1. Implementation Details We used the Seq2Seq formulation as speciﬁed in (Sutskever et al., 2014). We used the gated recurrent unit (GRU) (Chung et al., 2014) RNN cells and a bidirectional RNN (Schuster and Paliwal, 1997) encoder. We used the copying mechanism proposed by (Gu et al., 2016). The r
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2154268919	introduce provides a new type of target meaning representations (Bash1 commands), and is signiﬁcantly larger (from two to ten times) than most existing semantic parsing benchmarks (Dahl et al., 1994; Popescu et al., 2003). Other recent work in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkows
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2224454470	k, we evaluated two neural machine translation models that have demonstrated strong performance in both NL-to-NL translation and NL-to-code translation tasks, namely, Seq2Seq (Sutskever et al., 2014; Dong and Lapata, 2016) and CopyNet (Gu et al., 2016). We also evaluated a stage-wise natural language programing model, Tellina (Lin et al., 2017), which includes manually-designed heuristics for argument translation. Seq2
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2304113845	language and output commands are treated as sequences of tokens. At test time, the command sequences with the highest conditional probabilities were output as candidate translations. CopyNet CopyNet (Gu et al., 2016) is an extension of Seq2Seq which is able to select sub-sequences of the input sequence and emit them at proper places while generating the output sequence. The copy action is mixed with the regular t
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2158396456	marks (Dahl et al., 1994; Popescu et al., 2003). Other recent work in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkowski et al., 2013; Iyer et al., 2017; Zhong et al., 2017). However, the shell command data we consider raises unique challenges, due to its irregular syntax (no syntax tree r
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2130942839	NL2Bash is a challenging task which is worthy of future study. We build on recent work in neural semantic parsing (Dong and Lapata, 2016; Ling et al., 2016), by evaluating the standard Seq2seq model (Sutskever et al., 2014) and the CopyNet model (Gu et al., 2016). We also include a recently proposed stage-wise neural semantic parsing model, Tellina, which uses manually deﬁned heuristics for better detecting and translat
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2158396456	ode base modelDataset PL # # # Avg. # Avg. # NL Code Semantic Introduced pairs words tokens w. in nl t. in code collection collection alignment by IFTTT DSL 86,960 – – 7.0 21.8 scraped scraped Noisy (Quirk et al., 2015) C#2NL* C# 66,015 24,857 91,156 12 38 (Iyer et al., 2016) SQL2NL* SQL 32,337 10,086 1,287 9 46 RegexLib Regex 3,619 13,491 179Y 36.4 58.8Y Good@ (Zhong et al., 2018) HeartStone Python 665 – – 7 352Y g
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	1655078475	ort the automation of highly repetitive tasks such as ﬁle manipulation, search, and application-speciﬁc scripting (Wilensky et al., 1984; Wilensky et al., 1988; Dahl et al., 1994; Quirk et al., 2015; Desai et al., 2016). This work presents new data and semantic parsing methods on a novel and ambitious domain — natural language control of the operating system. Our long-term goal is to enable any user to perform tasks
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2610403318	pattern matching and each command can have idiomatic syntax rules (e.g. to specify an ssh remote, the format needs to be [USER@]HOST:SRC). Syntax-tree-based parsing approaches (Yin and Neubig, 2017; Guu et al., 2017) are hence difﬁcult to apply. 6. Baseline System Performance To establish performance levels for future work, we evaluated two neural machine translation models that have demonstrated strong performan
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2304113845	ranslation models that have demonstrated strong performance in both NL-to-NL translation and NL-to-code translation tasks, namely, Seq2Seq (Sutskever et al., 2014; Dong and Lapata, 2016) and CopyNet (Gu et al., 2016). We also evaluated a stage-wise natural language programing model, Tellina (Lin et al., 2017), which includes manually-designed heuristics for argument translation. Seq2Seq The Seq2Seq (sequence-to-s
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2611818442	rk in semantic parsing has also focused on programming languages, including regular expressions (Locascio et al., 2016), IFTTT scripts (Quirk et al., 2015), and SQL queries (Kwiatkowski et al., 2013; Iyer et al., 2017; Zhong et al., 2017). However, the shell command data we consider raises unique challenges, due to its irregular syntax (no syntax tree representation for the command options), wide domain coverage (
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2130942839	Seq2Seq (sequence-to-sequence) model deﬁnes the conditional probability of an output sequence given the input sequence using an RNN (recurrent neural network) encoder-decoder (Jain and Medsker, 1999; Sutskever et al., 2014). When applied to the NL-to-code translation problem, the input natural language and output commands are treated as sequences of tokens. At test time, the command sequences with the highest conditiona
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2158396456	sible and would support the automation of highly repetitive tasks such as ﬁle manipulation, search, and application-speciﬁc scripting (Wilensky et al., 1984; Wilensky et al., 1988; Dahl et al., 1994; Quirk et al., 2015; Desai et al., 2016). This work presents new data and semantic parsing methods on a novel and ambitious domain — natural language control of the operating system. Our long-term goal is to enable any
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2304113845	Sutskever et al., 2014). We used the gated recurrent unit (GRU) (Chung et al., 2014) RNN cells and a bidirectional RNN (Schuster and Paliwal, 1997) encoder. We used the copying mechanism proposed by (Gu et al., 2016). The rest of the model architecture is the same as the Seq2Seq model. We evaluated both Seq2Seq and CopyNet at three levels of token granularities: token, character and sub-token. Pre-processing We u
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2031016524	tural language as a programming medium would be universally accessible and would support the automation of highly repetitive tasks such as ﬁle manipulation, search, and application-speciﬁc scripting (Wilensky et al., 1984; Wilensky et al., 1988; Dahl et al., 1994; Quirk et al., 2015; Desai et al., 2016). This work presents new data and semantic parsing methods on a novel and ambitious domain — natural language control
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	1522301498	txt”, SUB END. Hyperparameters The dimension of our decoder RNN is 400. The dimension of the two RNNs in the bi-directional encoder is 200. We optimized the learning objective with mini-batched Adam (Kingma and Ba, 2014), using the default momentum hyperparameters. Our initial learning rate is 0.0001 and the mini-batch size is 128. We used variational RNN dropout (Gal and Ghahramani, 2016) with 0.4 dropout rate. For
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2158396456	these venues. The problem with these data is that they are loosely aligned and cannot be directly used for training. ing (Nie et al., 2018). Extracting good alignments from them is very challenging (Quirk et al., 2015; Iyer et al., 2016; Yao et al., 2018). That being said, these datasets signiﬁcantly surpasses the manually gathered ones in terms of size and diversity, hence demonstrating signiﬁcant potential for f
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2163274265	ynthesized expert Very Good (Haas and Riezler, 2016) given code written Jobs640H DSL 640 391 58= 9.8 22.9 user written expert written given NL (Tang and Mooney, 2001) GEO880 DSL 880 284 60= 7.6 19.1 (Zelle and Mooney, 1996) Freebase917 DSL 917 – – – – (Cai and Yates, 2013) ATISH DSL 5,410 936 176= 11.1 28.1 (Dahl et al., 1994) WebQSP DSL 4,737 – – – – search log (Yih et al., 2016) NL2RX-KB13 Regex 824 715 85Y= 7.1 19.0Y
2788109215	NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System.	2096979215	Zhong et al., 2017) NLMAPS DSL 2,380 1,014 – 10.9 16.0 synthesized expert Very Good (Haas and Riezler, 2016) given code written Jobs640H DSL 640 391 58= 9.8 22.9 user written expert written given NL (Tang and Mooney, 2001) GEO880 DSL 880 284 60= 7.6 19.1 (Zelle and Mooney, 1996) Freebase917 DSL 917 – – – – (Cai and Yates, 2013) ATISH DSL 5,410 936 176= 11.1 28.1 (Dahl et al., 1994) WebQSP DSL 4,737 – – – – search log (
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2059488546	. Semantic and grammar templates are used in (Oliveira 2009), while genetic algorithms are employed in (Manurung 2004; Manurung, Ritchie, and Thompson 2012; Zhou, You, and Ding 2010). In the work of (Jiang and Zhou 2008; Zhou, You, and Ding 2010; He, Zhou, and Jiang 2012), poetry generation is treated as a statistical machine translation problem, where the next line is generated by translating the previous line. Ano
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2115221470	ased on neural networks have been proposed with the paradigm of sequence-to-sequence learning, where poems are generated line by line and each line is generated by taking the previous lines as input (Zhang and Lapata 2014; Yi, Li, and Sun 2016; Wang et al. 2016a; 2016b; Zhang et al. 2017). However, restrictions exist in previous works, including topic drift and semantic inconsistency which are caused by only consideri
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	197120736	ing 2010; He, Zhou, and Jiang 2012), poetry generation is treated as a statistical machine translation problem, where the next line is generated by translating the previous line. Another approach in (Yan et al. 2013) generates poetry by summarizing users’ queries. More recently, deep neural networks have been applied in automatic poetry generation. An RNN-based framework is proposed in (Zhang and Lapata 2014) whe
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2059488546	Nakatsu 2009; Netzer et al. 2009; Oliveira 2009; 2012), genetic algorithms (Manurung 2004; Zhou, You, and Ding 2010; Manurung, Ritchie, and Thompson 2012) and statistical machine translation methods (Jiang and Zhou 2008; He, Zhou, and Jiang 2012) have been developed. More recently, with the signiﬁcant advances in deep neural networks, a number of poetry generation algorithms based on neural networks have been propos
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2115221470	nerated by all the models. Models Recall Models Recall RNNPG-A 12.85% PPG-R 33.5% RNNPG-H 11.82% PPG-H 33.7% SMT 19.79% MIPG 58.8% RNNPG. A poetry generation model based on recurrent neural networks (Zhang and Lapata 2014), where the ﬁrst line is generated with a template-based method taking keywords as input and the other three lines are generated sequentially. In our implementation, two schemes are used to select the
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2115221470	this paper can be easily generalized to generate other types of poetry. To construct the dataset of image-poem pairs, we collect 68,715 images from the Internet and use the poem dataset provided in (Zhang and Lapata 2014), which contains 65,559 7-character quatrains. Given the large numbers of images and poems, it is impractical to match them manually. Instead, we exploit the key concepts of the images and poems and m
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2115221470	roach in (Yan et al. 2013) generates poetry by summarizing users’ queries. More recently, deep neural networks have been applied in automatic poetry generation. An RNN-based framework is proposed in (Zhang and Lapata 2014) where each poem line is generated by taking the previously generated lines as input. In the work of (Yi, Li, and Sun 2016), the attention mechanism is introduced into poetry generation, where an atte
2788148803	How Images Inspire Poems: Generating Classical Chinese Poetry from Images with Memory Networks	2026351760	s have been made on automatic generation of classical Chinese poems from text information such as keywords. Among them, rule-based approaches (Tosa, Obara, and Minoh 2008; Wu, Tosa, and Nakatsu 2009; Netzer et al. 2009; Oliveira 2009; 2012), genetic algorithms (Manurung 2004; Zhou, You, and Ding 2010; Manurung, Ritchie, and Thompson 2012) and statistical machine translation methods (Jiang and Zhou 2008; He, Zhou, a
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	1985741469,2096476966,2250222000	2018, Lyon, France Powerdynamicsindebatesandothertypesofcoverage.Studies have shown that language use and topic control in debates can reflect influence between candidates and indicate power dynamics [35, 36, 41]. More recently, social media have also become an important channel to monitor public opinion on debates in real time [7, 14] and potentially change news media coverage. 6 CONCLUSION In this paper, we
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2096476966	ame speaker.4 We consider a sentence highlighted if it was among the most quoted t% sentences from the corresponding debate. We opted for 4There exist alternate ways to account for topic shift, e.g., [35]. WWW 2018, April 23–27, 2018, Lyon, France Tan, Peng, and Smith category % humans circular (sound bite, newsworthy) 30.0 provocative, sensational 25.5 surprising, funny 17.0 issues, informative 16.0
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2047221353	atures from each sentence and take the difference between them. We use logistic regression with ℓ2-regularization. This approach is equivalent to a linear framework for the ranking task within a pair [28]. We grid search the best ℓ2 coefficient based on five-fold cross-validated accuracy on the training set over 2x , where x ranges over 20 values evenly spaced between –8 and 1. 3.2 Human Interpretatio
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2170857652,2271245358	factors such as who the speaker is and what state the debate is in. Inspired by “natural experiments” and previous studies about the effect of wording on message sharing, memorability, and persuasion [13, 15, 54, 55], we propose a classification task that asks humans and machines to decide which sentence was quoted more in the news media between two “similar” sentences. Binary classification framework.To formally
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2166273866	hts holds promise for understanding media bias and polarization. Existing studies have shown that non-textual factors such as the media’s preferences and biases can affect the selection of highlights [2, 22, 32, 37]. In particular, Niculae et al. [37] demonstrate implicit structure in the media (e.g., international vs. domestic) by analyzing the patterns in quotes of President Barack Obama. Since televised presi
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2170857652,2271245358	Informativeness We use length as a proxy of informativeness. Longer sentences are more likely to be highlighted despite our control on length as discussed in §3.1. This echoes findings in Tan et al. [54, 55]. length ↑↑↑↑ Emotions We consider positive and negative words in Pennebaker et al. [40]. Highlighted sentences use significantly more negative words, while there is no difference in positive words. T
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2127492100,2166273866	ise a longstanding and often-studied segment of the media and are highly amenable to replication studies. We leave exploration of additional media sources to future work. Inspired by existing studies [31, 37, 47, 52], we define “highlights” based on quotes in news articles that directly come from the debates. In this work, we differentiate quotes from quotations. We refer to any texts in news articles that are en
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	126222424	pecifically, we train 1, 2, and 3-gram lexical level language models on the NYT corpus11, and we use the WSJ portion of Penn Treebank12 to train POS-level models. Our implementation is based on SRILM [50]. •Parallelism. We measure parallelism using the average length of the longest common sequences between sub-sentences following Song et al. [48]. The basic unit in the longest common sequence is a wor
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2021097538	can reflect influence between candidates and indicate power dynamics [35, 36, 41]. More recently, social media have also become an important channel to monitor public opinion on debates in real time [7, 14] and potentially change news media coverage. 6 CONCLUSION In this paper, we conduct the first systematic study on media selection of highlights from presidential debates, using a three-decade dataset.
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2013580886	sentences do not use more superlatives. superlatives Generality We count indefinite articles to measure generality. Our findings are consistent with Danescu-Niculescu-Mizil et al. [13], Shahaf et al. [45], Tan et al. [54]. indef. articles ↑↑↑ Language model To capture surprise or conspicuousness, we compute language model scores based on NYT texts and part-of-speech (POS) tags in the WSJ portion of Pe
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2342255891	tion-flow features.Although only a handful of participants in our human experiment mentioned that context matters, conversational dynamics in the debates may contribute to the selection of highlights [58]. We propose a novel set of conversation-flow features and indeed observe intriguing conversational dynamics around the highlights. In order to capture the local context of a sentence (s), we compare
2788268216	“You are no Jack Kennedy”: On Media Selection of Highlights from Presidential Debates	2170857652	use more superlatives. superlatives Generality We count indefinite articles to measure generality. Our findings are consistent with Danescu-Niculescu-Mizil et al. [13], Shahaf et al. [45], Tan et al. [54]. indef. articles ↑↑↑ Language model To capture surprise or conspicuousness, we compute language model scores based on NYT texts and part-of-speech (POS) tags in the WSJ portion of Penn Treebank. Howe
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2076804559	. The users would be more intrinsically motivated if the test were conducted in a scenario closer to the real-world experience of the participants. One example would be setting up conversations as in Hara and Iqbal (2015). A social media scenario could be an alternative to two-way communication: participants could be given a message to promote and asked whether they would be willing to use the output of MT to post in
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2103288873	adequacy. 2 Related Work To the best of our knowledge, there has been no prior work focused on evaluating user trust in MT. Automatically generated conﬁdence scores have been referred to as “trust” (Soricut and Echihabi, 2010), but user trust has not been evaluated. There has, however, been extensive work in human evaluation of MT quality and in user trust of other types of automation. 2.1 Human Evaluation of Machine Trans
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2147192413	ency is slightly more like the typical user experience in that they only see one translation. WMT initially adopted a direct assessment approach for the 2006 and 2007 workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007), but the interannotator agreement was low that the metric was abandoned until WMT16 (Bojar et al., 2016). The change came as a result of a pilot direct assessment, conducted in parallel to the ofﬁcia
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2152311128	ent of adequacy and ﬂuency is slightly more like the typical user experience in that they only see one translation. WMT initially adopted a direct assessment approach for the 2006 and 2007 workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007), but the interannotator agreement was low that the metric was abandoned until WMT16 (Bojar et al., 2016). The change came as a result of a pilot direct assessment, condu
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2147192413	er effect than ﬂuency errors. This indicates that either ﬂuency more strongly affects user trust than adequacy does or users have a more difﬁcult time judging adequacy than ﬂuency. Previous research (Callison-Burch et al., 2007; Graham et al., 2017) has shown that ﬂuency and adequacy scores are highly correlated, leading to only adequacy being measured in WMT17 (Bojar et al., 2017). The correlation was taken to mean that ad
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2101105183	has, however, been extensive work in human evaluation of MT quality and in user trust of other types of automation. 2.1 Human Evaluation of Machine Translation Automated quality metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) have been invaluable for improving and training MT systems, and have been shown to correlate with human rankings at the system level (Papineni et al., 2002), but
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2252166243	that the metric was abandoned until WMT16 (Bojar et al., 2016). The change came as a result of a pilot direct assessment, conducted in parallel to the ofﬁcial rankings, that tested the technique from Graham et al. (2013) on a larger scale. Graham et al. had found that they could improve inter-annotator agreement by accounting for the personal preferences of the judges: They used a near-continuous scale (100 points in
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2625092622	oves to neural machine translation (NMT), as NMT has been shown to consistently provide more ﬂuent translations than previous MT paradigms (Bentivogli et al., 2016; Toral and Sanchez-Cartagena, 2017; Koehn and Knowles, 2017), but is also prone to producing output´ that is ﬂuent but unrelated to the input when there is out of domain or insufﬁcient training data (Koehn and Knowles, 2017). We propose a survey-based approach
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2169279899	quality rather than the user experience and particularly trust. Common approaches to human evaluation of MT include ranking and direct assessment. Ranking was the ofﬁcial metric for WMT 2008 - 2016 (Callison-Burch et al., 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016). In each evaluation, participants ranked ﬁve translations of individual segments (usually sentences) from different MT systems against e
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2147192413	rrelate with human rankings at the system level (Papineni et al., 2002), but human judgments remain the gold standard for competitions like the Workshop (now Conference) on Machine Translation (WMT) (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017). However, these human judgments have focused almost entirely on intrinsic quality rather than the user experience and partic
2788274683	Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT	2076804559	t or acceptance of the system, and it would be difﬁcult to determine from this feedback since those who chose to contact the company may not be a representative sample of the pool of potential users. Hara and Iqbal (2015) conducted a user study of MT for communication. They used spoken language translation, which adds speech-to-text and text-to-speech technology to the MT. Participants were not asked to directly rate
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2130942839	6). Given the training set S, our training objective is to maximise the log probability of the segmented sequences T where the unsegmented sequences Sare given. The training objective is to maximise (Sutskever et al., 2014) 1 jSj X (T;S)2S logp(TjS) For a new sentence, we need to output a sequence T0 with maximum likelihood for the given input (Sutskever et al., 2014). T0 = argmax T p(TjS) LSTMs are used both at the enc
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2130942839	et al. (2016), originally proposed for neural machine translation. We consider the pair of sandhied and unsandhied sentences as source and target sentences, respectively. Following the insights from Sutskever et al. (2014), we reverse the sequence order at the input and we ﬁnd that the reversal of the string leads to improvement in the results. We also use a deep architecture with 3 layers each at the encoder and decod
2788285277	Building a Word Segmenter for Sanskrit Overnight.	1522301498	ion. The outputs are then passed to the loss function which calculates the log-perplexity over the data samples in the batch. We then update the parameters via backpropagation and use Adam optimiser (Kingma and Ba, 2015) for our model. VocabularyEnhancementforthemodel- Sanskrit, being a resource poor language, the major challenge is to obtain enough data for the supervised task. While there are plenty of sandhied tex
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2163377725	lem is a well studied problem across various languages where the segmentation is nontrivial. For languages such as Chinese and Japanese, where there is no explicit boundary markers between the words (Xue, 2003), numerous sequence labelling approaches have been proposed. In Sanskrit, it can be seen that the merging of word boundaries is the discretion of the writer. In this work, we propose a purely engineer
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2122228338	om a dataset of 25000 split points. Using the same dataset, Natarajan and Charniak (2011) proposed a sandhi splitter for Sanskrit. The method is an extension of Bayesian word segmentation approach by Goldwater et al. (2006). Krishna et al. (2016) is currently the state of the art in Sanskrit word segmentation. The system treats the problem as an iterative query expansion problem. Using a shallow parser for Sanskrit (Goy
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2251957808	of ours takes less than 12 hours to arXiv:1802.06185v1 [cs.CL] 17 Feb 2018 train in a ‘Titan X’ 12 GB memory, 3584 GPU Cores system. Our title for the paper is inspired from the title for the work by Wang et al. (2015). As with the original paper, we want to emphasise on the ease with which our system can be used for training and at runtime, as it do not require any linguistically involved preprocessing. Such requi
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2613904329	usion of attention is highly beneﬁcial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017), where different novel approaches in using attention are experimented with. Our experiments in line with the measures reported in Krishna et al. (2016) show that our system performs robustly across s
2788285277	Building a Word Segmenter for Sanskrit Overnight.	2626778328	Since we ﬁnd that the inclusion of attention is highly beneﬁcial in improving the performance of the system, we intend to experiment with recent advances in the encoder-decoder architectures, such as Vaswani et al. (2017) and Gehring et al. (2017), where different novel approaches in using attention are experimented with. Our experiments in line with the measures reported in Krishna et al. (2016) show that our system
2788321882	SHAPED: SHARED-PRIVATE ENCODER-DECODER FOR TEXT STYLE ADAPTATION	2133564696	D models given by p(zjx) is approximating the desired output style. 4 Model Instantiation As an implementation of the encoder-decoder model, we use the attention-based sequenceto-sequence model from (Bahdanau et al., 2015), with an RNN architecture using GRU units (Chung et al.,2014). The input token sequences are ﬁrst projected into an embedding space via an embedding matrix E, resulting in a sequence of vectors as in
2788321882	SHAPED: SHARED-PRIVATE ENCODER-DECODER FOR TEXT STYLE ADAPTATION	1514535095	t al.,2015;Wu et al., 2016;Vaswani et al.,2017), text summarization (Rush et al.,2015;Nallapati et al.,2016;See et al.,2017), dialog systems (Li et al.,2016;Asghar et al.,2017), and image captioning (Xu et al., 2015;Ranzato et al.,2015;Liu et al.,2017). This framework consists of an encoder that reads the input data and encodes it as a sequence of vectors, which is in turn used by a decoder to generate anWork do
2788367456	Live Blog Corpus for Summarization	2046133424	, 2014). 5.2. Upper bound For comparison, we compute two upper bounds. The upper bound for extractive summarization is retrieved by solving the maximum coverage of n-grams from the reference summary (Takamura and Okumura, 2010; Peyrard and EckleKohler, 2016; P.V.S. and Meyer, 2017). This is cast as an Integer Linear Programming (ILP) and depends on two parameters: N, the size of n-grams considered and L, the maximum length
2788367456	Live Blog Corpus for Summarization	2606974598	/Daily Mail dataset (Hermann et al., 2015). The latter contains large pairs of 312k online news articles and multi-sentence summaries used for neural summarization approaches (Nallapati et al., 2016; See et al., 2017). However, their dataset contains only one source document, whereas live blogs have a larger number of information snippets, typically more than 100. Another recent work uses social media’s reactions
2788367456	Live Blog Corpus for Summarization	2150869743	com live blog on “FIFA corruption inquiry” KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback-Leibler (KL) divergence between the word distributions in summary and the documents. ICSI (Gillick and Favre, 2009) is a global linear optimization that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents. ICSI has been among the state-of-theart MD
2788367456	Live Blog Corpus for Summarization	2182572585	which jointly aggregate news documents and reader comments. Update summarization. After the DUC series, the Text Analysis Conference5 (TAC) series (’08, ’09) introduced the update summarization task (Dang and Owczarzak, 2008). In this task, two summaries are provided for two sets of documents and the summary of the second set of documents is an update of the ﬁrst set. Although the importance of text to be included in the
2788367456	Live Blog Corpus for Summarization	1975579663	le systems compared to the extractive upper bounds for ROUGE-1 (UB-1) and ROUGE-2 (UB-2) extractive for summary lengths of L and 2L Figure 3: BBC.com live blog on “FIFA corruption inquiry” KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback-Leibler (KL) divergence between the word distributions in summary and the documents. ICSI (Gillick and Favre, 2009) is a global linear optimization that extracts a summary by s
2788367456	Live Blog Corpus for Summarization	2106081394	live blogs have a larger number of information snippets, typically more than 100. Another recent work uses social media’s reactions on Twitter to create large-scale multi-document summaries for news (Lloret and Palomar, 2013; Cao et al., 2016). Cao et al. (2016) use hashtags to cluster the documents into the same topic and use tweets with hyperlinks to generate optimal reference summaries. Their corpus consists of 204 do
2788367456	Live Blog Corpus for Summarization	1520857482	maries. In our work, we set N = 1 and N = 2 and compute the upper bound for ROUGE-1 (UB-1) and ROUGE-2 (UB-2) respectively. 5.3. Experimental Setup We report scores for the ROUGE metrics identiﬁed by Owczarzak et al. (2012) as strongly correlating with human evaluation methods: ROUGE-1 (R1) and ROUGE-2 (R2) recall with stemming and stop words not removed. For completeness, we also report the best skip-grams matching met
2788367456	Live Blog Corpus for Summarization	2341401723	t al., 2008) and the CNN/Daily Mail dataset (Hermann et al., 2015). The latter contains large pairs of 312k online news articles and multi-sentence summaries used for neural summarization approaches (Nallapati et al., 2016; See et al., 2017). However, their dataset contains only one source document, whereas live blogs have a larger number of information snippets, typically more than 100. Another recent work uses social
2788367456	Live Blog Corpus for Summarization	2173487081	umber of information snippets, typically more than 100. Another recent work uses social media’s reactions on Twitter to create large-scale multi-document summaries for news (Lloret and Palomar, 2013; Cao et al., 2016). Cao et al. (2016) use hashtags to cluster the documents into the same topic and use tweets with hyperlinks to generate optimal reference summaries. Their corpus consists of 204 document clusters wit
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2338266296	(Neubig et al., 2017). We train using the Adam optimisation algorithm (Kingma and Ba, 2014) over a maximum of 10 epochs, using early stopping. We make two modiÞcations to the bi-LSTM architecture of Plank et al. (2016). First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations. This choice was made so as to encourage the model not to rely on langu
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2351252181	80 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 NAACL-HLT 2018 Submission ***. ConÞdential Review Copy. DO NOT DISTRIBUTE. 2010) simultaneously for different languages (Tsvetkov et al., 2016; Ostling and Tiedemann¬ , 2017). In these recurrent multilingual language models with long short-term memory cells (LSTM, Hochreiter and Schmidhuber, 1997), languages are embedded into a n-dimensiona
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2351252181	development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Ostling and Tiedemann¨ ,2017;Malaviya et al.,2017). We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameter
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2338266296	et (Neubig et al.,2017). We train using the Adam optimisation algorithm (Kingma and Ba,2014) over a maximum of 10 epochs using early stopping. We make two modiﬁ- cations to the bi-LSTM architecture ofPlank et al. (2016). First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations. This choice was made so as to encourage the model not to rely on langu
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2740900542	ing, the neural network is encouraged to use the language embeddings to encode features of language. Other work has explored learning language embeddings in the context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by¬Ostling and Tiedemann (2017 ), both in their original state, and by further tuning them for PoS tagging. 2.3 Typological data In the experiments fo
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2740900542	ing, the neural network is encouraged to use the language embeddings to encode features of language. Other work has explored learning language embeddings in the context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by Ostling and Tiedemann¬ (2017), both in their original state, and by further tuning them for PoS tagging. 2.3 Typological data In the experiments fo
2788425346	FROM PHONOLOGY TO SYNTAX: UNSUPERVISED LINGUISTIC TYPOLOGY AT DIFFERENT LEVELS WITH LANGUAGE EMBEDDINGS	2338266296	Net (Neubig et al.,2017). We train using the Adam optimisation algorithm (Kingma and Ba,2014) over a maximum of 10 epochs, using early stopping. We make two modiÞcations to the bi-LSTM architecture ofPlank et al. (2016). First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations. This choice was made so as to encourage the model not to rely on langu
2788465415	Evaluating Scoped Meaning Representations.	2252123671	ated meaning representations by comparing them to gold standard annotations. Our work shows many similarities with recent annotation and parsing efforts around Abstract Meaning Representations, (AMR; Banarescu et al., 2013) in that we abstract away from syntax, use ﬁrstorder meaning representations, and use an adapted version of SMATCH (Cai and Knight, 2013) for evaluation. However, we deviate from AMR on several points
2788465415	Evaluating Scoped Meaning Representations.	2158847908	ivated treatment of modals, negation, presupposition, and quantiﬁcation), and the nonlogical symbols that we use are grounded in WordNet (concepts) and VerbNet (thematic roles), rather than PropBank (Palmer et al., 2005). We also provide a syntactic analysis in the annotated corpus, in order to derive the semantic analyses in a compositional way. We make the following contributions: A meaning representation with exp
2788465415	Evaluating Scoped Meaning Representations.	2570099837	mantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined
2788465415	Evaluating Scoped Meaning Representations.	1509171848	p in this pipeline, a single component produces the automatic annotation for all four languages, using language-speciﬁc models. Human annotators can correct machine output by adding ‘Bits of Wisdom’ (Basile et al., 2012). These corrections serve as data for training better models, and create a gold standard annotated subset of the data. Annotation quality is deﬁned per layer and language, at three levels: bronze (ful
2788465415	Evaluating Scoped Meaning Representations.	2250748818	resent in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation effort
2788465415	Evaluating Scoped Meaning Representations.	2163108052	s, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-speciﬁcation tec
2788465415	Evaluating Scoped Meaning Representations.	86887328	s were translated (e.g. ‘United States’ and ‘Stati Uniti’). This is not accounted for, since we do not currently make use of grounding proper names to a unique identiﬁer, for instance by wikiﬁcation (Cucerzan, 2007), or by using a languageindependent transliteration of names. In 13% of cases the translation was either non-literal or incorrect. Examples are ‘Tom lacks experience’ with the Dutch translation ‘Tom S
2788465415	Evaluating Scoped Meaning Representations.	2760854376	speciﬁc annotation models. There are ﬁve annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projec
2788465415	Evaluating Scoped Meaning Representations.	2525907473	t away from language-speciﬁc annotation models. There are ﬁve annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic ana
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	2250539671	effective use of word embeddings. In a wide range of NLP tasks, it is beneﬁcial to initialize the word embeddings with ones learnt from large text corpora like word2vec Mikolov et al. (2013) or GLoVe Pennington et al. (2014) and tune them as a part of a target task e.g. text classiﬁcation. It is therefore a natural question to ask whether such standardized representations of whole sentences that can be widely used in dow
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	2310102669	three labels - entailment, contradiction and neutral. As shown in Fig. 1(b), for each of the SUFISENT models, the encodings of the premise and hypothesis sentences are computed as u and v. Following Mou et al. (2016), a feature vector consisting of u, v, ju vjand uv is fed into a fully connected layer(s), before computing the 3-way softmax in the classiﬁcation layer. 3 TRAINING AND RESULTS The encodings deﬁned by
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	1840435438	ll et al. (2016). More recently, the work of Conneau et al. (2017) takes a supervised learning approach. They train a sentence encoding model on the Stanford Natural Language Inference (SNLI) dataset Bowman et al. (2015) and show that the learnt encoding transfers well to to a set of transfer tasks encapsulated in the SentEval benchmark. This is reminiscent of the approach taken by ImageNet Deng et al. (2009) in the
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	1486649854	nces that can be widely used in downstream tasks, is possible. There are two classes of approaches to this problem. Taking cue from word2vec, an unsupervised learning approach is taken by SkipThought Kiros et al. (2015) and FastSent Hill et al. (2016). More recently, the work of Conneau et al. (2017) takes a supervised learning approach. They train a sentence encoding model on the Stanford Natural Language Inference
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	2153579005	as become standard through the effective use of word embeddings. In a wide range of NLP tasks, it is beneﬁcial to initialize the word embeddings with ones learnt from large text corpora like word2vec Mikolov et al. (2013) or GLoVe Pennington et al. (2014) and tune them as a part of a target task e.g. text classiﬁcation. It is therefore a natural question to ask whether such standardized representations of whole senten
2788531807	SufiSent - Universal Sentence Representations Using Suffix Encodings	2108598243	taset Bowman et al. (2015) and show that the learnt encoding transfers well to to a set of transfer tasks encapsulated in the SentEval benchmark. This is reminiscent of the approach taken by ImageNet Deng et al. (2009) in the computer vision community. One of the most effective ways of encoding a sentence s is to pass it through a recurrent neural network like a LSTM Hochreiter &amp; Schmidhuber (1997) and use the
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1861492603,1889081078,2017814585,2070148066	[6, 7, 16, 17, 19, 20, 23, 28, 32, 39, 40, 43, 45]. Historically, progress in the research community on a given computer vision problem is typically preceded by a largescale, publicly-shared dataset [12, 25, 30, 33, 42]. However, a limitation of available VQA datasets is that all come from artiﬁcially created VQA settings. Moreover, none are “goal oriented” towards the images and questions that come from blind peopl
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1905882502	. Baselines. We benchmark eight methods. We include three variants of the only publicly-available method for predicting when a question is not relevant for an image [27]. This method uses NeuralTalk2 [21] pre-trained on the MSCOCO captions dataset [25] to generate a caption for each image. The algorithm then measures the similarity between the proposed caption and the question to predict a relevance s
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	627986001,1488163396,1575833922,2136462581,2416885651	algorithm training and evaluation. We next conduct experiments to characterize the images, questions, and answers and uncover unique aspects differentiating VizWiz from the many existing VQA datasets [6, 7, 16, 17, 19, 20, 23, 28, 32, 39, 40, 43, 45]. We ﬁnally evaluate numerous algorithms for predicting answers [17, 22] and predicting if a visual question can be answered [27]. Our ﬁndings highlight VizWiz is a difﬁcult dataset for modern vision
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	125693051	answers to their daily visual questions. This dataset is built off of previous work [8] which accrued 72,205 visual questions over Dataset Which Images? Who Asked? How Asked? DAQUAR [28] NYU Depth V2 [34] Automatically generated (templates) ——– VQA v1.0: Abstract [7] Abstract Scenes Crowd workers (AMT) Typed VQA v1.0: Real [7] MSCOCO [25] Crowd workers (AMT) Typed Visual Madlibs [43] MSCOCO [25] Autom
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	627986001,1575833922,1933349210	ctions for further algorithm improvements. Answering Visual Questions. The prevailing assumption when collecting answers to visual questions is that the questions are answerable from the given images [6, 7, 16, 17, 19, 23, 28, 32, 40, 39, 43, 45]. The differences when constructing VQA datasets thus often lies in whether to collect answers from anonymous crowd workers [6, 7, 16, 20, 23], automated methods [19, 28], or inhouse annotators [28, 3
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2108598243	d (templates) ——– SHAPES [6] Synthetic Shapes Automatically generated (templates) ——– Visual Genome [23] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT) Typed FVQA [39] MSCOCO [25] &amp; ImageNet [14] In-house participants Typed TDIUC [20] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT), In-house participants, Automatically generated Typed Ours - VizWiz Blind people use mobile phones to take a
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2416885651	e the content is appropriate. Alternatively, artiﬁcially constructed images come from controlled settings where either computer graphics is employed to synthesize images with known objects and scenes [6, 19] or crowd workers are employed to add any pre-deﬁned clipart objects to pre-deﬁned indoor and outdoor scenes [7]. In contrast, images collected “in the wild” can contain inappropriate or private conte
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1933349210	to enable the training and evaluation of algorithms, we collected new answers to all visual questions for this purpose. To collect answers, we modiﬁed the excellent protocol used for creating VQA 1.0 [7]. As done before, we collected 10 answers per visual question from AMT crowd workers located in the US by showing crowd workers a question and associated image and instructing them to return “a brief
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1861492603,1889081078,2017814585,2070148066	eople, which in turn yields new vision-based and language-based challenges. Images in Vision Datasets. When constructing vision datasets, prior work typically used images gathered from the web (e.g., [12, 25, 30, 33, 42]) or created artiﬁcially (e.g., [6, 7, 19]). Such images are typically high quality and safe for public consumption. For example, images curated from the web intrinsically pass a human quality assessm
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1990373827	es and questions that come from blind people. Yet, blind people arguably have been producing the big data desired to train algorithms. For nearly a decade, blind people have been both taking pictures [4, 8] and asking questions about the pictures they take [8, 11, 24]. Moreover, blind people often are early adopters of computer vision tools to support their real daily needs. We introduce the ﬁrst public
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1861492603	et Which Images? Who Asked? How Asked? DAQUAR [28] NYU Depth V2 [34] Automatically generated (templates) ——– VQA v1.0: Abstract [7] Abstract Scenes Crowd workers (AMT) Typed VQA v1.0: Real [7] MSCOCO [25] Crowd workers (AMT) Typed Visual Madlibs [43] MSCOCO [25] Automatically generated (templates) ——– FM-IQA [16] MSCOCO [25] Crowd workers (Baidu) Typed KB-VQA [40] MSCOCO [25] In-house participants Typ
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1933349210	ff of previous work [8] which accrued 72,205 visual questions over Dataset Which Images? Who Asked? How Asked? DAQUAR [28] NYU Depth V2 [34] Automatically generated (templates) ——– VQA v1.0: Abstract [7] Abstract Scenes Crowd workers (AMT) Typed VQA v1.0: Real [7] MSCOCO [25] Crowd workers (AMT) Typed Visual Madlibs [43] MSCOCO [25] Automatically generated (templates) ——– FM-IQA [16] MSCOCO [25] Crow
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2416885651	guage-based challenges. Images in Vision Datasets. When constructing vision datasets, prior work typically used images gathered from the web (e.g., [12, 25, 30, 33, 42]) or created artiﬁcially (e.g., [6, 7, 19]). Such images are typically high quality and safe for public consumption. For example, images curated from the web intrinsically pass a human quality assessment of “worthy to upload to the internet”
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2022699114	hms that also address the interests of blind people. Our work builds off previous work [8] which established a mobile phone application that supported blind people to ask over 70,000 visual questions [10] by taking a photo and asking a question about it. We begin our work by implementing a rigorous ﬁltering process to remove all visual questions that could risk compromising the safety and/or privacy o
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	627986001,1488163396,1575833922,1933349210,2136462581	hms. VQA Datasets. Over the past three years, a plethora of VQA datasets have been publicly shared to encourage a larger community to collaborate on developing algorithms that answer visual questions [6, 7, 16, 17, 19, 20, 23, 28, 32, 39, 40, 43, 45]. While a variety of approaches have been proposed to assemble VQA datasets, in all cases the visual questions were contrived. For example, all images were either taken from an existing vision dataset
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2022699114	ing the VizWiz mobile application1, an application available for iPhone and Android mobile phone platforms. Blind people used this application to ask about their daily visual accessibility challenges [8, 10]. A person asked a visual question by taking a picture and then recording a spoken question. The VizWiz application was released May 2011, and used by 11,045 users. 48,169 of the collected visual ques
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1981633181	ions are deemed unanswerable by crowd workers, despite the availability of several automated systems designed to guide blind photographers to improve the image focus [3], lighting [8], or composition [18, 37, 44]. We propose the ﬁrst VQA dataset which naturally promotes the problem of predicting whether a visual question is answerable. We construct our dataset by explicitly asking crowd workers whether a visu
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2012339105,2116680608	llenges. Success in developing automated methods would mitigate concerns about the many undesired consequences from today’s status quo for blind people of relying on humans to answer visual questions [8, 11, 24]; e.g., humans often must be paid (i.e., potentially expensive), can take minutes to provide an answer (i.e., slow), are not always available (i.e., potentially not scalable), and pose privacy issues
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2095781211	located paid crowd workers [8, 24] or volunteers [1]. Such VQA systems have been shown to be valuable for many daily tasks including grocery shopping [8], locating a speciﬁc object in a complex scene [9], and choosing clothes to wear [11]. Yet, these systems are limited because they rely on humans to provide answers. An automated solution would be preferred for reasons such as cost, latency, scalabil
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1488163396,1933349210,2136462581	MSCOCO [25]) or artiﬁcially constructed (e.g., Abstract Scenes [7], computer graphics [6, 19]). In addition, questions were generated either automatically [6, 19, 20, 28, 32, 43], from crowd workers [7, 16, 17, 20, 23, 45], or from in-house participants [20, 40]. We introduce the ﬁrst VQA dataset which reﬂects visual questions asked by people who were authentically trying to learn about the visual world. This enables u
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1861492603	MSCOCO [25] Crowd workers (AMT) Typed CLEVR [19] Synthetic Shapes Automatically generated (templates) ——– SHAPES [6] Synthetic Shapes Automatically generated (templates) ——– Visual Genome [23] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT) Typed FVQA [39] MSCOCO [25] &amp; ImageNet [14] In-house participants Typed TDIUC [20] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT), In-house participan
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2116680608	new perspectives about the VQA problem. 2. Related Works VQA for Blind Users. For nearly a decade, humanpowered VQA systems have enabled blind people to overcome their daily visual challenges quickly [1, 8, 24]. With such systems, users employ a mobile phone application to capture a photo (or video), ask a question about it, and then receive an answer from remotely located paid crowd workers [8, 24] or volu
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	627986001,1488163396,1575833922,2136462581,2416885651	ng (VQA) problem, which aims to accurately answer any question about any image. Over the past three years, 14 VQA datasets have emerged in the vision community to catalyze research on the VQA problem [6, 7, 16, 17, 19, 20, 23, 28, 32, 39, 40, 43, 45]. Historically, progress in the research community on a given computer vision problem is typically preceded by a largescale, publicly-shared dataset [12, 25, 30, 33, 42]. However, a limitation of avai
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2101105183	one, and we refer to these as VizWiz [17] and VizWiz [22]. Evaluation Metrics. We employ ﬁve metrics commonly used for evaluating VQA and image description algorithms: Accuracy [7], CIDEr [38], BLEU4 [29], and ME5Practical issues led to a dataset with ˘2,000 fewer visual questions. TEOR [15]. The accuracy method [7] was introduced based on the observation that most answers in the VQA 1.0 dataset were
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2012339105,2116680608	people arguably have been producing the big data desired to train algorithms. For nearly a decade, blind people have been both taking pictures [4, 8] and asking questions about the pictures they take [8, 11, 24]. Moreover, blind people often are early adopters of computer vision tools to support their real daily needs. We introduce the ﬁrst publicly-available vision dataset originating from blind people, whi
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2166789485	could risk compromising the safety and/or privacy of any individuals associated with them, since blind people often willingly share personal information with strangers to overcome personal obstacles [5]. We then crowdsource answers to support algorithm training and evaluation. We next conduct experiments to characterize the images, questions, and answers and uncover unique aspects differentiating Vi
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2250384498	rkers (AMT) Typed CLEVR [19] Synthetic Shapes Automatically generated (templates) ——– SHAPES [6] Synthetic Shapes Automatically generated (templates) ——– Visual Genome [23] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT) Typed FVQA [39] MSCOCO [25] &amp; ImageNet [14] In-house participants Typed TDIUC [20] MSCOCO [25] &amp; YFCC100M [35] Crowd workers (AMT), In-house participants, Automatically ge
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1861492603	ty of approaches have been proposed to assemble VQA datasets, in all cases the visual questions were contrived. For example, all images were either taken from an existing vision dataset (e.g., MSCOCO [25]) or artiﬁcially constructed (e.g., Abstract Scenes [7], computer graphics [6, 19]). In addition, questions were generated either automatically [6, 19, 20, 28, 32, 43], from crowd workers [7, 16, 17,
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2166789485	uals involved with the dataset. This is especially important for visually impaired people, because they often make the tradeoff to reveal personal information to a stranger in exchange for assistance [5]; e.g., credit card numbers and personal mail. This is also important for those reviewing the dataset since visual questions can contain “adult-like” (e.g., nudity), and so potentially offensive conte
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2012339105	or volunteers [1]. Such VQA systems have been shown to be valuable for many daily tasks including grocery shopping [8], locating a speciﬁc object in a complex scene [9], and choosing clothes to wear [11]. Yet, these systems are limited because they rely on humans to provide answers. An automated solution would be preferred for reasons such as cost, latency, scalability, and enhanced privacy. For exam
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	1956340063	zWiz data alone, and we refer to these as VizWiz [17] and VizWiz [22]. Evaluation Metrics. We employ ﬁve metrics commonly used for evaluating VQA and image description algorithms: Accuracy [7], CIDEr [38], BLEU4 [29], and ME5Practical issues led to a dataset with ˘2,000 fewer visual questions. TEOR [15]. The accuracy method [7] was introduced based on the observation that most answers in the VQA 1.0 d
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2108598243	since our ﬁltering process erred on removing “suspicious” scene-based and blurry images from our dataset paired with containing many object recognition questions (see Figure 2). Following prior work [14], we ﬁrst computed the average image from all images in VizWiz. Figure 3 shows the result. As desired from a diverse dataset, the resulting gray image conﬁrms our dataset does not conform to a particu
2788643321	VizWiz Grand Challenge: Answering Visual Questions from Blind People	2143449221	ﬁve metrics commonly used for evaluating VQA and image description algorithms: Accuracy [7], CIDEr [38], BLEU4 [29], and ME5Practical issues led to a dataset with ˘2,000 fewer visual questions. TEOR [15]. The accuracy method [7] was introduced based on the observation that most answers in the VQA 1.0 dataset were one word in length. However, since nearly half of the answers in VizWiz exceed one word
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2113596997	:Physics :Albert_Einstein dbo:birthPlace :Ulm :Albert_Einstein dbo:deathPlace :Princeton Listing 1: An excerpt of RDF triples. 3.2. Approach RDF2PT approach is akin to the approach SPARQL2NL (Ngonga Ngomo et al., 2013) from which the project SemWeb2NL3 originated. SemWeb2NL comprises rule-based and template-based approaches which aim to verbalize texts and concepts not only from RDF triples but also from ontologies
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2273063868	2004; Hewlett et al., 2005; Sun and Mellish, 2006). Despite the plethora of works written on handling SWT data, only a few have exploited the generation of languages other than English, for instance, Keet and Khumalo (2017) to Zulu language. Additionally, a considerable number of NLG approaches can be found to European or Brazilian 1https://github.com/dice-group/RDF2PT arXiv:1802.08150v1 [cs.CL] 22 Feb 2018 Portuguese l
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2516703979	alize all the verbs in the past tense. 11See the complete list of dependency parsing tags in Ngonga Ngomo et al. (2013). 4. Evaluation We based our evaluation methodology on Gardent et al. (2017) and Ferreira et al. (2016). Our main goal was to evaluate how well RDF2PT represents the information obtained from the data. We hence divided our evaluation set into expert and non-expert users. Both sets were made up of nativ
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	1600480717	anguage. Additionally, a considerable number of NLG approaches can be found to European or Brazilian 1https://github.com/dice-group/RDF2PT arXiv:1802.08150v1 [cs.CL] 22 Feb 2018 Portuguese languages (Pereira and Paraboni, 2008; Cuevas and Paraboni, 2008; de Novais et al., 2009; de Novais et al., 2010; de Novais et al., 2012; de Novais and Paraboni, 2013; De Oliveira and Sripada, 2014; Pereira et al., 2015), however, none o
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2251652254	bject of research has only recently gained signiﬁcant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques such as (Sleimi and Gardent
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2146579954	data. However, the subject of research has only recently gained signiﬁcant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) challenge, which used deep learning techniques s
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2113596997	date of a given resource through some predicate, for example dbo:deathDate, RDF2PT is able to lexicalize all the verbs in the past tense. 11See the complete list of dependency parsing tags in Ngonga Ngomo et al. (2013). 4. Evaluation We based our evaluation methodology on Gardent et al. (2017) and Ferreira et al. (2016). Our main goal was to evaluate how well RDF2PT represents the information obtained from the data
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2250459435	h, German, Italian and Spanish. To this end, we will exploit the similarity among their syntaxes in the micro-planning task and we will reuse their respective SimpleNLG versions (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) for the realization task. Acknowledgments This work has been supported by the H2020 project HOBBIT (GA no. 688227) and supported by the Brazilian N
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2766347905	im is to create Brazilian Portuguese silver standard datasets which are able to be uploaded into GERBIL(Usbeck et al., 2015) for an easy evaluation. To this end, we aim to implement RDF2PT in BENGAL (Ngomo et al., 2017), which is an approach for automatically generating NER benchmarks based on RDF triples and Knowledge Graphs. This application has already resulted in promising datasets which we have used to investig
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2113596997	r of inputs have been taken for NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of natural language from SW, more precisely from RDF data, has gained substantial attention (Bouayad-Agha et al., 2014; Staykova, 2014). Some ch
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	1832719046	ion variables and only after that, turning to other variables. This method has already been used by other approaches and is the most effective method to follow regarding rule-based approaches to RDF (Bouayad-Agha et al., 2014). As an example, consider the following triples in Listing 3. :Albert_Einstein dbo:deathPlace :Princeton. :Princeton dbo:Country :USA. 4The predicates can vary according to the classes and KB 5This ch
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	1832719046	lly from RDF). The version of RDF2PT used in this paper, all experimental results and the texts generated for the experiments are publicly available.1 2. Related Work According to Staykova (2014) and Bouayad-Agha et al. (2014), there has been a plenty of works which investigated the generation of Natural Language (NL) texts from Semantic Web Technologies (SWT) as an input data. However, the subject of research has only rec
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2113596997	masculine and feminine which in turn results in the gender. If the resource is not a person, we use Tree-tagger. Properties - The lexicalization of properties relies on one of the results of Ngonga Ngomo et al. (2013), i.e., that most property labels are either nouns or verbs. To determine which lexicalization to use automatically, we rely on the insight that the ﬁrst and last words of a property label in Portugue
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	1832719046	n taken for NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of natural language from SW, more precisely from RDF data, has gained substantial attention (Bouayad-Agha et al., 2014; Staykova, 2014). Some challenges have been proposed
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2516703979	n) is the task responsible for generating syntagms (references) to discourse entities, for example, whether the text should refer to an entity using a deﬁnite description, a pronoun or a proper noun (Ferreira et al., 2016). In the following, we describe the challenges behind the tasks entailed. Sentence aggregation This task is based on Ngonga Ngomo et al. (2013). It is divided into two phases, subject grouping and obj
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2113596997	ng a deﬁnite description, a pronoun or a proper noun (Ferreira et al., 2016). In the following, we describe the challenges behind the tasks entailed. Sentence aggregation This task is based on Ngonga Ngomo et al. (2013). It is divided into two phases, subject grouping and object grouping. Subject grouping collapses the predicates and objects of two triples if their subjects are the same. Object grouping collapses th
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2516703979	rowdFlower and is publicly available.13 The experiment was performed by 30 participants (10 per list). They were asked to rate each text considering the clarity and ﬂuency based on two questions from Ferreira et al. (2016) on a scale from 1 (Very Bad) to 5 (Very Good). The questions were: (1) Fluency: Does the text present a consistent, logical ﬂow? (2) Clarity: Is the text easy to understand? 4.1. Results Experts Figu
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	42170355	S tags http://www.cis.uni-muenchen. de/˜schmid/tools/TreeTagger/data/ Portuguese-Tagset.html 7Note that it could be any property which returns a natural language representation of the given URI, see (Ell et al., 2011). resource is recognized as a person, RDF2PT applies a string similarity measure (0.8 threshold) between the lexicalized word with a list of names provided by SemWeb2NL. This list is divided by mascul
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	74376760	Semantic Web Technologies (SWT) as an input data. However, the subject of research has only recently gained signiﬁcant momentum. This attention comes from the great number of published works such as (Cimiano et al., 2013; Duma and Klein, 2013; Ell and Harth, 2014; Biran and McKeown, 2015) which used RDF as an input data and achieved promising results. Also, the works published in the WebNLG (Colin et al., 2016) chall
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	92357348	siderable number of NLG approaches can be found to European or Brazilian 1https://github.com/dice-group/RDF2PT arXiv:1802.08150v1 [cs.CL] 22 Feb 2018 Portuguese languages (Pereira and Paraboni, 2008; Cuevas and Paraboni, 2008; de Novais et al., 2009; de Novais et al., 2010; de Novais et al., 2012; de Novais and Paraboni, 2013; De Oliveira and Sripada, 2014; Pereira et al., 2015), however, none of them have exploited the g
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	2250711464	an and Spanish. To this end, we will exploit the similarity among their syntaxes in the micro-planning task and we will reuse their respective SimpleNLG versions (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) for the realization task. Acknowledgments This work has been supported by the H2020 project HOBBIT (GA no. 688227) and supported by the Brazilian National Council for Scient
2788689700	RDF2PT: Generating Brazilian Portuguese Texts from RDF Data.	1514535095	text and speech output of these systems, there is far less consensus on what the input should be (Gatt and Krahmer, 2017). A large number of inputs have been taken for NLG systems, including images (Xu et al., 2015), numeric data (Gkatzia et al., 2014), semantic representations (Theune et al., 2001) and Semantic Web (SW) data (Ngonga Ngomo et al., 2013; Bouayad-Agha et al., 2014). Presently, the generation of na
2788784374	Learning Word Vectors for 157 Languages	2153579005	(2015) for Chinese. One of the contributions of this work is the introductionof word analogy datasets for French, Hindi and Polish. To build these datasets, we use the English analogies introduced by Mikolov et al. (2013a) as a starting point. Most of the word pairs are directly translated, and we introduced some modiﬁcations,which are speciﬁc for each language. French. We directly translated all the word pairs in th
2788784374	Learning Word Vectors for 157 Languages	1523296404	100B tokens) were published with word2vec(Mikolov et al., 2013b). Penningtonet al. (2014) released GloVe models trained on Wikipedia, Gigaword and Common Crawl (840B tokens). AnotableeffortistheworkofAl-Rfou et al. (2013), in which word vectors have been trained for 100 languages using Wikipedia data. 2. Training Data We train our word vectors using datasets composed of a mixture of Wikipedia and Common Crawl. 2.1. Wi
2788784374	Learning Word Vectors for 157 Languages	2153579005	corpora have been released alongside open source implementation of word embedding models. English word vectors trained on a part of the Google News dataset (100B tokens) were published with word2vec(Mikolov et al., 2013b). Penningtonet al. (2014) released GloVe models trained on Wikipedia, Gigaword and Common Crawl (840B tokens). AnotableeffortistheworkofAl-Rfou et al. (2013), in which word vectors have been trained
2788784374	Learning Word Vectors for 157 Languages	2493916176	details the procedure for splitting the data by language and pre-processingit in Section 2. Using this data, we trained word vectors using an extension of the fastText model with subword information(Bojanowski et al., 2017),as describedinSection3. In Section 4, we introduce three new word analogy datasets for French, Hindi and Polish and evaluate our word representations on word analogy tasks. Overall, we evaluate our w
2788784374	Learning Word Vectors for 157 Languages	2161494021	ed, the correspondingtext is of high quality, making Wikipedia a great resource for (multilingual) natural language processing. It has been applied to many different tasks,suchasinformationextraction(Wu and Weld, 2010), orwordsensedisambiguation(Mihalcea, 2007). We downloaded the XML Wikipedia dumps from September 11, 2017. Theﬁrst preprocessingstep is to extractthe text content from the XML dumps. For this purpose
2788784374	Learning Word Vectors for 157 Languages	1580825123	making Wikipedia a great resource for (multilingual) natural language processing. It has been applied to many different tasks,suchasinformationextraction(Wu and Weld, 2010), orwordsensedisambiguation(Mihalcea, 2007). We downloaded the XML Wikipedia dumps from September 11, 2017. Theﬁrst preprocessingstep is to extractthe text content from the XML dumps. For this purpose, we used a modiﬁed version of the wikifil.
2788784374	Learning Word Vectors for 157 Languages	2501208544	nce :: Rome : Italy. Such datasets are usually composed of all the possible combinations of pairs such as Paris : France, Berlin : Germany or Beijing : China. In our evaluation, we use the dataset of Svoboda and Brychcin (2016) for Czech, that of Ko¨per et al. (2015) for German, that of Cardellino (2016) for Spanish, that of Venekoski and Vankka (2017) for Finnish, that of Berardi et al. (2015) for Italian, the European var
2788784374	Learning Word Vectors for 157 Languages	2493916176	ngth 3 to 6. One motivation for using fewer n-grams is that the corresponding models are much more efﬁcient to learn. • CBOW: using the model described in Sec. 3. instead of the skipgram variant from Bojanowski et al. (2017). • +negatives: using more negative examples. By default, the fastTextlibrary samples 5 negative examples. Here, we proposeto use 10 negatives. • +epochs: using more epochs to train the models. By def
2788784374	Learning Word Vectors for 157 Languages	2493916176	odels Inthis section, we brieﬂydescribethe two methodsthat we compareto train our word vectors. Skipgram. The ﬁrst model that we consider is the skipgram model with subword information, introduced by Bojanowski et al. (2017). This model, available as part of the fastText3 software, is an extension of the skipgram model, where word representations are augmented using character ngrams. A vector representation is associated
2788784374	Learning Word Vectors for 157 Languages	2394700483	r Spanish, that of Venekoski and Vankka (2017) for Finnish, that of Berardi et al. (2015) for Italian, the European variant of the dataset proposed by Hartmann et al. (2017) for Portugueseand that of Chen et al. (2015) for Chinese. One of the contributions of this work is the introductionof word analogy datasets for French, Hindi and Polish. To build these datasets, we use the English analogies introduced by Mikolo
2788784374	Learning Word Vectors for 157 Languages	2493916176	vectors of the character ngrams appearing in the word. The full word is always includedas part of the characterngrams, so that the model still learns one vector for each word. We refer the reader to Bojanowski et al. (2017) for a more thorough description of this model. CBOW. The second model that we consider is an extension of the CBOW model (Mikolovet al., 2013b), with position weights and subword information. Similar
2788784374	Learning Word Vectors for 157 Languages	1523296404	wordvectorsdirectlydependsontheamountandqualityof data they were trained on. A common source of data to learn word representations, available in many languages, is the online encyclopedia Wikipedia (Al-Rfou et al., 2013). This provides high quality data which is comparable across languages. Unfortunately, for many languages, the size of Wikipedia is relatively small, and often not enough to learn high quality word ve
2788844247	LIdioms: A Multilingual Linked Idioms Data Set.	2102153514	), which was released with the´ arXiv:1802.08148v1 [cs.CL] 22 Feb 2018 main purpose of describing translations among lexical entries. Another resource that describes multilingual content is BabelNet (Navigli and Ponzetto, 2010), which integrates knowledge from various lexical resources, such as WordNet (Miller, 1995). Additionally, BabelNet has adopted the lemon structure for representing lexical entries (Ehrmann et al., 20
2788844247	LIdioms: A Multilingual Linked Idioms Data Set.	2250214110	nd Ponzetto, 2010), which integrates knowledge from various lexical resources, such as WordNet (Miller, 1995). Additionally, BabelNet has adopted the lemon structure for representing lexical entries (Ehrmann et al., 2014). Although these resources are linked lexical multilingual data sets, they contain a limited number of idioms described correctly along with their respective translations across languages. This lack o
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2612467493	(Bollacker et al., 2008) and WordNet (Miller et al., 1990). The knowledge present in these knowledge bases includes common knowledge and partially covers common-sense knowledge and domain knowledge (Song and Roth, 2017). Knowledge Graphs and Knowledge Bases are conceptually equivalent for our purpose and we will use the name interchangeably in this paper. We illustrate the signiﬁcance of world knowledge using a few
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2127426251	(Shi and Weninger, 2017). Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic information. Neural Tensor Network(NTN) (Socher et al., 2013) was the pioneering work in this ﬁeld which initialized entity vectors with the average word embeddings followed by tensor-based operations. Recent works involving this idea are “Joint Alignment” (Zho
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2250807343	13) was the pioneering work in this ﬁeld which initialized entity vectors with the average word embeddings followed by tensor-based operations. Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017). DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model. Pretra
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2094728533	amples of such systems are NELL (Mitchell et al., 2015) and DeepDive (Niu arXiv:1802.05930v2 [cs.CL] 21 May 2018 et al., 2012). There are human created knowledge bases as well, like Freebase (FB15k) (Bollacker et al., 2008) and WordNet (Miller et al., 1990). The knowledge present in these knowledge bases includes common knowledge and partially covers common-sense knowledge and domain knowledge (Song and Roth, 2017). Kno
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2283196293	The convolution based model, helped to reduce the space over entities and relationships over which attention had to be generated. However more sophisticated techniques using similarity based search (Wang et al., 2014a; Mu and Liu, 2017) can be pursued towards this purpose. The results from the initial experiments illustrates the effectiveness of our proposed approach, advocating further investigations in these di
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2250539671	e models were implemented using TensorFlow (Abadi et al., 2015). The relevant hyperparameters are listed in Table 2. The word embeddings for the experiments were obtained using the pre-trained GloVe (Pennington et al., 2014)2 vectors. For words missing in the pre-trained vectors, the local GloVe vectors which was trained on the corresponding dataset was used. 4.2 Results &amp; Discussion Table 3 shows the results of test
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2510312297	ed model, helped to reduce the space over entities and relationships over which attention had to be generated. However more sophisticated techniques using similarity based search (Wang et al., 2014a; Mu and Liu, 2017) can be pursued towards this purpose. The results from the initial experiments illustrates the effectiveness of our proposed approach, advocating further investigations in these directions. References
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2612467493	eing infused into the learning model for any given task. By the same logic, our work is different from domain adaptation (Glorot et al., 2011) as well. There has been attempts to use world knowledge (Song and Roth, 2017) for creating more labeled training data and providing distant supervision etc. Incorporating Inductive Biases (Ridgeway, 2016) based on the known information about a domain onto the structure of the
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2094728533	experiments on DBPedia ontology classiﬁcation dataset1, with a very strong baseline. These datasets are chosen as they share domain knowledge with two most popular knowledge bases, Freebase (FB15k) (Bollacker et al., 2008) and WordNet (WN18) (Bordes et al., 2013). The training and test size of the datasets are mentioned in Table 1. 1http://wiki.dbpedia.org/ services-resources/dbpedia-data-set-2014 Dataset Train Size Te
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	1840435438	our experiments, we have mainly used the popular text classiﬁcation dataset 20Newsgroups (Lichman, 2013) and the Natural Language Inference dataset, Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015). We have also done experiments on DBPedia ontology classiﬁcation dataset1, with a very strong baseline. These datasets are chosen as they share domain knowledge with two most popular knowledge bases,
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2172684358	iﬁc hyperplane to translate the entities. Similar work utilizing only the structure of the graph include ManifoldE (Xiao et al., 2015b), TransG (Xiao et al., 2015a), TransD (Ji et al., 2015), TransM (Fan et al., 2014), HolE (Nickel et al., 2016b) and ProjE (Shi and Weninger, 2017). Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic i
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2556343638	lizing only the structure of the graph include ManifoldE (Xiao et al., 2015b), TransG (Xiao et al., 2015a), TransD (Ji et al., 2015), TransM (Fan et al., 2014), HolE (Nickel et al., 2016b) and ProjE (Shi and Weninger, 2017). Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic information. Neural Tensor Network(NTN) (Socher et al., 2013) was
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2336384382	n this ﬁeld which initialized entity vectors with the average word embeddings followed by tensor-based operations. Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017). DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model. Pretrained word2vec (Mikolov et al
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	1663973292	ntation of similar entity/relation vectors and attending over them. Figure 4: Convolution model cluster representation In order to cluster similar entity/relation vectors, we used k-means clustering (Bishop, 2006) and formed lclusters with equal number of entity/relation vectors in each cluster. Each of the clusters were then encoded using convolutional ﬁlters. The output of the k-means clustering is a sequenc
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2283196293	y to object entity using one-dimensional relation vector (h+ r = t). Variants of the TransE (Bordes et al., 2013) model uses translation of the entity vectors over relation speciﬁc subspaces. TransH (Wang et al., 2014b) introduced the relation-speciﬁc hyperplane to translate the entities. Similar work utilizing only the structure of the graph include ManifoldE (Xiao et al., 2015b), TransG (Xiao et al., 2015a), Tra
2788902803	LEARNING BEYOND DATASETS: KNOWLEDGE GRAPH AUGMENTED NEURAL NETWORKS FOR NATURAL LANGUAGE PROCESSING	2499696929	zed entity vectors with the average word embeddings followed by tensor-based operations. Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017). DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model. Pretrained word2vec (Mikolov et al., 2013) is used to form
2789065247	MULTIMODAL NAMED ENTITY RECOGNITION FOR SHORT SOCIAL MEDIA POSTS	2250539671	15%), and test sets (15%). The captions data have average length of 30.7 characters (5.81 words) with vocabulary size 15,733, where 6,612 are considered unknown tokens from Stanford GloVE embeddings (Pennington et al., 2014). Named entities annotated in the SnapCaptions dataset include many of new and emerging entities, and they are found in various surface forms (various nicknames, typos, etc.) To the best of our knowle
2789065247	MULTIMODAL NAMED ENTITY RECOGNITION FOR SHORT SOCIAL MEDIA POSTS	2004763266	dings, especially when there are many missing tokens in the given word embedding matrix. Note that we do not explore the use of gazetteers information or other auxiliary information (POS tags, etc.) (Ratinov and Roth, 2009) as it is not the focus of our study. Attention modules are widely applied in several deep learning tasks (Xu et al.,2015;Chan et al.,2015;Sukhbaatar et al.,2015;Yao et al., 2015). For example, they u
2789065247	MULTIMODAL NAMED ENTITY RECOGNITION FOR SHORT SOCIAL MEDIA POSTS	2179519966	mbeddings, and visual embeddings representations, respectively. 3.1 Features Similar to the state-of-the-art NER approaches (Lample et al.,2016;Ma and Hovy,2016;Aguilar et al.,2017;Passos et al.,2014;Chiu and Nichols, 2015;Huang et al.,2015), we use both word embeddings and character embeddings. Word embeddings are obtained from an unsupervised learning model that learns co-occurrence statistics of words from a large e
2789065247	MULTIMODAL NAMED ENTITY RECOGNITION FOR SHORT SOCIAL MEDIA POSTS	1895577753	is an open challenge, and multiple approaches can be considered. For instance, one may provide visual contexts only as an initial input to decoder as in some encoderdecoder image captioning systems (Vinyals et al., 2015). However, we empirically observe that an NER decoder which takes as input the visual embeddings at every decoding step (Section 3.2), combined with the modality attention module (Section3.3), yields
2789065247	MULTIMODAL NAMED ENTITY RECOGNITION FOR SHORT SOCIAL MEDIA POSTS	1586939924	S tags, etc.) (Ratinov and Roth, 2009) as it is not the focus of our study. Attention modules are widely applied in several deep learning tasks (Xu et al.,2015;Chan et al.,2015;Sukhbaatar et al.,2015;Yao et al., 2015). For example, they use an attention module to attend to a subset within a single input (a part/region of an image, a speciﬁc token in an input sequence of tokens, etc.) at each decoding step in an en
2789277863	A SIMPLE AND EFFECTIVE APPROACH TO THE STORY CLOZE TEST	2586358499	t al.(2017) established a neural baseline for models trained on the validation set, with a test-set accuracy of 74.7%. They were also able to achieve a marginally better accuracy of 72.5% (compared toSchwartz et al. (2017b)) when using just the sentence endings and ignoring the context; and this approach did not require any feature engineering. They showed that a human can distinguish ‘right’ from ‘wrong’ endings with
2789472613	Explain Yourself: A Natural Language Interface for Scrutable Autonomous Robots.	2160379528	(AxV) now routinely operate in regions that are dangerous or impossible for humans to reach, such as the deep underwater environment. Typically, remote robots instil less trust than those co-located [1, 6]. This combined with high vulnerability in hazardous, high-stakes environments, such as that described in [7], means that the interface between operator and AxV is key in maintaining situation awarene
2789472613	Explain Yourself: A Natural Language Interface for Scrutable Autonomous Robots.	2767535632	e. In this paper, we focus on explanations of behaviours and describe a method that is agnostic to the type of autonomy method. With respect to providing communication for monitoring, please refer to [5] for further details and an overview of the system. 2 EXPLANATIONS FOR REMOTE AUTONOMY Types of explanations include why to provide a trace or reasoning and whynotto elaborate on the system’s control
2789472613	Explain Yourself: A Natural Language Interface for Scrutable Autonomous Robots.	2154157725	e system. 2 EXPLANATIONS FOR REMOTE AUTONOMY Types of explanations include why to provide a trace or reasoning and whynotto elaborate on the system’s control method or strategy [4]. Lim et al. (2009) [10] show that both why and why not explanations increase understanding but only why increases trust. We adopt here the ‘speak-aloud’ method whereby an expert provides rationalisation of the AxV behaviour
2789472613	Explain Yourself: A Natural Language Interface for Scrutable Autonomous Robots.	581437709	ther robots and systems are used. In previous work, explanations have been categorised as either explaining 1) machine learning as in [11] who showed that they can increase trust; 2) explaining plans [2, 13]; 3) verbalising robot [12] or agent rationalisation [3]. However, humans do not present a constant verbalisation of their actions but they do need to be able to provide information on-demand about wh
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2533667621	al Neural Networks (CNNs) are a powerful method and show very good results in natural language processing. There has been much NLP research that used CNNs, such as [23], [24] and [13]. In particular, [25] presented an integrated CNN and Lexicon models, with one of them called Naïve Concatenation. Some modifications were applied to this model in order to implement the sentiment analysis on our Arabic H
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	1984708705	Arabic language using an adaption from other languages, such as English; [4] introduced a tool for preprocessing Arabic text, which include root stemmer, part-of-speech tagger (POS-tagger), etc.; and [9] reported some challenges in dealing with the Arabic language in NLP and described some solutions for these. Moreover, has received individual attention: [10] used some machine learning methods for se
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	937194849	bic language has also increased: [2] is a book for researchers dealing with Arabic NLP; [3]presented a rule-based approach for Arabic language using an adaption from other languages, such as English; [4] introduced a tool for preprocessing Arabic text, which include root stemmer, part-of-speech tagger (POS-tagger), etc.; and [9] reported some challenges in dealing with the Arabic language in NLP and
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2591716527	dataset (AHS) from 0.85 to 0.92 for the Main dataset, and from 0.87 to 0.95 for the Sub-dataset. Finally, this paper presents an improved accuracy, reaching 0.92, compared to our previous results in [1] that were 0.90 on the Main-Dataset. We plan future studies to deal with negation words in Arabic, as the negative or the opposite word meaning might be avoided by viewing it as a compound word of two
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2591716527	to download from a Bitbucket repository: [https://bitbucket.org/a_alayba/arabic-health-services-ahs-dataset/src] B. Sentiment classification Previous experiments on the main dataset were described in [1], where the accuracy results were between 0.85 and 0.90 using Naïve Bayes, Support Victor Machine, Logistic Regression and Basic Deep and Convolutional Neural Networks. In this experiment, different t
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	1832693441	nal Deep Neural Networks Convolutional Neural Networks (CNNs) are a powerful method and show very good results in natural language processing. There has been much NLP research that used CNNs, such as [23], [24] and [13]. In particular, [25] presented an integrated CNN and Lexicon models, with one of them called Naïve Concatenation. Some modifications were applied to this model in order to implement th
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2166706824	nd the way the features are used within the machine learning classifiers. Finally, Section V presents our conclusions and plans for future work. II. RELATED WORK Sentiment analysis gained exposure in [6], where three machine learning algorithms were used: Naïve Bayes, Maximum Entropy, and Support Vector Machines. Since [6] the amount of research on sentiment analysis has significantly increased. For
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2251137535	Networks Convolutional Neural Networks (CNNs) are a powerful method and show very good results in natural language processing. There has been much NLP research that used CNNs, such as [23], [24] and [13]. In particular, [25] presented an integrated CNN and Lexicon models, with one of them called Naïve Concatenation. Some modifications were applied to this model in order to implement the sentiment ana
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2251137535	rphology-based and lexical features for Arabic sentiment analysis; [12] improved the performance of sentiment analysis for Arabic, using different techniques like stemming, POS and expanding lexicon; [13] used deep neural networks with three different architectures for Arabic sentiment analysis; and [14] considered building an Arabic lexicon, manually and automatically. III. ARABIC WORD EMBEDDING Word
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	937194849	using Part of Speech (POS) taggers, using lexicon based approaches, as well as by combining with word distributing techniques. in NLP for the Arabic language in recent years, for example [2], [3] and [4]. However, there is still a need to tackle the complexity of NLP tasks in Arabic. This complexity comes from many aspects, such as morphology, orthography, dialects, short vowels and word order. For e
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2591716527	study is the CBOW with 200 dimensions. IV. SENTIMENT ANALYSIS A. by Dataset In this experiment, the Main dataset is our previously proposed dataset of Arabic tweets about health services described in [1]. The dataset was collected from Twitter and contains 628 positive tweets, and 1398 negative tweets, to give a total of 2026. As the dataset was labeled by only three human annotators, it can be hard
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	1614298861	t is no longer the semantic or syntactic word distribution that is still a crucial challenge. There are two methods that are widely used for word semantic distribution, which are Word2Vec proposed by [15], and GloVe introduced by (s[16]. These methods can take unstructured text and implement some mathematical equations, to implementby representing each word in the text by a vector. All vectors that wa
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2088394786	t-of-speech tagger (POS-tagger), etc.; and [9] reported some challenges in dealing with the Arabic language in NLP and described some solutions for these. Moreover, has received individual attention: [10] used some machine learning methods for sentiment classification; [11] presented an annotated Arabic dataset and applied morphology-based and lexical features for Arabic sentiment analysis; [12] impro
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2591716527	volutional neural networks with different text feature selections, we report improved accuracy of sentiment classification (91%-95%) on our publicly available Arabic language health sentiment dataset [1]. Keywords — Arabic Sentiment Analysis, Machine Learning, Convolutional Neural Networks, Word Embedding, Word2Vec for Arabic, Lexicon. I. INTRODUCTION Sentiment Analysis is one of the Natural Language
2789618396	Improving Sentiment Analysis in Arabic Using Word Representation	2250539671	yntactic word distribution that is still a crucial challenge. There are two methods that are widely used for word semantic distribution, which are Word2Vec proposed by [15], and GloVe introduced by (s[16]. These methods can take unstructured text and implement some mathematical equations, to implementby representing each word in the text by a vector. All vectors that was used to builare close to each
2789812779	The Rapidly Changing Landscape of Conversational Agents.	178897730	akers of English. They came up with Lets Go project (Raux et al, 2003 [9]) that was designed to provide Pittsburgh area bus information. Later, this was opened to the general public (Raux et al, 2005 [10]). Their work is important in terms of the techniques they used. The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Fre
2789812779	The Rapidly Changing Landscape of Conversational Agents.	1486649854	aknesses in existing metrics and provide recommendations for the future development of better automatic evaluation metrics for dialogue systems. According to them, the metrics (like Kiros et al, 2015 [33]) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to ﬁnd a sign
2789812779	The Rapidly Changing Landscape of Conversational Agents.	2001050921	earning methods in the dialogue and personal agents. 6.1. Initial reinforcement methods One of the ﬁrst main papers that thought of using reinforcement learning for this came in 2005 by English et al [26]. They used an on-policy Monte Carlo method and the objective function they used was a linear combination of the solution quality (S) and the dialog length (L), taking the form: o(S,I) = w 1S - w 2L.
2789812779	The Rapidly Changing Landscape of Conversational Agents.	2581637843	erative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans. In the paper by Li et al, 2017 [29], the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utte
2789812779	The Rapidly Changing Landscape of Conversational Agents.	2513380446	t learning methods into the area of dialog and personal agents. 6.2. End-to-End Reinforcement Learning of Dialogue Agents for Information Access Lets have a look at KB-InfoBot (by Dhingra et al, 2017 [27]): a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. In this paper, they replace the symbolic queries (which break the differentiability
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	1531374185	= argmax y 1;::y n YN i=1 P(y ijy 1;::y i 1;w 1::w M;)(2) Equation (2) is to be realized using a RNN-based architecture, which is described in detail in Section 6.1. 3 Related Work Heilman and Smith [9] use a set of hand-crafted syntax-based rules to generate questions from simple declarative sentences. The system identifies multiple possible answer phrases from all declarative sentences using the c
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	2137006453	the approaches proposed for the QGSTEC challenge [12] are also rule-based systems, some of which put to use sentence features such as part of speech (POS) tags and named entity relations (NER) tags. [3] use ASSERT (an automatic statistical semantic role tagger that can annotate naturally occurring text with semantic arguments) for semantic role parses, generate questions based on rules and rank them
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	2410082850	ase triples (subject, relation, object). Additionally, recent studies suggest that the sharp learning capability of neural networks does not make linguistic features redundant in machine translation. [16] suggest augmenting each word with its linguistic features such as POS, NER. [8] suggest a tree-based encoder to incorporate features, although for a different application. We build on the recent sequ
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	2072385577,2133459682	composer journalist and newspaper editor william henry wills , ron goodwin , and journalist angela rippon and comedian dawn french&quot;,theanswerpointersproduced are: Pointer(s) by answer sequence: [6,11,20] !journalist henry rippon Pointer(s) by answer boundary: [10,12] !william henry wills Fig.2: Answer selection using boundary pointer network. 6 Automating reading comprehension by generating question
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	2072385577	eclarative sentences using the constituency parse tree structure of each sentence. The system then over-generates questions and ranks them statistically by assigning scores using logistic regression. [20] use semantics of the text by converting it into the Minimal Recursion Semantics notation [5]. Rules specific to the summarized semantics are applied to generate questions. Most of the approaches prop
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	2133459682	also evaluated our system on other standard metrics to enable comparison with other systems. However, as explained earlier, the standard metrics used in machine translation such as BLEU [13], METEOR [6], and ROUGE-L [10], might not be appropriate measures to evaluate the task of question generation. To appreciate this, consider the candidate question \who was the widow of mcdonald ’s owner ?&quot; a
2789831953	Automating Reading Comprehension by Generating Question and Answer Pairs	1531374185,2072385577	is paper. Fig.1: Example: sample questions generation from text by our models. Initial attempts at automated question generation were heavily dependent on a limited, ad-hoc, hand-crafted set of rules [9,20]. These rules focus mainly on the syntactic structure of the text and are limited in their application, only to sentences of simple structures. Recently, the success of sequence to sequence learning m
2789851222	A Deep Learning Approach for Multimodal Deception Detection.	1522734439	]. 3D-CNN has achieved state-of-the-art results in object classication on tridimensional data [16]. 3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features [17] from the whole video which helps in identifying the facial expressions such as smile, fear, or stress. The input to 3D-CNN is a video v of dimension (c;f;h;w), where c represents the number of channe
2789851222	A Deep Learning Approach for Multimodal Deception Detection.	1983364832	to extract unimodal features from each video. We extract textual, audio and visual features as described below. Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN [16]. 3D-CNN has achieved state-of-the-art results in object classication on tridimensional data [16]. 3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features [
2789851222	A Deep Learning Approach for Multimodal Deception Detection.	2153579005	sion 300 for an input video, v. Textual Features Extraction We use Convolutional Neural Networks (CNN) [18,19] to extract features from the transcript of a video, v. First, we use pretrained Word2Vec [20] model to extract the vector representations for every word in the transcript. These vectors are concatenated and fed as input vector to the CNN. We use a simple CNN with one convolutional layer and a
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2251861449	and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average a
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1681397005	52 52.4 296.1 80.4/85.93 84.54 Table 3: Transfer test results for various baseline methods. We include supervised results trained directly on each task (no transfer). Results 1 correspond to AdaSent (Zhao et al., 2015), 2 to BLSTM-2DCNN (Zhou et al., 2016), 3 to TF-KLD (Ji and Eisenstein, 2013) and 4 to Illinois-LH system (Lai and Hockenmaier, 2014). • dropout(ﬂoat): dropout rate in the case of MLP. For use cases w
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2780932362	al baseline models are evaluated in Table 3: • Continuous bag-of-words embeddings (average of word vectors). We consider the most commonly used pretrained word vectors available, namely the fastText (Mikolov et al., 2017) and the GloVe (Pennington et al., 2014) vectors trained on CommonCrawl. • SkipThought vectors (Ba et al., 2016) • InferSent vectors (Conneau et al., 2017) In addition to these methods, we include the
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1840435438	he approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec3Antonio Rivera - CC BY 2.0 - ﬂickr ognizing textual entailment (RTE) which consi
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1486649854	used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concerning what evaluations to use. Recent works in which various alternative sentence encoders are compared use a similar set of tasks LIUM, Universit´e Le Mans 1See also recent workshops on evalua
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1905882502	ce. We measure Recall@K, with K 2f1;5;10g, i.e., the percentage of images/captions for which the corresponding caption/image is one of the ﬁrst K retrieved; and median rank. We use the same splits as Karpathy and Fei-Fei (2015), i.e., we use 113k images (each containing 5 captions) for training, 5k images for validation and 5k images for test. For evaluation, we split the 5k images in 5 random sets of 1k images on which we
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2114524997	d both binary and ﬁne-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classiﬁer on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2251861449	ding to much discussion about the best way to go about it1. On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be ap
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2515741950	ditional tasks as the consensus on the best evaluation for sentence embeddings evolves. In particular, tasks that probe for speciﬁc linguistic properties of the sentence embeddings (Shi et al., 2016; Adi et al., 2017) are interesting directions towards understanding how the encoder understands language. We hope that our toolkit will be used by the community in order to ensure that fully comparable results are publ
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2160660844	e classiﬁcation, including sentiment analysis (MR and both binary and ﬁne-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classiﬁer on top, either in the form of a Logistic Regression
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2133458109	e with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spe
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2251939518	e use a set of binary classiﬁcation tasks (see Table 1) that covers various types of sentence classiﬁcation, including sentiment analysis (MR and both binary and ﬁne-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We gen
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2028175314	ee Table 1) that covers various types of sentence classiﬁcation, including sentiment analysis (MR and both binary and ﬁne-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classiﬁer on top, eit
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1522301498	ence representations. For the natural language inference tasks, where we are given two sentences uand v, we provide the classiﬁer with the input hu;v;ju vj;uvi. To ﬁt the Pytorch models, we use Adam (Kingma and Ba, 2014), with a batch size 64. We tune the L2 penalty of the classiﬁer with grid-search on the validation set. When using SentEval, two functions should be implemented by the user: • prepare(params, dataset)
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1980776243	eports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. Paraphrase detection The Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2194775991	r evaluation, we split the 5k images in 5 random sets of 1k images on which we compute the mean R@1, R@5, R@10 and median (Med r) over the 5 splits. We include 2048-dimensional pretrained ResNet-101 (He et al., 2016) features for all images. 4. Usage and Requirements Our evaluations comprise two different types: ones where we need to learn on top of the provided sentence representations (e.g. classiﬁcation/regres
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2251919380	ined directly on each task (no transfer). Results 1 correspond to AdaSent (Zhao et al., 2015), 2 to BLSTM-2DCNN (Zhou et al., 2016), 3 to TF-KLD (Ji and Eisenstein, 2013) and 4 to Illinois-LH system (Lai and Hockenmaier, 2014). • dropout(ﬂoat): dropout rate in the case of MLP. For use cases where there are multiple calls to SentEval, e.g when evaluating the sentence encoder at every epoch of training, we propose the follow
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2250790822	LP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score b
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2104246439	ng SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collect
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	1861492603	s only 2 classes, i.e., the aim is to predict whether the sentences are paraphrases or not. Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Lin et al., 2014). The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Ca
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2612953412	s are compared use a similar set of tasks LIUM, Universit´e Le Mans 1See also recent workshops on evaluating representations for NLP, e.g. RepEval: https://repeval2017.github.io/ (Hill et al., 2016a; Conneau et al., 2017). Implementing pipelines for this large set of evaluations, each with its own peculiarities, is cumbersome and induces unnecessary wheel reinventions. Another wellknown problem with the current status
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2126400076	tween two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each sub
2790235966	SentEval: An Evaluation Toolkit for Universal Sentence Representations.	2612953412	word vectors available, namely the fastText (Mikolov et al., 2017) and the GloVe (Pennington et al., 2014) vectors trained on CommonCrawl. • SkipThought vectors (Ba et al., 2016) • InferSent vectors (Conneau et al., 2017) In addition to these methods, we include the results of current state-of-the-art methods for which both the encoder and the classiﬁer are trained on each task (no transfer). For GloVe and fastText ba
2790259362	The importance of Being Recurrent for Modeling Hierarchical Structure	1840435438	al inference. The ﬁrst task was proposed byLinzen et al.(2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced byBowman et al. (2015b) to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of
2790259362	The importance of Being Recurrent for Modeling Hierarchical Structure	1840435438	RNNs? In this work, we provide the ﬁrst answer to this question. Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations (Bowman et al., 2015b;Tran et al.,2016;Linzen et al.,2016). We choose Transformer as a non-recurrent model to study in this paper. We refer to Transformer as Fully Attention Network (FANs) to emphasize this characteristi
2790259362	The importance of Being Recurrent for Modeling Hierarchical Structure	2613904329	s appear to be a natural choice for modeling sequential data, recently a class ofnon-recurrentmodels (Gehring et al.,2017; Vaswani et al.,2017) have shown competitive performance on sequence modeling.Gehring et al. (2017) propose a fully convolutional sequence-tosequence model that achieves state-of-the-art performance in machine translation.Vaswani et al. (2017) introduce Transformer networks that do not use any conv
2790613664	A Factoid Question Answering System for Vietnamese.	1894439495	by available search engines. Thestate-of-the-art techniques in open-domainQA canbeclassiﬁed into two main categories, namely semantic parsing based techniques and informationretrieval based techniques[2].Semantic parsing systems try to interpret the meaning of a question correctly by semantic analysis. A correct interpretation converts the This paper is published under the Creative Commons Attributio
2790613664	A Factoid Question Answering System for Vietnamese.	1801721664,1894439495	performanceofoursystembyintegratingrecentlyavailabledependency parser [8], semantic rolelabeller [18] and named entity recognizer for Vietnamese [7]. Finallyrecentworks onopen-domainquestionanswering [2,3] have shown the eﬃciency of embedding models, which learn low 7The temporary demo link of our system is at http://124.158.5.68:8080/wiki-qa/ dimensional vectorrepresentations ofwordsandknowledgebases
2790613664	A Factoid Question Answering System for Vietnamese.	1552847225	validated on a much larger test set of diverse questions, totaling nearly 900 question and answer pairs. 3 METHODOLOGY 3.1 DBPedia and Graph Model OurQAsystemusesanontologydevelopedbytheDBPediaproject[13].2 DBpediaisacrowd-sourcedcommunityeﬀorttoextractstructured information from Wikipedia and make this information available ontheWebforawidenumberoflanguages,includingVietnamese. The DBPedia knowledge
2790647712	Neural architectures for open-type relation argument extraction	2250539671	400g GRU, hidden size: f50;100;200;400g The resulting hyper-parameter choices are reported in the Sections describing the respective submodels. We use the 100-dimensional pretrained GloVe vectors of Pennington et al. (2014) and did not experiment with other word vector variants. The the size of the relation vector is equal to the number of relations (12, as for one-hot-encoding, but with the exibility to arrange similar
2790647712	Neural architectures for open-type relation argument extraction	2158899491	al Random Field (CRF) tagger model predicts the answer span by predicting the label &quot;I&quot; for the answer, and &quot;O&quot; otherwise. As in previous work combining neural networks with CRFs (Collobert et al., 2011; Lample et al., 2016), the CRF combines local label scores, obtained from the features of the previous layers, with learned transition weights in order to obtain sequential label consistency: For an
2790647712	Neural architectures for open-type relation argument extraction	2626778328	catenating the relation embeddings at each position: hCNN i = [h (3;3);h(3;5);h(3;7);h(3;9);e(r)] (9) 2.2.3 Self-attention encoder A third encoder uses the multi-headed self-attention architecture of Vaswani et al. (2017) to get an encoding for each position in the sequence. In self attention, the input representation for each position is used as a query to compute attention 3 We omit the bias term in ane transformat
2790647712	Neural architectures for open-type relation argument extraction	2107598941	ce prediction is not conditioned on a query entity; apart from the dierent problem formulation, this also implies that the model cannot be trained with incomplete annotation via distant supervision (Mintz et al., 2009), since training needs all labels to be present (not just those for the query Q). Zheng et al. (2017) use a tagging scheme similar to (Katiyar and Cardie, 2016) to annotate relation arguments in sente
2790647712	Neural architectures for open-type relation argument extraction	2155454737	e(s i) are the embeddings of the prex and sux of w i. Position embedding. Since the rst experiments using convolutional neural networks (CNNs) for relation extraction (Collobert et al., 2011; dos Santos et al., 2015) encoding the relative position to relation arguments has been key to good performance. We encode the relative position with respect to the query. Position encoding is used for all extractors, not onl
2790647712	Neural architectures for open-type relation argument extraction	2064675550	ecently been extended (Katiyar and Cardie, 2016) to a neural tagging scheme, where relations (and the distance to the related token) are predicted per token by a long shortterm memory network (LSTM, (Hochreiter and Schmidhuber, 1997)). This setting is quite dierent from ours since prediction is not conditioned on a query entity; apart from the dierent problem formulation, this also implies that the model cannot be trained with
2790647712	Neural architectures for open-type relation argument extraction	2427527485	entences for relation classication (Verga et al., 2016; Xu et al., 2016). Another related eld is that of question answering (QA). The introduction of the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has given rise to a large body of work on answer extraction. Seo et al. (2016) and Chen et al. (2017) introduce an ecient method of aligning question and paragraph words through an attention mechani
2790647712	Neural architectures for open-type relation argument extraction	2158899491	experiments: |We compare dierent neural architectures (encoders) for computing a sentence representation suitable for argument extraction. The proposed encoders are based on convolutional networks (Collobert et al., 2011), recurrent networks (Chung et al., 2014), and self-attention (Vaswani et al., 2017). |We compare dierent neural architectures (extractors) for extracting answers from this sentence representation. T
2790647712	Neural architectures for open-type relation argument extraction	2251847161	is the focus of this work, is even more challenging. Comparison of end-to-end relation extraction systems, as in the Knowledge Base Population (KBP) English Slot Filling shared task (Surdeanu, 2013; Angeli et al., 2014), indicates that recall is the most dicult metric to optimize in entity-driven relation extraction. Further analysis (Pink et al., 2014) showed that named entity tagging is, after relation prediction
2790647712	Neural architectures for open-type relation argument extraction	2099120987	a more general model should best be estimated. 5 Related work In opinion recognition, early work has focused on extracting opinion holders and opinion items with CRFs and integer linear programming (Choi et al., 2006). See (Culotta et al., 2006) and (Homann et al., 2010) for other approaches to argument tagging using traditional feature-based CRFs. This line of research has recently been extended (Katiyar and Car
2790647712	Neural architectures for open-type relation argument extraction	2133564696	given rise to a large body of work on answer extraction. Seo et al. (2016) and Chen et al. (2017) introduce an ecient method of aligning question and paragraph words through an attention mechanism (Bahdanau et al., 2014) to obtain an answer span. Wang et al. (2017) propose an architecture that, based on match LSTM, builds a question aware passage representation and uses an attention-based pointer network (Vinyals et
2790647712	Neural architectures for open-type relation argument extraction	1924770834	hitectures (encoders) for computing a sentence representation suitable for argument extraction. The proposed encoders are based on convolutional networks (Collobert et al., 2011), recurrent networks (Chung et al., 2014), and self-attention (Vaswani et al., 2017). |We compare dierent neural architectures (extractors) for extracting answers from this sentence representation. The proposed extractors are based on point
2790647712	Neural architectures for open-type relation argument extraction	2510759893	on already identied named entity spans. We compare a variant of neural table lling that does not rely on any of these conditions with a range of alternative argument extraction methods. Wikireading (Hewlett et al., 2016) is the task of extracting infobox properties from Wikipedia articles about a certain entity (similar to (Homann et al., 2010)). Some aspects of Wikireading are easier than the problem we are dealing
2790647712	Neural architectures for open-type relation argument extraction	2251091211	ition on a query entity and need to downweight non-argument labels to overcome sparsity in the training data. Similarly, table lling models have been developed to extract entities and relations, see (Miwa and Sasaki, 2014) for the original feature-based formulation and (Miwa and Bansal, 2016) for an RNN-based extension of the model. In contrast to Open-type relation argument extraction 21 our work, this model requires
2790647712	Neural architectures for open-type relation argument extraction	2155454737	NNs with position embeddings and CRFs for semantic role labeling. Subsequent work conrmed that convolutional neural networks are appropriate models for relation classication (Zeng et al., 2014; dos Santos et al., 2015; Adel et al., 2016; Vu et al., 2016). Other approaches have employed RNN variants for representing sentences for relation classication (Verga et al., 2016; Xu et al., 2016). Another related eld is t
2790647712	Neural architectures for open-type relation argument extraction	2626778328	okup layer, Section 2.1). The function computing the resulting vector (from q = Wq(a)e i, K= WK (a)Eand V = WV E) is dened by: Attention(q;K;V) = Vsoftmax(KT q) (11) We follow the setup described in Vaswani et al. (2017) and use 8 attention heads (each with a hidden size of 25 resulting in an overall hidden size of 200). The input to the self-attention mechanism is transformed by a feed-forward layer (output size 200
2790647712	Neural architectures for open-type relation argument extraction	2158899491	ors). The vectors e(p i) and e(s i) are the embeddings of the prex and sux of w i. Position embedding. Since the rst experiments using convolutional neural networks (CNNs) for relation extraction (Collobert et al., 2011; dos Santos et al., 2015) encoding the relative position to relation arguments has been key to good performance. We encode the relative position with respect to the query. Position encoding is used f
2790647712	Neural architectures for open-type relation argument extraction	2626778328	a relation P21 (sex or gender) has a non-standard argument slot, but only a handful of distinct possible values are attested in WikiData; so P21 is not a relation that we consider for our 4 Following Vaswani et al. (2017), we use 8 heads. Open-type relation argument extraction 13 relation example sentence per:occupation [Alan Aubry] Q ( born 24 September 1974 ) is a French [photographer] A . per:position_held Under pr
2790647712	Neural architectures for open-type relation argument extraction	2626778328	s. More repetitions did not yield signicant improvements on development data. See gure 3 for a diagram depicting the architecture of one self-attention layer. We deviated from the setup described in Vaswani et al. (2017) in the following ways, each of which improved the performance on the development data: 1.We included residual connections that add the input of the self-attention mechanism directly to the output, ra
2790647712	Neural architectures for open-type relation argument extraction	2251091211	sentence representation. The proposed extractors are based on pointer models (Vinyals et al., 2015), linear chain conditional random elds (Laerty et al., 2001; Lample et al., 2016), and table lling (Miwa and Sasaki, 2014). 2 Encoding and extraction architectures A big class of errors in end-to-end relation extraction systems are missing or inexact named entity tags and, in a pipelined model, lost recall cannot be rega
2790647712	Neural architectures for open-type relation argument extraction	2158899491	stance-based paradigm (Ren et al., 2017). Traditional relation classication and, more generally, work deciding whether a relation holds between two identied subparts of a sentence is also relevant. Collobert et al. (2011) combined CNNs with position embeddings and CRFs for semantic role labeling. Subsequent work conrmed that convolutional neural networks are appropriate models for relation classication (Zeng et al.,
2790647712	Neural architectures for open-type relation argument extraction	1806891645	sumably due to the overwhelming majority of cells with a negative label. 2.4 Hyper-parameters The following hyper-parameters were tuned on the development data (according to instance level accuracy) (Bengio, 2012) over the ranges given below. For tuning, the encoders were paired with the pointer network extractor (which is most similar 12 Open-Type Relation Argument Extraction to the Bidirectional Attention-Fl
2790647712	Neural architectures for open-type relation argument extraction	2626778328	tence representation suitable for argument extraction. The proposed encoders are based on convolutional networks (Collobert et al., 2011), recurrent networks (Chung et al., 2014), and self-attention (Vaswani et al., 2017). |We compare dierent neural architectures (extractors) for extracting answers from this sentence representation. The proposed extractors are based on pointer models (Vinyals et al., 2015), linear ch
2790647712	Neural architectures for open-type relation argument extraction	1924770834	ternative instantiations. 2.2.1 RNN encoder In the recurrent neural network (RNN) encoder architecture, each candidate sentence is encoded by two layers of bi-directional Gated Recurrent Units (GRU) (Chung et al., 2014) with a hidden size of 200 (100 per direction). The hidden representation for position iin the rst GRU layer is the concatenation of a left-to-right and a right-to-left GRU hidden state. It is denoted
2790647712	Neural architectures for open-type relation argument extraction	2510759893	time) as standard types (Chinchor and Robinson, 1997). (b) Open class. There must be a wide range of admissible values for the slot in question (i.e., the answers must be relational, not categorical (Hewlett et al., 2016)). For example, the WikiData relation P21 (sex or gender) has a non-standard argument slot, but only a handful of distinct possible values are attested in WikiData; so P21 is not a relation that we co
2790647712	Neural architectures for open-type relation argument extraction	2174833404	tion classication (Zeng et al., 2014; dos Santos et al., 2015; Adel et al., 2016; Vu et al., 2016). Other approaches have employed RNN variants for representing sentences for relation classication (Verga et al., 2016; Xu et al., 2016). Another related eld is that of question answering (QA). The introduction of the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has given rise to a large body
2790647712	Neural architectures for open-type relation argument extraction	2539469848	tity recognition in relation extraction is to do segmentation of text heuristically based on part-of-speech patterns and cooccurrences, and then to proceed in the traditional instance-based paradigm (Ren et al., 2017). Traditional relation classication and, more generally, work deciding whether a relation holds between two identied subparts of a sentence is also relevant. Collobert et al. (2011) combined CNNs wi
2790647712	Neural architectures for open-type relation argument extraction	2604368306	uestion answering (QA). The introduction of the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has given rise to a large body of work on answer extraction. Seo et al. (2016) and Chen et al. (2017) introduce an ecient method of aligning question and paragraph words through an attention mechanism (Bahdanau et al., 2014) to obtain an answer span. Wang et al. (2017) propose an architecture that,
2790647712	Neural architectures for open-type relation argument extraction	1986398135	uld best be estimated. 5 Related work In opinion recognition, early work has focused on extracting opinion holders and opinion items with CRFs and integer linear programming (Choi et al., 2006). See (Culotta et al., 2006) and (Homann et al., 2010) for other approaches to argument tagging using traditional feature-based CRFs. This line of research has recently been extended (Katiyar and Cardie, 2016) to a neural taggi
2790647712	Neural architectures for open-type relation argument extraction	2229639163	vercome sparsity in the training data. Similarly, table lling models have been developed to extract entities and relations, see (Miwa and Sasaki, 2014) for the original feature-based formulation and (Miwa and Bansal, 2016) for an RNN-based extension of the model. In contrast to Open-type relation argument extraction 21 our work, this model requires fully annotated data (no distant supervision), and therefore has only b
2790718951	HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for Commonsense Reading Comprehension	2512457506	ension (MRC) has become a spotlight topic in recent natural language processing ﬁeld. MRC consists of various subtasks, such as cloze-style reading comprehension (Hermann et al.,2015;Hill et al.,2015;Cui et al., 2016,2018), span-extraction reading comprehension (Rajpurkar et al.,2016) and open-domain reading comprehension (Chen et al.,2017), etc. One key problem in reading comprehension is that how the machine ut
2790804200	Polyglot Semantic Parsing in APIs	2102258316	languages by extracting general rule templates from all representations in the dataset, and exploited additional information and patterns using the Geobase database and the semantic grammars used in (Wong and Mooney, 2006b). This resulted in a graph with 2,419 nodes, 4,936 edges and 39,482 paths over an output vocabulary of 164. For Sportscaster, we directly translated the semantic grammar provided inChen and Mooney(2
2790804200	Polyglot Semantic Parsing in APIs	2014611589	tasks averaged over all datasets and compared against the best monolingual results fromRichardson and Kuhn (2017b,a), or RK 5.3 Implementation and Model Details We use the Foma ﬁnite-state toolkit ofHulden (2009) to construct all graphs used in our experiments. We also use the Cython version of Dynet (Neubig et al.,2017) to implement all the neural models (see supp. materials for more details). In the results
2790896538	Corpus Statistics in Text Classification of Online Data.	1822239915	ata (Charalampakis, Spathis, Kouslis, &amp; Kermanidis, 2016). However, selection of appropriate ML algorithms is still dominated by comparison of algorithms’ performance on one or several data sets (Taboada 2016). In the current work, we focus on corpus characteristics of textual data sets and their correspondence to text classification results. We analyze how corpus statistics can be used to estimate and, co
2791038416	Quality Expectations of Machine Translation	2123301721	the development set) cannot be guaranteed to deliver the optimal score on the test set, i.e. in order to deliver (say) the best BLEU score in testing, one might be better off tuning on (say) METEOR (Banerjee and Lavie 2005) rather than BLEU itself, as might be expected. More importantly, as any pair of translators will tell you, there is no such thing as the correct translation. In a discussion regarding translator resi
2791038416	Quality Expectations of Machine Translation	2106818711	out, BLEU weights all items in the reference sentence equally, so the fact that for the most part (3) is word salad makes no difference to its overall BLEU score. 18 Nonetheless, more recent papers (Agarwal and Lavie 2008; Farrús et al, 2012) have also demonstrated that BLEU correlates extremely well with human judgement of translation quality. 19 This was introduced to prevent systems from outputting very short targe
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2250539671	0 times are ﬁltered. This results in a vocabulary of around 276k words and 188k distinct nouns without the postﬁx “ PP”. We set the context window size as two and keep other parameters the same as in Pennington et al. (2014). We report results for 100 dimension embeddings, though similar trends were also observed with 200 and 300 dimensions. For comparison, we also trained a 100 dimension word embeddings (GloVe Giga) on
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2153579005	els explore the distributional hypothesis which states that words occurring in similar contexts have similar meanings (Harris, 1954). State-of-the-art word representations such as word2vec skip-gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been shown to perform well across a variety of NLP tasks, including textual entailment (Rockta¨schel et al., 2016),reading comprehension (Chen et al.,2016), a
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2171549925	u et al., 2014). In recentyears, variouscomputational approaches have been developed for bridging anaphora recognition (Markert et al., 2012; Hou et al., 2013a) and for bridging antecedent selection (Poesio et al., 2004; Hou et al., 2013b). This work falls into the latter category and we create a new lexical knowledge resource for the task of choosing antecedents for bridging anaphors. Previous work on bridging anap
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2046416131	ia lexico-semantic, frame or encyclopedic relations. Bridging resolution has to recognize bridging anaphors and ﬁnd links to antecedents. There has been a few works tackling full bridging resolution (Hahn et al., 1996; Hou et al., 2014). In recentyears, variouscomputational approaches have been developed for bridging anaphora recognition (Markert et al., 2012; Hou et al., 2013a) and for bridging antecedent selecti
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2171549925	ight cabinet ministers had received ﬁve million yen from the industry. Choosing the right antecedents for bridging anaphors is a subtask of bridging resolution. For this substask, most previous work (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b) calculate semantic relatedness between an anaphor and its antecedent based on word co-occurrence count using certain syntactic patterns. Most recently, w
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2250539671	above NPs: travelers PP–station ,travelers PP–airport hotels PP– travelers, destination PP–travelers. 3.2 Word Embeddings Based on PP Contexts (embeddings PP) Our PP context model is based on GloVe (Pennington et al., 2014), which obtains state-ofthe-art results on various NLP tasks. We extract noun pairs asdescribed inSection 3.1fromtheauTarget Word embeddings PP GloVe Giga president minister, mayor, governor, clinton
2791054563	ENHANCED WORD REPRESENTATIONS FOR BRIDGING ANAPHORA RESOLUTION	2250539671	pothesis which states that words occurring in similar contexts have similar meanings (Harris, 1954). State-of-the-art word representations such as word2vec skip-gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have been shown to perform well across a variety of NLP tasks, including textual entailment (Rockta¨schel et al., 2016),reading comprehension (Chen et al.,2016), and information status classiﬁcation
2791148095	Attention on Attention: Architectures for Visual Question Answering (VQA).	2293453011	cs.CL] 21 Mar 2018 relate to one another. Much of the progress in VQA parallels developments made in other problems, such as image captioning [XBK +15] [VTBE16] and textual question answering [KIS 15][XMS16]. The primary method to approach VQA tasks is based on three subcomponents: creating representations for the image and question; passing these inputs through a neural network to create a co-dependent
2791148095	Attention on Attention: Architectures for Visual Question Answering (VQA).	2293453011	dules of dynamic memory networks (DMN), which were originally developed for textual question answering, in order to show that a basic DMN architecture could be utilized for visual question answering. [XMS16] However, since then model architectures for visual and textual question answering have been specializing to their domains. With the models for visual question answering preferring more and more sophi
2791148095	Attention on Attention: Architectures for Visual Question Answering (VQA).	2250539671	estions and answers are tokenized and then trimmed/padded to a maximum length of 14 words. These tokens are then represented using 300-dimentional pre-trained Wikipedia+Gigaword GloVe word embeddings [PSM14]. Thirty six features per image are created via passing the VQA v2.0 images through a Faster R-CNN, with bottom-up attention, as proposed by [AHB+17]. The Faster R-CNN detects object centric elements
2791176029	Fine-grained attention mechanism for neural machine translation	2465346528	. This enables ﬁner-grained attention, meaning that the attention mechanism may choose to focus on one of many possible interpretations of a single word encoded in the high-dimensional context vector Choi et al. (2017); Van der Maaten and Hinton (2012). This is done by letting the attention mechanism output as many scores as there are dimensions in a context vectors, contrary to the existing variants of attention m
2791176029	Fine-grained attention mechanism for neural machine translation	1753482797	als how the ﬁne-grained attention mechanism exploits the internal structure of context vectors. 1 Introduction Neural machine translation (NMT), which is an end-to-end approach to machine translation Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2015), has widely become adopted in machine translation research, as evidenced by its success in a recent WMT’16 translation task Sennrich et al. (2016); Ch
2791176029	Fine-grained attention mechanism for neural machine translation	2418388682	brenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2015), has widely become adopted in machine translation research, as evidenced by its success in a recent WMT’16 translation task Sennrich et al. (2016); Chung et al. (2016). The attention-based approach, proposed by Bahdanau et al. (2015), has become the dominant approach among others, which has resulted in state-of-the-art translation qualities on,
2791176029	Fine-grained attention mechanism for neural machine translation	2465346528	the context. This looks similar to our ﬁne-grained attention in a sense that each dimension of the representation is treated in different ways. We evaluate the contextualization (Context) proposed by Choi et al. (2017). The contextualization enriches the word embedding vector by incorporating the context information: cx = 1 T XT t=1 NN (x t); where NN  is a feedforward neural network parametrized by . We closely
2791176029	Fine-grained attention mechanism for neural machine translation	2133564696	d attention mechanism indeed exploits the internal structure of each context vector. 2 Background: Attention-based Neural Machine Translation The attention-based neural machine translation (NMT) from Bahdanau et al. (2015) computes a conditional distribution over translations given a source sentence X= (w x 1 ;w x 2 ;:::;w T ): p(Y = (wy 1 ;w y 2 ;:::;w y T0 )jX): (1) This is done by a neural network that consists of a
2791176029	Fine-grained attention mechanism for neural machine translation	2195405088	the dominant approach among others, which has resulted in state-of-the-art translation qualities on, for instance, En-Fr Jean et al. (2015a), En-De Jean et al. (2015b); Sennrich et al. (2016), En-Zh Shen et al. (2016), En-Ru ? and En-Cz ?Luong and Manning (2016). These recent successes are largely due to better handling a large target vocabulary Jean et al. (2015a); ?); ?); Luong and Manning (2016), incorporating
2791176029	Fine-grained attention mechanism for neural machine translation	2133564696	ences.) We use newstest2013 and newstest2015 as the validation and test sets for En-De, and newsdev2015 and newstest2015 for En-Fi. 4.3 Models We use the attention-based neural translation model from Bahdanau et al. (2015) as a baseline, except for replacing the gated recurrent unit (GRU) with the long short-term memory unit (LSTM). The vocabulary size is 30K for both source and target languages, the dimension of word
2791176029	Fine-grained attention mechanism for neural machine translation	2133564696	where the gradient of the log-likelihood is efﬁciently computed by the backpropagation algorithm. 2.1 Variants of Attention Mechanism Since the original attention mechanism was proposed as in Eq. (3) Bahdanau et al. (2015), there have been several variants ?. ? presented a few variants of the attention mechanism on the sequence-to-sequence model Sutskever et al. (2014). Although their work cannot be directly compared t
2791176029	Fine-grained attention mechanism for neural machine translation	2227523508	handling a large target vocabulary Jean et al. (2015a); ?); ?); Luong and Manning (2016), incorporating a target-side monolingual corpus ?Gulcehre et al. (2015) and advancing the attention mechanism ?Cohn et al. (2016); Tu et al. (2016). We notice that all the variants of the attention mechanism, including the original one by Bahdanau et al. (2015), are temporal in that it assigns a scalar attention score for each
2791176029	Fine-grained attention mechanism for neural machine translation	2339995566	ich has resulted in state-of-the-art translation qualities on, for instance, En-Fr Jean et al. (2015a), En-De Jean et al. (2015b); Sennrich et al. (2016), En-Zh Shen et al. (2016), En-Ru ? and En-Cz ?Luong and Manning (2016). These recent successes are largely due to better handling a large target vocabulary Jean et al. (2015a); ?); ?); Luong and Manning (2016), incorporating a target-side monolingual corpus ?Gulcehre et
2791176029	Fine-grained attention mechanism for neural machine translation	2130942839	inal attention mechanism was proposed as in Eq. (3) Bahdanau et al. (2015), there have been several variants ?. ? presented a few variants of the attention mechanism on the sequence-to-sequence model Sutskever et al. (2014). Although their work cannot be directly compared to the attention model in Bahdanau et al. (2015), they introduced a few variants for score function of attention model–content based and location base
2791176029	Fine-grained attention mechanism for neural machine translation	1923211482	ion generation by ? assigns a scalar attention score for each context vector, which corresponds to a spatial location in an input image, treating all the dimensions of the context vector equally. See Cho et al. (2015) for more of such examples. 1 arXiv:1803.11407v1 [cs.CL] 30 Mar 2018 On the other hand, in Choi et al. (2017), it was shown that word embedding vectors have more than one notions of similarities by an
2791176029	Fine-grained attention mechanism for neural machine translation	2133564696	l corpus ?Gulcehre et al. (2015) and advancing the attention mechanism ?Cohn et al. (2016); Tu et al. (2016). We notice that all the variants of the attention mechanism, including the original one by Bahdanau et al. (2015), are temporal in that it assigns a scalar attention score for each context vector, which corresponds to a source symbol. In other words, all the dimensions of a context vector are treated equally. Th
2791176029	Fine-grained attention mechanism for neural machine translation	2465346528	location in an input image, treating all the dimensions of the context vector equally. See Cho et al. (2015) for more of such examples. 1 arXiv:1803.11407v1 [cs.CL] 30 Mar 2018 On the other hand, in Choi et al. (2017), it was shown that word embedding vectors have more than one notions of similarities by analyzing the local chart of the manifold that word embedding vectors reside. Also, by contextualization of wor
2791176029	Fine-grained attention mechanism for neural machine translation	2130942839	mechanism exploits the internal structure of context vectors. 1 Introduction Neural machine translation (NMT), which is an end-to-end approach to machine translation Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2015), has widely become adopted in machine translation research, as evidenced by its success in a recent WMT’16 translation task Sennrich et al. (2016); Chung et al. (2016). The at
2791176029	Fine-grained attention mechanism for neural machine translation	2220350356	n’, ‘strategy’, ‘election’ and ’Obama’). We ﬁnd it an interesting future work to test the ﬁne-grained attention with other NMT models like character-level models or multi-layered encode/decode models Ling et al. (2015); ?. Also, the ﬁnegrained attention mechanism can be applied to different tasks like speech recognition. Acknowledgments The authors would like to thank the developers of Theano Bastien et al. (2012).
2791176029	Fine-grained attention mechanism for neural machine translation	2064675550	ncatenation results in a context Cthat is a tuple of annotation vectors: C= fh 1;h 2;:::;h T g: The recurrent activation functions ! ˚and ˚are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al. (2014)). The decoder consists of a recurrent network and the attention mechanism. The recurrent network is a unidirectional language model to compute the co
2791176029	Fine-grained attention mechanism for neural machine translation	2133564696	nternal structure of context vectors. 1 Introduction Neural machine translation (NMT), which is an end-to-end approach to machine translation Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al. (2015), has widely become adopted in machine translation research, as evidenced by its success in a recent WMT’16 translation task Sennrich et al. (2016); Chung et al. (2016). The attention-based approach,
2791176029	Fine-grained attention mechanism for neural machine translation	2465346528	ontext at a time, and that it may be beneﬁcial to assign a score for each dimensionof the context vector, as each dimension represents a different perspective into the captured internal structure. In Choi et al. (2017), it was shown that each dimension in word embedding could have different meaning and the context could enrich the meaning of each dimension in different ways. The insight in this paper is similar to
2791176029	Fine-grained attention mechanism for neural machine translation	2410539690	get vocabulary Jean et al. (2015a); ?); ?); Luong and Manning (2016), incorporating a target-side monolingual corpus ?Gulcehre et al. (2015) and advancing the attention mechanism ?Cohn et al. (2016); Tu et al. (2016). We notice that all the variants of the attention mechanism, including the original one by Bahdanau et al. (2015), are temporal in that it assigns a scalar attention score for each context vector, wh
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2626778328	(Hasler et al., 2017), or beam search for NMT. SGNMT can also be used to analyze search errors. Tab. 1 compares ﬁve different search conﬁgurations for SMT lattice rescoring with a Transformer model (Vaswani et al., 2017) on a subset5 of the Japanese-English Kyoto Free Translation Task (KFTT) test set (Neubig, 2011). Following Stahlberg et al. (2016) we measure time complexity in number of node expansions. Our depth-ﬁ
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2311921240	(NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenge
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2626778328	15) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility poses major challenges in MT-related research, teaching, and industry. Researchers
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2407166119	arch conﬁgurations for SMT lattice rescoring with a Transformer model (Vaswani et al., 2017) on a subset5 of the Japanese-English Kyoto Free Translation Task (KFTT) test set (Neubig, 2011). Following Stahlberg et al. (2016) we measure time complexity in number of node expansions. Our depth-ﬁrst search algorithm stops when a partial hypothesis score is worse than the current best complete hypothesis score (admissible pru
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	932413789	eaching and industry, respectively. 3Making all models of the T2T library (Google, 2017) available to SGNMT took less than 200 lines of code. 4For example, the neural language modeling software NPLM (Vaswani et al., 2013) is written in C++, but can be accessed in SGNMT via its Python interface. Figure 2: Greedy decoding with the predictor constellation nmt,fst for lattice rescoring. 2 The Predictor Interface Predictor
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2133564696	er all other existing tools for rapid prototyping and assessment of new research avenues. Among other Neural MT innovations, SDL Research used SGNMT to prototype and assess attention-based Neural MT (Bahdanau et al., 2015), Neural MT model shrinking (Stahlberg and Byrne, 2017) and the recent Transformer model (Vaswani et al., 2017). As described in Sec. 5, the Transformer model is trivially supported by the SGNMT decod
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	1816313093	er (Google, 2017) 21.7 19.3 22.5 Table 2: BLEU scores of SGNMT with different NMT back ends on the complete KFTT test set (Neubig, 2011) computed with multi-bleu.pl. All neural systems are BPE-based (Sennrich et al., 2016) with vocabulary sizes of 30K. The SMT baseline achieves 18.1 BLEU. 4 Output Formats SGNMT supports ﬁve different output formats. text: Plain text ﬁle with ﬁrst best translations. nbest: n-best list
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2527845440	es demanding speed requirements on its deployment processes. Another practical challenge many researchers are struggling with is the large number of available NMT tools (van Merrienboer et al., 2015; Junczys-Dowmunt et al., 2016; Klein et al., 2017; Sennrich et al.,¨ 2017; Helcl and Libovick´y, 2017; Bertoldi et al., 2017; Hieber et al., 2017). 1 Committing to one particular NMT tool bears the risk of being outdated soon, as
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2133564696	f innovation in machine translation (MT) has gathered impressive momentum over the recent years. The discovery and maturation of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vas
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2114912785	implemented, it can be directly combined with all other predictors which are already available in SGNMT. Therefore, general techniques like lattice and n-best list rescoring (Stahlberg et al., 2016; Neubig et al., 2015), ensembling, MBR-based NMT (Stahlberg et al., 2017a), etc. only need to be implemented once (as predictor), and are automatically available for all models. This does not only speed up the transition
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2626778328	innovations, SDL Research used SGNMT to prototype and assess attention-based Neural MT (Bahdanau et al., 2015), Neural MT model shrinking (Stahlberg and Byrne, 2017) and the recent Transformer model (Vaswani et al., 2017). As described in Sec. 5, the Transformer model is trivially supported by the SGNMT decoder through its predictor framework, and is easy to combine with other predictors. It is worth noting that at th
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2130942839	Introduction The rate of innovation in machine translation (MT) has gathered impressive momentum over the recent years. The discovery and maturation of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; G
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	1902237438	machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architecture. This volatility p
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2608395138	nd assessment of new research avenues. Among other Neural MT innovations, SDL Research used SGNMT to prototype and assess attention-based Neural MT (Bahdanau et al., 2015), Neural MT model shrinking (Stahlberg and Byrne, 2017) and the recent Transformer model (Vaswani et al., 2017). As described in Sec. 5, the Transformer model is trivially supported by the SGNMT decoder through its predictor framework, and is easy to comb
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2407166119	Once a new predictor is implemented, it can be directly combined with all other predictors which are already available in SGNMT. Therefore, general techniques like lattice and n-best list rescoring (Stahlberg et al., 2016; Neubig et al., 2015), ensembling, MBR-based NMT (Stahlberg et al., 2017a), etc. only need to be implemented once (as predictor), and are automatically available for all models. This does not only sp
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2136657878	in OpenFST (Allauzen et al., 2007) format with standard arcs. fst: Lattices with sparse tuple arcs (Iglesias et al., 2015) which keep predictor scores separate. ngram: MBR-style n-gram posteriors (Kumar and Byrne, 2004; Tromble et al., 2008) as used by Stahlberg et al. (2017a) for NMT. 5 SGNMT for Research SGNMT is designed for environments in which implementation time is far more valuable than computation time. Th
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2626778328	r (Google, 2017) back end based on TensorFlow (Abadi et al., 2016). Without reimplementation, we could validate that MBRbased NMT holds up even under a much stronger NMT model, the Transformer model (Vaswani et al., 2017). Tab. 2 compares the performance of lattice rescoring and MBR-based combination across four different NMT implementations using SGNMT. 6 SGNMT for Teaching SGNMT is being used for teaching at the Uni
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2574872930	s on its deployment processes. Another practical challenge many researchers are struggling with is the large number of available NMT tools (van Merrienboer et al., 2015; Junczys-Dowmunt et al., 2016; Klein et al., 2017; Sennrich et al.,¨ 2017; Helcl and Libovick´y, 2017; Bertoldi et al., 2017; Hieber et al., 2017). 1 Committing to one particular NMT tool bears the risk of being outdated soon, as keeping up with the
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2407166119	software if 6https://github.com/ehasler/tensorflow 7https://github.com/tensorflow/nmt, trained with Tensor2Tensor (Google, 2017) needed. For example, our previous research work on lattice rescoring (Stahlberg et al., 2016) and MBR-based NMT (Stahlberg et al., 2017a) used the NMT package Blocks (van Merrienboer¨ et al., 2015) which is based on Theano (Bastien et al., 2012). Since both Blocks and Theano have been discont
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2100238596	t al., 2007) format with standard arcs. fst: Lattices with sparse tuple arcs (Iglesias et al., 2015) which keep predictor scores separate. ngram: MBR-style n-gram posteriors (Kumar and Byrne, 2004; Tromble et al., 2008) as used by Stahlberg et al. (2017a) for NMT. 5 SGNMT for Research SGNMT is designed for environments in which implementation time is far more valuable than computation time. This basic design decisio
2791212065	Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation.	2100664567	tion of the neural machine translation (NMT) paradigm (Sutskever et al., 2014; Bahdanau et al., 2015) has led to steady and substantial improvements of translation performance (Williams et al., 2014; Jean et al., 2015; Luong et al., 2015; Chung et al., 2016; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Fig. 1 shows that this progress is often driven by signiﬁcant changes in the network architectur
2791322409	The emergent algebraic structure of RNNs and embeddings in NLP.	2131494463,2293453011	ged as the clear victor in, e.g., language translation [6,7,8], and are typically more capable of identifying important contextual points through attention mechanisms for, e.g., reading comprehension [9,10,11,12]. With an interest in NLP, we thus turn to RNNs. RNNs nominally aim to solve a general problem involving sequential inputs. For various more specied tasks, specialized and constrained implementations
2791322409	The emergent algebraic structure of RNNs and embeddings in NLP.	2753093910	P, we thus turn to RNNs. RNNs nominally aim to solve a general problem involving sequential inputs. For various more specied tasks, specialized and constrained implementations tend to perform better [13,14,15,8,16,17,18,11, 12,9,10]. Often, the improvement simply mitigates the exploding/vanishing gradient problem [19,20], but, for many tasks, the improvement is more capable of generalizing the network’s training for that task. U
2791322409	The emergent algebraic structure of RNNs and embeddings in NLP.	2110485445	ral language processing (NLP) have been enabled by novel deep neural network architectures and word embeddings. Historically, convolutional neural network (CNN)[1,2] and recurrent neural network (RNN)[3,4] topologies have competed to provide state-of-the-art results for NLP tasks, ranging from text classication to reading comprehension. CNNs identify and aggregate patterns with increasing feature size
2791322409	The emergent algebraic structure of RNNs and embeddings in NLP.	2626778328	rule-based construction of language. While both networks display great ecacy at certain tasks [5], RNNs tend to be the more versatile, have emerged as the clear victor in, e.g., language translation [6,7,8], and are typically more capable of identifying important contextual points through attention mechanisms for, e.g., reading comprehension [9,10,11,12]. With an interest in NLP, we thus turn to RNNs. R
2791322409	The emergent algebraic structure of RNNs and embeddings in NLP.	2250539671	tain networks excel at certain NLP tasks can lead to more performant networks, and networks that solve new problems. Advances in word embeddings have furnished the remainder of recent progress in NLP [21,22,23,24, 25,26]. Although it is possible to train word embeddings end-to-end with the rest of a network, this is often either prohibitive due to exploding/vanishing gradients for long corpora, or results in poor emb
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2164585080	decided to base the question collection on script scenarios rather than speciﬁc texts. As a starting point for our data collection, we use 1www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and ca
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	1544827683	thus deﬁned as: p(ajt;q) = softmax(p&gt;a) (2) Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text ﬁt. Bidirectional GRUs ar
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2107901333	estion collection on script scenarios rather than speciﬁc texts. As a starting point for our data collection, we use 1www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new sc
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2145374219	hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehe
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2158794898	hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framew
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2574002207	ing steps and the dataset validation. Lastly, Section 2.4. gives statistics about the ﬁnal dataset. 2.1. Pilot Study As a starting point for our pilots, we made use of texts from the InScript corpus (Modi et al., 2016), which provides stories centered around everyday situations (see Section 2.2.2.). We conducted three different pilot studies to determine the best way of collecting questions that require inference o
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	1549997466	ontribution of script knowledge to machine comprehension. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake or taking a bus (Schank and Abelson, 1975). To illustrate the importance of script knowledge, consider Example (1): (1)The waitress brought Rachel’s order. She ate the food with great pleasure. Without using commonsense knowledge, it may be d
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2164585080	proposed in recent years. However, systems have been evaluated for speciﬁc aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards,
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2411480514	q) = softmax(p&gt;a) (2) Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text ﬁt. Bidirectional GRUs are used to encode que
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2574002207	we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. 2.2.2. Texts For the collection of texts, we followed Modi et al. (2016), where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequent
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	2557764419	and test systems regarding different aspects of language understanding, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. A
2791374212	MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge.	1544827683	umber of reading comprehension datasets have been proposed, including MCTest (Richardson et al., 2013), BAbI (Weston et al., 2015), the Children’s Book Test (CBT, Hill et al. (2015)), CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)), and RACE (Lai et al., 2017). These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mod
2791412059	Neural Lattice Language Models	2625092622	-accepted linguistic phenomena. As a result, standard models lack linguistically informed inductive biases, potentially limiting their accuracy, particularly in lowdata scenarios (Adams et al., 2017; Koehn and Knowles, 2017). In this work, we present a novel modiﬁcation to the standard LSTM language modeling framework that allows us to incorporate some varieties of these linguistic intuitions seamlessly: neural lattice l
2791412059	Neural Lattice Language Models	2259472270	). In particular, the LSTM cell (Hochreiter and Schmidhuber, 1997) is a speciﬁc RNN architecture which has been shown to be effective on many tasks, including language modeling (Press and Wolf, 2017; Jozefowicz et al., 2016; Merity et al., 2016; Inan et al., 2017).1 LSTM language models recursively cal1In this work, we utilize an LSTM with linked input and forget gates, as proposed by Greff et al. (2016). culate the hid
2791412059	Neural Lattice Language Models	2143612262	arly models conditioned on a particular input, have many applications including in machine translation (Bahdanau et al., 2016), abstractive summarization (Chopra et al., 2016), and speech processing (Graves et al., 2013). * Now at Google: buckman@google.com Figure 1: Lattice decomposition of a sentence and its corresponding lattice language model probability calculation Similarly, state-of-the-art language models are
2791412059	Neural Lattice Language Models	2259472270	bability mass assigned to the sentinel value, p main(&lt;s&gt; jh t;), is distributed across all possible tokens sequences of length less than L, using another LSTM with parameters  sub. Similar to Jozefowicz et al. (2016), this sub-LSTM is initialized by passing in the hidden state of the main lattice LSTM at that timestep. This gives us a probability for each sequence p sub(c 1;c 2;:::;c L jh t; sub). The ﬁnal formu
2791412059	Neural Lattice Language Models	2610780044	ble to represent all meanings effectively. There has been past work in word embeddings which has shown that using multiple embeddings for each word is helpful in constructing a useful representation. Athiwaratkun and Wilson (2017) represented each word with a multimodal Gaussian distribution and demonstrated that embeddings of this form were able to outperform more standard skipgram embeddings on word similarity and entailment
2791412059	Neural Lattice Language Models	2259472270	its corresponding lattice language model probability calculation Similarly, state-of-the-art language models are almost universally based on RNNs, particularly long short-term memory (LSTM) networks (Jozefowicz et al., 2016; Inan et al., 2017; Merity et al., 2016). While powerful, LSTM language models usually do not explicitly model many commonly-accepted linguistic phenomena. As a result, standard models lack linguisti
2791412059	Neural Lattice Language Models	2527133236	dings to all words. 6 Related Work Past work that utilized lattices in neural models for natural language processing centers around using these lattices in the encoder portion of machine translation. Su et al. (2016) utilized a variation of the Gated Recurrent Unit that operated over lattices, and preprocessed lattices over Chinese characters that allowed it to effectively encode multiple segmentations. Additiona
2791412059	Neural Lattice Language Models	2138660131	e language models improve accuracy by helping the gradient ﬂow over smaller paths, preventing vanishing gradients. Many hierarchical neural language models have been proposed with a similar objective Koutnik et al. (2014; Zhou et al. (2017). Our work is distinguished from these by the use of latent token-level segmentations that capture meaning directly, rather than simply being highlevel mechanisms to encourage grad
2791412059	Neural Lattice Language Models	2514713644	ed by taking the 10,000 words in the vocabulary and adding the most common 10,000 n-grams with 1 &lt;nL. The weights on the ﬁnal layer of the network were tied with the input embeddings, as done by (Press and Wolf, 2017; Inan et al., 2017). In all lattice models, hidden states were computed using weighted expectation (x3.3.3) unless mentioned otherwise. In multi-embedding models, embedding Table 1: Results on Englis
2791412059	Neural Lattice Language Models	2514713644	h to approximating p(X). In particular, the LSTM cell (Hochreiter and Schmidhuber, 1997) is a speciﬁc RNN architecture which has been shown to be effective on many tasks, including language modeling (Press and Wolf, 2017; Jozefowicz et al., 2016; Merity et al., 2016; Inan et al., 2017).1 LSTM language models recursively cal1In this work, we utilize an LSTM with linked input and forget gates, as proposed by Greff et a
2791412059	Neural Lattice Language Models	2527133236	or i2A j We then sum the local hidden and cell states: h j = X i2A j ~h i c j = X i2A j ~c i 3This framework has been used before for calculating neural sentence representations involving lattices by Su et al. (2016) and Sperber et al. (2017), but not for the language models that are the target of this paper. This formulation is powerful, but comes at the cost of sacriﬁcing the probabilistic interpretation of whi
2791412059	Neural Lattice Language Models	2220350356	kens x 1;x 2;:::;x jX such that X = [x 1;x 2;:::;x jX ], i.e. the concatenation of the tokens (with an appropriate delimiter). These tokens can be either on the character level (Hwang and Sung, 2017; Ling et al., 2015) or word level (Inan et al., 2017; Merity et al., 2016). Using the chain rule, language models generally factorize p(X) in the following way: p(X) = p(x 1;x 2;:::;x jX ) = YjXj t=1 p(x t jx 1;x 2;:::;
2791412059	Neural Lattice Language Models	2212703438	ltilattice with 2 and 3 embeddings per word. The implementation of our networks was done in DyNet (Neubig et al., 2017).All LSTMs had 2 layers, each with hidden dimension of 200. Variational dropout (Gal and Ghahramani, 2016) of .2 was used on the Chinese experiments, but hurt performance on the English data, so it was not used. The 10,000 word embeddings each had dimension 256. For lattice models, chunk vocabularies were
2791412059	Neural Lattice Language Models	2271595638	ord with a multimodal Gaussian distribution and demonstrated that embeddings of this form were able to outperform more standard skipgram embeddings on word similarity and entailment tasks. Similarly, Chen et al. (2015) incorporate standard skip-gram training into a Gaussian mixture framework and show that this improves performance on several word similarity benchmarks. When a polysemous word is represented using on
2791412059	Neural Lattice Language Models	2064675550	rocessing. These models typically share a common backbone: recurrent neural networks (RNN), which have proven themselves to be capable of tackling a variety of core natural language processing tasks (Hochreiter and Schmidhuber, 1997; Elman, 1990). One such task is language modeling, in which we estimate a probability distribution over sequences of tokens that corresponds to observed sentences (x2). Neural language models, partic
2791412059	Neural Lattice Language Models	2110485445	share a common backbone: recurrent neural networks (RNN), which have proven themselves to be capable of tackling a variety of core natural language processing tasks (Hochreiter and Schmidhuber, 1997; Elman, 1990). One such task is language modeling, in which we estimate a probability distribution over sequences of tokens that corresponds to observed sentences (x2). Neural language models, particularly models
2791412059	Neural Lattice Language Models	2064675550	ted by spaces, and no word contains a space. 2.2 Recurrent Neural Networks Recurrent neural networks have emerged as the state-of-the-art approach to approximating p(X). In particular, the LSTM cell (Hochreiter and Schmidhuber, 1997) is a speciﬁc RNN architecture which has been shown to be effective on many tasks, including language modeling (Press and Wolf, 2017; Jozefowicz et al., 2016; Merity et al., 2016; Inan et al., 2017).1
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	2296076036	and 14,093 940 3,758 Table 2: Number of instances in each dataset split. lexical-full, in which the train, test, and validation sets each consists of a distinct vocabulary. The split was suggested by Levy et al. (2015), and it randomly assigns words to distinct sets, such that for example, including travel guide in the train set promises that ﬁshing guide would not be included in the test set, and the models do not
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	22977213	C paraphrasing extracts texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY, or BE ORGANIZED BY students (e.g. Nakov and Hearst, 2006; Kim and Nakov, 2011; Hendrickx et al., 2013; Nulty and Costello, 2013). Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its co
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	1889268436	ch AN. The full-additive model (Zanzotto et al., 2010; Dinu et al., 2013) is a similar approach that works on any two-word composition, multiplying each word by a square matrix: nc=A·~v w1 +B·~v w2 . Socher et al. (2012) suggested a non-linear composition model. A recursive neural network operates bottom-up ontheoutput ofaconstituency parser to represent variable-length phrases. Each constituent is represented by a v
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	2250539671	ctor~v ncasadditional distributional input, providing the contexts in which w1w2 occur as an NC: ~v nc = [~v w 1,~v w2,~v nc,~v P(w ,w 2)]. Like Dima (2016), welearnNCvectors using theGloVealgorithm (Pennington et al., 2014), by replacing each NC occurrence in the corpus with a single token. This information can potentially help clustering NCs that appear in similar contexts despite having low pairwise similarity scores
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	22977213	f its constituent words (Spa¨rck Jones, 1983). Previous work on the task falls into two main approaches. The ﬁrst maps NCs to paraphrases that express the relation between the constituent words (e.g. Nakov and Hearst, 2006; Nulty and Costello, 2013), such as mapping coffee cup and garbage dump to the pattern [w1]CONTAINS [w2]. The second approach computes a representation for NCs from the distributional representation
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	1889268436	in lexical-head it is the modiﬁer frequency baseline, and in lexical-mod it is the head frequency baseline. 5Unseen heads/modiﬁers are assigned a random relation. resentations (Zanzotto et al., 2010; Socher et al., 2012) and single word embeddings in a fully connected network.6 4.3 Results Table 1 shows the performance of various methods on the datasets. Dima’s (2016) compositional models perform best among the basel
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	2296076036	lts, recently, Dima (2016) showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and attributed ittothe lexical memorization phenomenon (Levy et al., 2015). In this paper we apply lessons learned from the parallel task of semantic relation classiﬁcation. We adapt HypeNET (Shwartz et al., 2016) to the NC classiﬁcation task, using their path embeddings to
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	1608322251	proposed 3 simple combinations of~v w1 and~v w2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. Baroni and Zamparelli (2010) focused onadjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distan
2791524052	OLIVE OIL IS MADE OF OLIVES, BABY OIL IS MADE FOR BABIES: INTERPRETING NOUN COMPOUNDS USING PARAPHRASES IN A NEURAL MODEL	73310205	texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY, or BE ORGANIZED BY students (e.g. Nakov and Hearst, 2006; Kim and Nakov, 2011; Hendrickx et al., 2013; Nulty and Costello, 2013). Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its constituents, e.g. spel
2791554110	IcoRating: A Deep-Learning System for Scam ICO Identification.	1915485278	1. The proportion of positive (sam) data points is 47%. non-linear transformations. Various techniques have been proposed to make neural models interpretable (Koh and Liang,2017; Montavon et al.,2017;Mahendran and Vedaldi, 2015;Li et al.,2015a,2016). The basic idea of these models is to build another learning or visualization model on top of a pre-trained neural model to for interpretation. We adopt two widely used methods
2791554110	IcoRating: A Deep-Learning System for Scam ICO Identification.	1915485278	e m= 1 setting, when all features are considered. 5.2 Qualitative Analysis We need to rationalize the output from the model. Deep learning models are hard to be rationalized directly (Lei et al.,2016;Mahendran and Vedaldi, 2015;Weinzaepfel et al.,2011;Vondrick et al., 2013). This is because neural network models operate like a black box: using vector representations (as opposed to humaninterpretable features) to represent i
2791600991	The morphospace of language networks	2157365695	cation networks that dene the Pareto front of the morphospace. Finally, dedicated, data-driven studies exist about different optimality aspects of language, from prosody to syntax among many others (Jaeger and Levy, 2006; Frank and Jaeger, 2008; Jaeger, 2010; Piantadosi et al., 2011; Mahowald et al., 2013). But discussion of the least-eort language model has focused on its information theoretical characterization. T
2791600991	The morphospace of language networks	2586145851	ne the Pareto front of the morphospace. Finally, dedicated, data-driven studies exist about different optimality aspects of language, from prosody to syntax among many others (Jaeger and Levy, 2006; Frank and Jaeger, 2008; Jaeger, 2010; Piantadosi et al., 2011; Mahowald et al., 2013). But discussion of the least-eort language model has focused on its information theoretical characterization. The hypothesis that human
2791666059	NATURAL LANGUAGE TO STRUCTURED QUERY GENERATION VIA META-LEARNING	2550120381	eitas,2016;Neelakantan et al.,2015;Graves et al.,2014;Yin et al.,2015; Devlin et al.,2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al.,2017;Parisotto et al., 2017) aim to generate explicit programs and then execute them to get output. The learner model we used in this work follows the line of program synthesis models and trains on pairs of natural language (que
2791689110	Monitoring Targeted Hate in Online Environments.	2252065932	is arguably the simplest possible approach to hate speech monitoring, and many types of reﬁnements are possible, such as weighting of the dictionary entries (Eisenstein, 2017), handling of negation (Reitan et al., 2015), and scope detection. We will return to a more detailed discussion of problems with the proposed approach in Section 4.3.. At this point, we note that one possible advantage of using such a simple ap
2791689110	Monitoring Targeted Hate in Online Environments.	2595653137	ate speech annotation. Waseem and Hovy (2016) examine the effect of various types of features on hate speech detection, and ﬁnd that character n-grams and gender information provide the best results. Davidson et al. (2017) argues that lexical methods suffer from low precision and aims to separate hate speech from other instances of offensive language. Their results show that while racist and homophobic content are clas
2791689110	Monitoring Targeted Hate in Online Environments.	2153579005	erm suggestions based on the expert’s feedback (Gyllensten and Sahlgren, 2018). As embedding, we use Gensim’s (Rehˇ u˚ˇrek and Sojka, 2010) implementation of the Continuous Bag of Words (CBOW) model (Mikolov et al., 2013), which builds word vectors by training a 2-layer neural network to predict a target word based on a set of context words. The network learns two sets of vectors, one for the target terms (the embeddi
2791689110	Monitoring Targeted Hate in Online Environments.	2473555522	est that hate speech should be seen as a continuous rather then as a binary problem, and that detailed instructions for the annotators are needed to improve the reliability of hate speech annotation. Waseem and Hovy (2016) examine the effect of various types of features on hate speech detection, and ﬁnd that character n-grams and gender information provide the best results. Davidson et al. (2017) argues that lexical me
2791689110	Monitoring Targeted Hate in Online Environments.	2515268015	onments have been made. For example, Warner and Hirschberg (2012) use machine learning coupled with template-based features to detect hate speech in user-generated web content with promising results. Wester et al. (2016) examine the effects of various types of linguistic features for detecting threats of violence in a corpus of YouTube comments, and ﬁnd promising results even using simple bag-of-words representations
2791689110	Monitoring Targeted Hate in Online Environments.	2585712495	of various types of linguistic features for detecting threats of violence in a corpus of YouTube comments, and ﬁnd promising results even using simple bag-of-words representations. On the other hand, Ross et al. (2016) examine the reliability of annotations of hate speech, and ﬁnd that the annotator agreement is very low, indicating that hate speech detection is a very challenging problem. The authors suggest that
2791689110	Monitoring Targeted Hate in Online Environments.	2508035001	understanding we have translated some of the words to English. Note that the dictionaries may contain both unigrams and multiword expressions. The dictionaries are constructed in a manner similar to Tulkens et al. (2016b; 2016a); human experts (psychologist and computer scientist) manually study a large number of posts from the text domain of interest (see further Section 4.1.) and record signiﬁcant words and phrase
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2251329024	ataset with 300d GloVe word vectors, normalized (left) and unnormalized (right) 3.2 The Uniﬁed 2-gram and Token Space Modifying an implementation of GloVe in TensorFlow, tf-glove [16], by using spaCy [17] to identify twoword phrase4, the algorithm randomly treated a two-word phrase as a single token 50% of the time, and the remaining times replaced it with one of the underlying words (To prevent the p
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2250539671	halanobis distance of words in a corpus compared to their tf-idf. The context used here was the Stanford Sentiment Analysis Treebank dataset [14]. Both normalized and unnormalized GloVe word vectors3 [15] were compared, to see how much of the distance relationship was encoded in the initial distance of the word vectors. The normalized approach resulted in substantially better results, presumably becau
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2251329024	n this paper for very large corpora 72 million word 300d vectors trained with Common Crawl, crawl-300d-2M 4 A recursive approach was also tested, where the sentence was broken down by phrase by spaCy [17] and then constituent phrases were recursively merged according to the sigmoid. Unfortunately, it clearly underperformed the bag-of-words approach from the start, so was dismissed. Figure 3: A visuali
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2251939518	One necessary initial test was an implementation of the Mahalanobis distance of words in a corpus compared to their tf-idf. The context used here was the Stanford Sentiment Analysis Treebank dataset [14]. Both normalized and unnormalized GloVe word vectors3 [15] were compared, to see how much of the distance relationship was encoded in the initial distance of the word vectors. The normalized approach
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	1015675232	a standard technique to evaluate word importance, comparing the frequency of a word’s use in a document to the frequency in the overall corpus, used by at least 70% of text based recommender-systems [4]. This premise also features in the inverse-frequency sentence embedding model in Arora et al.’s “A Simple but Tough to Beat Baseline for Sentence Embeddings” [5], which remains nearly state-of-the-ar
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2780932362	thout roots generally ranks words in the same order, this method maintains the shape of the distribution. 5 Sentence Embeddings From this point in this paper onwards, GloVe is replaced with fastText7 [19], which, has less predictive word vectors [20], but can perform a character level prediction to predict word vectors for misspelled or extremely esoteric words. This was used with PyMagnitude, which o
2791720719	Context is Everything: Finding Meaning Statistically in Semantic Spaces.	2164019165	tors based on global context using a biLSTM [7]. While techniques have been developed for small scale transfer learning [1], the learning of global context [2], and multiple-word meaning word vectors [8], CoSal provides a more transparent, easy-to-implement technique which also works on tiny contexts. 2.2 tf-idf tf-idf compares a word’s frequency in a document to its frequency in an overall corpus to
2791882636	Studio Ousia’s Quiz Bowl Question Answering System	2406945108	which aims to predict the entity types for a question. For example, if the target question is the one shown in Table 1, the target entity types are person and author. We use the FIGER entity type set [10], which consists of 112 ﬁne-grained entity types, as the target entity types. We automatically assign entity types to each answer by resolving the answer’s Wikipedia entity to its corresponding entity
2791882636	Studio Ousia’s Quiz Bowl Question Answering System	2131357087	detection Because the model requires a list of the entities appearing in a question, we automatically annotate entity names using a simple entity linking method. The method is based on keyphraseness [6], which is the probability that an entity name is used as an anchor in Wikipedia. We detect an entity name if its keyphraseness is larger 4 than 2%. Furthermore, as an entity name can be ambiguous (e.
2791882636	Studio Ousia’s Quiz Bowl Question Answering System	2120615054	owing: u i =relu(W convs i +b conv); (5) where relu is a rectiﬁer function, W conv 2Rd conv hd word is a weight matrix, and b conv 2Rd conv is a bias vector. Note that because we use wide convolution [11], m equals N +h+1 in our model. Then, we use max pooling to combine the m vectors into a single d conv-dimensional feature vector c, each of whose components is computed as follows: c j = max 1&lt;im
2791882636	Studio Ousia’s Quiz Bowl Question Answering System	2118091490	tputs two scores: the sum and the maximum probability8 based on the predicted probabilities of the entity types assigned to the answer. 2.4 Information retrieval models As others have in past studies [2, 13, 14], we use conventional IR models to enhance the performance of our QA system. In particular, we compute multiple relevance scores against the documents associated with the target answer using the words
2791882636	Studio Ousia’s Quiz Bowl Question Answering System	2250539671	yper-parameters of the CNN, we use H = f2;3;4;5g, d word = 300, and d conv =1;000. We use ﬁlter window sizes of 2, 3, 4, and 5, and 1,000 feature maps for each ﬁlter. We use the GloVe word embeddings [12] trained on the 840 billion Common Crawl corpus to initialize the word representations. As in the neural network model explained previously, a question is truncated at a random position before it is i
2791948357	Towards the Creation of a Large Corpus of Synthetically-Identified Clinical Notes.	2030498706	his realization presents a unique challenge within the clinical domain. Electronic Health Record (EHR) notes are an important source of information for improving our current understanding of patients [4]. However, text typically contains sensitive protected health information (PHI) and consequently cannot be shared easily among researchers due to legal limitations established to ensure patient privac
2792145158	MULTIMODAL EMOJI PREDICTION	2194775991	nt. 3 Models We present and motivate the models that we use to predict an emoji given an Instagram post composed by a picture and the associated comment. 3.1 ResNets Deep Residual Networks (ResNets) (He et al., 2016) are Convolutional Neural Networks which were competitive in several image classiﬁcation tasks (Russakovsky et al.,2015;Lin et al.,2014) and showed to be one of the best CNN architectures for image re
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2130942839	b.com/neulab/xnmt. 1 Introduction Due to the effectiveness and relative ease of implementation, there is now a proliferation of toolkits for neural machine translation (Kalchbrennerand Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), as many as 51 according to the tally by nmt-list.1 The common requirements for such toolkits are speed, memory efﬁciency, and translation accuracy, which are essential for th
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2195405088	hingfor speed-up instead. • XNMT of course contains standard NMT models, but also includes functionality for optimization using reinforcement learning (Ranzato et al., 2015) or minimum risk training (Shen et al., 2016), ﬂexible multi-task learning (Dai and Le, 2015), encoders for speech (Chan et al.,2016),andtrainingandtestingofretrieval-basedmodels(Huanget al.,2013). In theremainderof the paper,we providesome conc
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2327501763	Modeling Techniques XNMTaims toprovidea widelibraryofstandardmodelingtoolsofusein performingNMT,or sequence-to-sequencemodelingexperimentsingeneral. Forexample,ithassupportforspeechoriented encoders (Chan et al., 2016; Harwath et al., 2016) that can be used in speech recognition, preliminary support for self-attentional “Transformer” models (Vaswani et al., 2017). It also has the ability to performexperimentsin re
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2170973209	ns standard NMT models, but also includes functionality for optimization using reinforcement learning (Ranzato et al., 2015) or minimum risk training (Shen et al., 2016), ﬂexible multi-task learning (Dai and Le, 2015), encoders for speech (Chan et al.,2016),andtrainingandtestingofretrieval-basedmodels(Huanget al.,2013). In theremainderof the paper,we providesome concreteexamplesof thedesign principles behind XNMT,
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2626778328	ral. Forexample,ithassupportforspeechoriented encoders (Chan et al., 2016; Harwath et al., 2016) that can be used in speech recognition, preliminary support for self-attentional “Transformer” models (Vaswani et al., 2017). It also has the ability to performexperimentsin retrieval (Huanget al., 2013) instead of sequence generation. 3.2 Parameter Sharing and Multi-task Learning Modern deep learning architectures often i
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2133564696	roduction Due to the effectiveness and relative ease of implementation, there is now a proliferation of toolkits for neural machine translation (Kalchbrennerand Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), as many as 51 according to the tally by nmt-list.1 The common requirements for such toolkits are speed, memory efﬁciency, and translation accuracy, which are essential for the use of such systems in
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	1869752048	sk MT + Parsing We performed a multi-task training of a sequence-to-sequence model for parsing and a machine translation task. The main task is the parsing task, and we followed the general setup in (Vinyals et al., 2015), butwe onlyusedthe standardWSJ trainingdata. It is jointlytrainedwith English-German translation system. Since the source side is English sentence for both tasks, they can share the source embedder a
2792296443	XNMT: The eXtensible Neural Machine Translation Toolkit.	2327501763	t al. (2015) is simpler because it does not use BPE and only a unidirectionalencoder. 4.2 Speech Recognition We performedexperimentsin a speech recognitiontask with a simple listen-attend-spellmodel (Chan et al., 2016). This model features a 4-layer pyramidal LSTM encoder, subsampling the input sequence by factor 2 at every layer except the ﬁrst, resulting in an overall subsampling factor of 8. The layer size is se
2792376130	An Analysis of Neural Language Modeling at Multiple Scales.	2618854269	uage modeling baseline, they achieve state-of-the-art results similarly to Melis et al.(2018). The most recent results on character-level datasets generally involve more complex architectures however.Mujika et al. (2017) introduce the Fast-Slow RNN, which splits the standard language model architecture into a fast changing RNN cell and a slow changing RNN cell. They show the slow RNN’s hidden state experiences less c
2792494424	On the difficulty of a distributional semantics of spoken language	2586148577	.5 Audio2vec-U 5 105 0.0 Audio2vec-C 2 647 0.0 Mean MFCC 1 1,414 0.0 Chance 0 3,955 0.0 Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model provided by [17]. spoken utterances in the validation data, and rank the others according to the cosine similarity. We then measure: (a) Median rank of the top-ranked paraphrase; and (b) recall@K: the proportion of p
2792494424	On the difficulty of a distributional semantics of spoken language	2586148577,2601713192	al context. Datasets of images paired with spoken captions can be used to train multimodal models that extract visually salient semantic information from speech, without access to textual information [14, 15, 16, 17, 18, 19]. On the other hand, the success of word embeddings derived by distributional semantic principles has shown how rich the semantic information within the structure of language itself is. Semantic repre
2792494424	On the difficulty of a distributional semantics of spoken language	2131744502	us composition operators have been applied on word representations (e.g. [4, 5, 6, 7]. Alternatively, sentence representations are induced via the objective to predict the surrounding sentences (e.g. [8, 9]). Such representations capture aspects of the meaning of the encoded sentences, which can be used in a variety of tasks such as semantic entailment or text understanding. In the case of spoken langua
2792494424	On the difficulty of a distributional semantics of spoken language	1924770834	cture of [17], i.e.itconsistsofa1-dimensionalconvolutionallayerwhichsubsamples the input, followed by a stack of recurrent layers, followed by a self-attention operator. Unlike [17] we use GRU layers [22] instead of RHN layers [23], and do not implement residual connections. These modiﬁcations are made in order to exploit the fast native CUDNN implementation of a GRU stack and thus speed up experiment
2792494424	On the difficulty of a distributional semantics of spoken language	2767224889	d to sentences in the Skip-thought model [9]. The sequence-to-sequence Audio2Vec model learns semantic embeddings for audio segments corresponding to words, by predicting the audio segments around it [21]. This work is closely related to our work, but we do not segment speech into words. 3. Models 3.1. Encoder Allthe models inthethissectionusethesame encoder architecture. The encoder is loosely based
2792494424	On the difficulty of a distributional semantics of spoken language	1882958252	er for training, speaker identity can be decoded from them. We thus implemented a version of SegMatch where an auxiliary speaker classiﬁer is connected to the encoder via a gradient reversal operator [29]. This architecture optimizes the main loss, while at the same timepushing the encoder toremove information about speaker identity from the representation it outputs. In preliminary experiments we saw
2792494424	On the difficulty of a distributional semantics of spoken language	2117539524	g the diagonal) and compute Pearson’s correlation coefﬁcient between them. The image features for this evaluation are obtained from the ﬁnal fully connect layer of VGG-16 [27] pre-trained on Imagenet [28] and consist of 4096 dimensions. 4.3. Settings We preprocess the audio by extracting 12-dimensional melfrequency cepstral coefﬁcients (MFCC) plus log of the total energy. We use 25 milisecond windows,
2792494424	On the difficulty of a distributional semantics of spoken language	2153579005	h LatentSemanticAnalysis have proven toclosely resemble human semantic knowledge [2, 1]. Word2vec models produce semantically rich word embeddings by learning to predict the surrounding words in text [20, 3] and this principle is extended to sentences in the Skip-thought model [9]. The sequence-to-sequence Audio2Vec model learns semantic embeddings for audio segments corresponding to words, by predicting
2792494424	On the difficulty of a distributional semantics of spoken language	2114347655	ined on raw audio data gives discrete encodings that are closely related to phonemes [11]. Words and phrase units in continuous speech can be discovered using algorithms based on dynamic time warping [12], and as a by-product in end-to-end tasks such as speech-to-speech translation [13]. Semantic information encoded in speech is used in studies that ground speech to the visual context. Datasets of ima
2792494424	On the difficulty of a distributional semantics of spoken language	2767224889	ned by an MLP with learned parametersUandW,andpassedthroughthetimewisesoftmax function: αt = exp(Utanh(Wxt)) P t′ exp(Utanh(Wxt′)) (3) 3.2. Audio2vec We deﬁne two versions of a model based on [9] and [21]. Both versions use the multilayer GRU encoder described above, and a single-layer GRU decoder, conditioned on the output of the encoder. The decoder predicts the MFCC features at time t+1 based on th
2792494424	On the difficulty of a distributional semantics of spoken language	2586148577	om the same utterance and distinguish them from encoded segments from different utterances withinthe same minibatch. The loss function issimilar to the one for matching spoken utterances to images in [17], with the difference that here we are matching utterance segments to each other: (11) X b,e X b′ max[0,α +d(b,e)− d(b′,e)] + X e′ max[0,α +d(b,e)− d(b,e′)] ! where (b,e)are beginning andend segments
2792494424	On the difficulty of a distributional semantics of spoken language	2586148577	ple photographic images with their spoken descriptions. Thanks to the structure of these data we can use the evaluation metrics detailed in section 4.2. SyntheticallyspokenCOCO Thisdatasetwascreatedby[17], basedontheoriginalCOCOdataset[24],usingtheGoogle TTS API.Thecaptions are spoken bya single synthetic voice, which is realistic but simpler than human speakers, lacking variability and ambient noise.
2792494424	On the difficulty of a distributional semantics of spoken language	1681397005	can predict their surrounding context. In search for similarly generic and versatile representations of whole sentences, various composition operators have been applied on word representations (e.g. [4, 5, 6, 7]. Alternatively, sentence representations are induced via the objective to predict the surrounding sentences (e.g. [8, 9]). Such representations capture aspects of the meaning of the encoded sentences
2792494424	On the difficulty of a distributional semantics of spoken language	2586148577	related to our work, but we do not segment speech into words. 3. Models 3.1. Encoder Allthe models inthethissectionusethesame encoder architecture. The encoder is loosely based on the architecture of [17], i.e.itconsistsofa1-dimensionalconvolutionallayerwhichsubsamples the input, followed by a stack of recurrent layers, followed by a self-attention operator. Unlike [17] we use GRU layers [22] instead
2792494424	On the difficulty of a distributional semantics of spoken language	1686810756	s of these matrices (excluding the diagonal) and compute Pearson’s correlation coefﬁcient between them. The image features for this evaluation are obtained from the ﬁnal fully connect layer of VGG-16 [27] pre-trained on Imagenet [28] and consist of 4096 dimensions. 4.3. Settings We preprocess the audio by extracting 12-dimensional melfrequency cepstral coefﬁcients (MFCC) plus log of the total energy.
2792494424	On the difficulty of a distributional semantics of spoken language	1861492603	spoken descriptions. Thanks to the structure of these data we can use the evaluation metrics detailed in section 4.2. SyntheticallyspokenCOCO Thisdatasetwascreatedby[17], basedontheoriginalCOCOdataset[24],usingtheGoogle TTS API.Thecaptions are spoken bya single synthetic voice, which is realistic but simpler than human speakers, lacking variability and ambient noise. There are 300,000 images, each wit
2792494424	On the difficulty of a distributional semantics of spoken language	2153579005	tion learning 1. Introduction In the realm of NLP for written language, unsupervised approaches to inducing semantic representations of words have a long pedigree and a history of substantial success [1, 2, 3]. The core idea behind these models is to build word representations that can predict their surrounding context. In search for similarly generic and versatile representations of whole sentences, vario
2792494424	On the difficulty of a distributional semantics of spoken language	2767224889	ut does not have access to the frame at t. yt+1 =Fht (7) ht =gru(ht−1,Enc(x)) (8) In this version h 0 is a learned parameter. For both versions the loss function is the Mean Squared Error. Themodel of[21]works onword-segmented speech: the encoder encodes the middle word of a ﬁve word sequence, and the decoder decodes each of the surrounding words. Similarly, the Skip-thought model of [9] works with a
2792494424	On the difficulty of a distributional semantics of spoken language	2752796333	ver the phonemic or lexical building blocks of the language signal. A VQ-VAE model with a convolutional encoder trained on raw audio data gives discrete encodings that are closely related to phonemes [11]. Words and phrase units in continuous speech can be discovered using algorithms based on dynamic time warping [12], and as a by-product in end-to-end tasks such as speech-to-speech translation [13].
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	2410539690	2013; Cho et al., 2014; Sutskever et al., 2014b) and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015b; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	1577834172	amazaki (1997) proposed to use statistical or dictionary information to build alignment corpus. Resnik (1998, 1999) proposed to extract parallel corpus from the Internet with a system called Strands. Wang and Ren (2005) proposed to use the logarithmic linear model for Chinese-Japanese clause alignment. Besides features such as sentence lengths, matching patterns, Chinese character co-occurrence in Japanese and Chine
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	2100664567	to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015b; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. Experimental results show that a copy mechanism can improve performance of seq-to-seq model remarkably
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	1902237438	ention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015b; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017), which also enhanced the performance of the NMT model. Experimental results show that a copy mechanis
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	1577834172	ese sentence alignment as a baseline. Following the previous work, we implement this model with combination of three features, sentence lengths, matching patterns and Chinese character co-occurrence (Wang and Ren, 2005; Lin and Wang, 2007; Liu and Wang, 2012). We split the data into training set (2,999) and test set(1,545) to train the log-linear model. Our unsupervised method does not need training data. Both thes
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	2130942839	esults show that our simple algorithm works very well (F1 score 99.4), which is even better than the supervised algorithms. Deep learning has achieved great success in tasks like machine translation. Sutskever et al. (2014a) proposed a sequence to sequence (seqto-seq) model that generates good translation results on machine translation. Bahdanau et al. (2014) proposed to use attention mechanism to allow the decoder to
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	2304113845	ge and target language. 2.3 Copy Mechanism As is stated above, ancient and contemporary Chinese share many common characters and most of the name entities use the same representation. Copy mechanism (Gu et al., 2016) is very suitable in this situation, where the source and target sequence share some of the words. We apply pointer-generator framework in our model, which follows the same intuition as the copy mecha
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	169228892	Japanese clause alignment. Besides features such as sentence lengths, matching patterns, Chinese character co-occurrence in Japanese and Chinese is also taken into consideration. Lin and Wang (2007); Liu and Wang (2012) adapted this method to ancient-contemporary Chinese translation alignment based on the observation that Chinese character co-occurrence also exists in ancientcontemporary Chinese. The method above wo
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	169228892	llowing the previous work, we implement this model with combination of three features, sentence lengths, matching patterns and Chinese character co-occurrence (Wang and Ren, 2005; Lin and Wang, 2007; Liu and Wang, 2012). We split the data into training set (2,999) and test set(1,545) to train the log-linear model. Our unsupervised method does not need training data. Both these two methods are evaluated on the test s
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	1753482797	roposed to use attention mechanism to allow the decoder to extract phrase alignment information from the hidden states of the encoder. Most of the existing NMT systems are based on the Seq2Seq model (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014b) and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu e
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	2527133236	Seq model (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014b) and the attention mechanism. Some of them have variant architectures to capture more information from the inputs (Su et al., 2016; Xiong et al., 2017; Tu et al., 2016), and some improve the attention mechanism (Luong et al., 2015b; Meng et al., 2016; Mi et al., 2016; Jean et al., 2015; Feng et al., 2016; Calixto et al., 2017),
2792511229	Automatic Transferring between Ancient Chinese and Contemporary Chinese	1902237438	states s t of the decoder at time step t and all the hidden states h in the encoder, which is also known as the attention mechanism. Instead of the normal global attention, we apply local attention (Luong et al., 2015a; Tjandra et al., 2017). Because most of the time ancient and contemporary Chinese have similar word order, when calculating the context vector c t, we calculate a pivot position in the hidden states
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	2120699290	used to add distributional features to the Framester proﬁles to enable a better word frame disambiguation. 2.1. Resources and Datasets Our approach makes use of three linguistic resources: BabelNet (Navigli and Ponzetto, 2012): a multilingual encyclopedic dictionary that connects concepts and named entities in a very large network of semantic relations (see Table 1). Framester (Gangemi et al., 2016a): a linguistic LOD hub
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	2023460117	age symbolic (Gangemi et al., 2016b) and statistical (Chen et al., 2014) semantics for frame parsing, which, in turn, can be exploited for many different applications ranging from sentiment analysis (Recupero et al., 2015) all the way to content-based recommendations (De Clercq et al., 2014). The contributions of this paper are the following ones: 1.We present the LOaDing lexical resource, an extension of Framester tha
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	2153225416	ation extraction systems like NELL (Carlson et al., 2010) or Knowledge Vault (Dong et al., 2014) can acquire massive amounts of machine-readable knowledge from the Web, whereas projects like DBpedia (Bizer et al., 2009), YAGO (Rebele et al., 2016) or BabelNet (Navigli and Ponzetto, 2012) have turned collaboratively-generated content into large knowledge bases. However, all of these resources are entitycentric in tha
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	2016753842	itnessed an impressive amount of work on the automatic construction of wide-coverage knowledge resources. Web-scale information extraction systems like NELL (Carlson et al., 2010) or Knowledge Vault (Dong et al., 2014) can acquire massive amounts of machine-readable knowledge from the Web, whereas projects like DBpedia (Bizer et al., 2009), YAGO (Rebele et al., 2016) or BabelNet (Navigli and Ponzetto, 2012) have tu
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	2120699290	Knowledge Vault (Dong et al., 2014) can acquire massive amounts of machine-readable knowledge from the Web, whereas projects like DBpedia (Bizer et al., 2009), YAGO (Rebele et al., 2016) or BabelNet (Navigli and Ponzetto, 2012) have turned collaboratively-generated content into large knowledge bases. However, all of these resources are entitycentric in that they are primarily built around the notion of entities, as either p
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	1987863801	meaning representations directly from the data. Bringing together the ‘best of both worlds’ has the potential to combine the beneﬁts of wide-coverage symbolic (Gangemi et al., 2016b) and statistical (Chen et al., 2014) semantics for frame parsing, which, in turn, can be exploited for many different applications ranging from sentiment analysis (Recupero et al., 2015) all the way to content-based recommendations (De
2792599664	Enriching Frame Representations with Distributionally Induced Senses.	1512387364	ntics. 1. Introduction Recent years have witnessed an impressive amount of work on the automatic construction of wide-coverage knowledge resources. Web-scale information extraction systems like NELL (Carlson et al., 2010) or Knowledge Vault (Dong et al., 2014) can acquire massive amounts of machine-readable knowledge from the Web, whereas projects like DBpedia (Bizer et al., 2009), YAGO (Rebele et al., 2016) or BabelN
2792918004	Learning to Recognize Musical Genre from Audio.	2580221632	them to get familiar with it. That challenge was part of a wider effort to promote open evaluation in machine learning for music data, of which the release of the open FMA dataset was the first step [1]. The goal of this initiative is to establish a reference benchmark based on open data. MIR research has historically suffered from the lack of publicly available benchmark datasets, which stem from t
2792918004	Learning to Recognize Musical Genre from Audio.	2580221632	h song only has one target genre. Other metadata, e.g. the song title or artist name, were not to be used for the prediction. The data for this challenge comes from the recently published FMA dataset [1], a dump of the Free Music Archive (FMA).1 The dataset is a collection of 917 GiB and 343 days of Creative Commonslicensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in
2792918004	Learning to Recognize Musical Genre from Audio.	2580221632	Moreover, we developed a starter kit with code to handle the data and make a submission.4 It also featured some examples and a baseline. Finally, participants were encouraged to review the FMA paper [1] for a detailed description of the data as well as the GitHub repository for Jupyter notebooks showing how to use the data, explore it, and train baseline models.5 3 RESULTS At the end of the first ro
2792918004	Learning to Recognize Musical Genre from Audio.	2101151533	racterize similarities between compositions and organize music collections. Yet the boundaries between genres still remain fuzzy, making the problem of music genre recognition (MGR) a nontrivial task [5]. While its utility has been debated, mostly because of its ambiguity and cultural definition, it is widely used and understood by endusers who find it useful to discuss musical categories [3]. This p
2793065281	THE WEB AS A KNOWLEDGE-BASE FOR ANSWERING COMPLEX QUESTIONS	2465265491	-100 web snippets using manually-engineered features. We re-train the model on our data with one new feature: for every question qand candidate answer mention in a snippet, we run RASOR, a RC model bylee et al. (2016), and add the output logit score as a feature. We found that combining the web-facing model of Talmor et al.(2017) and RASOR, resulted in improved performance. Evaluation For evaluation, we measure pr
2793065281	THE WEB AS A KNOWLEDGE-BASE FOR ANSWERING COMPLEX QUESTIONS	1544827683	ion from multiple sources. Recently, interest in question answering (QA) has surged in the context of reading comprehension (RC), where an answer is sought for a question given one or more documents (Hermann et al., 2015;Joshi et al.,2017;Rajpurkar et al.,2016). q:What city is the birthplace of the author of `Without end&apos;, and hosted Euro 2012? Decompose: q 1:Author of `Without End&apos;? fKen Follett, Adam Zaga
2793065281	THE WEB AS A KNOWLEDGE-BASE FOR ANSWERING COMPLEX QUESTIONS	2510759893	rk requires a dataset of broad and complex questions that examine the importance of question decomposition. While many QA datasets have been developed recently (Yang et al.,2015;Rajpurkar et al.,2016;Hewlett et al., 2016;Nguyen et al.,2016;Onishi et al.,2016;Hill et al.,2015;Welbl et al.,2017), they lack a focus on the importance of question decomposition. Most RC datasets contain simple questions that can be answere
2793065281	THE WEB AS A KNOWLEDGE-BASE FOR ANSWERING COMPLEX QUESTIONS	2411480514	the ﬁnal answer a. Neural models trained over large datasets led to great progress in RC, nearing human-level performance (Wang et al.,2017). However, analysis of models revealed (Jia and Liang,2017;Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are ava
2793265220	CliNER 2.0: Accessible and Accurate Clinical Concept Extraction.	2104381725	ask of concept extraction from discharge summaries. The winning system achieved an exact F measure of 0.852 by using a discriminative semi-Markov HMM, trained using passive-aggressive online updates [deBruijn et al., 2011]. Many other top performing methods used a Conditional Random Field (CRF) to model the sequence learning problem [Roberts and Harabagiu, 2011]. In the years following the shared task workshop, the da
2793265220	CliNER 2.0: Accessible and Accurate Clinical Concept Extraction.	2046844528	ccessful attempts utilized the strengths of workshop participants (sequential models, such as a CRF) and added generalized word representations using distributional semantics [Fu and Ananiadou, 2014, Jonnalagadda et al., 2012, Wu et al., 2015]. Since then, deep learning and recurrent neural networks have increased in popularity and easiness-to-implement, leading to a many LSTM-based approaches to clinical concept extracti
2793265220	CliNER 2.0: Accessible and Accurate Clinical Concept Extraction.	2208683342	g this dataset. Early successful attempts utilized the strengths of workshop participants (sequential models, such as a CRF) and added generalized word representations using distributional semantics [Fu and Ananiadou, 2014, Jonnalagadda et al., 2012, Wu et al., 2015]. Since then, deep learning and recurrent neural networks have increased in popularity and easiness-to-implement, leading to a many LSTM-based approaches t
2793265220	CliNER 2.0: Accessible and Accurate Clinical Concept Extraction.	2143017621	oys feature extraction using both linguistic features (e.g., ngrams and wordshapes) and domain knowledge (e.g., UMLS Metathesaurus). POS tagging was performed with the general-domain nltk pos_tagger [Bird, 2002]. Table 1 shows a full list of features that are extracted for each token. These features are extracted for each individual token, except for prev1-all-feats and next1-all-feats, which include all wo
2793265220	CliNER 2.0: Accessible and Accurate Clinical Concept Extraction.	2208683342	s approach is sensitive to misspellings such as “cholorolesteral.” Table 2: Precision, recall, and F1 of selected concept extraction models. Exact Class Match Precision Recall F1 Truecasing CRFSuite [Fu and Ananiadou, 2014] 0.808 0.715 0.759 Binarized Neural Embedding CRF [Wu et al., 2015] 0.851 0.806 0.828 LSTM-CRF: GloVe Chalapathy et al. [2016] 0.844 0.834 0.839 CliNER 2.0: feats+CRF 0.835 0.758 0.795 CliNER 2.0: w+
2793276877	Preparing Bengali-English Code-Mixed Corpus for Sentiment Analysis of Indian Languages.	2139645402	em by combining a lexicon based module with supervised classiﬁers like SVM, CRF and decision trees. Some of them have also been made as a sub-part for a part-of-speech tagging system like the one by (Vyas et al., 2014). For sentiment analysis on code-mixed, binary polarity classiﬁcation has been tried using different classes of supervised modelsby (Ghosh et al., 2017b) andfor ternarypolarityby (Ghosh et al., 2017a)
2793348910	Tensor2Tensor for Neural Machine Translation.	2525778437	.8 23 2016). These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible (Wu et al., 2016). 3 Tensor2Tensor Tensor2Tensor (T2T) is a library of deep learning models and datasets designed to make deep learning research faster and more accessible. T2T uses TensorFlow (Abadi et al., 2016) thr
2793348910	Tensor2Tensor for Neural Machine Translation.	2545625743	ard RNN on top of the convolution to generate the output, which creates a bottleneck and hurts performance. Fully convolutional neural machine translation without this bottleneck was ﬁrst achieved in Kaiser and Bengio (2016) and Kalchbrenner et al. (2016). The Extended Neural GPU model (Kaiser and Bengio, 2016) used a recurrent stack of gated convolutional layers, while the ByteNet model (Kalchbrenner et al., 2016) did a
2793348910	Tensor2Tensor for Neural Machine Translation.	2525778437	ce length nis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (Wu et al., 2016) and byte-pair (Sennrich et al., 2015) representations. A single convolutional layer with kernel width k &lt;ndoes not connect all pairs of input and output positions. Doing so requires a stack of O(n
2793348910	Tensor2Tensor for Neural Machine Translation.	2581624817	DE EN-FR ByteNet (Kalchbrenner et al., 2016) 23.75 Deep-Att + PosUnk (Zhou et al., 2016) 39.2 100 GNMT + RL (Wu et al., 2016) 24.6 39.92 23 140 ConvS2S (Gehring et al., 2017) 25.16 40.46 9.6 150 MoE (Shazeer et al., 2017) 26.03 40.56 20 120 GNMT + RL Ensemble (Wu et al., 2016) 26.30 41.16 180 1100 ConvS2S Ensemble (Gehring et al., 2017) 26.36 41.29 77 1200 Transformer (base model) 27.3 38.1 3:3 Transformer (big) 28.4
2793348910	Tensor2Tensor for Neural Machine Translation.	2064675550	ep neural networks achieved great success with sequence-tosequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) that used recurrent neural networks (RNNs) with LSTM cells (Hochreiter and Schmidhuber, 1997). The basic sequence-to-sequence architecture is composed of an RNN encoder which reads the source sentence one token at a time and transforms it into a ﬁxed-sized state vector. This is followed by an
2793348910	Tensor2Tensor for Neural Machine Translation.	2737740651	es heavily on many of the attention building blocks in Tensor2Tensor and adds many of its own.  tf.contrib.layers.rev block, implementing a memory-efﬁcient block of reversible layers as presented in Gomez et al. (2017), was ﬁrst implemented and exercised in Tensor2Tensor.  The Adafactor optimizer (pending publication), which signiﬁcantly reduces memory requirements for second-moment estimates, was developed within
2793348910	Tensor2Tensor for Neural Machine Translation.	2525778437	a fraction of the training cost. Model BLEU Training Cost (in FLOPS * 1018) EN-DE EN-FR EN-DE EN-FR ByteNet (Kalchbrenner et al., 2016) 23.75 Deep-Att + PosUnk (Zhou et al., 2016) 39.2 100 GNMT + RL (Wu et al., 2016) 24.6 39.92 23 140 ConvS2S (Gehring et al., 2017) 25.16 40.46 9.6 150 MoE (Shazeer et al., 2017) 26.03 40.56 20 120 GNMT + RL Ensemble (Wu et al., 2016) 26.30 41.16 180 1100 ConvS2S Ensemble (Gehring
2793348910	Tensor2Tensor for Neural Machine Translation.	1753482797	ially overcome in Bahdanau et al. (2014) by using a neural model of attention. Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from Kalchbrenner and Blunsom (2013) and later in Meng et al. (2015). These early models used a standard RNN on top of the convolution to generate the output, which creates a bottleneck and hurts performance. Fully convolutional neural
2793348910	Tensor2Tensor for Neural Machine Translation.	2613904329	his idea, introduced in WaveNet (van den Oord et al., 2016), signiﬁcantly improves efﬁciency of the model. The same technique was improved in a number of neural translation models recently, including Gehring et al. (2017) and Kaiser et al. (2017). 2 Self-Attention Instead of convolutions, one can use stacked self-attention layers. This was introduced in the Transformer model (Vaswani et al., 2017) and has signiﬁcantly
2793348910	Tensor2Tensor for Neural Machine Translation.	2133564696	the whole input sentence needs to be encoded into a single ﬁxed-size vector. This clearly manifests itself in the degradation of translation quality on longer sentences and was partially overcome in Bahdanau et al. (2014) by using a neural model of attention. Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from Kalchbrenner and Blunsom (2013) and late
2793348910	Tensor2Tensor for Neural Machine Translation.	2130942839	mplementation of the state-of-the-art Transformer model. 1 Neural Machine Translation Background Machine translation using deep neural networks achieved great success with sequence-tosequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) that used recurrent neural networks (RNNs) with LSTM cells (Hochreiter and Schmidhuber, 1997). The basic sequence-to-sequence architecture is composed of an
2793348910	Tensor2Tensor for Neural Machine Translation.	2130942839	n RNN decoder, which generates the target sentence, one token at a time, from the state vector. While a pure sequence-to-sequence recurrent neural network can already obtain good translation results (Sutskever et al., 2014; Cho et al., 2014), it suffers from the fact that the whole input sentence needs to be encoded into a single ﬁxed-size vector. This clearly manifests itself in the degradation of translation quality
2793348910	Tensor2Tensor for Neural Machine Translation.	2626778328	recently, including Gehring et al. (2017) and Kaiser et al. (2017). 2 Self-Attention Instead of convolutions, one can use stacked self-attention layers. This was introduced in the Transformer model (Vaswani et al., 2017) and has signiﬁcantly improved state-of-the-art in machine translation and language modeling while also improving the speed of training. Research arXiv:1803.07416v1 [cs.LG] 16 Mar 2018 Figure 1: The T
2793348910	Tensor2Tensor for Neural Machine Translation.	2626778328	Recurrent O(n d2) O(n) O(n) Convolutional O(k n d2) O(1) O(log k(n)) Self-Attention (restricted) O(r n d) O(1) O(n=r) More details about multi-head attention and overall architecture can be found in Vaswani et al. (2017). 2.1 Computational Performance As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)se
2793348910	Tensor2Tensor for Neural Machine Translation.	2132043663	sing a neural model of attention. Convolutional architectures have been used to obtain good results in word-level neural machine translation starting from Kalchbrenner and Blunsom (2013) and later in Meng et al. (2015). These early models used a standard RNN on top of the convolution to generate the output, which creates a bottleneck and hurts performance. Fully convolutional neural machine translation without this
2793348910	Tensor2Tensor for Neural Machine Translation.	2133564696	te-of-the-art Transformer model. 1 Neural Machine Translation Background Machine translation using deep neural networks achieved great success with sequence-tosequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) that used recurrent neural networks (RNNs) with LSTM cells (Hochreiter and Schmidhuber, 1997). The basic sequence-to-sequence architecture is composed of an RNN encoder which reads
2793348910	Tensor2Tensor for Neural Machine Translation.	2613904329	Training Cost (in FLOPS * 1018) EN-DE EN-FR EN-DE EN-FR ByteNet (Kalchbrenner et al., 2016) 23.75 Deep-Att + PosUnk (Zhou et al., 2016) 39.2 100 GNMT + RL (Wu et al., 2016) 24.6 39.92 23 140 ConvS2S (Gehring et al., 2017) 25.16 40.46 9.6 150 MoE (Shazeer et al., 2017) 26.03 40.56 20 120 GNMT + RL Ensemble (Wu et al., 2016) 26.30 41.16 180 1100 ConvS2S Ensemble (Gehring et al., 2017) 26.36 41.29 77 1200 Transformer (ba
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2117278770	,2007). We train a 5-gram language model using KenLM (Heaﬁeld et al.,2013), and use target style sentences from YAFC and the sub-sampled target style sentences from out-of-domain Yahoo Answers, as in Moore and Lewis (2010), to create a large LM. 4.3 Neural Machine Translation Encoder-decoder based neural network models have become quite successful (Sutskever et al., 2014;Bahdanau et al.,2014;Cho et al.,2014). The ﬁeld
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2735642330	ansforming the formality style of the text. Style Transfer without Parallel Data: Another direction of research directly controls certain attributes of the generated text without using parallel data. Hu et al. (2017) control the sentiment and the tense of the generated text by learning a disentangled latent representation in a neural generative model. Ficler and Goldberg (2017) control several linguistic style as
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2732863878	ate a parallel corpus of 30K sentence pairs by scraping the modern translations of Shakespeare plays and train a PBMT system to translate from modern English to Shakespearean English.2 More recently, Jhamtani et al. (2017) show that a copy-mechanism enriched sequenceto-sequence neural model outperforms XU12 on the same set. In text simpliﬁcation, the availability of parallel data extracted from English Wikipedia and Si
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2732863878	ck and Tetreault (2016) (PT16) to extract informal sentences for YAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: Previous work on style transfer (Xu et al.,2012;Jhamtani et al., 2017;Niu et al.,2017;Sennrich et al.,2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan,2011) for evaluation. Additionally, XU12 introduce aut
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2735574368	e generated text without using parallel data. Hu et al. (2017) control the sentiment and the tense of the generated text by learning a disentangled latent representation in a neural generative model. Ficler and Goldberg (2017) control several linguistic style aspects simultaneously by conditioning a recurrent neural network language model. Under NMT models, Sennrich et al. (2016a) control the politeness of the translated t
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2130942839	entences from out-of-domain Yahoo Answers, as in Moore and Lewis (2010), to create a large LM. 4.3 Neural Machine Translation Encoder-decoder based neural network models have become quite successful (Sutskever et al., 2014;Bahdanau et al.,2014;Cho et al.,2014). The ﬁeld of style transfer, however, has not yet been able to fully take advantage of these advances owing to the lack of availability of large parallel data. W
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2250616809	f formal and informal words and phrases from different sources and use a natural language generation system to generate informal and formal texts by replacing lexical items based on user preferences. Xu et al. (2012) (henceforth XU12) was one of the ﬁrst works to treat style transfer as a sequence to sequence task. They generate a parallel corpus of 30K sentence pairs by scraping the modern translations of Shakes
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2101105183	ly evaluate system outputs. Evaluating Style Transfer: Previous work on style transfer (Xu et al.,2012;Jhamtani et al., 2017;Niu et al.,2017;Sennrich et al.,2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan,2011) for evaluation. Additionally, XU12 introduce automatic metrics that measure the degree to which the output matches the target style. Under human b
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2329847998	s work on detecting formality of a given text at the lexical level (Brooke et al.,2010; 2https://github.com/cocoxu/Shakespeare Brooke and Hirst,2014;Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen,2010;Peterson et al.,2011;Mosquera and Moreda,2012). In our work, we reproduce the sentence-level formality classiﬁer introduced in Pavlick and Tetreault
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	161156596	sults for both models and evaluation metrics. 1Results for this direction are in the supplementary material. 1 arXiv:1803.06535v1 [cs.CL] 17 Mar 2018 2 Related Work Style Transfer with Parallel Data: Sheikha and Inkpen (2011) collect pairs of formal and informal words and phrases from different sources and use a natural language generation system to generate informal and formal texts by replacing lexical items based on us
2793585215	DEAR SIR OR MADAM, MAY I INTRODUCE THE YAFC CORPUS: CORPUS, BENCHMARKS AND METRICS FOR FORMALITY STYLE TRANSFER	2732863878	th attention (Bahdanau et al.,2014).9 To train this model, we use the output of the rule-based method on YAFC and GloVE (Pennington et al.,2014) word embeddings pretrained on Yahoo Answers. NMT Copy: Jhamtani et al., (2017) introduce a copy-enriched NMT model for style transfer to 9We used the OpenNMT-py (Klein et al.,2017) toolkit with default parameters, a vocabulary size of 50K and embeddings of size 300. During tran
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2155893237	2017), and signiﬁcant effort can be required to develop research infrastructure 1http://pytorch.org/ for particular model classes. More specialized toolkits exist in some domains. For example, Caffe (Jia et al., 2014) includes strong reference models trained on ImageNet (Deng et al., 2009), signiﬁcantly lowering the barrier to entry for computer vision research. AllenNLP provides a similar type of support, for sem
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2610748790	Additional models are currently under development and should be released soon, including: endto-end neural coreference (Lee et al., 2017a), and semi-supervised learning for named entity recognition (Peters et al., 2017). We also expect the number of tasks and reference implementations to grow steadily over time.7 7The most up-to-date list of reference models is maintained online: http://allennlp.org/models 6 Semanti
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	1840435438	e facts in the second. The AllenNLP TE model is a re-implementation of the decomposable attention model (Parikh et al., 2016), a widely used TE baseline that was state-of-the-art on the SNLI dataset (Bowman et al., 2015) in late 2016. The AllenNLP TE model achieves an accuracy of 86.4% on the SNLI 1.0 test dataset, a 2% improvement on most publicly available implementations andasimilarscore as the original paper. Rat
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2618101654	eing determined by a conﬁguration ﬁle. SpanExtractor: A recent trend in NLP is to build models that operate on spans of text, instead of on tokens. State-of-the-art models for coreference resolution (Lee et al., 2017a), constituency parsing (Stern et al., 2017), and semantic role labeling (He et al., 2017) all operate in this way. Support for building this kind of model is built into AllenNLP, including a SpanExt
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2577255746	hitectures inaddition tomodelparameters. Most existing deep-learning toolkits are designed for general machine learning (Bergstra et al., 2010; Yu et al., 2014; Chen et al., 2015; Abadi et al., 2016; Neubig et al., 2017), and can require signiﬁ- cant effort to develop research infrastructure for particular model classes. Some, such as Keras (Chollet et al., 2015), do aim to make it easy to build deep learning models.
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2626778328	the same impact across the ﬁeld. 1 Introduction Neural network models are now the state-of-theart for a wide range of tasks such as text classiﬁ- cation (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difﬁcult to
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2158847908	nes for future research. AllenNLP includes reference implementations for several tasks, including: • Semantic Role Labeling (SRL) models recover the latent predicate argument structure of a sentence (Palmer et al., 2005). SRL builds representations thatanswerbasicquestions about sentence meaning; for example, “who” did “what” to “whom.” The AllenNLP SRL model is a re-implementation of a deep BiLSTM model (He et al.,
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2618101654	range of tasks such as text classiﬁ- cation (Howard and Ruder, 2018), machine translation (Vaswani et al., 2017), semantic role labeling (Zhou and Xu, 2015; He et al., 2017), coreference resolution (Lee et al., 2017a), and semantic parsing (Krishnamurthy et al., 2017). However it can be surprisingly difﬁcult to tune new models or replicate existing results. State-of-the-art deep learning models often take over a
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2610748790	rd representations. Deciding between pre-trained wordembeddings, wordembeddings concatenated with a character-level CNN encoding, or even using a pre-trained model to get token-in-context embeddings (Peters et al., 2017), is all done by conﬁguring the TextFieldEmbedder, allowing for very easy controlled experimentation. Seq2SeqEncoder: A very common paradigm in deep NLP is to take a sequence of word vectors and pass
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2108598243	ructure 1http://pytorch.org/ for particular model classes. More specialized toolkits exist in some domains. For example, Caffe (Jia et al., 2014) includes strong reference models trained on ImageNet (Deng et al., 2009), signiﬁcantly lowering the barrier to entry for computer vision research. AllenNLP provides a similar type of support, for semantic NLP problems. Many existing NLP pipelines (Manning et al., 2014; Bi
2793978524	AllenNLP: A Deep Semantic Natural Language Processing Platform	2618101654	t. There is a large number of ways to do this, including LSTMs (Hochreiter and Schmidhuber, 1997), GRUs (Cho et al., 2014), intra-sentence attention (Cheng et al., 2016), recurrent additive networks (Lee et al., 2017b), and many more. AllenNLP’s Seq2SeqEncoderabstracts away the decision ofwhichparticular encoder touse, allowing the user to build an encoder-agnostic model and specify the encoder via conﬁguration.
2794237618	Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes	1987458159	20, 21, 22]. Moreover, information extraction (IE) is widely used in NLP research. The results showed that IE is solid and promising in extracting data with various formats from unstructured reports [23, 24, 25, 26, 27]. In our previous work, we developed innovative semantic knowledge discovery techniques [28, 29, 30, 31, 32] to perform knowledge pattern analysis for detecting associations among different complicati
2794237618	Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes	2111437636	rocessing, consisting of three major components: dictionary based concepts indexing and keyword mention lookup, pattern based information extraction, and machine learning based mention identification [46, 47]. In this study, we used MedTagger to identify medical concepts in clinical notes. Concept Keywords Mutual Information We extracted patients’ clinical notes within 30 days from the surgery dates for t
2794237618	Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes	1680392829	ry) were obtained. Then, these concepts with their frequency were aligned in different postsurgical days. Decision Tree Algorithm Decision tree algorithm [48] with stratified 10-fold cross validation [49] was applied to detect SSIs using the concept keywords extracted by our approach. For each fold, we selected top k concept keywords based on PMI (Eq 1) from the training set and used them as features
2794237618	Detection of Surgical Site Infection Utilizing Automated Feature Generation in Clinical Notes	1593045043	ted with patients (i.e., remove medical concepts associated with other than patients such as family members), 3) removed negated medical concepts. Then, we applied point-wise mutual information (PMI) [44, 45] on these concepts to automatically generate salient concepts related to SSIs and ranked them based on their inequality score (details in the subsection of Concept Keywords Mutual Information). The ex
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2066134726	(e.g objects and their attributes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	8316075	butes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	1782590233	es may be included in these datasets, none of them speciﬁcally targets face descriptions. There are several datasets of faces that are widely used by the Image Processing community including the LFW (Huang et al., 2007, Learned-Miller et al., 2016), MegaFace (Kemelmacher-Shlizerman et al., 2016) and IJB-C datasets (Klare et al., 2015). These datasets however do not have labelled attributes. The LFWA (Huang et al.,
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2165542474	the faces in the dataset. These were then used to provide participants with some examples of what was expected of them (see below). Data 400 Images were selected2 from the Faces in The Wild dataset3 (Berg et al., 2005), with the aim of collecting as many descriptions as possible for each image. These are close-up images of the faces of public personalities, taken in naturalistic contexts (that is, without controlli
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2185175083	jects and events or relations shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), men
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	1834627138	lmacher-Shlizerman et al., 2016) and IJB-C datasets (Klare et al., 2015). These datasets however do not have labelled attributes. The LFWA (Huang et al., 2007, LearnedMiller et al., 2016) and CelebA (Liu et al., 2015) datasets on the other hand contain images that are labelled with features mainly referring not only to physical facial attributes, such as skin colour and hair style, but also attributes of the perso
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2185089786	ly targets face descriptions. There are several datasets of faces that are widely used by the Image Processing community including the LFW (Huang et al., 2007, Learned-Miller et al., 2016), MegaFace (Kemelmacher-Shlizerman et al., 2016) and IJB-C datasets (Klare et al., 2015). These datasets however do not have labelled attributes. The LFWA (Huang et al., 2007, LearnedMiller et al., 2016) and CelebA (Liu et al., 2015) datasets on th
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2474608001	n these datasets, none of them speciﬁcally targets face descriptions. There are several datasets of faces that are widely used by the Image Processing community including the LFW (Huang et al., 2007, Learned-Miller et al., 2016), MegaFace (Kemelmacher-Shlizerman et al., 2016) and IJB-C datasets (Klare et al., 2015). These datasets however do not have labelled attributes. The LFWA (Huang et al., 2007, LearnedMiller et al., 20
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2109586012	ptions of these images are returned. The descriptions are then either copied directly (which assumes that descriptions can be reused as-is with similar images) or synthesized from extracted phrases. (Ordonez et al., 2011) and (Kuznetsova et al., 2012) are examples of retrieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct g
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2149172860	returned. The descriptions are then either copied directly (which assumes that descriptions can be reused as-is with similar images) or synthesized from extracted phrases. (Ordonez et al., 2011) and (Kuznetsova et al., 2012) are examples of retrieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct generation attempts to generate
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2143449221	s shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), mentioning what is visible, while min
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	1811254738	s use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a signiﬁcant amount of de
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2105103432	tems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a signiﬁcant amount of descriptions that can be found as-is in the training set (Devlin et al., 2015, Tanti et al., 2018). This suggests that the datasets on which they are trained are very repetitive and lack diversity. State of the art image captioning requires large datasets for training and test
2794317666	Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions.	2560313346	usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a signiﬁcant amount of descriptions that can be found as-is in t
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2149327368	cusedonnewstranslationformorethanadecade. Deﬁning and measuring human quality in translation is challenging for a number of reasons. Traditional metrics of translation quality, such as BLEU [28], TER [33] and Meteor [10] measuretranslationqualitybycomparisonwithoneormorehumanreferencetranslations. However, the same source sentence can be translated in sometimes substantially diﬀerent but equally corre
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2194775991	dsthequalityofcrowd-sourcednon-professionaltranslations. 1 Introduction Recentyearshaveseenhumanperformancelevelsreachedorsurpassedintasksrangingfromgames suchasGo[32]toclassiﬁcationofimagesinImageNet[20]toconversationalspeechrecognitionon theSwitchboardtask[49]. In the area of machine translation, we have seen dramatic improvements in quality with the advent of attentional encoder-decoder neural netw
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2130942839	esearchevaluationcampaigns(e.g.WMT[6]),andalsoforlarge scale production systems [45,11]. NMT scales to train on parallel data on the order of tens of millionsofsentences. Currently,State-of-the-artNMT[3,34]isgenerallybasedonasequence-to-sequenceencoderdecoder model with an attention mechanism [3]. Attentional sequence-to-sequence NMT models the conditional probability p(yjx) of the translated sequence y
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2117278770	experiments reportedinTable2areunconstrainedsystemsusingadditionaldata. Firstweapplywordalignmentheuristicstoﬁlterverynoisydata. Thisﬁltersoutaround10% ofthedata. ThenweapplyCross-Entropydataselection[27]and[2]toorderthesentencesbased on their relevance to the CWMT part of the WMT data. We then select a speciﬁc number of sentencespairsbyrank. In a separate experiment, we also apply the SentVec similar
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2159755860	ingrelevantdata. 3.6 SystemCombinationandRe-ranking Inordertocombinethesystemsdescribedabove,wecombinen-besthypothesesfromallsystems andthentrainare-rankerusingk-bestMIRAonthevalidationset. K-bestMIRA[8]isaversionof MIRA(amargin-basedclassiﬁcationalgorithm)thatworkswithabatchtuningtolearnare-ranker forthek-besthypothesis. Thefeaturesweuseforre-rankingare: SYS Score: OriginalSystemScoreandidentiﬁer.
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2117278770	r out the very noisy data, similar to the approach in [18]. However, datathatislessegregiouslynoisyrepresentsabiggerproblemsinceitishardertorecognize. Thede-factostandardmethodfordataselectionforSMTis[27]and[2]. Unfortunatelyithas notprovedasusefulforNMT;whileitreducesthetrainingdataitdoesnotleadtoimprovements insystemquality[37]. Weproposeanewapproachthattacklesbothproblemsatonce: ﬁltering noisydataa
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2410539690	rds,IncorrectWords,Ungrammatical,andNamedEntity. Eachaccountsforroughly5%oferrors. Thisindicatesthatthereis stillroomtoimprovemachinetranslationqualityviavariousapproaches,suchasmodelingMissing Words [36,15], integration of high quality data for named-entity translation, as well as domain andtopicadaptationfortheissuesofincorrectwordsandungrammaticality. 7 Discussion and Future Work Inthispaper,wedescrib
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2579578355	as been successfully applied to various real-world problems such as question answering [35], image classiﬁcation [47], imagesegmentation[25],imagetoimagetranslation[50,52,24],faceattributemanipulation[31], andmachinetranslation[19,43,23,1]. Inthiswork,toachievestrongmachinetranslationperformance,wecombinetwodiﬀerentdual learningmethodsthatrespectivelyenhancetheusageofmonolingualandbilingualtrainingdat
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2117278770	ualsentencepairs. WeusetheChineseandEnglish languagemodelstrainedonthe18Msentencesofbilingualdatatoﬁlterthemonolingualsentences from“NewsCrawl: articlesfrom2016”and“CommonCrawl”providedbyWMT17usingCED[27]. Afterﬁltering,weretainabout7MEnglishandChinesemonolingualsentences. Themonolingual datawillbedeployedinbothduallearningandback-translationsetupsthroughtheexperiments. Newsdev2017 is used as the deve
2794365787	Achieving Human Parity on Automatic Chinese to English News Translation.	2130942839	versationalspeechrecognitionon theSwitchboardtask[49]. In the area of machine translation, we have seen dramatic improvements in quality with the advent of attentional encoder-decoder neural networks [34,3,38]. However, translation quality continues to vary a great deal across language pairs, domains, and genres, more or less in direct relationshiptotheavailabilityoftrainingdata. Thispapersummarizeshowweac
2794386110	Unpaired Image Captioning by Language Pivoting	648786980	9]. Exposure bias happens when a model is trained to predict a word given the previous ground-truth words but uses its own generated words during inference. The schedule sampling approach proposed in [30] can mitigate the exposure bias by selecting between the ground-truth words and the machine generated words according to the scheduled probability in training. Recently, the loss-evaluation mismatch p
2794386110	Unpaired Image Captioning by Language Pivoting	2133564696	asets [1,16,15], many studies [18,19, 3,2023,8,24] have used neural networks to generate image descriptions. Inspired by the success of encoder-decoder framework for neural machine translation (NMT) [25, 6], many researchers have proposed to use such a framework for image caption generation [5,3]. One representative work in this direction is the method proposed by Vinyals et al [5]. They encode the imag
2794386110	Unpaired Image Captioning by Language Pivoting	2560313346	r bounds of image-to-English captioning results on MSCOCO test split. Approach BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr Upper Bound i2tim ! en (ResNet101, XE Loss) 73.2 56.3 42.0 31.2 25.3 95.1 FC-2K [7] (ResNet101, XE Loss)    29.6 25.2 94.0 Lower Bound i2tim ! zh +nmt zh ! en (Independent) 42.0 20.6 9.5 3.9 12.0 12.3 Results of Unpaired Image English Captioning. Table 5 shows the comparisons amo
2794386110	Unpaired Image Captioning by Language Pivoting	2337363174,2443536229	btain for some language pairs. This is unfortunate because NMT usually needs a large amount of data to train. As a result, improving NMT on resource-scarce language pairs has attracted much attention [42,43]. Recently,manyworkshavebeendoneintheareaofpivotstrategiesofNMT[44,11, 12,4547,13].Pivot-basedapproachintroducesathirdlanguage,namedpivotlanguage for which there exist source-pivot and pivot-target p
2794386110	Unpaired Image Captioning by Language Pivoting	2560313346	en that is trained with paired English captions. i2t im ! en shares the same architecture as i2t im ! zh , except that they havedifferentvocabularysizes.WealsoreporttheresultsofourimplementationofFC2K[7],whichadoptsasimilararchitecture.AscanbeseeninTable4,i2t im ! en achieves slightly better performance in all metrics. The lower bound is achieved by pipelinining i2tim ! zh andnmt zh ! en .Inthepipeli
2794386110	Unpaired Image Captioning by Language Pivoting	1811254738	ge descriptions. Inspired by the success of encoder-decoder framework for neural machine translation (NMT) [25, 6], many researchers have proposed to use such a framework for image caption generation [5,3]. One representative work in this direction is the method proposed by Vinyals et al [5]. They encode the image with a CNN and use a Long Short-Term Memory (LSTM) network as the decoder, and the decode
2794386110	Unpaired Image Captioning by Language Pivoting	2176263492	hich can explore both long-term and temporal information in word sequences for caption generation. Exposure bias and loss-evaluation mismatch have been the major problems in sequence prediction tasks [29]. Exposure bias happens when a model is trained to predict a word given the previous ground-truth words but uses its own generated words during inference. The schedule sampling approach proposed in [3
2794386110	Unpaired Image Captioning by Language Pivoting	2293344577	ing. Gu et al [8] propose a coarse-to-ne learning approach which simultaneously solves the multi-stage training problem as well as the exposure bias issue. The most closely related to our approach is [21]. However, they construct a multilingual parallel dataset based on MSCOCO image corpus, while in our paper, we do not have such a multilingual corpus. 2.2 Neural Machine Translation Neural machine tra
2794386110	Unpaired Image Captioning by Language Pivoting	1905882502	ion. Likewise, each image in MSCOCO also has v e reference description, and most of these descriptions are depicting humans participating in various activities. We use the same test splits as that in [4]. For MSCOCO, we use5,000imagesforvalidationand5,000imagesfortesting,andforFlickr30K,weuse 1,000 images for testing. 4.2 Implementation Details Architecture. Fig. 4 shows the architectures of the thre
2794386110	Unpaired Image Captioning by Language Pivoting	2302086703,2552161745	mechanisms to incorporate the spatial attention on convolutional features of an image into decoder. Another improvement is to leverage the high-level visual attributes to enhance the sentence decoder [23,27,28]. Recently, Gu et al [9] propose a CNN-based image captioning model, which can explore both long-term and temporal information in word sequences for caption generation. Exposure bias and loss-evaluati
2794386110	Unpaired Image Captioning by Language Pivoting	2101105183	t the next time step. We set a xed beam search size of k = 5 fori2t im ! zh and k = 10 fornmt zh ! en .Weevaluatethequalityofthegenerated image descriptions with the standard evaluation metrics: BLEU [53], METEOR [54], and CIDEr [55]. Since BLEU aims to assess how similar two sentences are, we also evaluate the diversity of the generated sentence with Self-BLEU [56], which takes one sentence as the hy
2794386110	Unpaired Image Captioning by Language Pivoting	2422843715	in the target language. However, such pivot-based approach has a major problem that the errors made in the source-to-pivot model will be forwarded to the pivot-to-target model. Recently, Cheng et al [48] introduce an autoencoder to reconstruct monolingual corpora. They further improve it in [49], in which they propose a joint training approach for pivot-based NMT. 3 Unpaired Image Captioning Let D i;
2794386110	Unpaired Image Captioning by Language Pivoting	2176263492,2560313346,2599772929	und-truth words and the machine generated words according to the scheduled probability in training. Recently, the loss-evaluation mismatch problem has been well-addressed in sequence prediction tasks [29,7,31,32, 8]. Rennie et al [7] address both exposure bias and loss-evaluation problems with a self-critical learning, which utilizes the inference mode as the baseline in training. Gu et al [8] propose a coarse-t
2794386110	Unpaired Image Captioning by Language Pivoting	1956340063	a xed beam search size of k = 5 fori2t im ! zh and k = 10 fornmt zh ! en .Weevaluatethequalityofthegenerated image descriptions with the standard evaluation metrics: BLEU [53], METEOR [54], and CIDEr [55]. Since BLEU aims to assess how similar two sentences are, we also evaluate the diversity of the generated sentence with Self-BLEU [56], which takes one sentence as the hypothesis and the others as th
2794421184	CAESAR: Context Awareness Enabled Summary-Attentive Reader.	2123442489	ent details which gave the optimal result. We apply pretrained GloVe word embeddings trained on common crawl with 840B tokens and 300 dimensions [15]. The Stanford CoreNLP toolkit served as tokenizer [16], while the document are truncated to a length of 500 while questions are similar truncated to 35 words as questions/passages longer than this length are few. We also adopt the following hyperparamete
2794421184	CAESAR: Context Awareness Enabled Summary-Attentive Reader.	2250539671	processing settings, we settle on the following experiment details which gave the optimal result. We apply pretrained GloVe word embeddings trained on common crawl with 840B tokens and 300 dimensions [15]. The Stanford CoreNLP toolkit served as tokenizer [16], while the document are truncated to a length of 500 while questions are similar truncated to 35 words as questions/passages longer than this le
2794421184	CAESAR: Context Awareness Enabled Summary-Attentive Reader.	1902237438	question and context tokens are fed to the same LSTM unit to obtain their encoding. And, there is an additional linear layer on top op the question encoding which results in general scoring function [14] instead of simple dot product between question and document encoding for attention weights calculation. After this is completed, we obtain two matrices D2Rm l and Q2Rn l. Each of the same dimension a
2794421184	CAESAR: Context Awareness Enabled Summary-Attentive Reader.	2125436846	sing, one that holds promise to revolutionize the way people interact with and retrieve information from machines [1]. The goal was outlined by Richardson et al. with the MCTest dataset they proposed [2], in which questions are provided for which the answer can only be found in the associated passage text. To perform well on such task, machine comprehension models are expected to possess some sort of
2794426516	CRoss-lingual and Multilingual Speech Emotion Recognition on English and French	2045528981	(such as ’anger’, ’happiness’, ’sadness’) and with 5-point scales on the dimensions valence, arousal and dominance (1 - low/negative, 5 - high/positive). The corpus contains 10,039 utterances. Recola [12] is a multimodal database of French speech consisting of dyadic conversations during a video conference where participants had to solve a collaborative task. From 46 speakers in total, we use the free
2794426516	CRoss-lingual and Multilingual Speech Emotion Recognition on English and French	1522301498	l experiments ﬁve times and report the means. Hyper-parameters The ACNN model is implemented with the Tensorﬂow library [14]. We apply stochastic gradient descent with an adaptive learning rate (Adam [15]) for training. The systems hyper-parameters are the following: 200 kernels with a size of 26x10 in the convolutional layer (spanning all 26 logMel ﬁlter-banks); a mini-batch size of 32; and a pool si
2794426516	CRoss-lingual and Multilingual Speech Emotion Recognition on English and French	2045528981	n of 23 speakers in this study, consisting of 1,308 utterances. Recola is annotated with continuous labels for arousal and valence in the range [-1, 1] on a 40ms rate. Annotation was done with ANNEMO [12], a tool similar to Feeltrace [13]. Since we are interested in recognition of emotions on utterance level, we calculated the mean of all values for one turn, and then took the average across all annot
2794487997	Near-lossless Binarization of Word Embeddings	2153579005	ects its semantic and syntactic information extracted from the language (Bengio et al. 2003). They are usually created from a large corpus by moving closer the vectors of words co-occurring together (Mikolov et al. 2013) or factorizing the matrix containing co-occurrences statistics (Pennington, Socher, and Manning 2014), and commonly require several gigabytes of memory space. For example, with a vocabulary of 2 mill
2794487997	Near-lossless Binarization of Word Embeddings	2153579005	er, and Manning 2013), SimLex (Hill, Reichart, and Korhonen 2015), SimVerb (Gerz et al. 2016) and WordSim (Finkelstein et al. 2001). Word analogy This evaluation follows the standard protocol used by Mikolov et al. (2013). The task consists in ﬁnding the word din questions like “ais to bas cis to d”. The evaluation ﬁrst computes the vector v b v a + v c and then look at its closest neighbours. If the closest one is th
2794487997	Near-lossless Binarization of Word Embeddings	2251939518	ng real-valued vectors. 1 Introduction Word embeddings models play a central role in many NLP applications like document classiﬁcation (Joulin et al. 2017; Conneau et al. 2017) or sentiment analysis (Socher et al. 2013; Qian et al. 2017). The real-valued vector representation associated to each word of a vocabulary Vreﬂects its semantic and syntactic information extracted from the language (Bengio et al. 2003). The
2794487997	Near-lossless Binarization of Word Embeddings	2493916176	oduces binary vectors from several pre-trained embeddings: dict2vec (Tissier, Gravier, and Habrard 2017) which contains 2.3M words and has been trained on the full English Wikipedia corpus; fasttext (Bojanowski et al. 2017) which contains 1M words and has also been trained on the English Wikipedia corpus; and GloVe (Pennington, Socher, and Manning 2014) which contains 400k words and has been trained on both English Wiki
2794487997	Near-lossless Binarization of Word Embeddings	2218741211	random projections to produce binary codes that approximates the cosine similarity of the corresponding orginal vectors. However, these methods generally fail to fully preserve semantic similarities (Xu et al. 2015). Faruqui et al. (2015) propose to binarize real-valued vectors by ﬁrst increasing the vector size to create sparse vectors, and then applying the naive binarization function. Although this method pre
2794487997	Near-lossless Binarization of Word Embeddings	2140610559	s to produce binary codes that approximates the cosine similarity of the corresponding orginal vectors. However, these methods generally fail to fully preserve semantic similarities (Xu et al. 2015). Faruqui et al. (2015) propose to binarize real-valued vectors by ﬁrst increasing the vector size to create sparse vectors, and then applying the naive binarization function. Although this method preserves the semantic sim
2794487997	Near-lossless Binarization of Word Embeddings	2067438047	or size. The similarity datasets used are MEN (Bruni, Tran, and Baroni 2014), RW (Luong, Socher, and Manning 2013), SimLex (Hill, Reichart, and Korhonen 2015), SimVerb (Gerz et al. 2016) and WordSim (Finkelstein et al. 2001). Word analogy This evaluation follows the standard protocol used by Mikolov et al. (2013). The task consists in ﬁnding the word din questions like “ais to bas cis to d”. The evaluation ﬁrst computes
2794487997	Near-lossless Binarization of Word Embeddings	2295030615	in other tasks as the vocabulary size is drastically reduced to around 1000 words. Although binary vectors can speed up vector computations, some NLP applications only work with real-valued vectors (Ma and Hovy 2016); being able to reconstruct realvalued vectors from the binary ones is therefore required for such applications. As a consequence, one can store the binary embeddings and compute on the ﬂy real-valued
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2142920810	e a method that captures this sequence of words and also learn a distance metric between them. Long Short-Term Memory (LSTM) based recurrent neural networks have proven helpful in capturing sequences [19]. Also, siamese networks have shown promising results in distance-based learning methods [5]. Hence, we use a combination of these by utilizing a bidirectional LSTM to map articles to a semantic space
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2087735403	es data is taken from Europe Media Monitor (EMM) [3], data of Plagiarism is taken from corpus of plagiarized short answers [8] and data of Postediting is taken from WMT quality estimation shared task [6]. 4.2 Baselines We test CREDO system against the previous approaches in the problem [22] and [21] and evaluate it against the same metrics. {LG + SR: This approach uses language stylistic features and
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2136189984	etc, which are considered good evidences by the system but are semantically dierent. In this model, siamese architecture with LSTMs calculates the semantic similarity, similar to how the DSSM model [12] computes semantic similarity with primarily two dierences: { A fully connected neural network is the DSSM model’s basis, whereas, we employ an LSTM here instead. LSTMs capture sequential information
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2136189984	hen, there have been attempts to understand their relevance in the context of sentence similarity. In SCQA model [9], siamese networks solve the task of community question answering and in DSSM model [12], they handle the task of website ranking. The above methods use the siamese network based on the task. Here, the task is the semantic similarity. Hence, we use LSTM models [19] to project our article
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2148506018	ing a bidirectional LSTM to map articles to a semantic space in conjunction with a siamese network to learn the similarity metric between them. A factual article has more probability of being neutral [18]. Hence, a system to capture this feature is essential. To tackle the problem, we use a sentiment analysis [20] tool to evaluate the neutrality of a given article. The rest of the paper is organized a
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2149706766	overall problem is of classication and hence the choice of classier has to be appropriate for the data points. Quadratic Discriminant Analysis (QDA) [26], Gaussian Naive Bayes [13], Decision Trees [23], Random Forests (ensemble of Decision Trees) [4], AdaBoost Classier [16], Support Vector Methods with Radial Basis Function kernel (SVM-RBF) [1] and Multi Layer Perceptron Neural Network (MLP-NN) [2
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	1967925097	r and Question-Question is taken from Stack Exchange Q&amp;A Forums4, Headlines data is taken from Europe Media Monitor (EMM) [3], data of Plagiarism is taken from corpus of plagiarized short answers [8] and data of Postediting is taken from WMT quality estimation shared task [6]. 4.2 Baselines We test CREDO system against the previous approaches in the problem [22] and [21] and evaluate it against t
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2084591134	redibility analysis. [29] discusses the belief system of computers and credibility analysis’ necessity. [28] discusses fact checking, its denition and motivations, posing it as a classication task. [7] proposes credibility analysis in a social media context, thereby using features that describe users’ posting behaviour. Joint model based on CRF [22] employs linguistic features like assertive verbs,
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2142920810	ring and in DSSM model [12], they handle the task of website ranking. The above methods use the siamese network based on the task. Here, the task is the semantic similarity. Hence, we use LSTM models [19] to project our articles in the semantic space to learn a similarity metric between them. Article Keywords In May 1946, Einstein made a rare public appearance outside of Princeton, New Jersey, when he
2794602508	Neural Network Architecture for Credibility Assessment of Textual Claims.	2097726431	similarity metric between them. A factual article has more probability of being neutral [18]. Hence, a system to capture this feature is essential. To tackle the problem, we use a sentiment analysis [20] tool to evaluate the neutrality of a given article. The rest of the paper is organized as follows. We discuss previous approaches in the eld, motivating us for the task in section 2. In section 3, we
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2506483933	ation. In fact, the value of scene graph representation has already been proven in a wide range of visual tasks, including semantic image retrieval (Johnson et al., 2015), caption quality evaluation (Anderson et al., 2016), etc. In this paper, we focus on scene graph generation from textual descriptions. Previous attempts at this problem (Schuster et al.,2015;Anderson et al.,2016) follow the same spirit. They ﬁrst use
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2250378130	er retrieval performance across all three evaluation metrics: recall@5, recall@10, and median rank. We also notice that the numbers in our retrieval setting are higher than those (even with oracle) inSchuster et al. (2015)’s retrieval setting. This strongly suggests that generating accurate scene graphs from images is a very promising research direction in image retrieval, and grounding parsed scene graphs to bounding
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2506483933	ExcludingJohnson et al. (2015) which used ground truth, scene graphs are obtained either from images (Dai et al.,2017;Xu et al.,2017;Li et al.,2017) or from textual descriptions (Schuster et al.,2015;Anderson et al., 2016). In this paper we focus on the latter. In particular, parsed scene graphs are used in Schuster et al.(2015) for image retrieval. We show that with our more accurate scene graph parser, performance on
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2416885651	h generation as future work. More broadly, our task also relates to entity and relation extraction, e.g.Katiyar and Cardie (2017), but there object attributes are not handled. Neural module networks (Andreas et al., 2016) also use dependency parses, but they translate questions into a series of actions, whereas we parse descriptions into their graph form. Finally,Krishnamurthy and Kollar(2013) connected parsing and gr
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2296308987	on Kiperwasser and Goldberg(2016) for both its simplicity and good performance. Apart from dependency parsing, Abstract Meaning Representation (AMR) parsing (Flanigan et al.,2014;Werling et al.,2015;Wang et al., 2015;Konstas et al.,2017) may also beneﬁt scene graph generation. However, as ﬁrst pointed out inAnderson et al.(2016), the use of dependency trees still appears to be a common theme in the literature, an
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2030904529	nt the action set with a REDUCE action, that pops the stack without adding to the arc set (see Table1). This action is often used in other transition-based dependency parsing systems (e.g. arc-eager (Nivre, 2004)). More recently,Hershcovich et al.(2017) andBuys and Blunsom(2017) also included this action when parsing sentences to graph structures. Parser F-score Stanford (Schuster et al.,2015) 0.3549 SPICE (A
2794733740	SCENE GRAPH PARSING AS DEPENDENCY PARSING	2250378130	on tasks, such as image retrieval. We directly compare our parser with the Stanford Scene Graph Parser (Schuster et al.,2015) on the development set and test set of the image retrieval dataset used inSchuster et al. (2015) (not Visual Genome). For every region in an image, there is a humanannotated region description and region scene graph. The queries are the region descriptions. If the region graph corresponding to t
2794752572	CLICR: A DATASET OF CLINICAL CASE REPORTS FOR MACHINE READING COMPREHENSION	2028175314	closed-domain reading comprehension datasets. Size: number of questions. We did not include remotely related datasets which concern a different task (e.g. information retrieval) (Roberts et al. ,2015;Voorhees and Tice 2000). 20% F1, which leaves ample space for further study of machine readers on our dataset. In brief, the contributions of our paper are: We propose a large dataset for reading comprehension in the medic
2794752572	CLICR: A DATASET OF CLINICAL CASE REPORTS FOR MACHINE READING COMPREHENSION	2168041406	entity outside of it, and we adjust the entity boundary so that it does not include a parenthetical at the end of the entity. Clamp assigns entities following the i2b2-2010 shared task speciﬁcations (Uzuner et al., 2011). For each entity, a concept unique identiﬁer (CUI) is also available, which links it to the UMLS RMetathesaurus (Lindberg et al.,1993). To check the quality of the recognized entities, we carried out
2794752572	CLICR: A DATASET OF CLINICAL CASE REPORTS FOR MACHINE READING COMPREHENSION	2126209950	et al.,2016; Joshi et al.,2017;Rajpurkar et al.,2016), web search queries (Nguyen et al.,2016), news articles (Hermann et al.,2015;Onishi et al.,2016;Trischler et al.,2017), books (Bajgar et al.,2016;Hill et al., 2016;Paperno et al.,2016) and English exams (Lai et al.,2017). In Table1, we compare our dataset to several domain-speciﬁc datasets for machine comprehension. In Quasar-S, the queries are constructed from
2794752572	CLICR: A DATASET OF CLINICAL CASE REPORTS FOR MACHINE READING COMPREHENSION	2126209950	they could not be used as training sets for statistical NLP models. Cloze datasets require the reader to ﬁll in gaps by relying on accompanying text. Representative datasets are Children’s Book Test (Hill et al., 2016) and Book Test (Bajgar et al.,2016), in which queries are created by removing a word or a named entity from the running text in a book; andHermann et al.(2015), who similarly to us blank out entities
2794945088	DEEP COMMUNICATING AGENTS FOR ABSTRACTIVE SUMMARIZATION	2603266952	ins a challenge. Simultaneous work has investigated the use of deep communicating agents (Sukhbaatar et al., 2016) for collaborative tasks such as logic puzzles (Foerster et al.,2016), visual dialog (Das et al., 2017), and reference games (Lazaridou et al., 2016). Our work builds on these approaches to propose the ﬁrst study on using communicating agents to encode long text for summarization. The key idea of our m
2794945088	DEEP COMMUNICATING AGENTS FOR ABSTRACTIVE SUMMARIZATION	1869752048	s. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the context that the decoder should focus on within the input encoding (Luong et al.,2015;Vinyals et al., 2015b;Bahdanau et al.,2015). For example,Luong et al.(2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document clas
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2250539671	. Alternatively, the skip gram model uses a given word to predict what are the most likely words that will be used in a similar context. Building off of this framework, Pennington, Socher and Manning [87] uniﬁed vector space models by combining features of the count-based models (like LSA and LDA) and context-based models (like word2vec) to a more robust model named GloVe, or global vectors of word re
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2072644219	, not documents, are a multinomial distribution over the topics [19]. Soon after, many metadata topic model extensions were created for a variety of metadata attributes like time (dynamic topic model [20]), geography (geographical topic model [21]), and emotion (emotion topic model [22]). Given the large collection of metadata topic model extensions, Mimno and McCallum (2009) categorized metadata exte
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2130978632	, the Hidden Markov Model (HMM) extended the normal BoW assumption to facilitate a consideration of word context into the topic model framework [34]. In the case of network data, the link-topic model [35] and the relational topic model (RTM) [36] are two additional topic models that incorporate relational based information into the model for further analysis. Finally, Teh et al. (2006) introduce a gen
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2481795101	) make point but not standard error estimates to facilitate statistical hypothesis testing. The next problem is that computational methods for topic model inference, as it is an NP-hard problem [68], [69], can provide local optima but cannot guarantee global optima, which is termed multi-modality. This problem threatens the stability of a topic model output and can lead researchers to question whether
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2134731454	) relied on a Gaussian noise assumption that could not be justiﬁed for word counts (document-term matrix). Second, LSA could not account for polysemy, the multiple uses of words in different contexts [13]. To address these problems, Hoffman (2001) introduced probabilistic latent semantic index (pLSI) model through the addition of a probabilistic (mixture) component to the LSA model by assuming each wo
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	1947594277	A) model [24]. Whereas in the up-stream approach, the algorithm is conditioned on the metadata covariates such that the document-topic distributions are mixtures of the covariate-speciﬁc distribution [32]. The classic example of this approach is the authortopic model or dynamic topic model. Essentially, the up-stream models “learn an assignment of the words in each document to one of a set of entities
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2112050062	can affect the topic proportions. The model 8As noted earlier, simulation approaches are ideal for topic models because they are (1) theoretically backed; (2) unbiased; (3) computationally convenient [23]. Fig. 9. Covariate inference for LDA and STM on simulated datasets by Roberts et al. (2016) replaces LDA’s assumption of a Dirichlet prior for the topic distribution with a Dirichlet-Multinomial regr
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2063904635	ages in which all messages are less than 140 characters. As an alternative, they cite alternatives like aggregating messages to transform the documents to a user-level to expand the size of documents [44]. Third, they ﬁnd that collections with too many topics lend statistical inference methods to be inefﬁcient. Fourth, they ﬁnd that LDA performance is affected by how well-separated the underlying topi
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	184934546	alyzing PPC is to identify systematic errors (e.g. caused by a poor local optimal solution or initialization) like this across multiple topics that can threaten the legitimacy of the model as a whole [72]. Last, Chuang et al. (2015) provide an interactive solution to the problem of multi-modality: TopicCheck, a visualization Fig. 12. TopicCheck with 50 Iterations of a 20 Topic STM to assess topic stab
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2090491854	Such approaches focus on the task of document summarization, information retrieval and relationships between documents. Common examples of these approaches include matrix representations like Termite [47] and Serendip [48] (see Figure 4) as well as parallel coordinates visualizations as in ParallelTopics [49]. Chuang et al. (2012) provide a general design framework for topic-oriented interactive visua
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	1498269992	ationships and hierarchical topic structure. For example, the Hidden Markov Model (HMM) extended the normal BoW assumption to facilitate a consideration of word context into the topic model framework [34]. In the case of network data, the link-topic model [35] and the relational topic model (RTM) [36] are two additional topic models that incorporate relational based information into the model for furt
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2081459780	cus on the task of document summarization, information retrieval and relationships between documents. Common examples of these approaches include matrix representations like Termite [47] and Serendip [48] (see Figure 4) as well as parallel coordinates visualizations as in ParallelTopics [49]. Chuang et al. (2012) provide a general design framework for topic-oriented interactive visual systems based on
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2481795101	DA’s results. The spectral approach utilizes the connection of LDA with non-negative matrix factorization (see [71]) that provides theoretical guarantees that the optimal parameters will be recovered [69]. Essentially, this approach makes stronger model assumptions (matrix decomposition elements must be non-negative) in order to avoid the problems of multi-modality. However, Roberts et al. (2015) iden
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2053075547	e role of explainable user interfaces for machine learning tasks like STM inference. Therefore, a key research opportunity for STM is the development of an explainable, intelligent interactive system [81] to analyze for interpretation, model evaluation, multi-modality, pre-processing steps17 and validation. Such an interface could be built integrating high level visualization tools like Shiny [82] and
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2016196732	e as such a ﬁeld because of its “take it or leave it” problem because many social scientists “have extensive domain knowledge but lack the machine learning expertise to modify topic model algorithms” [45]. A shortcoming of their argument is they omit the role that visualizations can play within such interactive systems. Further, they fail to recognize the body of research by the visualization communit
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2024783975	en documents. Common examples of these approaches include matrix representations like Termite [47] and Serendip [48] (see Figure 4) as well as parallel coordinates visualizations as in ParallelTopics [49]. Chuang et al. (2012) provide a general design framework for topic-oriented interactive visual systems based on how an analyst makes inference on the topics (interpretation) and the actual and percei
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2063904635	estimating causal effects with LDA, their approach has two limiting factors. First, to analyze author and time, they did not directly model. Instead, employing the “aggregation” approach suggested by [44], they combined Tweets by author and day to modify the deﬁnition of a document from a tweet to the collection of all tweets in a day by each author. By doing aggregation, this enables them to control
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2159544539	et al. (2015) analyzed bilingual social media messages through automated machine translation to estimate the effect language (Arabic or Chinese) has on Twitter users’ topics regarding Edward Snowden [75]. Sachdeva et al. (2016) used STM to analyze smoke-related tweets and the potential spatialtemporal effects of wildﬁres have on users’ tweets relative to those individuals who reside or work close to
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2019676294	framework for topic-oriented interactive visual systems based on how an analyst makes inference on the topics (interpretation) and the actual and perceived accuracy of the analyst’s inference (trust) [50]. Other interfaces like HierarchicalTopics have generalized the model and facilitated interfaces that focus on a hierarchical structure within the topics that can aid in drill down on multiple levels
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2096974619	and not within the generative topic model itself. As noted in the appendix of Roberts et al. (2014), the problem with this approach is the measurement of uncertainty that can lead to spurious results [8].7 A general theme of these applications represent the ﬂexibility of how LDA-based framework can be modiﬁed to address a unique theoretical question for a speciﬁc document-level covariate (e.g. time,
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2105617746	h that provides a more robust initialization with the computational complexity of using LDA’s results. The spectral approach utilizes the connection of LDA with non-negative matrix factorization (see [71]) that provides theoretical guarantees that the optimal parameters will be recovered [69]. Essentially, this approach makes stronger model assumptions (matrix decomposition elements must be non-negati
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2097940802	hat can aid in exploring the trend, evolution, lead-lag effect and event-detection relative to the topics. TIARA is an interface created to visualize topical trends by using an enhanced stacked graph [54], [55]. Similarly, TextFlow was introduced with the goal of exploring the evolution of topics including identifying how topics merge and split over time [56]. Further, TextPioneer is a visual interfac
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2296760094	hat have aligned topics in more STM iterations. Chuang et al. (2015) ﬁndings suggest the need to consider more than one topic model as one “single topic may not capture all perspectives on a dataset” [73]. Further, another contribution of this paper is analyzing the impact of including or excluding rare-words on the stability of the topics. This is a novel approach to provide users an interactive unde
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2165599843	hreshold is met. Hoffman et al. (2010) extended the variational inference to introduce a faster online batch algorithm that can be used to massively scale LDA for very large corpora or streaming data [18]. Ultimately, given that sampling-based or variational inference methods are estimates and never ever exact solutions, neither method is perfect and the decision of each depends on the trades off of s
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2096974619	ial science research. This overview leads itself to the newly created structural topic model (STM) that extends the general topic model framework to estimate causal effects within text documents [7], [8]. In section 1, I review the evolution of topic models by introducing latent Dirichlet allocation (LDA) [9] and related seminal models. I also consider computational methods for topic models and discu
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2296760094	k, a visualization Fig. 12. TopicCheck with 50 Iterations of a 20 Topic STM to assess topic stability from Chuang et al. (2015) interface to assess the stability of topics across multiple runs of STM [73]. Figure 12 shows TopicCheck for 50 iterations of a 20 topic STM for a dataset of 13,250 political blogs. Each rectangle is a topic, with each column being one run of STM. Topics are aligned across ea
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	1947594277	r LDA and STM on simulated datasets by Roberts et al. (2016) replaces LDA’s assumption of a Dirichlet prior for the topic distribution with a Dirichlet-Multinomial regression for the given covariates [32]. On the other hand, researchers found that the same approach (Dirichlet-multinomial regression) was not feasible for the word distributions. Eisenstein et al. (2009) identify three main problems when
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2112050062	ll predict items based on the latent topics that the observations suggest, but the CTM will predict items associated with additional topics that are correlated with the conditionally probable topics” [23]. However, like many other topic model extensions, relaxation of model assumptions comes at the expense of model complexity and even intractability for existing methods. In this case, simulation techn
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2481795101	ly, Roberts et al. (2015) note that such sensitivity to starting positions is well known by computer scientists yet infrequently discussed. relative model improvement for optimizing the initial state [69]. In the STM model, a researcher has the option to a spectral initialization that provides a quick starting point that minimizes the chance of ﬁnding sub-optimal local minima [69]. One approach for ST
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2134731454	mework. A. LDA and Seminal Papers As a generalization, there are two approaches to computerassisted text analysis: natural language processing (NLP) and statistical-based algorithms like topic models [13]. Unlike NLP methods that tags parts-of-speech and grammatical structure, statistical-based models like topic models are largely based on the “bag-of-words” (BoW) assumption. In BoW models, collection
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	184934546	the modiﬁcation of the model’s assumptions have the potential of leading to less interpretable models. Posterior predictive checks (PPC) provide insight on how well the model’s assumptions hold [10], [72]. Figure 11 provides one such PPC (instantaneous mutual information from [72]) for three topics, each plot representing the top ten most likely words for each of the three topics. In these examples, t
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2165279024	n on the number of parameters, this approach (1) reduces overﬁtting issues and (2) can combine generative facets through simple addition in log space, avoiding the need for latent switching variables [33]. Given the inclusion of these predecessor models, its important to review the terminology to distinguish between the two types of covariates used in the model: prevalence and content. Prevalence cova
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2124672527	he normal BoW assumption to facilitate a consideration of word context into the topic model framework [34]. In the case of network data, the link-topic model [35] and the relational topic model (RTM) [36] are two additional topic models that incorporate relational based information into the model for further analysis. Finally, Teh et al. (2006) introduce a generalized hierarchical structure to the top
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	168564468	ntists) by providing the model in a high-level (R) rather than the traditional lowlevel languages most topic models methods have previously been available (e.g. Java in Mallet [65], Python for Gensim [66]). 10STM is able to model non-linear patterns through its extension to allow spline transformations and covariate interactions. Fig. 11. Posterior predictive checks (PPC) using instantaneous mutual in
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2212107515	o link across these sources (see Figure 5. Further, another avenue of topic analysis includes the impact of analyzing topics within streaming data sources like Twitter or other social media platforms [59]. Fig. 5. The integration of multiple corpora within TopicPanorama from Liu et al. (2016) D. LDA-based Topic Model Applications in Social Sciences One of the ﬁrst applications of topic models within s
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	171636962	o social science research. This overview leads itself to the newly created structural topic model (STM) that extends the general topic model framework to estimate causal effects within text documents [7], [8]. In section 1, I review the evolution of topic models by introducing latent Dirichlet allocation (LDA) [9] and related seminal models. I also consider computational methods for topic models and
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2481795101	onclusion of this paper. B. Multi-Modality The second problem with topic models is its lack of stability due to its inherent computational complexity. Topic model inference is a NP-hard problem [68], [69]. This hardness leads to the problem of multi-modality, i.e., an optimization problem (like maximum likelihood) can be solved locally but cannot, with certainty, be solved globally.11 Multi-modality i
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2251803266	ons are limited. Fig. 15. Functions within stm package from Roberts et al. (2015b) on the Distributional Hypothesis of language, i.e., words are used in similar context to words with similar meanings [85]. Under this interpretation, bag-of-words models (like LSA and LDA) are considered as count-based models as they encode language as a series of vector counts. As mentioned earlier, one downside of thi
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	1990995255	rarchicalTopics have generalized the model and facilitated interfaces that focus on a hierarchical structure within the topics that can aid in drill down on multiple levels for document summarization [51], [52]. Moreover, new research has used (network) graphs to represent the correlations between topics, especially when coupled with models with a more ﬂexible correlation structure like CTM [53]. Fig.
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2140124448	Rosen-Zvi et al. (2004) introduced the author-topic model to incorporate the author attribute by modifying LDA’s assumption that author, not documents, are a multinomial distribution over the topics [19]. Soon after, many metadata topic model extensions were created for a variety of metadata attributes like time (dynamic topic model [20]), geography (geographical topic model [21]), and emotion (emoti
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2096974619	t al. (2014) acknowledge that many survey analyses simply ignore open-ended survey responses in favor of closed-ended surveys given the lack of tools to analyze such results. rather than assume them” [8]. Second, it provides a formal way of quantifying the content and prevalance effects on the topics, especially with a treatment variable, through a cheaper, more consistent semi-automated process. For
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2513458147	t sum to one), interpretation of covariates’ marginal effects in STM are difﬁcult as an increase in one topic proportion must be tied to a decrease in similar magnitude to the other topic proportions [67]. Last, the model uses are approximation methods that are more complex than simulation-based (e.g. MCMC or Gibbs Sampling) methods. Last, as the model is more complex than LDA, so too is its output as
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2147152072	on technique, to reduce the document-term matrix to latent factors. Their goal was to identify broad semantic (correlation) structure within the documents by removing noise from uninformative factors [11]. Later, Landauser and Dumais (1997) extended the LSI model to create the latent semantic analysis (LSA) model [12]. Further, these methods could be improved by substituting the term-frequency inverse
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2165279024	ters, the computational complexity and the lack of sparsity. This is especially a problem when considering the word-topic relationship for a large corpus of documents can have an extensive vocabulary [33]. To address this problem, they introduce the Sparse Additive Generative Model (SAGE). The SAGE consists of an alternative framework that uses deviations in log-frequency from a benchmark distribution
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2096974619	tify words that have high probabilities for only a few topics rather than many topics. Roberts et al. argue that a “topic that is both cohesive and exclusive is more likely to be semantically useful” [8]. Finally, other researchers have studied the effectiveness of topic modeling while controlling for other key factors (e.g. 3Two limitations to Wallach et al. (2009a) is that these results were only t
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2142889507	tion over the topics [19]. Soon after, many metadata topic model extensions were created for a variety of metadata attributes like time (dynamic topic model [20]), geography (geographical topic model [21]), and emotion (emotion topic model [22]). Given the large collection of metadata topic model extensions, Mimno and McCallum (2009) categorized metadata extension models into two groups: down-stream a
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2098062695	a variables (m) and the words in the text (w) are conditionally generated by the hidden topics (z). The most common example of this approach is the supervised latent Dirichlet allocation (sLDA) model [24]. Whereas in the up-stream approach, the algorithm is conditioned on the metadata covariates such that the document-topic distributions are mixtures of the covariate-speciﬁc distribution [32]. The cla
2794957125	Computer-Assisted Text Analysis for Social Science: Topic Models and Beyond.	2096974619	vised approach because they have enough volume and were not used as stop words. Nevertheless, they ﬁnd that the beneﬁts of STM largely outstrip any such costs relative to human coding in this example [8]. Another signiﬁcant application of STM has been by political scientists to analyze political rhetoric. One of the ﬁrst examples come from Milner and Tingley (2015) in which they analyzed the text wit
2794978288	Meta-Learning a Dynamical Language Model	2589889860	capture long-range connections more explicitly. Unfortunately, the very local context is often so highly informative that LMs typically end up using their memories mostly to store short term context (Daniluk et al., 2016). In this work, we study the possibility of combining short-term representations stored in hidden states with medium term representations encoded in a set of dynamical weights of the language model. O
2794978288	Meta-Learning a Dynamical Language Model	2036317923	Second, multiple time-scales dependencies in sequential data can naturally be encoded by using a hierarchical representation where higher-level features are changing slower than lower-level features (Schmidhuber, 1992; Chung et al., 2016). As a consequence, we would like our model to store information in a multi-scale hierarchical way where 1. short time-scale representations can be encoded in neural activations (
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2605131327	[15] used an end-to-end system for translating audio books between French and English. Sequence-tosequence models to both transcribe Spanish speech and translate it in English have also been proposed [16], by jointly training the two tasks in a multitask scenario with two decoders sharing the speech encoder. This model was extended by us [17], with the translation decoder receiving information both fr
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2133564696	show that it actually is beneﬁcial in the cases where the single-source speech transcription model has greatest difﬁculty. 2. Model Our models are based on a sequence-to-sequence model with attention [6]. In general, this type of model is composed of three parts: a recurrent encoder, the attention, and a recurrent decoder (see Figure 1a).1 Let X1 = x1 1 :::x 1 N be a sequence of speech frames, X2 = x
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	1902237438	anisms provide two context to the decoder. Note that for clarity’s sake there are dependencies not shown. The attention mechanisms produce the attention weights with the following computations, as in [8], with v1, v2, Ws 1 , W s 2 , W h 1 , and W k2 being parameters to be learnt: 1 kn= softmax(v 1 tanh( h Ws 1s k 1;W h 1h 1 n i )) (6) 2 km= softmax(v 2 tanh( h Ws 2s k 1;W h 2h 2 m i )): (7) Since bot
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2133564696	We focus on this language documentation scenario and explore methods that learn from a small number of transcribed speech utterances along with their translations. We use the neural attentional model [6] and experiment with extensions that take both speech utterances and their translations as input sources. We assume that the translations are in a high-resource language that can be automatically tran
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2113106066	lation problem has been traditionally approached by feeding the output of a speech recognition system uncertainty was integrated with MT by using speech output lattices as input to translation models [10, 11]. A sequence-tosequence model for speech translation without transcriptions has been introduced [12], but was only evaluated on alignment. Synthesized speech data were translated in [13] using a model
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2327501763	lation without transcriptions has been introduced [12], but was only evaluated on alignment. Synthesized speech data were translated in [13] using a model similar to the Listen Attend and Spell model [14], while a larger-scale study [15] used an end-to-end system for translating audio books between French and English. Sequence-tosequence models to both transcribe Spanish speech and translate it in Eng
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2582956876	on models [10, 11]. A sequence-tosequence model for speech translation without transcriptions has been introduced [12], but was only evaluated on alignment. Synthesized speech data were translated in [13] using a model similar to the Listen Attend and Spell model [14], while a larger-scale study [15] used an end-to-end system for translating audio books between French and English. Sequence-tosequence
2794997795	Leveraging Translations for Speech Transcription in Low-resource Settings.	2101105183	across the training and the test set. In this setting, the acoustic modelling part is harder than encoding the translation sentence. For completeness, in Table 2 we also report word-level BLEU scores [26], the most common evaluation metric for Machine Translation. A higher BLEU almost always translates to lower WER. We only report results on Ainu and Spanish as Mboshi does not have standardized word s
2795159688	What we talk about when we talk about monads	115425660	bindover mapandjoin(theformerismoreconvenientforprogramming,butthelatteris simplerforproving).Similarly,thinkingofmonadsmoregenerallyasnon-standard computationslikelycontributedtotheirintroductioninF#[46].Asanimpure language,F#doesnotneedmonadsforimplementingstateorexceptions,butthey becomeusefulforasynchronouscomputations[52]. AsdiscussedinSection3,monadsarerootedinthemathematicalAlgolresearch progra
2795159688	What we talk about when we talk about monads	115425660	eofIO,thenon-standardaspectisthatthe computationcanperformeffects,whichisnormallynotpossibleinHaskell. Otherlanguagesintroducedotherkindsofnon-standardcomputations.Forexample, F#computationexpressions[46],amechanismsimilartothe‘do‘notationhasbeen introducedinordertosimplifywritingofasynchronouscomputations[52].ThenonstandardaspecthereisthatthecomputationcanperformasynchronousI/Owithout blockingthecurr
2795159688	What we talk about when we talk about monads	1578976048	ievethereismuchlefttobedonein understandinghowweusemetaphorswhenprogramming. Therearealsonumeroususesofmonadsthatwewerenotabletocoverinthis papersuchasthelinkbetweenmonadsandaspect-orientedprogramming[8,32]. Thisrelationshipmightprovideyetanothermetaphorforthinkingaboutmonadsthat isperhapsequallyinterestingasthetwometaphors,containersandcomputations, presentedinthispaper. 12:20 TomasPetricek 7.2Summary–
2795159688	What we talk about when we talk about monads	115425660	touseamonad(asopposedtoaconcretenotion)isoftenneglected.Theassumption thatabstractionisdesirableisanotherpartoftheAlgolresearchprogramme.Similarly, thereareonlyfewpapersdiscussingsyntaxforusingmonads[16,46]andthosefocus onformalpropertiesofthenotation,ratherthanonitscognitiveaspects[3]. 1Forexample,thingsstoredinacontainerarenotordered,sowecouldarguethatmonadic bindshouldbecommutative(i.e.a &gt;&gt;= x
2795355797	COLORLESS GREEN RECURRENT NETWORKS DREAM HIERARCHICALLY	2549835527	including the human judgments in Italian, can be found at https://github.com/ facebookresearch/colorlessgreenRNNs. at language modeling. Since our positive results contradict, to some extent, those ofLinzen et al. (2016), we also replicate their relevant experiment using our best RNN (an LSTM). We outperform their models, suggesting that a careful architecture/hyperparameter search is crucial to obtain RNNs that are
2795355797	COLORLESS GREEN RECURRENT NETWORKS DREAM HIERARCHICALLY	2549835527	that the LSTM is quite robust to the presence of attractors, in contrast to what was reported byLinzen et al.(2016). We directly compared our English LSTM LM to theirs by predicting verb number on theLinzen et al. (2016) test set. We extracted sentences where all of the words between subject and verb were in our LM vocabulary. Out of those sentences, we sampled 2000 sentences with 0, 1 and 2 attractors and kept all t
2795441105	Chart Parsing Multimodal Grammars.	2152096446	e chart parser). This paper presupposes the reader has at least a basic familiarity with multimodal categorial grammars (Moortgat 2010, Moortgat 2011, Moot &amp; Retor´e 2012) and with chart parsing (Shieber et al. 1995). 2 Chart rules In this section, I will dicuss the inference rules used by the chart parser. I will start with the simplest rules and gradually introduce more detail. 2.1 AB rules The elimination rule
2795441105	Chart Parsing Multimodal Grammars.	1495161316	ibes the chart parser for multimodal categorial grammars which has been developed in conjunction with the type-logical treebank for French, which is described in more detail in (Moot 2010, Moot 2012, Moot 2015b) and which is available at (Moot 2015a). The chart parser itself can be downloaded as a part of Grail light at (Moot 2018). https://github.com/RichardMoot/GrailLight The chart parser is an instance
2795441105	Chart Parsing Multimodal Grammars.	2152096446	r of the type explained in (Shieber 1The use of pairs of string positions to represent substrings of an input string is widely used in parsing algorithms; see for example (Pereira &amp; Shieber 1987, Shieber et al. 1995, Jurafsky &amp; Martin 2009). 2 et al. 1995), so we start with an agenda containing items 1-5 (the lexical lookup for the words in the sentence) and then successively add the items of the agenda to t
2795455201	MODELING SEMANTIC PLAUSIBILITY BY INJECTING WORLD KNOWLEDGE	2624729087	+ EMBEDDING. The model will be listed below as NN + WK-GOLD (i.e. with GOLD, Turker-annotated World Knowledge features). For question (ii), we select a data-efﬁcient feature learning model. FollowingForbes and Choi (2017) we evaluate the models with 5% or 20% of training data. We experiment with several previously proposed techniques: (a) label spreading; (b) factor graph; (c) multi-LDA. As a baseline we employ a simp
2795455201	MODELING SEMANTIC PLAUSIBILITY BY INJECTING WORLD KNOWLEDGE	2624729087	ge features. The feature types derive from inspecting the high agreement event triples for knowledge missing in distributional selection (e.g. relative sizes in man-swallowpaintball/desk). Previously,Forbes and Choi (2017) proposed a three level (3-LEVEL) featurization scheme, where an object-pair can take 3 values for, e.g. relative size: f 1;0;1g(i.e. lesser, similar, greater). This method, however, does not explain
2795455201	MODELING SEMANTIC PLAUSIBILITY BY INJECTING WORLD KNOWLEDGE	2210838531	istributional signals. Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky,2017). Existing methods for these tasks, however, draw predominantly (if not only) on distributional da
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	2473329891	documents, MC on spoken content is a much less investigated ﬁeld. Different kinds of DNN systems have been used in slot ﬁlling task including Recurrent Neural Network (RNN) [6, 7], bidirectional RNN [8] and Convolutional Neural Network(CNN) [9, 10]. However, all these previous models work on sequence labeling task, while SQA requires machine to perform more sophisticated reasoning. There were also w
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	1938755728	ences of words are used here. We adopt CNN to generate the embeddings from the phoneme sequence of a word, and this network is called Phoneme-CNN. Our proposed approach is the reminiscent of Char-CNN [31, 32], which apply CNN on characters to generate distributed representation of word for text classiﬁcation task. Phoneme-CNN is illustrated in Figure 2. We explain how we obtain feature for one word with o
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	68293321,1505680913	of k above). All the parameters of ﬁlters and phoneme embedding matrix H are end-to-end learned with reading comprehension model. It is also possible to incorporate other sub-word units like syllable [33, 34, 35] by the same CNN architecture described above. We will experiment with syllable sequences of words. 4. Experiments Setup and Results In this section, we ﬁrst show the performance of the published mode
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	2007261869,2059795879,2145077480,2561067925	n sequence labeling task, while SQA requires machine to perform more sophisticated reasoning. There were also works trying to estimate word conﬁdence scores or error probability on ASR transcriptions [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. ASR conﬁdence measures have been introduced as additional features for spoken language understanding (SLU) [21]. SQA usually worked with automatic speech recognition (ASR) transcripts of the spoken
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	2512720747	segments including the answer. The two types of outputs will be evaluated by two different approaches. tree-structured RNN is used to construct sentence representation from their syntactic structure [24]. However, the scale of TOEFL dataset used in the previous study is too limited to develop powerfully expressive models. In addition, the test is multi-select, in which machine does not provide an ans
2795497335	Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension.	2295570185	SLU) [21]. SQA usually worked with automatic speech recognition (ASR) transcripts of the spoken content, and typical approaches used information retrieval (IR) techniques [14] or used knowledge bases [22] to ﬁnd the proper answer. Recently, deep learning is used to answer TOEFL listening comprehension test which is a task highly related to SQA. TOEFL is an English examination that tests the knowledge
2795525971	EVALUATING HISTORICAL TEXT NORMALIZATION SYSTEMS: HOW WELL DO THEY GENERALIZE?	2143092851	nt training data sizes. To evaluate the downstream effects of normalization, we applied the models to a collection of unseen documents and then tagged them with the Stan6English:Markus (1999); German:Scheible et al. 2011); Hungarian:Simon(2014); Icelandic:Rögnvaldsson et al. (2012); Swedish:Fiebranz et al.(2011). For details of their dates and contents, seePettersson et al.(2014). ford POS tagger, which comes pre-trai
2795525971	EVALUATING HISTORICAL TEXT NORMALIZATION SYSTEMS: HOW WELL DO THEY GENERALIZE?	619445499	Rayson,2008;Bollmann,2012; Hauser and Schulz,2007;Bollmann et al.,2011; Pettersson et al.,2013a;Mitankin et al.,2014;Pettersson et al.,2014), statistical machine translation (Pettersson et al.,2013b;Scherrer and Erjavec, 2013), and most recently neural network models (Bollmann and Søgaard,2016;Bollmann et al., 2017;Korchagina,2017). However, most of these systems have been developed and tested on a single language (or even
2795643381	360\deg Stance Detection	1638051351	stance detection focused on Twitter (Rajadesingan and Liu,2014;Mohammad et al.,2016;Augenstein et al.,2016), particularly with regard to identifying rumors (Qazvinian et al.,2011;Lukasik et al.,2015;Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos,2016), which require recognizing entailment relations between claim and article. 1The demo
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2149995043	forts have focused on the word-level or mixed-models, rather than purely character-level models based on results that indicate “tonal diacritics can simply not be solved on the level of the grapheme” [9]. With some studies using corpora as small as 5k words from a 3.5k lexicon [6], the dearth of accurate diacritized electronic text has been the object of study as well as a limiting factor on progress
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2508579679	ing Czech, Polish, Romanian and Hungarian [14, 15, 16], Turkish [7], Arabic [17, 18, 19, 20], Maori [21], Uyghur [22], Urdu [23], Vietnamese [24, 25, 26],¯ as well as West African languages like Igbo [27] and Yorub` ´a [3, 6, 8, 9]. ADR investigations have relied on ruled-based morphological analyzers [14, 23], or used a variety of statistical learning techniques including conditional random ﬁelds (CR
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2493916176	matic text processing and search applications. Across the full corpus, to detect incorrect variants of both single and multiple diacritized forms, we can ﬁrst train a word vector model using fastText [36]. Then given a word that was incorrectly predicted in the test set, we can look at it’s nearest neighbours and amend the word forms in the training set that are not valid diacritizations. Lastly, rega
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2058316855	on-going efforts in a wide variety of languages, including Czech, Polish, Romanian and Hungarian [14][15][16], Turkish [7], Arabic [17][18][19][20], Maori [21], Uyghur [22], Urdu [23], Vietnamese [24][25][26], as well as West African languages like Igbo [27] and Yorub` ´a [9][6][3][8]. ADR investigations have relied on ruled-based morphological analyzers [14][23], or used a variety of statistical lear
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2149995043	Romanian and Hungarian [14][15][16], Turkish [7], Arabic [17][18][19][20], Maori [21], Uyghur [22], Urdu [23], Vietnamese [24][25][26], as well as West African languages like Igbo [27] and Yorub` ´a [9][6][3][8]. ADR investigations have relied on ruled-based morphological analyzers [14][23], or used a variety of statistical learning techniques including conditional random ﬁelds (CRFs) [20], support
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2012542894	ture directions. 2. Related work ADR is an active ﬁeld of study with on-going efforts in a wide variety of languages, including Czech, Polish, Romanian and Hungarian [14][15][16], Turkish [7], Arabic [17][18][19][20], Maori [21], Uyghur [22], Urdu [23], Vietnamese [24][25][26], as well as West African languages like Igbo [27] and Yorub` ´a [9][6][3][8]. ADR investigations have relied on ruled-based mo
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2149995043	u uˇ u mu (drink), mu` (sink), m´u (sharp) n` ´n n¯ n n (I), n (continuous aspect marker)´ s. s sa´ (run), s.a´ (fade), s.`a (choose) Undiacritized Yorub` ´a text has a high degree of ambiguity [3][8][9]. Adegbola et al. state that for ADR the “prevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within
2795877110	Attentive Sequence-to-Sequence Learning for Diacritic Restoration of YorùBá Language Text.	2149995043	y-based learning is a variant of the classical k-Nearest Neighbors (k-NN) approach to classiﬁcation, common for NLP tasks. Trained on a corpus of 65.6k words, their best word-level accuracy was 76.8% [9]. Scannell implemented a Na¨ıve Bayes classiﬁer using word and character-level models. For character-level prediction, each ambiguous character was treated a separate classiﬁcation problem, disregardi
2795935804	ESPNet: End-to-end speech processing toolkit	2024490156	[12], many variations of attention methods. With these state-of-theart end-to-end ASR techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc. Thus, ESPnet provides publicly available state-
2795935804	ESPNet: End-to-end speech processing toolkit	854541894	1 7.6 + BLSTM layers (4 !6) CER 8.5 5.9 + char-LSTMLM CER 8.3 5.2 + joint decoding CER 5.5 3.8 + label smoothing CER 5.3 3.6 WER 12.4 8.9 seq2seq + CNN (no LM) [33] WER N/A 10.5 seq2seq + FST word LM [35] CER N/A 3.9 WER N/A 9.3 CTC + FST word LM [11] WER N/A 7.3 Method Wall Clock Time # GPUs ESPnet (Chainer) 20 hours 1 ESPnet (PyTorch) 5 hours 1 seq2seq + CNN [33] 120 hours 10 Table 2 also compares t
2795935804	ESPNet: End-to-end speech processing toolkit	2530876040	3 eval92 ESPnet with VGG 2-BLSTM CER 10.1 7.6 + BLSTM layers (4 !6) CER 8.5 5.9 + char-LSTMLM CER 8.3 5.2 + joint decoding CER 5.5 3.8 + label smoothing CER 5.3 3.6 WER 12.4 8.9 seq2seq + CNN (no LM) [33] WER N/A 10.5 seq2seq + FST word LM [35] CER N/A 3.9 WER N/A 9.3 CTC + FST word LM [11] WER N/A 7.3 Method Wall Clock Time # GPUs ESPnet (Chainer) 20 hours 1 ESPnet (PyTorch) 5 hours 1 seq2seq + CNN [
2795935804	ESPNet: End-to-end speech processing toolkit	2327501763,2608712415	connectionist temporal classiﬁ- cation (CTC) [10, 11, 12] and attention-based encoder-decoder 1Language modeling is often performed by external language model toolkits, for example SRILM [6] network [13, 14, 15, 16]. Attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, while CTC uses Markov assumptions to efﬁciently solve sequential problems by d
2795935804	ESPNet: End-to-end speech processing toolkit	2109886035	e can categorize the toolkits into two types based on CTC and attention architectures as follows: • CTC-based: EESEN [11], Stanford CTC [28], Baidu Deepsppech [12], • Attention-based: Attention-LVCSR [29], OpenNMT speech to text [30] Note that most of end-to-end ASR toolkits are based on CTC, while ESPnet is based on an attention-based encoder-decoder network. Compared with Attention-LVCSR and OpenNMT
2795935804	ESPNet: End-to-end speech processing toolkit	854541894	ers inspired by [33, 34], that is h 1:T0 = BLSTM(VGG 2(o )): (2) This yields better performance than the pyramid BLSTM in many cases. 3.2.2. Attention ESPnet uses a location-aware attention mechanism [35], as a default attention. A dot-product attention [36] is also supported. While the location-aware attention yields better performance, the dot-product attention is much faster in terms of the computa
2795935804	ESPNet: End-to-end speech processing toolkit	1736701665	on of ESPnet with publicly available toolkits within an end-to-end ASR framework. We can categorize the toolkits into two types based on CTC and attention architectures as follows: • CTC-based: EESEN [11], Stanford CTC [28], Baidu Deepsppech [12], • Attention-based: Attention-LVCSR [29], OpenNMT speech to text [30] Note that most of end-to-end ASR toolkits are based on CTC, while ESPnet is based on an
2795935804	ESPNet: End-to-end speech processing toolkit	1524333225	ion in an end-to-end manner. ESPnet adopts widely-used dynamic neural network toolkits, Chainer [8] and PyTorch [9], as a main deep learning engine. ESPnet also follows the style of Kaldi ASR toolkit [1] for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. ESPnet fully utilizes beneﬁts of two major end-
2795935804	ESPNet: End-to-end speech processing toolkit	2627092829	ional scaling parameter. This method corresponds to a shallow fusion of a decoder network and RNNLM originally proposed in neural machine translation [41] and applied to end-to-end speech recognition [34]. 3.5. ASR setup in adverse environments Although most of ASR recipes supported in ESPnet are standard English tasks, current ESPnet recipes deal with other languages including Japanese (CSJ), Mandari
2795935804	ESPNet: End-to-end speech processing toolkit	1736701665	LSTMLM CER 8.3 5.2 + joint decoding CER 5.5 3.8 + label smoothing CER 5.3 3.6 WER 12.4 8.9 seq2seq + CNN (no LM) [33] WER N/A 10.5 seq2seq + FST word LM [35] CER N/A 3.9 WER N/A 9.3 CTC + FST word LM [11] WER N/A 7.3 Method Wall Clock Time # GPUs ESPnet (Chainer) 20 hours 1 ESPnet (PyTorch) 5 hours 1 seq2seq + CNN [33] 120 hours 10 Table 2 also compares the result of ESPnet with the other reports. Sin
2795935804	ESPNet: End-to-end speech processing toolkit	2027499299	multilingual endto-end ASR system (e.g., 10 languages) by following our previous study [42]. In addition, the ESPnet recipes also include noise robust/far-ﬁeld speech recognition tasks including AMI [22], CHiME-4 [25], and CHiME-5 tasks [26]. Especially ESPnet is an ofﬁcial end-to-end ASR baseline for the CHiME-5 challenge. 4. Implementation 4.1. Standard recipe ﬂow Figure 2 shows a ﬂow of standard r
2795935804	ESPNet: End-to-end speech processing toolkit	2327501763	with its network. 3.2. Attention-based encoder-decoder 3.2.1. Encoder The default encoder network is represented by bidirectional long short-term memory (BLSTM) with subsampling (called pyramid BLSTM [15]) given T-length speech feature sequence o 1: Tto extract high-level feature sequence h 1: 0 as h 1:T0 = BLSTM(o T ); (1) where T0&lt;Tin general due to the subsampling. The Chainer backend also suppo
2795935804	ESPNet: End-to-end speech processing toolkit	37526647	o-end ASR techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc. Thus, ESPnet provides publicly available state-of-the-art endto-end ASR setups, which aim to accelerate the development of
2795935804	ESPNet: End-to-end speech processing toolkit	2530876040	omputational time for main end-to-end ASR network training with number of GPUs. ESPnet achieved very fast training especially for the PyTorch backend even with a single GPU (gtx1080ti), compared with [33] for the same WSJ task. However, one of the issues of these end-to-end ASR systems is that their performance does not reach that of the stateof-the-art hybrid HMM/DNN systems. For example, the WER of
2795935804	ESPNet: End-to-end speech processing toolkit	2626778328	onal cost. In addition to above attentions, the PyTorch backend supports more than 11 types of attention functions including additive attention [37], coverage mechanism [38], and multi-head attention [39]. 3.3. Hybrid CTC/attention ESPnet adopts hybrid CTC/attention end-to-end ASR [17], which effectively utilizes the advantages of both architectures in training and decoding. 3.3.1. Multiobjective trai
2795935804	ESPNet: End-to-end speech processing toolkit	1736701665	onents in the HMM/DNN and CTCsyllable systems. Tables 3 and 4 compare the best system of Table 4: HKUST Mandarin CTS task (CER %). eval ESPnet 28.3 HMM/LSTM (Kaldi nnet3) 33.5 CTC with language model [11] 34.8 HMM/TDNN, LF MMI [27] 28.2 ESPnet (i.e., VGG 2-BLSTM, char-RNNLM, and joint decoding) with the hybrid HMM/DNN systems. Especially, ESPnet almost reached the latest best performance of the HMM/DN
2795935804	ESPNet: End-to-end speech processing toolkit	1494198834	ons of attention methods. With these state-of-theart end-to-end ASR techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc. Thus, ESPnet provides publicly available state-of-the-art endto-e
2795935804	ESPNet: End-to-end speech processing toolkit	1524333225	in speech processing communities. Especially, these efforts have been driven by popular products including Google voice search, Amazon Alexa, and Apple Siri and open source activities including Kaldi [1], HTK [2], Sphinx [3], Julius [4], RASR [5] in addition to general research activities. These open source toolkits include feature extraction, acoustic modeling based on a hidden Markov model (HMM), G
2795935804	ESPNet: End-to-end speech processing toolkit	1736701665,2102113734	up for speech recognition and other speech processing experiments. ESPnet fully utilizes beneﬁts of two major end-to-end ASR implementations based on both connectionist temporal classiﬁ- cation (CTC) [10, 11, 12] and attention-based encoder-decoder 1Language modeling is often performed by external language model toolkits, for example SRILM [6] network [13, 14, 15, 16]. Attention-based methods use an attention
2795935804	ESPNet: End-to-end speech processing toolkit	2530876040,2627092829	T0&lt;Tin general due to the subsampling. The Chainer backend also supports convolutional neural networks based on initial two blocks of VGG layer (VGG 2()) [32] followed by BLSTM layers inspired by [33, 34], that is h 1:T0 = BLSTM(VGG 2(o )): (2) This yields better performance than the pyramid BLSTM in many cases. 3.2.2. Attention ESPnet uses a location-aware attention mechanism [35], as a default atten
2795935804	ESPNet: End-to-end speech processing toolkit	2027499299	techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc. Thus, ESPnet provides publicly available state-of-the-art endto-end ASR setups, which aim to accelerate the development of this emerg
2795935804	ESPNet: End-to-end speech processing toolkit	2293858598	ublicly available toolkits within an end-to-end ASR framework. We can categorize the toolkits into two types based on CTC and attention architectures as follows: • CTC-based: EESEN [11], Stanford CTC [28], Baidu Deepsppech [12], • Attention-based: Attention-LVCSR [29], OpenNMT speech to text [30] Note that most of end-to-end ASR toolkits are based on CTC, while ESPnet is based on an attention-based en
2795963335	Prediction and Localization of Student Engagement in the Wild	2094077339	agement in the Wild WOODSTOCK’97, July 1997, El Paso, Texas USA 3.4 Deep Multi-Instance Network for Engagement Localization and Prediction In the MIL paradigm the training data is in the form of bags [29], B = {Xi,yi}N i=1 , where Xi = {xij} M j=1 , yi ∈Y, M, the number of instances in Xi and Y ∈{0,1,2,3}are the labels for the bags. M is same across all the videos. Such a problem is common in computer
2795963335	Prediction and Localization of Student Engagement in the Wild	2145916170	aphy for breast cancer detection [41] and molecular activities of drugs [10]. Sikka et al. [29] studied pain localization in a video in a weakly supervised fashion using the MIL framework. Tax et al. [33] proposed an MIL based framework to select the concept frames using clustering multi-instance learning. The main contribution of this work is to formulate student engagement prediction and localizatio
2795963335	Prediction and Localization of Student Engagement in the Wild	2016053056	belearned by deep neural networks along with MIL technique for a weakly supervised problem. This technique has been successfully used in vision tasks like image/video classification, object detection [21, 35], medical image processing in the form of mammography for breast cancer detection [41] and molecular activities of drugs [10]. Sikka et al. [29] studied pain localization in a video in a weakly superv
2795963335	Prediction and Localization of Student Engagement in the Wild	2145916170	d learning. MIL has been found extensively useful for tasks, in which the dataset has weak or noisy labels [1] and has been recently used in affect analysis tasks such as facial expression prediction [33] and image emotion prediction [27]. MIL assumes that the data could be presented with the help of a set of sub-instances or bag of sub-instances as {Xi}, where each bag consists of sub-instances {xij}
2795963335	Prediction and Localization of Student Engagement in the Wild	2121927366	many datasets are released to study the Prediction and Localization of Student Engagement in the Wild WOODSTOCK’97, July 1997, El Paso, Texas USA problems such as object detection[9] and segmentation[24] in an image and video. The number of datasets related to user engagement in the online learning environment released is very limited [3, 31, 34]. With the advent of deep learning frameworks, larger s
2795963335	Prediction and Localization of Student Engagement in the Wild	2108598243	e research community many datasets are released to study the Prediction and Localization of Student Engagement in the Wild WOODSTOCK’97, July 1997, El Paso, Texas USA problems such as object detection[9] and segmentation[24] in an image and video. The number of datasets related to user engagement in the online learning environment released is very limited [3, 31, 34]. With the advent of deep learning
2795963335	Prediction and Localization of Student Engagement in the Wild	2161673930	el of engagement have their own relevance. Different methods such as self-reports: participants answer the set of questions related to their experience such as level of engagement, interest and so on [18]. Secondly, an observational study was done by the external expert: Focus on the behavioral analysis of students. Traditional measurement methods are not sufficient to measure engagement in all the co
2795963335	Prediction and Localization of Student Engagement in the Wild	18680518	esting works for engagement detection. Name Features Subj. Method D’Mello et. al[11] FACS &amp; Log 28 Grafsgaard et. al[18] FACS 67 Regression Whitehill et al[34] FACS 34 Regression Grafsgaard et. al[17] FACS, Log - HMM Gupta et. al[19] Facial Features 112 DNN Xian et. al[38] PPG 18 RBF+SVM Arroyo et. al[5] Video 60 Regression &amp;Physiological Bosch et. al[6] Facial Features 137 Regression Body Pos
2795963335	Prediction and Localization of Student Engagement in the Wild	2100629176	fsgaard et. al[18] FACS 67 Regression Whitehill et al[34] FACS 34 Regression Grafsgaard et. al[17] FACS, Log - HMM Gupta et. al[19] Facial Features 112 DNN Xian et. al[38] PPG 18 RBF+SVM Arroyo et. al[5] Video 60 Regression &amp;Physiological Bosch et. al[6] Facial Features 137 Regression Body Posture In an interesting work, Whitehill et al. analyzed the behavioral engagement and defines four level o
2795963335	Prediction and Localization of Student Engagement in the Wild	2100629176	lities to analyze affective states of students while solving mathematical problems of SAT. Dataset was self-annotated and linear regression was used to regress the value of different affective states [5]. In another study conducted by Bosch et al. in the real world settings, various affective states of engagement such as boredom, engaged, concentration, confusion, frustration and delight were used. F
2795963335	Prediction and Localization of Student Engagement in the Wild	2094077339	n MIL, if atleast one instance is positive then the bag is assigned a positive label [10]. This assumption holds good for binary classification problems like malignancy detection [41], pain detection [29] and image classification [35] etc. In our dataset, since the ground truth labels are given for the entire video spanning 5 minutes, we take the firstk largest intensities of instances from the rankin
2795963335	Prediction and Localization of Student Engagement in the Wild	2010792435	oblem. Multiple Instance Learning (MIL) is followed as the paradigm for this weakly supervised learning. MIL has been found extensively useful for tasks, in which the dataset has weak or noisy labels [1] and has been recently used in affect analysis tasks such as facial expression prediction [33] and image emotion prediction [27]. MIL assumes that the data could be presented with the help of a set of
2795963335	Prediction and Localization of Student Engagement in the Wild	2112564774	on. Automatic engagement prediction can be based on various kinds of data modalities. It is argued that the student response can be used as an indicator of engagement in intelligent tutoring systems [22]. Another approach is based on features extracted on the basis of facial movements [13, 37]. Automated measures such as response time of a student to problems and test quizzes are used in ITS [20]. Ph
2795963335	Prediction and Localization of Student Engagement in the Wild	2094077339	s like image/video classification, object detection [21, 35], medical image processing in the form of mammography for breast cancer detection [41] and molecular activities of drugs [10]. Sikka et al. [29] studied pain localization in a video in a weakly supervised fashion using the MIL framework. Tax et al. [33] proposed an MIL based framework to select the concept frames using clustering multi-instan
2795963335	Prediction and Localization of Student Engagement in the Wild	2161673930	sked. Along with this Photoplethysmography signals were recorded. Table 1: Interesting works for engagement detection. Name Features Subj. Method D’Mello et. al[11] FACS &amp; Log 28 Grafsgaard et. al[18] FACS 67 Regression Whitehill et al[34] FACS 34 Regression Grafsgaard et. al[17] FACS, Log - HMM Gupta et. al[19] Facial Features 112 DNN Xian et. al[38] PPG 18 RBF+SVM Arroyo et. al[5] Video 60 Regre
2796032388	QUICKEDIT: EDITING TEXT & TRANSLATIONS BY CROSSING WORDS OUT	2613904329	al guess target sentence annotated with rejected tokens. It can thendecodeanewtargetsentencetakingtherejection labels into account. 3.1 Model Architecture This model builds upon the architecture from Gehring et al. (2017). Compared to this initial model,ourmodeladdsasecondencodertoencode the annotated guess sentence. It also duplicate every attention layer to allow the decoder to attend both to the source and the gues
2796032388	QUICKEDIT: EDITING TEXT & TRANSLATIONS BY CROSSING WORDS OUT	2143927888	computerized language assistance tools such as spelling correctors (Brill and Moore,2000) or next word suggestions (Bickel et al.,2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005;Mallinson et al.,2017), compressing sentences (Rush et al.,2015) or simplifying sentences (Nisioi et al.,2017). This type of work expands the possibilities for interactive text generation tools, like
2796032388	QUICKEDIT: EDITING TEXT & TRANSLATIONS BY CROSSING WORDS OUT	2613904329	rages the fact that the explanation for a given target word in generally localized around a few source words. Recently, new architectures have proposed to replace recurrent modules with convolutions (Gehring et al., 2017) or self-attention (Vaswani et al.,2017) to further increase accuracy. These architecture also performattentionatmorethanonedecoderlayers, allowing for more complex attention patterns. In this work, w
2796032388	QUICKEDIT: EDITING TEXT & TRANSLATIONS BY CROSSING WORDS OUT	2251171258	tem and enable human translators to edit its output with different levels of computer assistance. This enables improving machine translation outputs with lesser effort than purely manual translation. Green et al. (2014) implement such a system relying on a phrase-based translation system. The system presents an initial translation to the user who can accept a prex and select among the most likely postx iteratively.
2796080846	Enrichment of OntoSenseNet : Adding a sense-annotated Telugu lexicon	2153804780	2795 442 5776 Telugu-Hindi Dictionary 9939 142 1253 English-Telugu Dictionary 4657 1893 6695 Table 1: Statistics of available lexical resources for Telugu 3.1 Validation of the Resource Cohen’s Kappa [4] was used to measure inter-annotator agreement which proves the reliability. The annotations are done by one human expert and it is crosschecked by another annotator who is equally proficient. Both th
2796080846	Enrichment of OntoSenseNet : Adding a sense-annotated Telugu lexicon	2075123415	considered an ontology through the hypernymyhyponymy relations that are present in it. WordNet of any language leaves a few loop holes that other ontologies can fill [1]. By summarizing the following [11], [7], [15], we state the four major problems: 1 http://www.learnsanskrit.org/tools/sanscript 2 http://www.cfilt.iitb.ac.in/indowordnet/index.jsp Enrichment of OntoSenseNet: Adding a sense-annotated T
2796080846	Enrichment of OntoSenseNet : Adding a sense-annotated Telugu lexicon	2775390892	an ontology through the hypernymyhyponymy relations that are present in it. WordNet of any language leaves a few loop holes that other ontologies can fill [1]. By summarizing the following [11], [7], [15], we state the four major problems: 1 http://www.learnsanskrit.org/tools/sanscript 2 http://www.cfilt.iitb.ac.in/indowordnet/index.jsp Enrichment of OntoSenseNet: Adding a sense-annotated Telugu lexic
2796080846	Enrichment of OntoSenseNet : Adding a sense-annotated Telugu lexicon	2166233568	sets sound intuitively too specific when compared to their siblings, from a formal point of view, we may often explain their “different generality” by means of the distinction between types and roles [8]. 2.3 Variants of WSD WSD is broadly categorized into two types [3]: – Target Word WSD: The target WSD system disambiguates a restricted set of target words, usually one per sentence. Supervised appro
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2130942839	(2016) focused on speciﬁc linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using th
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2410539690	(i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date,
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2165612380	-query as word vectors to ﬁnd the average similarity measure between them; 3) ﬁnally the word vector based similarity is combined with the term-overlap-based similarity. The Vector Space Model (VSM) (Salton et al., 1975) is one of the overlap based methods. Each document is represented as a vector of terms. The ith document D i in targetside is represented as a vector D i = [w 1;i;w 2;i;:::w k;i], in which kis the si
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2162245945	2 RNNsearch 31.36 24.74 Transformer 32.55 25.11 Table 2: Results of Chinese-to-Portuguese translation. case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) for statistical signiﬁcance test. SMT We employ Moses (Koehn et al., 2007) to build phrase-based SMT model. The 5-gram language model are trained using the SRI Language Toolkit (Stolcke, 2002). To ob
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2101105183	3.37 13.34 Transformer 17.00 17.43 Ours SMT 33.78 27.42 RNNsearch 31.36 24.74 Transformer 32.55 25.11 Table 2: Results of Chinese-to-Portuguese translation. case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) for statistical signiﬁcance test. SMT We employ Moses (Koehn et al., 2007) to build phrase-based SMT model. The 5-gram language model are trained
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2133564696	c linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performance is still low using these state-of-the-art ap
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	171476473	ed. Pivot-based machine translation is a commonly used method when large quantities of parallel data are not readily available for some language pairs. Utiyama and Isahara (2007), Wu and Wang (2007), Bertoldi et al. (2008) investigated phrase-level, sentence-level and system-level pivot strategies for low resource translation in SMT. A pivot language, which is usually English, can bridge the source and target languages
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	1753482797	et al. (2014) and Liu and Leal (2016) focused on speciﬁc linguistic phenomena (i.e. present articles and temporal adverbials) in translation. Although NMT has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016), Chinese–Portuguese MT has not received much attention using NMT because training data are not readily enough. Therefore the performan
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2419539795	gh. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as most of them are transcripts of conversations in
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2156985047	MT We employ Moses (Koehn et al., 2007) to build phrase-based SMT model. The 5-gram language model are trained using the SRI Language Toolkit (Stolcke, 2002). To obtain word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with NewsCommentary11 corpora. We use minimum error rate training (Och, 2003) to optimize the feature weights. The maximum length of sentences is set as 80. RNNsearch We
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2130942839	N j=0 P(y jjy &lt;j;x) (3) in which given x and previous target translations y &lt;j (y 1;:::;y j 1), we need to compute the probability of the next word y j (j 2f1;:::;Ng). We employ both RNNsearch (Sutskever et al., 2014) and Transformer (Vaswani et al., 2017) architectures in our experiments. 3. Experiments 3.1. Data The general domain parallel corpus is built using the approaches introduced in Section 2. We randomly
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	1906341845	oring various sentence-alignment methods (e.g. length-based, dictionary-based), we found that translation based alignment is a robust approach especially for comparable data (Sennrich and Volk, 2010; Sennrich and Volk, 2011). The idea is to use machine translated text and BLEU as a similarity score to ﬁnd reliable alignments which are used as anchor points. The gaps between these anchor points are then ﬁlled using BLEU-b
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	1904365287	or rate training (Och, 2003) to optimize the feature weights. The maximum length of sentences is set as 80. RNNsearch We use our re-implemented attention-based NMT system, which incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the most recently generated word. We limited the source and target vocabularies to the most frequent 50K and 30K words in Chinese and P
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2146574666	the SRI Language Toolkit (Stolcke, 2002). To obtain word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with NewsCommentary11 corpora. We use minimum error rate training (Och, 2003) to optimize the feature weights. The maximum length of sentences is set as 80. RNNsearch We use our re-implemented attention-based NMT system, which incorporates dropout (Hinton et al., 2012) on the
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2006969979	to-sequence prediction task, which aims to ﬁnd for the source language sentence the most probable target language sentence that shares the same meaning. We can formulate SMT as: ^y = argmax y p(yjx) (Brown et al., 1993), where x and y are sentences in source and target sides, respectively. ^y denotes the translation output with the highest translation probability. p(yjx) is usually decomposed using the log-linear mo
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	1753482797	twork is trained to maximize the conditional likelihood on the bilingual training data. It directly models the probability of translation from the source sentence to the target sentence word by word (Kalchbrenner and Blunsom, 2013): P(yjx) = YN j=0 P(y jjy &lt;j;x) (3) in which given x and previous target translations y &lt;j (y 1;:::;y j 1), we need to compute the probability of the next word y j (j 2f1;:::;Ng). We employ both
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2606032440	uguese MT research for comparison purposes. In the future, we will investigate other approaches such as universal low-resource NMT (Gu et al., 2018) and discourse-aware approaches (Wang et al., 2018; Wang et al., 2017; Wang et al., 2016a) for Chinese–Portuguese MT task. Furthermore, we will keep exploring simple yet effective methods to build larger and domain-speciﬁc Chinese– Portuguese parallel corpora to furthe
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	630532510	use training data are not readily enough. Therefore the performance is still low using these state-of-the-art approaches. To date, there are only a few Chinese–Portuguese parallel corpora available1 (Tiedemann, 2012). OpenSubtitles20182 (Lison and Tiedemann, 2016) has released 6.7 millions Chinese–Portuguese sentence pairs, which are extracted from movie subtitles. These sentences are usually short and simple as
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	2626778328	x and previous target translations y &lt;j (y 1;:::;y j 1), we need to compute the probability of the next word y j (j 2f1;:::;Ng). We employ both RNNsearch (Sutskever et al., 2014) and Transformer (Vaswani et al., 2017) architectures in our experiments. 3. Experiments 3.1. Data The general domain parallel corpus is built using the approaches introduced in Section 2. We randomly sampled 1000 sentences and found that
2796150856	Chinese-Portuguese Machine Translation: A Study on Building Parallel Corpora from Comparable Texts.	1816313093	a ﬁxed vocabulary, which results in the OOV problem. This might contribute to the under-performance of NMT models compared SMT as observed in our experimental results. Joint byte-pair encoding (BPE) (Sennrich et al., 2016) is a simpler and more effective method to handle the OOV problem. It encodes rare and unknown words as sequences of subword units. We use the BPE toolkit13 to process our corpus and train an NMT syst
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2468716020	, embedding models for images and audio captions were shown to be capable of performing semantic retrieval tasks, and more recent works have studied word and object discovery [4] and keyword spotting [8]. Other work has analyzed these models, and provided evidence that linguistic abstractions such as phones and words emerge in their internal representations [4, 5, 6, 7]. Machine Translation. Automati
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2132921748	answering [30], and text-to-image generation [31]. Other work has studied joint representation learning for images and speech audio in the absence of text data. The ﬁrst major effort in this vein was [32], but until recently little progress was made in this direction as the text-and-vision approaches have remained dominant. In [3], embedding models for images and audio captions were shown to be capabl
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1836465849	e normalized according to the off-the-shelf VGG mean and variance computed over ImageNet [40]. The audio architecture we use the same as the one presented in [4], but with the addition of a BatchNorm [41] layer at the very front of the network, enabling us to do away with any data-space mean and variance normalization; our inputs are simply raw log mel ﬁlterbank energies. Our data preprocessing follow
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2137010615,2468716020	es spoken worldwide [1]. Recently, researchers have investigated models of spoken language that can be trained in a weakly-supervised fashion by augmenting the raw speech data with multimodal context [2, 3, 4, 5, 6, 7, 8]. Rather than learning a mapping between speech audio and text, these models learn associations between the speech audio and visual images. For example, such a model is capable of learning to associat
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2405756170	ew problems within the intersection of language and vision continue to be introduced, such as object discovery via multimodal dialog [29], visual question answering [30], and text-to-image generation [31]. Other work has studied joint representation learning for images and speech audio in the absence of text data. The ﬁrst major effort in this vein was [32], but until recently little progress was made
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1905882502,1931639407	ext. Some have studied correspondence matching between categorical abstractions, such as words and objects [20, 21, 22, 23, 24, 25]. Recently, interest in caption generation has grown, popularized by [26, 27, 28]. New problems within the intersection of language and vision continue to be introduced, such as object discovery via multimodal dialog [29], visual question answering [30], and text-to-image generati
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2114347655	from labels. The dominant approaches cast the problem as joint segmentation and clustering of the speech signal into linguistic units at various granularities. Segmental Dynamic Time Warping (S-DTW) [10, 11, 12] attempts to discover repetitions of the same words in a collection of untranscribed acoustic data by ﬁnding repeated regions of high acoustic similarity. Other approaches use Bayesian generative mode
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1923162067,2137471889	ng, and speech processing. Most existing work has focused on still-frame images paired with text. Some have studied correspondence matching between categorical abstractions, such as words and objects [20, 21, 22, 23, 24, 25]. Recently, interest in caption generation has grown, popularized by [26, 27, 28]. New problems within the intersection of language and vision continue to be introduced, such as object discovery via m
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1933349210	own, popularized by [26, 27, 28]. New problems within the intersection of language and vision continue to be introduced, such as object discovery via multimodal dialog [29], visual question answering [30], and text-to-image generation [31]. Other work has studied joint representation learning for images and speech audio in the absence of text data. The ﬁrst major effort in this vein was [32], but unti
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2153653739	ranslation. Automatically translating text from one language into another is a well-established problem. At ﬁrst dominated by statistical methods combining count-based translation and language models [33], the current paradigm relies upon deep neural network models [34]. New ideas continue to be introduced, including models which take advantage of shared visual context [35], but the majority of MT res
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2295297373	rds in a collection of untranscribed acoustic data by ﬁnding repeated regions of high acoustic similarity. Other approaches use Bayesian generative models at multiple levels of linguistic abstraction [13, 14, 15]. Neural network models have also been used to learn acoustic feature representations which are more robust to undesirable variation [16, 17, 18, 19]. Vision and Language. Modeling correspondences bet
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1686810756	ree networks: one for the image, one for the English caption, and one for the Hindi caption (Figure 1). The image network is formed by taking all layers up to conv5 from the pre-trained VGG16 network [39]. For a 224x224 pixel, RGB input image, the output of the network at this point would be a downsampled image of width 14 and height 14, with 512 feature channels. We need a means of transforming this
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2533523411	for text transcriptions or directly parallel corpora. 2. RELATION TO PRIOR WORK Unsupervised Speech Processing. State-of-the-art ASR systems are close to reaching human parity within certain domains [9], but this comes at an enormous resource cost in terms of text transcriptions for acoustic model training, phonetic lexicons, and large text corpora for language model training. These exist for only a
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	1545920196,1796128977	ve models at multiple levels of linguistic abstraction [13, 14, 15]. Neural network models have also been used to learn acoustic feature representations which are more robust to undesirable variation [16, 17, 18, 19]. Vision and Language. Modeling correspondences between vision and language is a rapidly growing ﬁeld at the intersection of computer vision, natural language processing, and speech processing. Most e
2796156786	Vision as an Interlingua: Learning Multilingual Semantic Embeddings of Untranscribed Speech	2133564696	another is a well-established problem. At ﬁrst dominated by statistical methods combining count-based translation and language models [33], the current paradigm relies upon deep neural network models [34]. New ideas continue to be introduced, including models which take advantage of shared visual context [35], but the majority of MT research has focused on the text-to-text case. Recent work has moved
2796176247	SIMPLE AND EFFECTIVE SEMI-SUPERVISED QUESTION ANSWERING	2586581008	e datasets from different domains. SQuAD (Rajpurkar et al.,2016) consists of questions whose answers are free form spans of text from passages in Wikipedia articles. We follow the same setting as in (Yang et al., 2017), and split 10% of training questions as the test set, and report performance when training on subsets of the remaining data ranging from 1% to 90% of the full set. We also report the performance on t
2796176247	SIMPLE AND EFFECTIVE SEMI-SUPERVISED QUESTION ANSWERING	1981208470	irectly require factual information. We apply the proposed system on three datasets from different domains – SQuAD (Rajpurkar et al. ,2016), TriviaQA-Web (Joshi et al. 2017) and the BioASQ challenge (Tsatsaronis et al., 2015). We observe signiﬁcant improvements in a low-resource setting across all three datasets. For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data. Our sy
2796176247	SIMPLE AND EFFECTIVE SEMI-SUPERVISED QUESTION ANSWERING	2586581008	ss all three datasets. For SQuAD and TriviaQA, we attain an F1 score of more than 50% by merely using 1% of the training data. Our system outperforms the approaches for semi-supervised QA presented inYang et al. (2017), and a baseline which uses the same unlabeled data but with a language modeling objective for pretraining. In the BioASQ challenge, we outperform the best performing system from previous year’s chall
2796176247	SIMPLE AND EFFECTIVE SEMI-SUPERVISED QUESTION ANSWERING	2512077205	turns out to be more beneﬁcial in terms of performance as well (c.f. Section4). Several cloze datasets have been proposed in the literature which use heuristics for construction (Hermann et al.,2015;Onishi et al., 2016;Hill et al.,2016). We further see the usability of such a dataset in a semi-supervised setting. 3 Methodology Our system comprises of following three steps: Cloze generation: Most of the documents ty
2796176247	SIMPLE AND EFFECTIVE SEMI-SUPERVISED QUESTION ANSWERING	2415755012	use of the generated cloze dataset to pre-train an expressive neural network designed for the task of reading comprehension. We work with two publicly available neural network models – the GA Reader (Dhingra et al., 2017) (to enable comparison with prior work) and BiDAF + Self-Attention (SA) model fromClark and Gardner(2017) (which is among the best performing models on SQuAD and TriviaQA). After pretraining, the perf
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	88044417	anguage 9 (5) Semantic Error: The errors that do not violate English grammar rules, but make the sentence senseless or absurd, are called as semantic errors. A semantic error can be a contextual error[2] or wrong word choice error. When a wrongly typed word is a real word in the language, it is not detected as a spelling error, yet it does not fit in the given context; such errors are called as conte
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2117545199	atic literature review is a well-planned procedure to search, identify, extract from, analyze, evaluate and interpret the existing literature works that are relevant to a particular research interest [26],[9]. A systematic review is different from a conventional review as it summarizes the existing work in a more complete and unbiased manner [9]. Systematic reviews are undertaken to sum up the existin
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2251086143	further be classified asfragments and run-ons. A fragment is an incomplete sentence in which either subject or verb is missing or it may be a sentence having dependent clause without the main clause [24]. A run-on sentence is two independent clauses missing a punctuation or necessary conjunction between them, which affects the readability of text. Sentence structure errors may contain other type of e
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2113134176	creating a corpus which is large enough to include all type of correct sentences seems practically infeasible. No tool support is available for this approach. 5.8 LSPs based approach (2007): Sun et al[20] proposed an approach which combines pattern discovery and machine learning to classify a sentence into two classes: correct and erroneous. To build this classification model, labeled sequential patte
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2131216224	e Run-ons elling ors ements Vtense Det er osition Punctuation xtual ors choice [16] ✓ ✓ ✓ ✓ ✓ [21] ✓ ✓ ✓ ✓ ✓ ✓ [14] ✓ ✓ ✓ ✓ ✓ ✓ ✓ [1] ✓ ✓ ✓ ✓ [3] ✓ [4] ✓ [10] ✓ ✓ ✓ ✓ [20] ✓ ✓ ✓ ✓ ✓ ✓ [8] ✓ ✓ ✓ ✓ ✓ ✓ [5] ✓ ✓ [17] ✓ ✓ ✓ ✓ ✓ [6] ✓ ✓ ✓ ✓ ✓ ✓ ✓ 20 Madhvi Soni et al Table 2. Summary of Various Grammar Checking Approaches Approach Technique used Target error types Linguistic data used Results Strengths Lim
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2113134176	ecking approaches oach Fragments e Run-ons elling ors ements Vtense Det er osition Punctuation xtual ors choice [16] ✓ ✓ ✓ ✓ ✓ [21] ✓ ✓ ✓ ✓ ✓ ✓ [14] ✓ ✓ ✓ ✓ ✓ ✓ ✓ [1] ✓ ✓ ✓ ✓ [3] ✓ [4] ✓ [10] ✓ ✓ ✓ ✓ [20] ✓ ✓ ✓ ✓ ✓ ✓ [8] ✓ ✓ ✓ ✓ ✓ ✓ [5] ✓ ✓ [17] ✓ ✓ ✓ ✓ ✓ [6] ✓ ✓ ✓ ✓ ✓ ✓ ✓ 20 Madhvi Soni et al Table 2. Summary of Various Grammar Checking Approaches Approach Technique used Target error types Linguistic
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2123388068	ection or not. A Systematic Review of Automated Grammar Checking in English Language 13 Fig. 8. Schematic Diagram of Arboretum[1] 5.5 SMT based approach (2006): The approach proposed by Brockett et al[3] makes use of Statistical Machine Translation (SMT) to detect and correct grammar errors. Aiming at mass noun errors, the authors advocate translation of the whole erroneous phrase instead of individu
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2131216224	ed errors are spelling or phrasal errors. A Systematic Review of Automated Grammar Checking in English Language 21 Technique used Target error types Linguistic data used Results Strengths Limitations [5] Article and preposition errors NUCLE corpus, Gigaword Corpus, section 23 of Wall Street Journal. F1 = 19.29% (articles), F1 = 11.15% (Prepositions) Supports automatic correction, Performance is bette
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2131216224	English Language 17 and does not cover other types of grammar errors like run-on sentences. The demo webpage of auto-editing is currently not available. 5.10 ASO based approach (2011): Dahlmeier et al[5] proposed grammar error correction using a linear classifier. They aim at correction of article and prepositional errors. The article or preposition and its context is treated as feature vectors and t
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2113134176	ntences from book- Avoid Errors by A.K. Misra. Not specified Language independent method, Quicker response by frequent detectors. Any pattern outside corpus is flagged as error even if it is correct. [20] Syntax errors, word choice error, sentence structure error. Hiroshima English Learners corpus, Japanese Learners of English corpus &amp; Chinese Learner Error corpus. Accuracy = 81.3 , precision = 83
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2123388068	s, Tschumi et al[21] developed a tool aimed at French native speakers writing in English, Naber developed an tool named LanguageTool [14] to detect a variety of English Grammar errors, Brockett et al [3] presented error Authors’ addresses: Madhvi Soni, Jabalpur Engineering College, Department of Computer Science &amp; Engineering, Jabalpur, M.P., 482011, India, madhvi. soni21@gmail.com; Jitendra Sing
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2123388068	sentences Success rate = 80% Better error correction due to best first method used. Overflagging of errors, poor performance for S-V agreement errors, missing auxiliary, complement and vform errors. [3] Mass noun errors Reuters newswire articles, CLEC corpus. English sentences from Chinese websites. success rate = 61.81% Automatic correction Unable to detect when mass noun is also a count noun. [4]
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2123388068	ted by various Grammar Checking approaches oach Fragments e Run-ons elling ors ements Vtense Det er osition Punctuation xtual ors choice [16] ✓ ✓ ✓ ✓ ✓ [21] ✓ ✓ ✓ ✓ ✓ ✓ [14] ✓ ✓ ✓ ✓ ✓ ✓ ✓ [1] ✓ ✓ ✓ ✓ [3] ✓ [4] ✓ [10] ✓ ✓ ✓ ✓ [20] ✓ ✓ ✓ ✓ ✓ ✓ [8] ✓ ✓ ✓ ✓ ✓ ✓ [5] ✓ ✓ [17] ✓ ✓ ✓ ✓ ✓ [6] ✓ ✓ ✓ ✓ ✓ ✓ ✓ 20 Madhvi Soni et al Table 2. Summary of Various Grammar Checking Approaches Approach Technique used Tar
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2113134176	tion of detected errors is not supported. Also spelling errors are simply ignored. No tool support is available for this approach. 16 Madhvi Soni et al Fig. 12. Schematic diagram of LSP based Approach[20] 5.9 Auto-Editing (2010): Huang et al[8] developed an online tool for automatic grammar error correction. The approach uses a manually created corpus of paired sentences collected from a website lang-
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2123388068	urred in CLEC corpus. The sentences containing these errors are used to create training data which can map erroneous string to correct one. See figure 9 Fig. 9. Schematic Diagram of SMT based Approach[3] The system was tested on 123 example sentences taken from English websites in China. During testing, the approach was able to correct 61.81% of mass noun errors. Errors like subject verb agreement an
2796193757	A Systematic Review of Automated Grammar Checking in English Language.	2131216224	xt. Then the classifier can be trained for these auxiliary problems to classify articles into 3 classes and prepositions into 36 classes. See figure 14 Fig. 14. Schematic Diagram of ASO based Approach[5] The training was done on NUCLE corpus and Gigaword corpus and testing was done using Wall Street Journal. The results were compared with two baseline methods. The ASO model outperforms both with an F
2796227053	Contrastive Learning of Emoji-based Representations for Resource-Poor Languages.	2402354285	) classifier (given by [17]) on the Telugu Twitter dataset to structure our baseline for Telugu language. – Multinomial Naive Bayes (Hindi) (MNB-H): We train a multinomial naive bayes model (given by [19]) on the Hindi Tweets dataset to form our baseline for Hindi language. 10 Nurendra Choudhary, Rajat Singh⋆, Ishita Bindlish and Manish Shrivastava 7 Experiments and Evaluation In order to study the co
2796227053	Contrastive Learning of Emoji-based Representations for Resource-Poor Languages.	1869752048	availability of immense data. Usually, methods that require immutable words are ineffective. Applying languages’ characters instead of words is a better approach. Given their proven effectiveness in [9,7,21,1,11,13,22], we use Bidirectional LSTMs (Bi-LSTMs) based on character n-grams. This approach produces embeddings based on the sequence of character n-grams, thus eliminating the problems of spelling mistakes and
2796227053	Contrastive Learning of Emoji-based Representations for Resource-Poor Languages.	2402354285	echniques are highly accurate but susceptible to the problems of spelling errors and improper sentences. And these problems are very frequent in informal texts such as tweets. Also, in case of Hindi, [19] have trained a multinomial naive bayes model on annotated tweets to solve the problem. Additionally, there have been efforts by researchers [18] to generate more annotated resources by utilizing avai
2796227053	Contrastive Learning of Emoji-based Representations for Resource-Poor Languages.	2133975759,2148118556	he problem of signature verification. Later, [6] applied the architecture with discriminative loss function for face verification. These networks also effectively enhance the quality of visual search [15,10]. Recently, [8] solved the problem of community question answering applying these networks . Let F(X) be the family of functions with parameters W. F(X) is differentiable with respect to W. Siamese ne
2796227053	Contrastive Learning of Emoji-based Representations for Resource-Poor Languages.	2157364932	specific information about the classes. Figure1: Siamese Network 2.1 Siamese Networks [5] introduced siamese neural networks (shown in figure1) to solve the problem of signature verification. Later, [6] applied the architecture with discriminative loss function for face verification. These networks also effectively enhance the quality of visual search [15,10]. Recently, [8] solved the problem of com
2796323698	ATTENTIVE INTERACTION MODEL: MODELING CHANGES IN VIEW IN ARGUMENTATION	2123442489	and = 0 otherwise. Details are described in the supplementary material. The original dataset comes with training and test splits (Figure1a). After tokenization and POS tagging with Stanford CoreNLP (Manning et al., 2014), our vocabulary is restricted to the most frequent 40,000 words from the training data. For a validationsplit,werandomlychoose10%oftraining discussions for each topic. Wetrainourmodelontheseventopics
2796323698	ATTENTIVE INTERACTION MODEL: MODELING CHANGES IN VIEW IN ARGUMENTATION	2271245358	(u max ), H SENT (the last hidden state of the sentence encoder sC M C ), T F IDF (TFIDFweighted n -grams of the comment), and W D O (word overlap). 5.4 Baseline The most similar prior work to ours (Tan et al., 2016) predicted whether an OH would ever give a DFin a discussion. The work used logistic regression with bag-of-words features. Hence, we also use logistic regression as our baseline to predict P ( = 1) .
2796323698	ATTENTIVE INTERACTION MODEL: MODELING CHANGES IN VIEW IN ARGUMENTATION	2271245358	y term frequency. Word Overlap Although integration of handcrafted features is behind the scope of this paper, we test the word overlap features between a comment and the OH&apos;s post, introduced byTan et al. (2016), as simple proxy for the interaction. For each comment, given the set of its words C and that of the OH&apos;s post O , these features are dened as h jC \ O j; jC \ O j jC j ; jC \ O j jO j ; jC \ O
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	2278629362	] consists of 5006 movie reviews annotated into 3 classes (positive, neutral and negative) and 4 classes (very positive, positive, negative and very negative). – English - Twitter Dataset: The dataset[17] consists of 103035 tweets annotated into 3 classes - positive, neutral and negative. – Spanish - Twitter Dataset: The dataset[17] consists of 275589 tweets annotated into 3 classes - positive, neutra
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	1869752048	with any certain domain. Usually, methods that require the immutable words are ineffective. A better approach utilizes the languages’ characters instead of words. Given their proven effectiveness in [8,6,24,1,9,13,25], we use Bidirectional LSTMs (Bi-LSTMs) based on character n-grams. This approach produces embeddings based on the sequence of character n-grams, thus eliminating the problems of spelling mistakes and
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	2402354285	e highly accurate but susceptible to the problems of spelling errors and improper sentences. And these problems are frequent in any informal text including reviews and tweets. Also, in case of Hindi, [21] have trained a multinomial naive bayes model on annotated Hindi tweets to solve the problem. Additionally, there have been efforts by researchers [19] to generate annotated resources by utilizing ava
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	2157364932	a without requiring specific information about the classes. Figure1: Siamese Network 2.1 Siamese Networks [4] introduced siamese neural networks to solve the problem of signature verification. Later, [5] used the architecture with discriminative loss function for face verification. Recently, these networks solved the problem of community question answering [7]. Let, F(X) be the family of functions wi
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	2402354285	SNASA (Tel-Eng). MNB-H refers to Multinomial Bayes Model for Hindi and compares to SNASA (Hin-Eng). – Multinomial Naive Bayes Model (Hindi) (MNB-H): We train a multinomial naive bayes model (given by [21]) on the Hindi Review dataset to form our baseline for Hindi language. 7 Experiments and Evaluation In order to study the comparison of SNASA to the previous models, we performed an array of experimen
2796370603	Emotions are Universal: Learning Sentiment Based Representations of Resource-Poor Languages using Siamese Networks.	2084046180	t vector machines that assigns sentiment poEmotions are Universal 3 larity to words or phrases using classifiers. Polarity of its constituents totals the sentences’ polarity. Lexicon based approaches [23] utilize a manually constructed lexicon with sentiments of major words given. This information assigns the polarity. The limitation of these approaches is the information loss of the words’ sequence w
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2183341477,2194775991	in the context of medical data, recent examples have shown that transfer learning works very well for imaging tasks [16, 4], due in large part to the availability of pre-trained computer vision models[18, 36, 34] that were pre-trained on the ImageNet database [13]. Machine learning has enormous potential in healthcare [5], however many researchers lack access to large sources of non-imaging healthcare data du
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2251803266	Factorization of a Modiﬁed Co-occurrence Matrix Previous work [21] by Levy and Goldberg showed that the skip-gram model with negative sampling (SGNS), which is often considered to be state-of-the-art [3], is implicitly factorizing a shifted, positive pointwise mutual information (PMI) matrix of word-context pairs. Pointwise mutual information (PMI) is a measure of association between a word and a con
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2108598243	hat transfer learning works very well for imaging tasks [16, 4], due in large part to the availability of pre-trained computer vision models[18, 36, 34] that were pre-trained on the ImageNet database [13]. Machine learning has enormous potential in healthcare [5], however many researchers lack access to large sources of non-imaging healthcare data due to privacy concerns. This has resulted in a lack o
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2250539671	in healthcare and medicine relative to other areas of machine learning and NLP. Moreover, because healthcare data come in a variety of forms, popular word embedding algorithms like word2vec and GloVe [31] which were originally developed for text cannot be directly applied to many kinds of healthcare data. The primary goal of this work is to construct a comprehensive set of embeddings for medical conce
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	168564468	a package [1, 2] in the R programming language. For the comparison to the traditional word2vec algorithm on the articles from PubMed, we used the implementation available in the gensim python package [32]. We used the skip-gram algorithm, hierarchical softmax, 10 negative samples, and a window size of 10. We used the implementation of GloVe available in the R package text2vec [33]. We used the sum of
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2099307202	ses of this paper we conﬁne our review to papers that are directly seeking to create low dimensional representations of clinical concepts, in the spirit of word2vec and GloVe. The ﬁrst investigations [28, 11, 29] using word2vec for medical concepts were performed shortly after the original word2vec paper appeared in 2013 and reported mixed results, though De Vine et. al reported state of the art performance w
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	1615991656	smoothing for the context singleton-frequencies, and whether or not the context vectors are used to construct the ﬁnal embeddings are all options that the practitioner must choose. Levy and Goldberg [22] conducted a systematic set of experiments on the effects of these hyper-parameters on the performance of word2vec, and we follow their recommendations in this work. Speciﬁcally, we used the following
2796398651	Clinical Concept Embeddings Learned from Massive Sources of Medical Data.	2250539671	at training a simple and scalable model with more data results in better accuracy than a complex non-linear model on a variety of benchmarks. 2.2 GloVe Global Vectors for Word Representations (GloVe) [31] was introduced shortly after Mikolov et. al and differs in several important ways. GloVe produces word embeddings by ﬁtting a weighted log-linear model to co-occurrence statistics. Given that a targe
2796406113	Not Just About Size - A Study on the Role of Distributed Word Representations in the Analysis of Scientific Publications.	1615991656	evaluated using semantic and syntactic similarity test sets. The results show that the Skip-gram model signiﬁcantly outperforms other architectures, specially in the semantic evaluation. Levy et al. [14] showed that much of the performance gains of word embeddings generated with these approaches are due to hyperparameter optimizations and design choices, instead of the embedding algorithms themselves
2796406113	Not Just About Size - A Study on the Role of Distributed Word Representations in the Analysis of Scientific Publications.	1849277567	hich have shown good performance in text classiﬁcation tasks [11]. In the paper, we also reﬂect on the ability of CNNs to learn features for the task at hand, which has been proved in computer vision [23] but still is a matter of debate in text understanding. Evaluation results show a trade oﬀ between the knowledge speciﬁcity of the corpus used to train the word embeddings and its size. In our evaluat
2796406113	Not Just About Size - A Study on the Role of Distributed Word Representations in the Analysis of Scientific Publications.	1615991656	yield dense vectors so that words with similar distributional context appear in the same region in the embedding space [21]. Two main families of algorithms to generate embeddings have been identiﬁed [18,14]: global matrix factorization (count-based) [13,18,21], and local context window methods (prediction) [17]. Nevertheless, Levy and Goldberg [13] blurred that distinction, showing that local context wi
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	2171060319	he authors investigate the reactions to terrorist events (such as the Boston Marathon bombing in 2013 and an attack in London) within the Arabic twitter-sphere by employing the algorithm developed by Hopkins and King (2010), that, as already highlighted, shares the same aggregated and supervised approach as iSA. Quite interestingly, they find that the degree of explicit support in each of the two above mentioned terrori
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	2171060319	to bagging. These limitations can be partly attenuated increasing considerably the size of the training set or the size of the subset of stems to use in each bagging replication. Built on top of the Hopkins and King (2010) inversion formula, iSA (Ceron et al., 2016) is a fast and more accurate implementation of this inverse solution which does not require resampling method and uses the complete length of stems by a sim
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	2171060319	e problematic if one is mainly interested in estimating some type of aggregate measure through the analysis of social media, as we want to do here with respect to the sentiment towards ISIS. Luckily, Hopkins and King (2010), had the idea to change point of view and focus on what can be accurately estimated. Their solution to the problem is as follows: 1 In the literature, when applying automated data classification a pr
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	2171060319	ittle issue about this estimation step as this task involves the estimation over the large number L of stems. This fact makes a direct solution for the problem very difficult. In their seminal paper, Hopkins and King (2010) proposed a workaround to solve this problem. They introduced the algorithm called ReadMe based on the repeated random sampling of the number of stems. In ReadMe the estimate of P (S|D) is performed o
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	1993691981	ref = {D1, . . . , DM }. In this sense, D is the union of two populations D = Dnoise ∪ Dref and this is in fact a misspecified reference population or a noise-labeled classification problem (see e.g. Lu et al., 2015). Notice further that Dnoise can actually consists itself of multiple heterogeneous subpopulations as Dnoise is just the complementary set do D ref. This is why, in essence, any algorithm, including t
2796493025	ISIS at its apogee: the Arabic discourse on Twitter and what we can learn from that about ISIS support and Foreign Fighters.	2171060319	for sentiment analysis To investigate the online opinions toward ISIS in this paper we adopt the technique iSA (integrated Sentiment Analysis: Ceron et al. 2016), derived from the fundamental work of Hopkins and King (2010). iSA is a supervised and aggregate alghoritm. The idea of supervised learning is rather simple (Grimmer and Stewart 2013): it starts with acknowledging the fact that human coders are (still?) better
2796495872	Aesthetical Attributes for Segmenting Arabic Word.	2888843596	characters; or indirect segmentation, where the word is cut in primitive. 1) Principles Methods: The principal methods used for segmenting Arabic word can be resumed in four techniques, as following [16]:  Segmentation by Skeleton Analysis: all algorithms following this method segment Arabic word in skeletons.  Segmentation by Contour Analysis: algorithms adopting this method extract word contours
2796528463	DETECTING LINGUISTIC CHARACTERISTICS OF ALZHEIMER'S DEMENTIA BY INTERPRETING NEURAL MODELS	2071477014	agged and untagged models. (see Table1for more details).3 Compared to other related works,Orimaye et al.(2015,2017) used AUC instead of accuracy, andKonig et al.¨ (2015) did not use DementiaBank data.Rudzicz et al. (2014) achieved an accuracy of 67.0% on the DementiaBank dataset using audio features as well as transcripts.Orimaye et al.(2016) achieved an accuracy of 87:5% but only used 36 transcripts. Their test set i
2796592721	Experiments with Universal CEFR Classification	2250290162	d various aspects of AES research such as: dataset development, feature engineering, multi-corpus studies and the role of prompt and task information (Yannakoudakis et al., 2011; Phandi et al., 2015; Zesch et al., 2015; Alikaniotis et al., 2016; Taghipour and Ng, 2016; Vajjala, 2018). AES models developed for non-English languages, primarily using the CEFR scale (Hancke 2013 for German, Pil´an et al. 2016 for Swedi
2796592721	Experiments with Universal CEFR Classification	2124725212	ere is a considerable amount of work that explored various aspects of AES research such as: dataset development, feature engineering, multi-corpus studies and the role of prompt and task information (Yannakoudakis et al., 2011; Phandi et al., 2015; Zesch et al., 2015; Alikaniotis et al., 2016; Taghipour and Ng, 2016; Vajjala, 2018). AES models developed for non-English languages, primarily using the CEFR scale (Hancke 2013
2796592721	Experiments with Universal CEFR Classification	2124725212	grading student essays written in response to some prompt. Different approaches for AES have been proposed in literature, where it is modeled as a regression, ranking or a classiﬁcation problem (cf. Yannakoudakis et al., 2011; Taghipour and Ng, 2016; Pilan et al.,´ 2016). To our knowledge, all the previous work described approaches that work with a single language (mostly English). Feature representations that can work fo
2796592721	Experiments with Universal CEFR Classification	2124725212	hat are commonly used in AES systems, as well as others that can be generalized across languages. They are described below: 1.Word and POS n-grams, which were commonly used in AES models in the past (Yannakoudakis et al., 2011). 2.Task-speciﬁc word and character embeddings trained through a softmax layer. Although word embeddings were used in recent neural AES models(Alikaniotis et al., 2016), this paper is the ﬁrst to expl
2796592721	Experiments with Universal CEFR Classification	1728842521	ral network models are implemented using Keras (Chollet et al., 2015) with TensorFlow as the backend (Abadi et al., 2015) and other models were implemented using scikit-learn (Pedregosa et al., 2011; Buitinck et al., 2013).4 While it is also possible to model AES as a regression task, we report classiﬁcation results which is common in CEFR classiﬁcation tasks. Our initial experiments with linear regression gave Pearson
2796592721	Experiments with Universal CEFR Classification	2101234009	uage’s data. All our neural network models are implemented using Keras (Chollet et al., 2015) with TensorFlow as the backend (Abadi et al., 2015) and other models were implemented using scikit-learn (Pedregosa et al., 2011; Buitinck et al., 2013).4 While it is also possible to model AES as a regression task, we report classiﬁcation results which is common in CEFR classiﬁcation tasks. Our initial experiments with linear
2796592721	Experiments with Universal CEFR Classification	2250372169	of work that explored various aspects of AES research such as: dataset development, feature engineering, multi-corpus studies and the role of prompt and task information (Yannakoudakis et al., 2011; Phandi et al., 2015; Zesch et al., 2015; Alikaniotis et al., 2016; Taghipour and Ng, 2016; Vajjala, 2018). AES models developed for non-English languages, primarily using the CEFR scale (Hancke 2013 for German, Pil´an e
2796748812	Evaluating Word Embedding Hyper-Parameters for Similarity and Analogy Tasks.	2465912008	ng the corpus size has little impact on word similarity tasks. 2 Related Work There are a number of studies on evaluating the quality of the word embeddings (Chiu et al., 2016; Schnabel et al., 2015; Linzen, 2016; Gladkova and Drozd, 2016). Chiu et al. (2016) studied the relationship between intrinsic and extrinsic task performance produced by word embeddings. The authors found that models that performed well
2796779706	OBJECT ORDERING WITH BIDIRECTIONAL MATCHINGS FOR VISUAL REASONING	2416885651	lters (instead of a pointer network which addresses an unordered sequence of structured object representations). As shown in Table1, this CNN-BiATT model outperforms the neural module networks (NMN) (Andreas et al., 2016) previous-best result by 3.6% on the public test set and 4.1% on the unreleased test set. More details and the model ﬁgure are in the appendix. Output Example Analysis: Finally, in Fig.1, we show some
2796779706	OBJECT ORDERING WITH BIDIRECTIONAL MATCHINGS FOR VISUAL REASONING	2416885651	Xu et al.,2015) has been widely used for conditioned language generation tasks. It is further used to learn alignments between different modalities (Lu et al.,2016;Wang and Jiang,2016;Seo et al.,2016;Andreas et al., 2016;Chaplot et al.,2017). In our work, a bidirectional attention mechanism is used to learn a joint representation of the visual objects and the words by building matchings between them. Pointer network
2796864832	Mining Social Media for Newsgathering.	2293230138	by [4]. Their dashboard also enables visualisation of the temporal evolution of events, showing the most important news articles and tweets, mapped information, and the main entities being mentioned. [64] focused instead on geolocated social media posts when they developed CityBeat, a dashboard for exploration of local news reports. Having deployed their application in several newsrooms, an important
2796864832	Mining Social Media for Newsgathering.	2296706733	, 87]. Use of fact-checking techniques can be of limited help in the context of breaking news events, where much of the information is new and may not be available in knowledge bases. Others, such as [65] with Hoaxy, implementeddashboardfortrackingthespreadofmisinformation, whichcouldbeusedtobetterunderstanding this phenomenon and to get new insights for improving veriﬁcation systems. 5 3.5 Finding In
2796864832	Mining Social Media for Newsgathering.	2151451758	alists who cover speciﬁc subjects or types of journalism. Work in this direction is scarce. The ﬁrst, preliminary approach to recommending news stories from social media in real-time was described by [59]. They compared three diﬀerent ranking algorithms, which showed promising results through preliminary experimentation. The same authors documented more advanced work in [58]. They combined two datasou
2796864832	Mining Social Media for Newsgathering.	651477617	ct-checkable claims are checked against a database for accuracy. Most work on fact-checking claims has built knowledge graphs out of knowledge bases, such as Wikidata, to check the validity of claims [17, 67, 68, 87]. Use of fact-checking techniques can be of limited help in the context of breaking news events, where much of the information is new and may not be available in knowledge bases. Others, such as [65]
2796864832	Mining Social Media for Newsgathering.	2607700676	of datasets, they found the third approach performed best. Veriﬁcationhasalsobeentackledasataskassociatedwithrumoursthatspreadinsocialmedia.Thisisthecase of the RumourEval shared task at SemEval 2017[20]. Subtask B consisted in determining if eachof the rumours in the dataset were true, false or remained unveriﬁed. The best approach, by [25], used a stance classiﬁcation system to determine the stance
2796864832	Mining Social Media for Newsgathering.	2250463706	edia. 3.5 FindingInformationSources It is not only the content that is important in the newsgathering process. Journalists need to identify quality information sources they can follow during an event [21, 54, 72]. This information sources can be eyewitnesses on the ground, local people who know the area where the news is developing, experts in the topic, etc. They may want to follow them to get the latest upd
2796864832	Mining Social Media for Newsgathering.	2250463706	eobjectiveistoidentify whether users are located within the region aﬀected by the crisis event in question. A binary classiﬁer would then determine if a user is within a distance of the event or not. [54] described a classiﬁer that determined if a user was IR (inside the region) or OR (outside the region). They proposed to look only at the linguistic features of the content posted by the users. By usi
2796864832	Mining Social Media for Newsgathering.	2112699412	ewswherethereisnopriorinformationandthesummariserneeds to deal with new, unseen events. Most existing work has focused on extractive summarisation. For scheduled events such as football matches, both [95] and [55] showed that it is relatively easy to achieve highly accurate summaries including all major happenings,suchasgoalsandredcards,bysimplylookingatwordfrequencies.Indeedthevocabularyislargely pre
2796864832	Mining Social Media for Newsgathering.	1943989130	nded in ordertoensure collectionofrelevant data.Therehasbeenresearchlookinginto keywordexpansion forthese purposes. Improved collection of event-related tweets for increased coverage was pioneered by [53]. They used Latent Dirichlet Allocation (LDA) to enable discovery of relevant keywords associated with a known event, which was designed for and tested with leisure events, which in their case include
2796864832	Mining Social Media for Newsgathering.	2101196063	ournalism is being mainly used in the following three ways: (i) as a venue to freely post news stories to reach out to potential new readers and increase the number of visitors on their websites, cf. [42], (ii) as an analytical platform to explore the preferences of news consumers, analysing the news stories that users read and share most, cf. [22], and (iii) as a gold mine to catch the scoop on break
2796864832	Mining Social Media for Newsgathering.	2152935499	he top ranked news items from Twitter. The resulting ranking of news is personalised for the user of the system, as it incorporates information from their network. With a slightly diﬀerent objective, [41] described a system that, given a news story as input, recommends relevant tweets for gathering furthering evidence about the story from Twitter. They use language models and topic models to identify
2796864832	Mining Social Media for Newsgathering.	1479916289	real-time was described by [59]. They compared three diﬀerent ranking algorithms, which showed promising results through preliminary experimentation. The same authors documented more advanced work in [58]. They combined two datasources,tweetsandRSSfeedsfromnewsoutlets,tolookforoverlappingcontent.Thesystemwillthenrank Twitter content overlapping with RSS feeds based on the friends of the user in questi
2796864832	Mining Social Media for Newsgathering.	2062743035	simplylookingatwordfrequencies.Indeedthevocabularyislargely predictable for football matches, and the volume of posts associated with keywords such as ‘goal’ increases drastically when a team scores. [37] proposed an alternative, graph-based approach. First, they used LDA to identifytopicsofdiscussionwithinastream.Then,theybuiltagraphwiththetweetsineachtopic,basedonword co-occurrences. Finally, they u
2796864832	Mining Social Media for Newsgathering.	2051405935	t of the information is accurate. This raises concern about the inability of ordinary users to detect misinformation. Research in recent years has focused on developing automated veriﬁcation systems. [47]extended previous workby [89]and [16]by incorporatingwhat theycalled“veriﬁcation features”. These features were determined based on insights from journalists and included source credibility, source id
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2062955551	., 2011;˜ Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et a
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2185175083	) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantiﬁed characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazaridou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	1861492603	; and we use book-level holdout so that no images/text in the test set are from books in the training set. Captions and Tags. We also examine two popular existing datasets: Microsoft COCO (captions) (Lin et al., 2014) (COCO) and MIRFLICKR-1M (tags) (Huiskes et al., 2010) (Flickr). For COCO, we construct our own training/validation splits from the 123K images, each of which has 5 captions. For Flickr, as an initial
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	1931639407	2003; Cusano et al., 2004; Grangier and Bengio, 2008; Chen et al., 2013, inter alia) or short, literal natural language captions (Farhadi et al., 2010; Ordo´nez et al., 2011;˜ Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2115752676	2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Khan et al., 2009; Socher and Fei-Fei, 2010; Jia et al., 2011; Zhuang et al., 2013, inter alia). We provide experimental results on both types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	1486723856	2014a). Other work has quantiﬁed characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazaridou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	1897761818	asets couple images with a handful of unordered tags (Barnard et al., 2003; Cusano et al., 2004; Grangier and Bengio, 2008; Chen et al., 2013, inter alia) or short, literal natural language captions (Farhadi et al., 2010; Ordo´nez et al., 2011;˜ Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal retrieval settings, images are paired with long, only loosely thematically-related documents. (Kha
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2037407504	bstantially differing characteristics, and are used for different tasks (Baltrusaitis et al., 2017). Some commonlyˇ used datasets couple images with a handful of unordered tags (Barnard et al., 2003; Cusano et al., 2004; Grangier and Bengio, 2008; Chen et al., 2013, inter alia) or short, literal natural language captions (Farhadi et al., 2010; Ordo´nez et al., 2011;˜ Kulkarni et al., 2013; Fang et al., 2015, inter a
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	1714665356	bulary size to 7.5K. We next consider latent-variable bag-of-words models, including LDA (Blei et al., 2003) (256 topics, trained with Mallet (McCallum, 2002)) a specialized biterm topic model (BTM) (Yan et al., 2013) for short texts (30 topics), and paragraph vectors (PV) (Le and Mikolov, 2014) (PV-DBOW version, 256 dimensions, trained with Gensim (Rehˇ u˚ˇrek and Sojka, 2010)).13 Alignment of Text and Images. We
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2286410738	ces Previous work suggests that incorporating visual features for less concrete concepts can be harmful in word similarity tasks (Hill and Korhonen, 2014b; Kiela et al., 2014; Kiela and Bottou, 2014; Hill et al., 2014). However, it is less clear if this intuition applies to more practical tasks (e.g., retrieval), or if this problem can be overcome simply by applying the “right” machine learning algorithm. We aim to
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2162867699	characteristics, and are used for different tasks (Baltrusaitis et al., 2017). Some commonlyˇ used datasets couple images with a handful of unordered tags (Barnard et al., 2003; Cusano et al., 2004; Grangier and Bengio, 2008; Chen et al., 2013, inter alia) or short, literal natural language captions (Farhadi et al., 2010; Ordo´nez et al., 2011;˜ Kulkarni et al., 2013; Fang et al., 2015, inter alia). In other cross-modal
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2108598243	f MNI as deﬁned in Equation 1. We extract image features from the pre-softmax layer of a deep convolutional neural network, ResNet50 (He et al., 2016), pretrained for the ImageNet classiﬁcation task (Deng et al., 2009); this method is known to be a strong baseline (Sharif Razavian et al., 2014).6 For nearest neighbor search, we use the Annoy library,7 which computes approximate kNN efﬁciently. We use k= 50 nearest
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2286410738	g human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantiﬁed characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazaridou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kie
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2251970440	h types of data. Concreteness in datasets has been previously studied in either text-only cases (Turney et al., 2011; Hill et al., 2013) or by incorporating human judgments of perception into models (Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantiﬁed characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; J
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2145056192	Image/Text Correspondences Previous work suggests that incorporating visual features for less concrete concepts can be harmful in word similarity tasks (Hill and Korhonen, 2014b; Kiela et al., 2014; Kiela and Bottou, 2014; Hill et al., 2014). However, it is less clear if this intuition applies to more practical tasks (e.g., retrieval), or if this problem can be overcome simply by applying the “right” machine learning
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2097117768	lor histograms) for multimodal retrieval. We consider two different CNNs pretrained on different datasets: ResNet50 features trained on the ImageNet classiﬁcation task (RN-Imagenet), and InceptionV3 (Szegedy et al., 2015) trained on the OpenImages (Krasin et al., 2017) image tagging task (I3-OpenImages). 12Averaging is done for ease of presentation; the performance in both directions is similar. Among the parametric a
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2577784528	nabled a number of new applications, e.g., better accessibility via automatic generation of alt text (Garcia et al., 2016), cheaper training-data acquisition for computer vision (Joulin et al., 2016; Veit et al., 2017), and cross-modal retrieval systems, e.g., Rasiwasia et al. (2010); Costa Pereira et al. (2014). Multimodal datasets often have substantially differing characteristics, and are used for different task
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2131744502	ng LDA (Blei et al., 2003) (256 topics, trained with Mallet (McCallum, 2002)) a specialized biterm topic model (BTM) (Yan et al., 2013) for short texts (30 topics), and paragraph vectors (PV) (Le and Mikolov, 2014) (PV-DBOW version, 256 dimensions, trained with Gensim (Rehˇ u˚ˇrek and Sojka, 2010)).13 Alignment of Text and Images. We explore four algorithms for learning correspondences between image and text ve
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2106277773	o not match. This task is a good representative of multimodal learning because computing a joint embedding of text and images is often a “ﬁrst step” for downstream tasks, e.g., cross-modal retrieval (Rasiwasia et al., 2010), image tagging (Chen et al., 2013), and caption generation (Kiros et al., 2015). Evaluations. Following previous work in crossmodal retrieval, we measure performance using the top-k% hit rate (also c
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2575842049	s of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazaridou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image search to collect 50 images each for a variety of words and compute the average cosine si
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2145056192	Silberer and Lapata, 2012; Hill and Korhonen, 2014a). Other work has quantiﬁed characteristics of concreteness in multimodal datasets (Young et al., 2014; Hill et al., 2014; Hill and Korhonen, 2014b; Kiela and Bottou, 2014; Jas and Parikh, 2015; Lazaridou et al., 2015; Silberer et al., 2016; Lu et al., 2017; Bhaskar et al., 2017). Most related to our work is that of Kiela et al. (2014); the authors use Google image sea
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2194775991	sociated with at least 100 images, so as to ensure the stability of MNI as deﬁned in Equation 1. We extract image features from the pre-softmax layer of a deep convolutional neural network, ResNet50 (He et al., 2016), pretrained for the ImageNet classiﬁcation task (Deng et al., 2009); this method is known to be a strong baseline (Sharif Razavian et al., 2014).6 For nearest neighbor search, we use the Annoy librar
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2106277773	ty via automatic generation of alt text (Garcia et al., 2016), cheaper training-data acquisition for computer vision (Joulin et al., 2016; Veit et al., 2017), and cross-modal retrieval systems, e.g., Rasiwasia et al. (2010); Costa Pereira et al. (2014). Multimodal datasets often have substantially differing characteristics, and are used for different tasks (Baltrusaitis et al., 2017). Some commonlyˇ used datasets couple
2796886047	QUANTIFYING THE VISUAL CONCRETENESS OF WORDS AND TOPICS IN MULTIMODAL DATASETS	2070753207	xt representations down to independent dimensions of high multimodal correlation. CCA-based methods are popular within the IR community for learning multimodal embeddings (Costa Pereira et al., 2014; Gong et al., 2014). We use Wang et al. (2015b)’s stochastic method for training deep CCA (Andrew et al., 2013) (DCCA), a method that is competitive with traditional kernel CCA (Wang et al., 2015a) but less memory-inten
2796918693	Introduction to Iltis: an interactive, web-based system for teaching logic	18742077	system allows for training the transformation of propositional formulas [8, 9]. „e inference of new knowledge using calculi that are close to natural inference is supported by many systems, see e.g. [2, 4, 6, 11]. Resolution, which is used in the introduction to logic in Dortmund, is to the best of our knowledge only supported by the AELL system [6] (which is not publicly available and only has support for th
2796918693	Introduction to Iltis: an interactive, web-based system for teaching logic	2057405025	Catalan language). In Tarski’s World students can learn how to evaluate •rst-order logic in a 3D-world. A playful but prototypical approach towards topics in an introductory logic course is taken in [10]. Digital tools are also used in some interactive logic books. For example, the teaching environment [3] allows for transforming textual statements into logical formulas by a mark-and-replace techniqu
2796918693	Introduction to Iltis: an interactive, web-based system for teaching logic	137836376	rt websites of Power of Logic [13]. An inspiration for presenting models for modal formulas (i.e. Kripke structures) can be found in [7]. An overview over further, also older systems, can be found in [5]. In summary, some of the aspects Iltis aims at are covered by other systems. Yet, an integration of these systems seems to be impracticable or even impossible due to technological diversity. Organiza
2796932047	IMPROVING CHARACTER-BASED DECODING USING TARGET-SIDE MORPHOLOGICAL INFORMATION FOR NEURAL MACHINE TRANSLATION	2311921240	(OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model ofChung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the dec
2796932047	IMPROVING CHARACTER-BASED DECODING USING TARGET-SIDE MORPHOLOGICAL INFORMATION FOR NEURAL MACHINE TRANSLATION	1522301498	experimental setting used in CDNMT, where the GRU size is 1024, the afﬁx and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015).Chung et al.(2016) andSennrich et al. (2016) demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora1 to train the mod
2796932047	IMPROVING CHARACTER-BASED DECODING USING TARGET-SIDE MORPHOLOGICAL INFORMATION FOR NEURAL MACHINE TRANSLATION	1816313093	the GRU size is 1024, the afﬁx and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015).Chung et al.(2016) andSennrich et al. (2016) demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora1 to train the models, newstest-2013 for tuning and newstest-20
2796932047	IMPROVING CHARACTER-BASED DECODING USING TARGET-SIDE MORPHOLOGICAL INFORMATION FOR NEURAL MACHINE TRANSLATION	2311921240	rent models try to capture complexities on the encoder side, but to the best of our knowledge the only model which proposes a technique to deal with complex constituents on the decoder side is that ofChung et al. (2016), which should be an appropriate baseline for our comparisons. Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model. This model is referre
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2117621558	d more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2157435188	lly didnotappearinagiventrainingcorpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014).
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2344508595	LP,crosslingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). 9 Conclusion and Future Work We ﬁrst investigated the applicability of neural seq2seq models tomorphological surface segmentation for polysynthetic languages in minimalresource settings, i.e., for c
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2117621558	morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here,weadd three newquestions tothisline ofresearch: (i)Aredata-hungry neuralnetworkmodels applicable to
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2351252181	n approach for a morphological segmentation task. Inmanyother areas ofNLP,crosslingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). 9 Conclusion and Future Work We ﬁrst investigated the applicability of neural seq2seq models tomorphological surface segmentation
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	1905100302	ncoder-decoder RNN (Bahdanau et al., 2015) which has been trained only on the available annotated data. Semi-supervisedMORFESSOR(MORF). We further compare to the semi-supervised version of MORFESSOR (Kohonen et al., 2010), a wellknown morphological segmentation system. During training, wetune the hyperparameters for each language on the respective development set. The best performing model is applied to the test set.
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2101711363	ological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available fo
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	1905100302	oped: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological
2797051393	FORTIFICATION OF NEURAL MORPHOLOGICAL SEGMENTATION MODELS FOR POLYSYNTHETIC MINIMAL-RESOURCE LANGUAGES	2160097208	terell et al. (2015) trained a semi-Markov CRF (semi-CRF) (Sarawagi and Cohen, 2005) jointly on morphological segmentation, stemming and tagging. For the similar problem of Chinese word segmentation, Zhang and Clark (2008) trained a model jointly on part-of-speech tagging. However, we are not aware of any prior work on multi-task training ordataaugmentation forneural segmentation models. In fact, the two only neural se
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2123442489	according to the verbs in LOs. We start with identifying the action verbs in learning outcomes. Two lists are formed containing the action verbs from each LO respectively. We use Stanford POS Tagger [15] to tag the words and identify action verbs. Each layer in the hierarchy is given a numerical value starting from 1 and going up to 6 as we move up the hierarchy. The absolute distance between the num
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2121184547	ds use the hierarchical distribution of the words in the database [3][20][16]. Some techniques also integrate external corpus statistics with lexical database and in uence the nal semantic similarity [14][10]. These methods have the following general limitations: { The appropriate meaning of the word is not considered while computing the similarity between words which introduces inaccuracies during th
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2143017621	e of the word is part of \word sense disambiguation&quot; research area. We use ‘max similarity’ algorithm to identify the sense of the words[19], as implemented in Pywsd, an NLTK based python library[4]. argmax synset(a)( Xn i max synset(i)(sim(i;a)) (3) In this stage, we identify the meaning of the word and the synset corresponding to this denition from the WordNet. This information is stored in c
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2045929671	encouraging. Grammar-based methods Grammar-based methods are more useful to analyze the general language sentences. Such methods ultimately depend on some measure of semantic similarity between words[9] [13]. The Sentence Text Similarity method [9] focuses on the semantic similarity between words as well as the String Similarity. They also consider the order of occurrences of words. These methods wo
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2165897980	er of pages indexed by the search engine are huge, and words with opposite meaning frequently occur together on the web. We have implemented the methodology to calculate the Google Similarity Distance[6], but results are not encouraging. Grammar-based methods Grammar-based methods are more useful to analyze the general language sentences. Such methods ultimately depend on some measure of semantic sim
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	2123442489	he hierarchy. Each level in the hierarchy is assigned a number starting at 1 with the base ‘Remembering’ and going up to 6 with ‘Creating’. To tag the verbs in the LOs, we use the Stanford POS tagger [15]. 4.4 Illustrative Example This section explains the working of methodology to calculate the Bloom’s Index and the usage of corpus statistics. Calculating Bloom’s Index Consider following sentences: S
2797140873	Similarity between Learning Outcomes from Course Objectives using Semantic Analysis, Blooms taxonomy and Corpus statistics.	1659833910	hods Similarity based on lexical databases Various methods have been developed previously which use a lexical database. These methods use the hierarchical distribution of the words in the database [3][20][16]. Some techniques also integrate external corpus statistics with lexical database and in uence the nal semantic similarity [14][10]. These methods have the following general limitations: { The app
2797206276	Alquist: The Alexa Prize Socialbot.	2251235149	) [10, 11] has been widely used as a DM since it can handle the uncertainty caused by automatic speech recognition (ASR) or natural language understanding (NLU). The dialogue state tracking challenge [12] has provided several labeled dialogue data as well as the evaluation framework to accelerate research of DM. The recent work [13] shows how to reduce an amount of data required to train end-to-end RN
2797206276	Alquist: The Alexa Prize Socialbot.	2130942839	conversation about user’s (bot’s) mood, a question about the day, etc. It is triggered when the chit-chat module is recognized or if no other module generates a response. We use sequence-to-sequence [22] network architecture to generate the responses. This framework is commonly used for machine translation but was recently proposed as a conversational model as well [1]. This model consists of encoder
2797206276	Alquist: The Alexa Prize Socialbot.	2101105183	on cosine similarity) question in the database and the answer to it is returned. 5 Experiments Objective evaluation of the end-to-end dialogue system is a difﬁcult task. Several metrics such as BLEU [26] were adopted, but they do not reﬂect the real quality of the dialogue [27]. We consider 7https://github.com/suriyadeepan/easy_seq2seq 8https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_p
2797206276	Alquist: The Alexa Prize Socialbot.	2438667436	edicate-argument structure was introduced in [9]. Statistical approaches were proposed to avoid handcrafting the rules for dialogue management (DM). Partially observed Markov decision process (POMDP) [10, 11] has been widely used as a DM since it can handle the uncertainty caused by automatic speech recognition (ASR) or natural language understanding (NLU). The dialogue state tracking challenge [12] has p
2797206276	Alquist: The Alexa Prize Socialbot.	1986321089,2138605095	was lost by ASR. This is helpful for NER which has a model trained on sentences with original case. The entity recognition is implemented using two approaches. The ﬁrst one is Microsoft Concept Graph [17, 18] which provides a list of possible concepts for a given entity (for example “Frozen” has following concepts: ﬁlm, feature, processed food, ...). We have a snapshot of the Concept graph5 stored in Elas
2797206276	Alquist: The Alexa Prize Socialbot.	2250539671	owing, analyzer: word, ngram_range: (1,2), max_df: 0.9, min_df: 0.0, norm: l1, smooth_idf: False, sublinear_tf: True. The rest of parameters has default values.  Embeddings: We use pre-trained GloVe [29] vectors. We convert training examples to vector representation by calculating the average of embeddings of example’s words. We normalize vector representations of training examples to unit length. We
2797206276	Alquist: The Alexa Prize Socialbot.	2250750514	to query for a speciﬁc entity name and retrieve a concept quickly. The second tool is based on Label-lookup6 which is used in question answering system YodaQA [19]. Label-lookup uses the cross-wikis [20] dataset to match strings together with the nearest Wikipedia concept. Our version of the tool has two minor modiﬁcations. Firstly we include the Freebase ID in the database and thus eliminate a SPARQ
2797206276	Alquist: The Alexa Prize Socialbot.	2128242680	rkup Language (AIML). Various utterances and their corresponding responses can be stored in this format. More recent systems can be divided into task oriented and open-domain systems. Furthermore, in [4], the task oriented systems are divided into those which use a database or statistical text matching as the source of information. They are often used as a tourist guide [5], reservation assistants [6
2797206276	Alquist: The Alexa Prize Socialbot.	2094728533	s of information sources:  Knowledge Bases include the facts which do not change over time, or whose values change only occasionally. We obtain this type of information from an RDF database Freebase [14]. 3 We also use the Microsoft Concept graph1 to recognize concepts belonging to the individual entities in user’s message. We expanded the Concept graph data with additional entries containing movies
2797206276	Alquist: The Alexa Prize Socialbot.	2107305284	ses can be generated using a recurrent neural network with sequence-to-sequence architecture [1, 8]. Taking advantage of semantic parser outputs such as predicate-argument structure was introduced in [9]. Statistical approaches were proposed to avoid handcrafting the rules for dialogue management (DM). Partially observed Markov decision process (POMDP) [10, 11] has been widely used as a DM since it c
2797206276	Alquist: The Alexa Prize Socialbot.	2129842875	used in the Structured Topic Dialogue about the news. This system works in three stages. The ﬁrst stage simpliﬁes the article’s sentences which is done by generating triples from the sentences using [24], and joining these triples into simpler sentences. The second stage generates questions from the new article using [25]. The generated question-answer pairs are stored in a database. Both of these st
2797206276	Alquist: The Alexa Prize Socialbot.	2123442489	tates of Structured Topic Dialogues. An example of such topic-speciﬁc analysis component is specialized entity recognition of book or movie titles. General analysis components We use Stanford CoreNLP [16] for natural language processing (NLP) tasks. The tool provides several annotators. Currently, we use annotators for sentence splitting, tokenization, part of speech tagging, dependency parsing, lemma
2797206276	Alquist: The Alexa Prize Socialbot.	1552847225	trings together with the nearest Wikipedia concept. Our version of the tool has two minor modiﬁcations. Firstly we include the Freebase ID in the database and thus eliminate a SPARQL query to DBpedia [21] to retrieve it. Secondly, we deleted all cross-wiki labels with the Levenshtein distance greater than 3 from the canonical label. Topic-speciﬁc analysis components The group of topic-speciﬁc analysis
2797306258	Learning Disentangled Representations of Texts with Application to Biomedical Abstracts	1878518397	. As a more general setting, we also consider learning directly from triplet-wise supervision concerning relative similarity with respect to particular aspects (Amid and Ukkonen,2015;Veit et al.,2017;Wilber et al., 2014). The assumption is that such judgments can be solicited directly from annotators, and thus the approach may be applied to arbitrary domains, so long as meaningful aspects can be deﬁned implicitly via
2797306258	Learning Disentangled Representations of Texts with Application to Biomedical Abstracts	2527896214	with 200 ﬁlters in each layer; window size of 5) and the PReLU activation function (He et al.,2015) as f e. We use 200d word embeddings, initialized via pretraining over a corpus of PubMed abstracts (Pyysalo et al., 2013). We used the Adam optimization function with default parameters (Kingma and Ba,2014). We imposed ‘ 2 regularization over all parameters, the value of which was selected from the range (1e-2, 1e-6) as
2797306258	Learning Disentangled Representations of Texts with Application to Biomedical Abstracts	2127368645	of these models derive from Latent Dirichlet Allocation (LDA) (Blei et al.,2003), and some variants have explicitly represented topics and aspects jointly for sentiment tasks (Brody and Elhadad,2010;Sauper et al., 2010,2011;Mukherjee and Liu,2012;Sauper and Barzilay,2013;Kim et al.,2013). A bit more generally, aspects have also been interpreted as properties spanning entire texts, e.g., a perspective or theme which
2797306258	Learning Disentangled Representations of Texts with Application to Biomedical Abstracts	2148830595	ning entire texts, e.g., a perspective or theme which may then color the discussion of topics (Paul and Girju,2010). This intuition led to the development of the factorial LDA family of topic models (Paul and Dredze, 2012;Wallace et al.,2014); these model individLook : deep amber hue , this brew is topped with a ﬁnger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , cor
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1886716020	3]. {WCEP: In the Wikipedia Current Events Portal, events are represented through rather brief textual descriptions and refer to daily happenings. We extract WCEP events using the WikiTimes interface [22]. We manually evaluated a random sample of the events identied in this step in DBpedia and Wikidata including 100 events per KG and language edition, achieving precision of 98% on average. Step Ib: U
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	804133461	d to increase recall. Some of the identied subclasses are blacklisted manually. {DBpedia [16]: For each language edition, we identify DBpedia events as instances of dbo:Event or its subclasses. YAGO [17]: We do not use the YAGO ontology for event identication due to the noisy event subcategories (e.g. event &gt; act &gt; activity &gt; protection &gt; self-defense &gt; martial art). YAGO events are i
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1552847225	ents. Extracting event-centric information: Most approaches for automatic knowledge graph construction and integration focus on entities and related facts rather than events. Examples include DBpedia [16], Freebase [2], YAGO [17] and YAGO+F [3]. In contrast, EventKG is focused on events and temporal relations. In [22], the authors extract event information from WCEP. EventKG builds upon this work to i
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1886716020	example, only 33% of the events in Wikidata provide temporal and 11:70% spatial information. Second, a variety of manually curated semi-structured sources (e.g. Wikipedia Current Events Portal (WCEP) [22] and multilingual Wikipedia event lists) contain information on contemporary events. However, the lack of structured representations of events and temporal relations in these sources hinders their dir
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2094728533	g event-centric information: Most approaches for automatic knowledge graph construction and integration focus on entities and related facts rather than events. Examples include DBpedia [16], Freebase [2], YAGO [17] and YAGO+F [3]. In contrast, EventKG is focused on events and temporal relations. In [22], the authors extract event information from WCEP. EventKG builds upon this work to include WCEP ev
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1886716020	ion focus on entities and related facts rather than events. Examples include DBpedia [16], Freebase [2], YAGO [17] and YAGO+F [3]. In contrast, EventKG is focused on events and temporal relations. In [22], the authors extract event information from WCEP. EventKG builds upon this work to include WCEP events. Extraction of events and facts from news: Recently, the problem of building knowledge graphs di
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2014772881	l for a variety of real-world applications in the elds of Semantic Web, NLP and Digital Humanities. In Semantic Web and NLP, these applications include Question Answering [14] and timeline generation [1]. In Digital Humanities, multilingual event repositories can facilitate cross-cultural studies that aim to analyze language-specic and communityspecic views on historical and contemporary events (ex
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2107315187	makes all this information available through a canonical representation. EventKG follows best practices in data publishing and reuses existing data models and vocabularies (such as Simple Event Model [23] and the DBpedia ontology) to facilitate its ecient reuse in real-world applications through the application of semantic technologies and open standards (i.e. RDF and SPARQL). EventKG currently inclu
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2107315187	mation and temporal relations for historical and contemporary events directly comparable to EventKG. The heterogeneity of data models and vocabularies for event-centric and temporal information (e.g. [12,19,20,23]), the large scale of the existing knowledge graphs, in which events play only an insignicant role, and the lack of clear identication of event-centric information, makes it particularly challenging
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1552847225	nt representations and temporal relations are spread across heterogeneous sources. First, large-scale knowledge graphs (KGs) (i.e. graph-based knowledge repositories [7] such as Wikidata [6], DBpedia [16], and YAGO [17]) typically focus on entity-centric knowledge. Event-centric information included in these sources is often not clearly identied as such, can be incomplete and is mostly restricted to
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2107315187	ntkg 13 https://opensource.org/licenses/MIT 14 Simon Gottschalk and Elena Demidova 8 Related Work Data models and vocabularies for events: Several data models and the corresponding vocabularies (e.g. [12,19,20,23]) provide means to model events. For example, the ECKG model proposed by Rospocher et al. [19] enables ne-grained textual annotations to model events extracted from news collections. The Simple Event
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	804133461	ntric information: Most approaches for automatic knowledge graph construction and integration focus on entities and related facts rather than events. Examples include DBpedia [16], Freebase [2], YAGO [17] and YAGO+F [3]. In contrast, EventKG is focused on events and temporal relations. In [22], the authors extract event information from WCEP. EventKG builds upon this work to include WCEP events. Extra
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	804133461	ons and temporal relations are spread across heterogeneous sources. First, large-scale knowledge graphs (KGs) (i.e. graph-based knowledge repositories [7] such as Wikidata [6], DBpedia [16], and YAGO [17]) typically focus on entity-centric knowledge. Event-centric information included in these sources is often not clearly identied as such, can be incomplete and is mostly restricted to named events an
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2107315187	originating from heterogeneous sources. { Provide provenance for the information included in EventKG. EventKG schema and the Simple Event Model: In EventKG, we build upon the Simple Event Model (SEM) [23] as a basis to model events. SEM is a exible 6 Simon Gottschalk and Elena Demidova data model that provides a generic event-centric framework. Within the EvenKG schema (namespace eventKG-s1), we adopt
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2014772881	s, e.g. \What are the most important events related to Syrian Civil War that took place in Aleppo?&quot; Relevance for timeline generation applications: Timeline generation is an active research area [1], where the focus is to generate a timeline (i.e. a chronologically ordered selection) of events and temporal relations for entities from a knowledge graph. EventKG can facilitate the generation of de
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2107315187	tegy to maintain its URIs across the versions, ensuring that the same URIs are consistently reused for the same real-world objects. EventKG reuses and extends an established event model, which is SEM [23] to describe event-related information it includes, and reuses existing vocabularies (e.g. DBpedia ontology, Dublin Core). The EventKG metadata is provided using the VoID7 vocabulary. EventKG follows
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	1552847225	vents as subclasses of Wikidata’s \event&quot; and \occurrence&quot;. The \occurrence&quot; instances are added to increase recall. Some of the identied subclasses are blacklisted manually. {DBpedia [16]: For each language edition, we identify DBpedia events as instances of dbo:Event or its subclasses. YAGO [17]: We do not use the YAGO ontology for event identication due to the noisy event subcatego
2797309124	EventKG: A Multilingual Event-Centric Temporal Knowledge Graph	2169022614	work to include WCEP events. Extraction of events and facts from news: Recently, the problem of building knowledge graphs directly from plain text news [19], and extraction of named events from news [15] have been addressed. These approaches apply Open Information Extraction methods and develop them further to address specic challenges in the event extraction in the news domain. State-of-the-art wor
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2084046180,2128668540,2163114435	(SVM) are widely used in this problem [10,15,27]. The majority of these approaches either rely on n-gram features or manually designed features. Multiple sentiment lexicons are built for this purpose [14,18,22]. In the recent years, sentiment classiﬁcation has been advanced by neural networks signiﬁcantly. Neural network based approaches automatically learn feature representations and do not require intensi
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2516196286	aspects and texts simultaneously using LSTMs. Furthermore, the target representation and text representation generated from LSTMs interact with each other by an attention-over-attention (AOA) module [2]. AOA automatically generates mutual attentions not only from aspect-to-text but also text-to-aspect. This is inspired by the observation that only few words in a sentence contribute to the sentiment
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2465978385	is why we choose AOA to attend to the most important parts in both aspect and sentence. Compared to previous methods, our model performs better on the laptop and restaurant datasets from SemEval 2014 [17] 2 Related work Sentiment Classiﬁcation Sentiment classiﬁcation aims at detecting the sentiment polarity for text. There are various approaches proposed for this research question [12]. Most existing
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	1879966306	e engineering. Researchers proposed a variety of neural network architectures. Classical methods include Convolutional Neural Networks [7], Recurrent Neural Networks [9,24], Recursive Neural Networks [19,29]. These approaches have achieved promising results on sentiment analysis. Aspect Level Sentiment Classiﬁcation Aspect level sentiment classiﬁcation is a branch of sentiment classiﬁcation, the goal of
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2052935438,2148506018	ion, the goal of which is to identify the sentiment polarity of one speciﬁc aspect in a sentence. Some early works designed several rule based models for aspect level sentiment classiﬁcation, such as [3,13]. Nasukawa et al. ﬁrst perform dependency parsing on sentences, then they use predeﬁned rules to determine the sentiment about aspects [13]. Jiang et al. improve the target-dependent sentiment classiﬁ
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2250539671	nd all bias terms are set to zero. The L 2 regularization coefﬁcient is set to 10 4 and the dropout keep rate is set to 0.2[20]. The word embeddings are initialized with 300-dimensional Glove vectors [16] and are ﬁxed during training. For the out of vocabulary words we initialize them randomly from uniform distribution U( 0:01;0:01). The dimension of LSTM hidden states is set to 150. The initial learn
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2019759670	om SemEval 2014 [17] 2 Related work Sentiment Classiﬁcation Sentiment classiﬁcation aims at detecting the sentiment polarity for text. There are various approaches proposed for this research question [12]. Most existing works use machine learning algorithms to classify texts in a supervision fashion. Algorithms like Naive Bayes and Support Vector Machine(SVM) are widely used in this problem [10,15,27]
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2064675550	rget. Each Bi-LSTM is obtained by stacking two LSTM networks. The advantage of using LSTM is that it can avoid the gradient vanishing or exploding problem and is good at learning long-term dependency [5]. With an input s= [v 1;v 2;:::;v n] and a forward LSTM network , we generate a sequence of hidden states ! h s 2Rn d h, where d h is the dimension of hidden states. We generate another state sequence
2797364912	Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks	2516196286	tic representations of the text and the aspect target generated by Bi-LSTMs, we calculate the attention weights for the text by an AOA module. This is inspired by the use of AOA in question answering [2]. Given the target representation h t 2Rm 2d h and sentence representation h s 2Rn 2d h, we ﬁrst calculate a pair-wise interaction matrix I= h s hT t , where the value of each entry represents the cor
2797453758	What Happened? Leveraging VerbNet to Predict the Effects of Actions in Procedural Text.	2738015883	ey require extensive training data, and can still struggle with queries requiring complex inference (Hermann et al.,2015). The extent to which these systems truly understand language remains unclear (Jia and Liang, 2017). More recently, several neural systems have been developed for reading procedural text. Building on the general Memory Network architecture (Weston et al.,2014) and gated recurrent models such as GRU
2797498228	NEURAL MODELS FOR REASONING OVER MULTIPLE MENTIONS USING COREFERENCE	2473344385	for comparison to prior work. yp&lt;0:0001 using Mcnemar’s test compared to GA w/ GRU. LAMBADA dataset. Our last set of experiments is on the broad-context language modeling task of LAMBADA dataset (Paperno et al., 2016). This dataset consists of passages 4-5 sentences long, where the last word needs to be predicted. Interestingly, though, the passages are ﬁltered such that human volunteers were able to predict the m
2797583228	Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.	2407023693	7, and contained 64,727 utterances from 1,881 speakers. Training the default convolution model from the TensorFlow tutorial (based on Convolutional Neural Networks for Small-footprint Keyword Spotting[19]) using the V1 training data gave a TopOne score of 85.4%, when evaluated against the test set from V1. Training the same model against version 2 of the dataset[1], documented in this paper, 8 produce
2797583228	Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.	2038484192	ion of neural network operations for ARM microcontrollers, and uses Speech Commands to train and evaluate the results. Listening to the World[22] demonstrates how combining the dataset and UrbanSounds[23] can improve the noise tolerance of recognition models. Did you Hear That[24] uses the dataset to test adversarial attacks on voice interfaces. Deep Residual Learning for Small Footprint Keyword Spott
2797583228	Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.	1494198834	lar to public domain). This licensing makes it very easy to build on top of. It is aligned by sentence, and 1 was created by volunteers reading requested phrases through a web application. LibriSpeech[7] is a collection of 1,000 hours of read English speech, released under a Creative Commons BY 4.0 license, and stored using the open source FLAC encoder, which is widely supported. Its labels are align
2797583228	Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.	2108598243	s matured, the number of people who want to train and evaluate recognition models has grown beyond these traditional groups, but the availability of datasets hasn’t widened. As the example of ImageNet[4] and similar collections in computer vision has shown, broadening access to datasets encourages collaborations across groups and enables apples-for-apples comparisonsbetween diﬀerent approaches, helpi
2797585226	DISCOURSE-AWARE NEURAL REWARDS FOR COHERENT TEXT GENERATION	2136189984	re 2: The teacher encodes the sentences of the document in the forward and reverse order. 2.2 Absolute Order Teacher The ﬁrst teacher explored is motivated by work on deep semantic similarity models (Huang et al., 2013), which approximated the similarity between queries and documents in information retrieval tasks. We extend this approach to modeling temporal patterns by training a sentence encoder to minimize the s
2797585226	DISCOURSE-AWARE NEURAL REWARDS FOR COHERENT TEXT GENERATION	2560313346	ces text that is more coherent and less repetitive than models trained with cross-entropy or reinforcement learning with other commonly used scores. 2 Neural Teachers Recent work in image captioning (Rennie et al., 2017), machine translation (Wu et al.,2016), and summarization (Paulus et al.,2018) has investigated using policy gradient methods to ﬁne-tune neural generation models using automatic measures such as CIDE
2797585226	DISCOURSE-AWARE NEURAL REWARDS FOR COHERENT TEXT GENERATION	2607151106	es a neural teacher to reward a model for capturing discourse semantics. Most similar to our work is work on using neural and embedding rewards to improve dialogue (Li et al.,2016), image captioning (Ren et al., 2017), simpliﬁcation (Zhang and Lapata,2017), and paraphrase generation (Li et al.,2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to r
2797585226	DISCOURSE-AWARE NEURAL REWARDS FOR COHERENT TEXT GENERATION	2101105183	on et al., 2016;Yang et al.,2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin,2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al.,2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al.,2016;Ranzato et al.,20
2797585226	DISCOURSE-AWARE NEURAL REWARDS FOR COHERENT TEXT GENERATION	2560313346	t al.,2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al.,2016;Ranzato et al.,2015;Paulus et al.,2018;Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams,1992). Work done while author was at Microsoft Research Model Teacher Reward Wash the tomatoes and cut them length-wise.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2153982529	[12] built the text simplification system closely related with WordNet to obtain the word pairs as hypernym or synonym.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	32283444	[7] compiled the corpus based on the lexical substitution at SemEval-2007 with manually annotation.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2132767156	Abstract—Unlike previous unknown nouns tagging task [1] and [2], this is the first attempt to focus on out-of-vocabulary (OOV) lexical evaluation tasks that does not require any prior knowledge.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2610780044	As an attempt to solve the limitation of Word2Vec mentioned above, the word embedding with Gaussian mixtures (word2gm) [4] tried to represent each word with a Gaussian mixture in high-dimensional space R:
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2132767156	Based on supersense, [1] ar X iv :1 80 4.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	1858576077	Moreover, the concept of OOV handling is closely related with the emerging ”zero shot learning (ZSL)” [5] in other machine learning studies.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2119954850,2128514324	Although in NLP study there are various semantic evaluation tasks for computational semantic analysis such as [6]–[10], the majority of current evaluation tasks focus on word sense disambiguation and text simplification.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2132767156	In previous unknown nouns supersense tagging, [1] and [2], rule-based models were proposed within the scope of WordNet.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2153579005	Recently, [3] proposed two models (skip-gram and continuous bag-of-words) named Word2Vec for effective learning representation.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2128514324	Second, most of the test words and the candidates in lexical substitution tasks such as [6]are daily words.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2164019165,2610780044	After that, other related models [4], [18]–[21] have been inspired such as probabilistic model, Gaussian and Gaussian mixture models and so on.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2153579005,2610780044	Then, we utilize unsupervised word embedding methods such as Word2Vec [3] and Word2GM [4] to perform the baseline experiments on the categorical classification task and OOV words attribute prediction tasks.
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2610780044	It is yet unclear how to properly choose the number of Gaussians per word K, the boundaries of ‖μ‖2 and Σ and their initial values, thus we simply borrowed the model from the original paper of Word2GM [4].
2797671597	English Out-of-Vocabulary Lexical Evaluation Task.	2026185168	V words may not impede the performance in their tasks, while in our proposed task, the outcome is directly related with the OOV words. In previous unknown nouns supersense tagging,(Curran ,2005) and (Ciaramita and Johnson 2003), rule-based models were proposed within the scope of WordNet. The main limitations are: The models are heavily dependent on some particular features of the unknown words such as the part of speech t
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2513380446	’s goal for effective integration with knowledge bases (KBs). Robust dialogue state tracking has been shown (Jurˇc´ıˇcek et al. ,2012) to be critical in improving dialogue success in task completion. Dhingra et al. (2017) proposed an end-to-end RL dialogue agent for information access. Their model focuses on bringing differentiability to the KB query operation by introducing a “soft” retrieval process in selecting the
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2534274346	2017) proposed a taskoriented dialogue model using end-to-end memory networks. In the same line of research, people explored using query-regression networks (Seo et al., 2016), gated memory networks (Liu and Perez, 2017), and copy-augmented networks (Eric and Manning,2017) to learn the dialogue state. These systems directly select a ﬁnal response from a list of response candidates conditioning on the dialogue history
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2403702038	combination of supervised and deep RL methods, as it is shown that RL may effectively improve dialogue success rate by exploring a large dialogue action space (Henderson et al.,2008;Li et al.,2017). Bordes and Weston (2017) proposed a taskoriented dialogue model using end-to-end memory networks. In the same line of research, people explored using query-regression networks (Seo et al., 2016), gated memory networks (Liu a
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2311783643	e in designing end-to-end solutions for task-oriented dialogues, inspired by the success of encoder-decoder based neural network models in non-task-oriented conversational systems (Serban et al.,2015;Li et al., 2016). Wen et al. (Wen et al.,2017) designed an end-to-end trainable neural dialogue model with modularly connected system components. This system is a supervised learning model which is evaluated on ﬁxed
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	1522301498	nd 150 respectively. Word embedding size is 300. Embedding size for system action and slot values is set as 32. Hidden layer size of the policy network is set as 100. We use Adam optimization method (Kingma and Ba, 2014) with initial learning rate of 1e-3. Dropout rate of 0.5 is applied during supervised training to prevent the model from over-ﬁtting. In imitation learning, we perform mini-batch model update after co
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2772001136	ne from scratch typically requires a large number of interactive learning sessions before an agent can reach a satisfactory performance level. Recent works (Henderson et al.,2008;Williams et al.,2017;Liu et al., 2017) explored pre-training the dialogue model using human-human or human-machine dialogue arXiv:1804.06512v1 [cs.CL] 18 Apr 2018 corpora before performing interactive learning with RL to address this conc
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2403702038	ntional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods. Both supervised learning (SL) based (Wen et al.,2017;Bordes and Weston, 2017;Liu and Lane,2017a) and deep reinforcement learning (RL) based systems (Zhao and Eskenazi,2016;Li et al.,2017;Peng et al.,2017) have been studied in the literature. Comparing to chit-chat dialogue mo
2797760463	DIALOGUE LEARNING WITH HUMAN TEACHING AND FEEDBACK IN END-TO-END TRAINABLE TASK-ORIENTED DIALOGUE SYSTEMS	2403702038	such as user models. Once the KB query results are returned, we save the retrieved entities to a queue and encode the result summary to a vector. Rather then encoding the real KB entity values as in (Bordes and Weston, 2017;Eric and Manning,2017), we only encode a summary of the query results (i.e. item availability and number of matched items). This encoding serves as a part of the input to the policy network. 3.4 Dial
2797790703	Natural Language Statistical Features of LSTM-generated Texts	2139501017	, and one of the major successes has been that of generative models [4]. In the area of NLG, many studies have been dedicated to speciﬁc, focused applications, such as image and video captioning [5], [6], poem synthesis [7] or lyric generation [8]. In all these cases, the considered generated texts are relatively short (captions, poems, lyrics) These two authors contributed equally. Marco Lippi is wi
2797790703	Natural Language Statistical Features of LSTM-generated Texts	2087148916	. Statistical measures that capture structure at the sequence level in texts involve correlations and spectral analysis [27]. Correlations in language are known to be of the power-law type [9], [10], [11], decaying as C( ˝) / , where is the distance between symbols – e.g. words or characters. It is possible to characterize the structure of long-range correlations using the method of Detrended Fluctuat
2797790703	Natural Language Statistical Features of LSTM-generated Texts	2736889448	capturing correlations that Markov models instead fail to represent, yet the range of correlations they consider is still quite limited (up to 1,000 characters). Conversely, Takahashi and TanakaIshii [16] reported that LSTM language models have limitation in reproducing such long-range correlations if measured with a method based on clustering properties of rare words; note however that their analysis
2797790703	Natural Language Statistical Features of LSTM-generated Texts	1895577753	d that is widely used for the evaluation of artiﬁcial texts, is perplexity [39], which can be computed as the geometric mean of the inverse probability for each predicted word in the a document [40], [41], where probabilities are typically estimated on a language model trained on a larger corpus. D. Creativity and Authorship Attribution Providing a quantitative method able to address the creativity of
2797790703	Natural Language Statistical Features of LSTM-generated Texts	1905882502	mains, and one of the major successes has been that of generative models [4]. In the area of NLG, many studies have been dedicated to speciﬁc, focused applications, such as image and video captioning [5], [6], poem synthesis [7] or lyric generation [8]. In all these cases, the considered generated texts are relatively short (captions, poems, lyrics) These two authors contributed equally. Marco Lippi
2797790703	Natural Language Statistical Features of LSTM-generated Texts	2087148916	Natural language has been widely studied within this context, and notoriously it shows statistical properties in the distribution of terms, as well as long-range correlations between words [9], [10], [11]. In comparison to short texts such as captions, this is a much more challenging setting to imitate for machines. In this work, we aim to provide an extensive empirical evaluation of texts generated w
2797790703	Natural Language Statistical Features of LSTM-generated Texts	1645937837	us on the problem of Natural Language Generation (NLG), which encompasses the capability of machines to synthesize text in a way that resembles spoken or written language typically employed by humans [2]. This research ﬁeld has recently known a period of great excitement, mostly due to the huge development in the area of deep learning [3], whose methods and algorithms have certainly contributed to mo
2797790703	Natural Language Statistical Features of LSTM-generated Texts	1689711448	regained popularity, being now widely used in a huge number of research and industrial applications, including automatic machine translation, speech recognition, text-to-speech generation (e.g., see [21] and references therein). A. General framework RNNs allow to process sequences of arbitrary lengths, by exploiting Lhidden layers h‘ t , with ‘ = f1;:::;Lgwhose cells are functions not only of the lay
2797790703	Natural Language Statistical Features of LSTM-generated Texts	2115221470	or successes has been that of generative models [4]. In the area of NLG, many studies have been dedicated to speciﬁc, focused applications, such as image and video captioning [5], [6], poem synthesis [7] or lyric generation [8]. In all these cases, the considered generated texts are relatively short (captions, poems, lyrics) These two authors contributed equally. Marco Lippi is with the Department of
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2251332263	annotation is treated as a separate training instance. We also include as training data the exemplar sentences, each annotated for a single target, as they have been reported to improve performance (Kshirsagar et al., 2015;Yang and Mitchell,2017). For semantic dependencies, we use the English DM dataset from the SemEval 2015 Task 18 closed track (Oepen et al.,2015).10 DM contains instances from the WSJ corpus for train
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2001751238	ED REGION, ACCESSIBILITY, DEIXIS, etc.) which are not realized in this sentence. In this work, we assume gold targets and LUs are given, and parse each target independently, following the literature (Johansson and Nugues, 2007;FitzGerald et al.,2015;Yang and Mitchell, 2017;Swayamdipta et al.,2017, interalia). Moreover, followingYang and Mitchell(2017), we perform frame and argument identiﬁcation jointly. Most prior work ha
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2609782806	ic dependency graphs, the FULL model outperforms the baselines by more than 0.6% absolute F 1 scores under both settings. Previous state-of-the-art results on DM are due to the joint learning model ofPeng et al. (2017), denoted as NeurboParser (FREDA3). They adopted a multitask learning approach, jointly predicting three different parallel semantic dependency annotations. Our FULL model’s in-domain test performance
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	1810532455	ivergent formalisms, one dealing with semantic spans and the other with semantic arXiv:1804.05990v1 [cs.CL] 17 Apr 2018 dependencies. We experiment on frame-semantic parsing (Gildea and Jurafsky,2002;Das et al., 2010), a span-based semantic role labeling (SRL) task (x2.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM;Flickinger et al.,2012) task (x2.2). See Figure1for an exampl
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2251199578	ned as graphs from the start (Hajiˇc et al. ,2012;Banarescu et al., 2013), and have no gold alignment to spans. Conversely, some span-based formalisms are not annotated with syntax (Baker et al.,1998;He et al., 2015),3 and so head rules would require using (noisy and potentially expensive) predicted syntax. Inspired by the head rules ofSurdeanu et al. (2008), we design cross-task parts, without relying 3 In Frame
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2252123671	ns and dependencies. For many other semantic representations, such a direct relationship might not be present. Some semantic representations are designed as graphs from the start (Hajiˇc et al. ,2012;Banarescu et al., 2013), and have no gold alignment to spans. Conversely, some span-based formalisms are not annotated with syntax (Baker et al.,1998;He et al., 2015),3 and so head rules would require using (noisy and poten
2797849802	LEARNING JOINT SEMANTIC PARSERS FROM DISJOINT DATA	2738152205	rd embedding for each token: h i= ! h i; h i : (7) 7Semantic dependency parses over a sentence are not constrained to be identical for different frame-semantic targets. Span representations. FollowingLee et al. (2017), span representations are computed based on boundary word representations and discrete length and distance features. Concretely, given a target tand its associated argument a= (i;j;r) with boundary i
2797859625	ARE AUTOMATIC METHODS FOR COGNATE DETECTION GOOD ENOUGH FOR PHYLOGENETIC RECONSTRUCTION IN HISTORICAL LINGUISTICS	2008225289	Following List et al. (2017b), we set the threshold for LexStat-Infomap to 0.55. The OnlinePMI approach (Rama et al., 2017) estimates the sound-pair PMI matrix using the online procedure described in Liang and Klein (2009). The approach starts with an empty PMI matrix and a list of synonymous word pairs from all the language pairs. The approach proceeds by calculating the PMI matrix from alignments calculated for each
2797913374	Investigating Backtranslation in Neural Machine Translation	2626778328	anslated data by directly comparing 3http://pytorch.org 4http://opennmt.net/Models/ the translation quality of the resulting NMT systems; and (ii) while certain new architectures such as Transformer [Vaswani et al., 2017] or different settings might obtain even better results, our goal here is not to build the absolutely best possible systems, but rather use conﬁgurations that are representative of what is used in th
2797913374	Investigating Backtranslation in Neural Machine Translation	2130942839	ative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. to-English, and analyse the resulting translation performance. 1 Introduction Neural Machine Translation (NMT) [Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015] is a relatively new machine translation (MT) paradigm that has quickly become dominant in both academic and industry MT communities, achieving state-of-the-art results [Bentiv
2797913374	Investigating Backtranslation in Neural Machine Translation	2133564696	e, no derivative works, attribution, CCBY-ND. to-English, and analyse the resulting translation performance. 1 Introduction Neural Machine Translation (NMT) [Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015] is a relatively new machine translation (MT) paradigm that has quickly become dominant in both academic and industry MT communities, achieving state-of-the-art results [Bentivogli et al., 2016; Boja
2797913374	Investigating Backtranslation in Neural Machine Translation	2574872930	ents all come from the Translation Task of the Tenth Workshop on Machine Translation in 2015 (WMT 2015 [Bojar et al., 2015]).2 To build our NMT systems we use OpenNMT-py (the pytorch port of OpenNMT [Klein et al., 2017]) with standard settings that allows for easy replicability of our experiments. The remainder of the paper is structured as follows: Section 2 presents related work on using back-translated and other
2797913374	Investigating Backtranslation in Neural Machine Translation	2130942839	lation task. It consist of a collection of 2169 sentences from the news domain. These sentences have also been tokenized and truecased. 5 Experimental set-up We train sequence-to-sequence NMT models [Sutskever et al., 2014] based on recurrent neural networks with an attention mechanism [Bahdanau et al., 2015; Luong et al., 2015]. The NMT framework we use is OpenNMT [Klein et al., 2017] and in particular its pytorch3 po
2797913374	Investigating Backtranslation in Neural Machine Translation	2101105183	lts Tables 2 and 3 show the evaluation scores of the models we trained for the authentic-to-hybrid and authentic-to-synthetic cases, respectively. We use a number of common evaluation metrics – BLEU [Papineni et al., 2002], TER [Snover et al., 2006], METEOR [Banerjee and Lavie, 2005], and CHRF [Popovic, 2015] – to give a more comprehensive estimation of the comparative translation quality. With the exception of TER, t
2797913374	Investigating Backtranslation in Neural Machine Translation	2133564696	ntences have also been tokenized and truecased. 5 Experimental set-up We train sequence-to-sequence NMT models [Sutskever et al., 2014] based on recurrent neural networks with an attention mechanism [Bahdanau et al., 2015; Luong et al., 2015]. The NMT framework we use is OpenNMT [Klein et al., 2017] and in particular its pytorch3 port. Our set-up follows the OpenNMT guidelines,4 that indicate that the default training
2797913374	Investigating Backtranslation in Neural Machine Translation	2525778437	paradigm that has quickly become dominant in both academic and industry MT communities, achieving state-of-the-art results [Bentivogli et al., 2016; Bojar et al., 2016; Junczys-Dowmunt et al., 2016; Wu et al., 2016; Castilho et al., 2017; Shterionov et al., 2017] on a range of language pairs and domains. As a corpus-based paradigm, the translation quality strongly depends on the quality and quantity of the trai
2797913374	Investigating Backtranslation in Neural Machine Translation	2625092622	and quantity of the training data provided. In comparison to statistical machine translation (SMT) [Koehn, 2010], NMT typically requires more data to build a system with good translation performance [Koehn and Knowles, 2017]. In many use-cases, however, the amount of good-quality parallel data available is insufﬁcient to reach the translation standard required. In such cases, it has become the norm to resort to back-tra
2797913374	Investigating Backtranslation in Neural Machine Translation	2574872930	quence-to-sequence NMT models [Sutskever et al., 2014] based on recurrent neural networks with an attention mechanism [Bahdanau et al., 2015; Luong et al., 2015]. The NMT framework we use is OpenNMT [Klein et al., 2017] and in particular its pytorch3 port. Our set-up follows the OpenNMT guidelines,4 that indicate that the default training conﬁguration is reasonable for training a German-to-English model on WMT 2015
2797913374	Investigating Backtranslation in Neural Machine Translation	2123301721	we trained for the authentic-to-hybrid and authentic-to-synthetic cases, respectively. We use a number of common evaluation metrics – BLEU [Papineni et al., 2002], TER [Snover et al., 2006], METEOR [Banerjee and Lavie, 2005], and CHRF [Popovic, 2015] – to give a more comprehensive estimation of the comparative translation quality. With the exception of TER, the higher the score, the better the translation is estimated t
2797913374	Investigating Backtranslation in Neural Machine Translation	2149327368	valuation scores of the models we trained for the authentic-to-hybrid and authentic-to-synthetic cases, respectively. We use a number of common evaluation metrics – BLEU [Papineni et al., 2002], TER [Snover et al., 2006], METEOR [Banerjee and Lavie, 2005], and CHRF [Popovic, 2015] – to give a more comprehensive estimation of the comparative translation quality. With the exception of TER, the higher the score, the be
2797913374	Investigating Backtranslation in Neural Machine Translation	2527845440	y new machine translation (MT) paradigm that has quickly become dominant in both academic and industry MT communities, achieving state-of-the-art results [Bentivogli et al., 2016; Bojar et al., 2016; Junczys-Dowmunt et al., 2016; Wu et al., 2016; Castilho et al., 2017; Shterionov et al., 2017] on a range of language pairs and domains. As a corpus-based paradigm, the translation quality strongly depends on the quality and qua
2798081680	WHEN AND WHY ARE PRE-TRAINED WORD EMBEDDINGS USEFUL FOR NEURAL MACHINE TRANSLATION?	2525778437	neural network models for NLP tasks such as sequence tagging (Lample et al., 2016;Ma and Hovy,2016) and text classiﬁcation (Kim,2014). However, it is much less common to use such pre-training in NMT (Wu et al., 2016), largely because the large-scale training corpora used for tasks such as WMT2 tend to be several orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treeban
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2159636675	r 2018 and almost all parsers ignore them.1 Despite the sophisticated analysis of gapping within CCG (Steedman, 1990), sentences with gapping were deemed too difﬁcult to represent within the CCGBank (Hockenmaier and Steedman, 2007). Similarly the treebanks for the Semantic Dependencies Shared Task (Oepen et al., 2015) exclude all sentences from the Wall Street Journal that contain gapping. Finally, while the tectogrammatical la
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	1905555481	And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predicates in sentenc
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2735411217	atrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (201
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2250539671	be captured by penalizing mismatching POS tags, and the other two by computing the distance between argument embeddings. We compute these embeddings by averaging over the 100- dim. pretrained GloVe (Pennington et al., 2014) embeddings for each token in the argument. Given the POS tags t g and t f and the argument embeddings v g and v f, sim is deﬁned as follows.5 sim(g;f) = k v g v fk 2 +1 [t g = t f] pos_mismatch_penal
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2141184766	constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predica
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2138607469	curacy of ACCs, it is speciﬁc to ACCs and does not generalize to other gapping constructions. Moreover, they did not reconstruct gapped ACC clauses. Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms would be needed for reconstruction. In practice, though, dealing with gapping in a g
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2123311362	d only limited coverage. There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-process
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2507121281	e system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructio
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2153579005	grained and universal part-of-speech tags, for which we used the tagger by Dozat et al. (2017). We trained the tagger on the COMBINED training corpus. As pre-trained embeddings, we used the word2vec (Mikolov et al., 2013) embeddings that were provided for the CoNLL 2017 Shared Task (Zeman et al., 2017), and we used the same hyperparameters as Dozat et al. (2017). Evaluation We evaluated the parseability of the two dep
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2251641918	ing to existing nodes would not work in the case of verb clusters because they do not satisfy the subtree constraint. 3 Methods 3.1 Composite relations Our ﬁrst method adapts one of the procedures by Seeker et al. (2012), which represents gaps in dependency trees by attaching dependents of an elided predicate with composite relations. These relations represent the dependency path that would 3To enhance the readabilit
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2132893218	isms and provided only limited coverage. There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2149726194	lations, using the the COMPOSITE procedure would require additional manual annotations in practice. 6 Related work Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2140270230	nce sv-ud-train-1102as output by the ORPHAN procedure. The system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for pheno
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2140270230	ncy trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins, tänks Ullnaområdet öka med 9
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2735411217	to the output of a parser, which often fails to identify gapping, our methods achieve a sentence-level accuracy of 32% and 34%, signiﬁcantly outperforming the recently proposed constituent parser by Kummerfeld and Klein (2017). 2 Background 2.1 Gapping constructions Gapping constructions in English come in many forms that can be broadly classiﬁed as follows. 1 To the best of our knowledge, the parser by Kummerfeld and Klei
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2251641918	practice. 6 Related work Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties befor
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2140436026	tput by the ORPHAN procedure. The system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2251641918	truct the elided material. The methods differ in how much information is encoded in the dependency tree. The ﬁrst method adapts an existing procedure for parsing sentences with elided function words (Seeker et al., 2012), which uses composite labels that can be deterministically turned into dependency graphs in most cases. The second method is a novel procedure that relies on the parser only to identify a gap, and th
2798130592	SENTENCES WITH GAPPING: PARSING AND RECONSTRUCTING ELIDED PREDICATES	2168671000	zing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover
2798132978	NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning.	359818833,2171468534	at-map visualization. The color intensity corresponds to the weight given to each word by the self-attention mechanism. and Hurtado,2014;Tumasjan et al.,2010), stock market monitoring (Si et al.,2013;Bollen et al., 2011b) etc. The wide usage of ﬁgurative language, such as emojis and special language forms like abbreviations, hashtags, slang and other social media markers, which do not align with the conventional lan
2798132978	NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning.	2108598243	atch and instead are initialized with pretrained models. Notable examples include face recognition (Taigman et al.,2014) and visual QA (Agrawal et al.,2017), where image features trained on ImageNet (Deng et al., 2009) and word embeddings estimated on large corpora via unsupervised training are combined. Although model transfer has seen widespread success in computer vision, transfer learning beyond pretrained word
2798132978	NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning.	2397535132	etric between t i and w. The seed words t i are selected separately for each dimension, from the words available in the original manual annotations (see2.2). The S(_) metric is estimated as shown in (Palogiannidi et al., 2015) using word-level contextual feature vectors and adopting a scheme based on mutual information for feature weighting. Manually annotated norms. To generate affective norms, we need to start from some
2798132978	NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning.	2153579005	mbeddings, to initialize the ﬁrst layer (embedding layer) of our neural networks. Word2vec Embeddings. We leverage our unlabeled dataset to train Twitter-speciﬁc word embeddings. We use the word2vec (Mikolov et al., 2013) algorithm, with the skip-gram model, negative sampling of 5 and minimum word count of 20, utilizing Gensim’s (Rehu˚ˇ ˇrek and Sojka ,2010) implementation. The resulting vocabulary contains 800;000 wo
2798139452	ADVERSARIAL EXAMPLE GENERATION WITH SYNTACTICALLY CONTROLLED PARAPHRASE NETWORKS	1945616565	nguistic variation, which can hurt the generalization of models trained on them. Recent work has shown it is possible to easily break many learned models by evaluating them on adversarial examples (Goodfellow et al., 2015), which are generated by manually introducing lexical, pragmatic, and syntactic variation not seen in the training set (Ettinger et al.,2017). Robustness to such adversarial examples can potentially b
2798139452	ADVERSARIAL EXAMPLE GENERATION WITH SYNTACTICALLY CONTROLLED PARAPHRASE NETWORKS	2622000134	rge amount of paraphrase pairs for training, and (2) dening syntactic transformations with which to label these pairs. Since no large-scale dataset of sentential paraphrases exists publicly, we followWieting et al. (2017) and automatically generate millions of paraphrase pairs using neural backtranslation . Backtranslation naturally injects linguistic variation between the original sentence and its backtranslated coun
2798139452	ADVERSARIAL EXAMPLE GENERATION WITH SYNTACTICALLY CONTROLLED PARAPHRASE NETWORKS	2123442489	sible. Our key insight is that target transformations can be detected (with some noise) simply by parsing these pairs. 3 Specically, we parse the backtranslated paraphrases using the Stanford parser (Manning et al., 2014), 4 which yields a pair of constituency parses hp1 ;p2 i foreachsentencepair hs1 ;s2 i,where s1 is the reference English sentence in the CzEng corpus and s2 is its backtranslated counterpart. For synt
2798240283	A DISCOURSE-AWARE ATTENTION MODEL FOR ABSTRACTIVE SUMMARIZATION OF LONG DOCUMENTS	2130942839	put x0 t, decoder hidden state h(d) t , and some information about the input sequence. This framework is the general seq2seq framework employed in many generation tasks including machine translation (Sutskever et al., 2014;Bahdanau et al., ) and summarization (Nallapati et al. ,2016;Chopra et al. ). Attentive decoding The attention mechanism maps the decoder state and the encoder states to an output vector, which is a
2798269150	Harvesting Paragraph-level Question-Answer Pairs from Wikipedia	1544827683	ed task of answering cloze questions (Winograd,1972;Levesque et al., 2011). To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used. The (semi-)synthetic datasets (e.g.,Hermann et al. (2015)) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (Rajpurkar et al., 2016). In comparison, high-quality crowdsourced datasets are
2798269150	Harvesting Paragraph-level Question-Answer Pairs from Wikipedia	1544827683	nswer pairs. We also provide a qualitative analysis for this large-scale generated corpus from Wikipedia. 1 Introduction Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015;Rajpurkar et al.,2016;Joshi et al.,2017) with the goal of developing systems that can answer questions about the content of a given passage or document. Large-scale QA datasets are indispensable for
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2129657639	-based In the absence of large-scale data for training coreference models, early systems relied heavily on expert knowledge. A frequently used example of this is the Stanford multi-pass sieve system (Lee et al., 2011). A deterministic system, the sieve consists of multiple rule-based models which are applied in succession, from highest-precision to lowest. Gender is among the set of mention attributes identiﬁed in
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2129657639	(dotted). Regression line and 95% conﬁdence interval in blue. Pearson r = 0.67. 4 Results and Discussion We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neu
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2893425640	on 2 participants 3 pronoun genders. Validation Like Winograd schemas, each sentence template is written with one intended correct answer (here, either OCCUPATION or PAR450 are from the supplement of Caliskan et al. (2017), an additional 7 from personal communication with the authors, and three that we added: doctor, ﬁreﬁghter, and secretary. (1a) Theparamedic performed CPR on the passenger even though she/he/they knew
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2099115159	in bias and have the potential to introduce bias into the system. Noun Gender and Number Many coreference resolution systems, including those described here, make use of a common resource released by Bergsma and Lin (2006)3 (“B&amp;L”): a large list of English nouns and noun phrases with gender and 3This data was distributed in the CoNLL 2011 and 2012 shared tasks on coreference resolution. (Pradhan et al., 2011, 2012)
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2099115159	chemas was much higher to begin with (94.9% Winogender vs. 86.5% Winograd). 0 20 40 60 80 100 % Female by Occupation (Bureau of Labor Stats, 2015-16) 0 20 40 60 80 100 % Female by Occupation in Text (Bergsma and Lin, 2006) Figure 3: Gender statistics from Bergsma and Lin (2006) correlate with Bureau of Labor Statistics 2015. However, the former has systematically lower female percentages; most points lie well below the
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	1599016936	cupation based on pronoun gender, as observed in Figure 1. To this end, we create a specialized evaluation set consisting of 120 hand-written sentence templates, in the style of the Winograd Schemas (Levesque et al., 2011). Each sentence contains three referring expressions of interest: 1. OCCUPATION , a person referred to by their occupation and a deﬁnite article, e.g., “the paramedic.” 2. PARTICIPANT , a secondary (h
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2101268022	d by Bergsma and Lin (2006)3 (“B&amp;L”): a large list of English nouns and noun phrases with gender and 3This data was distributed in the CoNLL 2011 and 2012 shared tasks on coreference resolution. (Pradhan et al., 2011, 2012) number counts over 85GB of web news. For example, according to the resource, 9.2% of mentions of the noun “doctor” are female. The resource was compiled by bootstrapping coreference informatio
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	1599016936	English speakers, we validated all 720 sentences on Mechanical Turk, with 10-way redundancy. Each MTurk task included 5 sentences from our dataset, and 5 sentences from the Winograd Schema Challenge (Levesque et al., 2011)6, though this additional validation step turned out to be unnecessary.7 Out of 7200 binary-choice worker annotations (720 sentences 10-way redundancy), 94.9% of responses agree with our intended answ
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2725155646	ern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016). Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zhang et al. (2018). Prior work also analyzes social and gender stereotyping in existing NLP and vision datasets (van Miltenburg, 2016; Rudinger et al., 2017). Tatman (2017) investigates the impa
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2483215953	ers explore (gender) bias in English word embeddings: how they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016). Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zhang et al. (2018). Prior work also analyzes social and gender stereotyping in existing NLP and vi
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2251035762	g this information available throughout the system. Statistical Statistical methods, often with millions of parameters, ultimately surpassed the performance of rule-based systems on shared task data (Durrett and Klein, 2013; Bjorkelund and¨ Kuhn, 2014). The system of Durrett and Klein (2013) replaced hand-written rules with simple feature templates. Combinations of these features implicitly capture linguistic phenomena
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2099115159	ion (Bureau of Labor Stats, 2015-16) 100 75 50 25 0 25 50 75 100 Gendered Pronoun Resolution by Occupations (%Female - %Male) System STAT RULE NEURAL 0 20 40 60 80 100 % Female by Occupation in Text (Bergsma and Lin, 2006) 100 75 50 25 0 25 50 75 100 Gendered Pronoun Resolution by Occupations (%Female - %Male) System STAT RULE NEURAL Figure 4: These two plots show how gender bias in coreference systems corresponds with
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2893425640	IPANT , a secondary (human) participant, e.g., “the passenger.” 3. PRONOUN , a pronoun that is coreferent with either OCCUPATION or PARTICIPANT. We use a list of 60 one-word occupations obtained from Caliskan et al. (2017) (see supplement), with corresponding gender percentages available from the U.S. Bureau of Labor Statistics.4 For each occupation, we wrote two similar sentence templates: one in which PRONOUN is core
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2893425640	n-exhaustive) overview of prior work on gender bias in NLP systems and datasets. A number of papers explore (gender) bias in English word embeddings: how they capture implicit human biases in modern (Caliskan et al., 2017) and historical (Garg et al., 2018) text, and methods for debiasing them (Bolukbasi et al., 2016). Further work on debiasing models with adversarial learning is explored by Beutel et al. (2017) and Zh
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2099115159	nder disparities. Figure 4 shows that systems’ gender preferences for occupations correlate with realworld employment statistics (U.S. Bureau of Labor Statistics) and the gender statistics from text (Bergsma and Lin, 2006) which these systems access directly; correlation values are in Table 1. We also identify so-called “gotcha” sentences in which pronoun gender does not match the occupation’s majority gender (BLS) if
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2251064706	to a new domain. Neural The move to deep neural models led to more powerful antecedent scoring functions, and the subsequent learned feature combinations resulted in new state-of-the-art performance (Wiseman et al., 2015; Clark and Manning, 2016b). Global inference over these models further improved performance (Wiseman et al., 2016; Clark and Manning, 2016a), but from the perspective of potential bias, the informati
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2251035762	Results and Discussion We evaluate examples of each of the three coreference system architectures described in 2: the Lee et al. (2011) sieve system from the rulebased paradigm (referred to as RULE), Durrett and Klein (2013) from the statistical paradigm (STAT), and the Clark and Manning (2016a) deep reinforcement system from the neural paradigm (NEURAL). By multiple measures, the Winogender schemas reveal varying degree
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	1599016936	of Winograd schemas, wherein a pronoun must be resolved to one of two previouslymentioned entities in a sentence designed to be easy for humans to interpret, but challenging for data-driven systems (Levesque et al., 2011). In our setting, one of these mentions is a person referred to by their occupation; by varying only the pronoun’s gender, we are able to test the impact of gender on resolution. With these “Winogende
2798280648	GENDER BIAS IN COREFERENCE RESOLUTION	2893425640	y bound with questions of gender, for humans and automated systems alike (see Figure 1). As awareness grows of the ways in which data-driven AI technologies may acquire and amplify human-like biases (Caliskan et al., 2017; Barocas and Selbst, 2016; Hovy and Spruit, 2016), this work investigates how gender biases manifest in coreference resolution systems. There are many ways one could approach this question; here we f
2798280909	I ain't tellin' white folks nuthin: A quantitative exploration of the race-related problem of candour in the WPA slave narratives.	38739846	towards their master was by nding all sentences that included the word \master,&quot; measuring each sentences’ sentiment by adding up the sentiment scores of the words from the SentiWordNet lexicon [ES06], and then averaging the sentiment of the target sentences written by white interviewers versus black interviewers. The results of this, on the Arkansas dataset, were as follows: the average sentiment
2798280909	I ain't tellin' white folks nuthin: A quantitative exploration of the race-related problem of candour in the WPA slave narratives.	38739846	nducted by whites versus black people. I measured the sentiment of a sentence by simply nding all words in the sentence that had a sentiment value (from -1 to 1) in the SentiWordNet sentiment lexicon [ES06], and summed the sentiment values up.5 I then found the aggregate sentiment of all the sentences including the target word by averaging up the sentiments of the sentences. This is an extremely simplis
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2068971446	characteristics of the reader and their cognitive state. For example, Reichle et at. (2010) have shown that eye movement patterns are categorically different in attentive versus mindless reading. In Rello and Ballesteros (2015) eye movements were used to distinguish between readers with and without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the ﬁrst language of non-native En
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2609504523	feature set contains 9,077 features with a non-zero value for at least one participant for MET, and 8,132 such features for TOEFL. Words in Fixed Context (WFC) This feature set was previously used in Berzak et al. (2017) and consists of reading times for words within ﬁxed contexts. We extract FP and TF durations for the 900 words in the Fixed Text sentences, resulting in a total of 1,800 WFC features. 4 English Proﬁc
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2609504523	l benchmarks of their English proﬁciency. Michigan English Test (MET) Our primary indicator of English proﬁciency is the listening and grammar sections of the MET (Form-B), which were administered by Berzak et al. (2017) in-lab, and taken by all the 145 non-native participants upon completion of the reading experiment. The test has a total of 50 multiple choice questions, comprising 20 listening comprehension questio
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2609504523	mar questions. The test score is computed as the number of correct answers for these questions, with possible scores ranging from 0 to 50. The mean MET score in the dataset is 41.46 (std 6.27). TOEFL Berzak et al. (2017) also collected selfreported scores on the most recently taken ofﬁ- cial English proﬁciency test, which we use here as a secondary evaluation benchmark. We focus on the most commonly reported test, th
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2609504523	n6we survey related work. Finally, we conclude and discuss future work in section7. 2 Experimental Setup Our study uses the dataset of eye movement records and English proﬁciency scores introduced in Berzak et al. (2017)1, which we describe here in brief. The dataset contains gaze recordings of 37 native English speakers and 145 ESL speakers belonging to four native language backgrounds: 36 Chinese, 36 Japanese, 36 P
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2609504523	nd encode them as features. As each of the ﬁve models has three coefﬁcients and one intercept term, the resulting WP-Coefﬁcients feature set has 20 features. Syntactic Clusters (S-Clusters) Following Berzak et al. (2017), we extract average word reading times clustered by POS tags and syntactic functions. We utilize three metrics of reading times, FF, FP and TF durations. We then cluster words according to three type
2798281106	ASSESSING LANGUAGE PROFICIENCY FROM EYE MOVEMENT IN READING	2154414741	re typically prepared manually and require extensive resources for test development. Moreover, their validity can be undermined by test speciﬁc training, prior knowledge of the evaluation mechanisms (Powers et al., 2002), as well as plain cheating via unauthorized access to test materials. Further, the utilized testing and evaluation methodologies vary across different tests, and test materials are in most cases inac
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1840435438	(8,054) ✓ Relationship Extraction FACC1 (Gabrilovich et al., 2013) 25K (25,132) ✓✗ Sentiment Analysis (Kotzias et al., 2015) 6K (6,000) ✓ Combined Diverse NLI Collection (DNC) 570K (570,459) — SNLI (Bowman et al., 2015) 570K — Multi-NLI (Williams et al., 2017) 433K Table 2: Statistics summarizing the recast datasets. The ﬁrst column refers to the original annotation that was recast, the ‘Combined‘ rowrefers tothe co
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2605717780	dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labeling tasks to investigate whether neural machine translation (NMT) models learn different linguistic phenomena (Belinkov et al., 2017a,b; Dalvi et al., 2017; Marvin and Koehn, 2018). Recently, Poliak et al. (2018a) used recast NLI datasets toinvestigate semanticscaptured byNMT encoders. Targeted Tests for Natural Language Understan
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2063998312	e information. We are interested in determining whether general NLU models capture ‘subjective clues’ that can help identify and understand emotions, opinions, and sentiment within a subjective text (Wilson et al., 2006). We recast a sentiment analysis dataset since the task is the “expression of subjectivity as either a positive ornegative opinion” (Taboada,2016). We extract sentences fromproduct, movie, and restaur
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1516535577	easoning. Limit Biases Studies indicate that many NLI datasets contain signiﬁcant biases. Examples in the early Pascal RTE datasets could be correctly predicted based on syntax alone (Vanderwende and Dolan, 2006; Vanderwende et al., 2006). Statistical irregularities, and annotation artifacts, within class labels allow a hypothesis-only model to signiﬁcantly outperform the majority baseline on at least six re
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1840435438	ecomp.net,andwillgrow overtimeasadditionalresourcesarerecastand added from novel sources. 1 Introduction A plethora of new natural language inference (NLI)1 datasets has been created in recent years (Bowman et al., 2015; Williams et al., 2017; Lai et al., 2017; Khot et al., 2018). However, these datasets do not provide clear insight into what type of reasoning or inference a model may be performing. For example, the
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2124898653	ed. Determining whether an event occurred enables accurate inferences, e.g. monotonic inferences, based on the event (Rudinger et al., 2018b).4 Incorporating factuality has been shown to improve NLI (Sauri and Pustejovsky, 2007). We recast event factuality annotations from UW (Lee et al., 2015), MEAN3This changed as large NLI datasets have recently been used to train, or pre-train, models to perform NLI, or other tasks (Conn
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1599016936	hquality instances manually created by linguists. MacCartney (2009) created the FraCaS textual inference test suite by automatically “convert[ing] each FraCaS question into a declarative hypothesis.” Levesque et al. (2012)’s Winograd Schema Challenge forces a model to choose between two possible answers for a question based on a sentence describing an event. Recent benchmarks test whether NLI models handle adjective-no
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2129842875	ich are related by p2) a sample of sentences in which those entities co-occur and 3) the most frequent natural language strings which join entities related by p according to a OpenIE triple database (Schmitz et al., 2012; Fader et al., 2011) extracted from a large text corpus. We then manually write a simple template (e.g. Mention1 was born in Mention2) for p, ignoring any unclear relations. In total, we end up with
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2549835527	ing for a future thorough study. 5 Related Work Exploring what linguistic phenomena neural models learn Many tests have been used to probe how well neural models learn different linguistic phenomena. Linzen et al. (2016) use “number agreement in English subject-verb dependencies” to show that LSTMs learn about syntax-sensitive dependencies. In addition to syntax (Shi et al., 2016), researchers have used other labelin
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1840435438	limit types of biases observed in previous NLI data; and 3) generate examples cheaply, potentially at large scales. NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor Multi-NLI (Williams et al., 2017), were created by eliciting hypotheses from humans. Crowdsource workers were tasked with writing one sentence each that is entailed, neutral, and co
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	1599016936	De Meulder, 2003). Gendered Anaphora Resolution (GAR) The ability to perform pronoun resolution is essential to language understanding, in many cases requiring common-sense reasoning about the world (Levesque et al., 2012). White et al. (2017) 5WereplaceEvent withtheevent describedinthecontext. 6We ensure grammatical hypotheses by appropriately conjugating “is a” when needed. Sem. Phenomena Dataset # pairs Automated De
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2251784184	notonic inferences, based on the event (Rudinger et al., 2018b).4 Incorporating factuality has been shown to improve NLI (Sauri and Pustejovsky, 2007). We recast event factuality annotations from UW (Lee et al., 2015), MEAN3This changed as large NLI datasets have recently been used to train, or pre-train, models to perform NLI, or other tasks (Conneau et al., 2017; Pasunuru and Bansal, 2017). 4Appendix B.1 provide
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2139621418	om the most frequent semantic predicates to generate hypotheses (5d). (5) a. Michael swatted the ﬂy b. cause(E, Agent) c. Agent caused the E d. Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then ﬁll in the templates with the matched tokens (5d). We also decompose multi-argument predicates into unary predicates to increas
2798284509	Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations.	2251784184	ontext. 6We ensure grammatical hypotheses by appropriately conjugating “is a” when needed. Sem. Phenomena Dataset # pairs Automated Decomp (Rudinger et al., 2018b) 42K (41,888) ✓ Event Factuality UW (Lee et al., 2015) 5K (5,094) ✓ MeanTime (Minard et al., 2016) .7K (738) ✓ Groningen (Bos et al., 2017) 260K (261,406) ✓ Named Entity Recognition CoNLL (Tjong Kim Sang and De Meulder, 2003) 60K (59,970) ✓ Gendered Anap
2798293082	Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension	2626778328	as H (D;(s;e)) = [dSA s ;dSAe] and let G (Q) = [qSA s ;qSAe] (we loosely use subscripts s;eto indicate that we use different trainable weights for start and end). In practice, it is more beneﬁcial (Vaswani et al., 2017) to have multiple heads (instead of single head) of the attention. That is, we can compute multiple weight vectors u 1 :::u N, where Nis the number of heads, and concatenate the attended vectors to ob
2798293082	Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension	2126209950	rehension. Massive reading comprehension question answering datasets such as the CNN/Daily Mail Corpus (Hermann et al., 2015), Stanford Question Answering Dataset (Rajpurkar et al.,2016), and others (Hill et al., 2016;Dhingra et al.,2017;Dunn et al.,2017; Joshi et al.,2017) have driven a large number of successful neural approaches (Kadlec et al.,2016; Wang and Jiang,2017;Lee et al.,2016;Seo et al., 2017;Wang et a
2798293082	Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension	1544827683	tate of the art, showing that the task is yet far from being solved. 2 Related Work Reading comprehension. Massive reading comprehension question answering datasets such as the CNN/Daily Mail Corpus (Hermann et al., 2015), Stanford Question Answering Dataset (Rajpurkar et al.,2016), and others (Hill et al., 2016;Dhingra et al.,2017;Dunn et al.,2017; Joshi et al.,2017) have driven a large number of successful neural ap
2798299685	Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback	108987309	ive and question-parse pairs (Jia and Liang,2016;Dong and Lapata,2016)) or question-answer pairs (Neelakantan et al.,2017). Improving semantic parsers using weak feedback has previously been studied (Goldwasser and Roth (2013);Artzi and Zettlemoyer(2013); inter alia). More recently, several works have applied policy gradient techniques such as REINFORCE (Williams,1992) to train neural semantic parsers (Liang et al.(2017);M
2798302710	Joint Bootstrapping Machines for High Confidence Relation Extraction	2103931177	87 vs. 0.74) better than the state of the art for four relationships. 1 Introduction Traditional semi-supervised bootstrapping relation extractors (REs) such as BREDS (Batista et al.,2015), SnowBall (Agichtein and Gravano, 2000) and DIPRE (Brin,1998) require an initial set of seed entity pairs for the target binary relation. They ﬁnd occurrences of positive seed entity pairs in the corpus, which are converted into extraction
2798302710	Joint Bootstrapping Machines for High Confidence Relation Extraction	2103931177	cing expected number of negative entities. Brin(1998) developed the bootstrapping relation extraction system DIPRE that generates extractors by clustering contexts based on string matching. SnowBall (Agichtein and Gravano, 2000) is inspired by DIPRE but computes a TFIDF representation of each context. BREDS (Batista et al.,2015) uses word embeddings (Mikolov et al.,2013) to bootstrap relationships. Related work investigated
2798302710	Joint Bootstrapping Machines for High Confidence Relation Extraction	2103931177	ons from Table 1 and Figure 2. of the seeds. On line 05, we make the ﬁrst hop: different BREX conﬁgurations (see below). The collected instances are then clustered, similar to work on bootstrapping byAgichtein and Gravano (2000) andBatista et al.(2015). On line 06, we make the second hop: all instances that are within ˝ sim of a hop-1 instance are added; each such instance is only added to one cluster, the closest one; see d
2798302710	Joint Bootstrapping Machines for High Confidence Relation Extraction	2250560707	ur bootstrapping algorithm for k it iterations where k it is a parameter. A key notion is the similarity between two instances. We will experiment with different similarity measures. The baseline is (Batista et al., 2015)’s measure given in Figure4, ﬁrst line: the similarity of two instances is given as a weighted sum of the dot products of their before contexts (~v 1), their between contexts (~v 0) and their after co
2798308429	EXPLOITING SEMANTICS IN NEURAL MACHINE TRANSLATION WITH GRAPH CONVOLUTIONAL NETWORKS	2606780347	on a parallel corpus to maximize the conditional likelihood of the target sentences. 2.2 Graph Convolutional Networks Graph neural networks are a family of neural architectures (Scarselli et al.,2009;Gilmer et al., 2017) speciﬁcally devised to induce representation of nodes in a graph relying on its graph structure. Graph convolutional networks (GCNs) belong to this family. While GCNs were introduced John gave his wo
2798312813	Did the Model Understand the Question	2738015883	). It involves measuring how a network’s accuracy changes as words are systematically dropped from questions. We emphasize that, in contrast to modelindependent adversarial techniques such as that of Jia and Liang (2017), our method exploits the strengths and weaknesses of the model(s) at hand. This allows our attacks to have a high success rate. Additionally, using insights derived from attributions we were able to
2798312813	Did the Model Understand the Question	2557851318	1 Introduction Recently, deep learning has been applied to a variety of question answering tasks. For instance, to answer questions about images (e.g. (Kazemi and Elqursh, 2017)), tabular data (e.g. (Neelakantan et al., 2017)), and passages of text (e.g. (Yu et al., 2018)). Developers, end-users, and reviewers (in academia) would all like to understand the capabilities of these models. The standard way of measuring the go
2798312813	Did the Model Understand the Question	2557851318	aching contentfree preﬁxes (e.g., “in not many words, :::”) to questions drops the accuracy from 61:1% to 19%. QA on tables (section 5): We analyze a system called Neural Programmer (henceforth, NP) (Neelakantan et al., 2017) that answers questions on tabular data. NP determines the answer to a question by selecting a sequence of operations to apply on the accompanying table (akin to an SQL query; details in section 5). W
2798312813	Did the Model Understand the Question	2597425697	acing question subject with low attribution nouns. Ribeiro et al. (2016a) use a model explanation technique to illustrate overstability for two examples. They do not quantify their analysis at scale. Kaﬂe and Kanan (2017); Zhang et al. (2016) examine the VQA data, identify deﬁciencies, and propose data augmentation to reduce over-representation of certain question/answer types. Goyal et al. (2016) propose the VQA 2.0
2798312813	Did the Model Understand the Question	2594633041	aseline 0, the integrated gradient along the ithdimension is deﬁned as follows. IG i(x;x0) ::= (x i x0 i) Z 1 =0 @F ( x0+ 0)) @x i d (here @F(x) @x i is the gradient of Falong the ithdimension at x). Sundararajan et al. (2017) discuss several properties of IG. Here, we informally mention a few desirable ones, deferring the reader to Sundararajan et al. (2017) for formal deﬁnitions. IG satisﬁes the condition that the attrib
2798312813	Did the Model Understand the Question	2738015883	cy of a visual question answering model from 61:1% to 19%, and that of a tabular question answering model from 33:5% to 3:3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for
2798312813	Did the Model Understand the Question	1945616565	a detailed comparison with other attribution methods). Recently, there have been a number of techniques for crafting and defending against adversarial attacks on image-based deep learning models (cf. Goodfellow et al. (2015)). They are based on oversensitivity of models, i.e., tiny, imperceptible perturbations of the image to change a model’s response. In contrast, our attacks are based on models’ over-reliance on few qu
2798312813	Did the Model Understand the Question	2346578521	ereas theirs is not. We could use many other methods instead of Integrated Gradients (IG) to attribute a deep network’s prediction to its input features (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014). One could also use model agnostic techniques like Ribeiro et al. (2016b). We choose IG for its ease and efﬁciency of implementation (requires just a
2798312813	Did the Model Understand the Question	2101964891	examples; our attacks are on the question. 5 Question Answering over Tables 5.1 Task, model, and data We now analyze question answering over tables based on the WikiTableQuestions benchmark dataset (Pasupat and Liang, 2015). The dataset has 22033 questions posed over 2108 tables scraped from Wikipedia. Answers are either contents of table cells or some table aggregations. Models performing QA on tables translate the que
2798312813	Did the Model Understand the Question	2738015883	hension (Section 6): The task is to answer questions about paragraphs of text. We analyze the network by Yu et al. (2018). Again, we ﬁnd that the network often ignores words that should be important. Jia and Liang (2017) proposed attacks wherein sentences are added to paragraphs that ought not to change the network’s answers, but sometimes do. Our main ﬁnding is that these attacks are more likely to succeed when an a
2798312813	Did the Model Understand the Question	2136462581	low attribution. 4 Visual Question Answering 4.1 Task, model, and data The Visual Question Answering Task (Agrawal et al., 2015; Teney et al., 2017; Kazemi and Elqursh, 2017; Ben-younes et al., 2017; Zhu et al., 2016) requires a system to answer questions about images (ﬁg. 1). We analyze the deep network from Kazemi and Elqursh (2017). It achieves 61:1% accuracy on the validation set (the state of the art (Fukui e
2798312813	Did the Model Understand the Question	2273038706	with low attribution nouns. Ribeiro et al. (2016a) use a model explanation technique to illustrate overstability for two examples. They do not quantify their analysis at scale. Kaﬂe and Kanan (2017); Zhang et al. (2016) examine the VQA data, identify deﬁciencies, and propose data augmentation to reduce over-representation of certain question/answer types. Goyal et al. (2016) propose the VQA 2.0 dataset, which has pa
2798312813	Did the Model Understand the Question	2123045220	methods instead of Integrated Gradients (IG) to attribute a deep network’s prediction to its input features (Baehrens et al., 2010; Simonyan et al., 2013; Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014). One could also use model agnostic techniques like Ribeiro et al. (2016b). We choose IG for its ease and efﬁciency of implementation (requires just a few gradient-calls) and its axiomatic justiﬁcatio
2798312813	Did the Model Understand the Question	2594633041	n in question answering. 1.1 Our Contributions We follow an analysis workﬂow to understand three question answering models. There are two steps. First, we apply Integrated Gradients (henceforth, IG) (Sundararajan et al., 2017) to attribute the systems’ predictions to words in the questions. We propose visualizations of attributions to make analysis easy. Second, we identify weaknesses (e.g., relying on unimportant words) i
2798312813	Did the Model Understand the Question	2594633041	ould also use model agnostic techniques like Ribeiro et al. (2016b). We choose IG for its ease and efﬁciency of implementation (requires just a few gradient-calls) and its axiomatic justiﬁcation (see Sundararajan et al. (2017) for a detailed comparison with other attribution methods). Recently, there have been a number of techniques for crafting and defending against adversarial attacks on image-based deep learning models
2798312813	Did the Model Understand the Question	2738015883	rds) in the networks’ logic as exposed by the attributions, and leverage them to craft adversarial questions. A key contribution of this work is an overstability test for question answering networks. Jia and Liang (2017) showed that reading comprehension networks are overly stable to semantics-altering edits to the passage. In this work, we ﬁnd that arXiv:1805.05492v1 [cs.CL] 14 May 2018 such overstability also appli
2798312813	Did the Model Understand the Question	2557851318	rming QA on tables translate the question into a structured program (akin to an SQL query) which is then executed on the table to produce the answer. We analyze a model called Neural Programmer (NP) (Neelakantan et al., 2017). NP is the state of the art among models that are weakly supervised, i.e., supervised using the ﬁnal answer instead of the correct structured program. It achieves 33:5% accuracy on the validation set
2798312813	Did the Model Understand the Question	2606462007	robustness of VQA models by appending questions with semantically similar questions. Our preﬁx attacks in section 4.4 are in a similar vein and perhaps a more natural and targeted approach. Finally, Fong and Vedaldi (2017) use saliency methods to produce image perturbations as adversarial examples; our attacks are on the question. 5 Question Answering over Tables 5.1 Task, model, and data We now analyze question answer
2798312813	Did the Model Understand the Question	2250539671	s were constructed by (a) generating a sentence that answers the question, (b) replacing all the adjectives and nouns with antonyms, and named entities by the nearest word in GloVe word vector space (Pennington et al., 2014) and (c) crowdsourcing to check that the new sentence is grammatically correct. This suggests a use of attributions to improve the effectiveness of the attacks, namely ensuring that question words tha
2798312813	Did the Model Understand the Question	2427527485	s during training. 6 Reading Comprehension 6.1 Task, model, and data The reading comprehension task involves identifying a span from a context paragraph as an answer to a question. The SQuAD dataset (Rajpurkar et al., 2016) for machine reading comprehension contains 107.7K query-answer pairs, with 87.5K for training, 10.1K for validation, and another 10.1K for testing. Deep learning methods are quite successful on this
2798312813	Did the Model Understand the Question	2738015883	ution visualizations. Knowing which words were ignored, or which operations the words were mapped to, can help the user decide whether to trust a system’s response. 2 Related Work We are motivated by Jia and Liang (2017). As they discuss, “the extent to which [reading comprehension systems] truly understand language remains unclear”. The contrast between Jia and Liang (2017) and our work is instructive. Their main co
2798312813	Did the Model Understand the Question	2557851318	any words 15.6% 11.2% one way or another 23.5% 20.0% Union of above attacks 3.3% Baseline please answer 32.3% 30.7% do you know 31.2% 29.5% Union of baseline preﬁxes 27.1% Table 3: Neural Programmer (Neelakantan et al., 2017): Left: Validation accuracy when attack phrases are concatenated to the question. (Original: 33:5%) produce an incorrect answer. We now describe attacks that add or drop content-free words from the qu
2798312813	Did the Model Understand the Question	2594633041	words should matter. We discuss task-speciﬁc related work in corresponding sections (sections 4 to 6). 3 Integrated Gradients (IG) We employ an attribution technique called Integrated Gradients (IG) (Sundararajan et al., 2017) to isolate question words that a deep learning system uses to produce an answer. Formally, suppose a function F : Rn ![0;1] represents a deep network, and an input x = (x 1;:::;x n) 2Rn. An attributi
2798313955	Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations	2100693535	ore recent work broke the task into two steps: in the ﬁrst step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g.Mitchell and Lapata,2010;Zanzotto et al., 2010;Socher et al.,2012). In the second step, the noun-compound representations are used as feature vectors for classiﬁcation (e.g.Dima and Hinrichs,2015;Dima,2016). The datasets for this task differ in s
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2613526370	16; Lin et al., 2017).´ Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the program language, e.g. ﬁrst-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neel
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2189089430	2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with nat
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2227250678	69.0% to 74.4%. 1 Introduction We focus on semantic parsing that maps natural language utterances to executable programs (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Pasupat and Liang, 2015; Iyer et al., 2017; Iyyer et al., 2017). In this work, we regard SQL as the program language, which could be executed on a table or a relational database to obtain an outcome
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2163274265	r approach signiﬁcantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%. 1 Introduction We focus on semantic parsing that maps natural language utterances to executable programs (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Pasupat and Liang, 2015; Iyer et al., 2017; Iyyer et al., 2017). In this work, we regard SQL as the program language,
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2154268919	a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose c
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2269738476	e column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017)
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2154268919	E columns, and uses attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of syntactic parser or manually designed templates. We are aware of existing studies that
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2252136820	emantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ from (1) the form of t
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2224454470	et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 Methodology We ﬁrst describe the
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2130942839	eural architecture, despite we also implement a RL strategy (refer to x4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been d
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2342096063	which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2420948438	the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the program language, e.g. ﬁrst-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic e
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2307268612	gical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational d
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2304113845	hension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gate
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2121465811	improves the state-of-the-art execution accuracy from 69.0% to 74.4%. 1 Introduction We focus on semantic parsing that maps natural language utterances to executable programs (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Pasupat and Liang, 2015; Iyer et al., 2017; Iyyer et al., 2017). In this work, we regard SQL as the program language, which could be executed
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2133564696	ite we also implement a RL strategy (refer to x4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language mo
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2101964891	L queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2097647324	to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ from (1) the form of the knowledge
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2465944011	ms (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational d
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2101964891	ompared to column prediction, the quality of cell prediction is much better because cell content typically (partially) appears in the question. 5.7 Transfers to WikiTableQuestions WikiTableQuestions (Pasupat and Liang, 2015) is a widely used dataset for semantic parsing. To further test the performance of our approach, we conduct an additional transfer learning experiment. Firstly, we directly apply the STAMP model train
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2605887895	ould be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semant
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2511108736	parser that considers the structure of table and the syntax of SQL language. The approach is partly inspired by the success of structure/grammar driven neural network approaches in semantic parsing (Xiao et al., 2016; Krishnamurthy et al., 2017). Our approach is based on pointer networks, which encodes the question into continuous vectors, and synthesizes the SQL query with three channels. The model learns when t
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2224454470	pervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ information need for structured data on
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2610403318	s end-to-end learning and independent of syntactic parser or manually designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement a RL strategy (refer to x4.4). Structure/Grammar Guided Neu
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2161002933	Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk y et al., 2016; Lin et al., 2017).´ Existing studies differ f
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2307268612	SQL as the program language, which could be executed on a table or a relational database to obtain an outcome. Datasets are the main driver of progress for statistical approaches in semantic parsing (Liang, 2016). Recently, Zhong Work is done during internship at Microsoft Research Asia. et al. (2017) release WikiSQL, the largest handannotated semantic parsing dataset which is an order of magnitude larger tha
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2250539671	stribution of a channel (p w). l= X t logp z(z tjy &lt;t;x) X t logp w(y tjz t;y &lt;t;x) (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3. The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2546950329	t of syntactic parser or manually designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement a RL strategy (refer to x4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewe
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2289899728	tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequenc
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2101964891	tion We focus on semantic parsing that maps natural language utterances to executable programs (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Pasupat and Liang, 2015; Iyer et al., 2017; Iyyer et al., 2017). In this work, we regard SQL as the program language, which could be executed on a table or a relational database to obtain an outcome. Datasets are the main d
2798315459	Semantic Parsing with Syntax- and Table-Aware SQL Generation	2133564696	a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word x i in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc i is the
2798321501	DeepEmo: Learning and Enriching Pattern-Based Emotion Representations.	2251812186	nriched patterns extracted from a gender-based dataset. We collected user feeds from Twitter and classiﬁed users into male and female classes based on their content via Sap et al.’s gender predictor (Sap et al., 2014). This produces a gender dataset, which we also manually verify by ourselves. We randomly sampled 2000 males and 2000 females and then randomly sampled 100 tweets from each user feed. This generated 4
2798321501	DeepEmo: Learning and Enriching Pattern-Based Emotion Representations.	2088622183	t sequences of size two and three were used in this work since this setting experimentally worked best for us. We may sometimes refer to these candidate patterns as templates, similar to (Riloff,1996;Riloff and Wiebe, 2003;Tromp and Pechenizkiy,2015). The difference in our work is that we don’t impose grammatical heuristics or rules in the pattern extraction process, therefore, our patterns tend to naturally have highe
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2167170026	. Considerable research efforts have been focused on the identiﬁcation and extraction of paraphrases. One successful technique is associated with bilingual pivoting (Bannard and Callison-Burch, 2005; Zhao et al., 2008), in which alternative phrase translations are used as paraphrases at a certain probability. However, this technique requires large bilingual parallel corpora; moreover, word alignment errors likely c
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2251919380	4). We also compare our system with machine learning-based approaches: the current state-of-the-art deep learning model GRU (Yin and Schutze, 2017), a log-¨ linear regression model SemEval-2014 best (Lai and Hockenmaier, 2014), and a hybrid approach combining a logistic regression model and probabilistic logic PL+eclassif (Beltagy et al., 2016). 5.3 Extracted paraphrases We extracted 9445 axioms from the SICK training data
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2250863723	approaches have been successful in representing the meanings of complex sentences, ultimately having a positive impact on RTE (Bjerva et al., 2014; Beltagy et al., 2014; Mineshima et al., 2015, 2016; Abzianidze, 2015, 2016). Although logic-based approaches succeed in capturing the meanings of functional or logical words, it is difﬁcult to capture the meanings of content words or phrases using genuine logical infe
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2609600678	automatically. We parsed the tokenized sentences of the premises and hypotheses using three widecoverage CCG parsers: C&amp;C (Clark and Curran, 2007), EasyCCG (Lewis and Steedman, 2014), and depccg (Yoshikawa et al., 2017). CCG derivation trees (parses) were converted into logical semantic representations based on Neo-Davidsonian event semantics (Section 3.1). The validation of semantic templates used for semantic repr
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2250748818	and conducts natural deduction proofs automatically. We parsed the tokenized sentences of the premises and hypotheses using three widecoverage CCG parsers: C&amp;C (Clark and Curran, 2007), EasyCCG (Lewis and Steedman, 2014), and depccg (Yoshikawa et al., 2017). CCG derivation trees (parses) were converted into logical semantic representations based on Neo-Davidsonian event semantics (Section 3.1). The validation of sema
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2250983897	an et al., 2013). Logic-based approaches have been successful in representing the meanings of complex sentences, ultimately having a positive impact on RTE (Bjerva et al., 2014; Beltagy et al., 2014; Mineshima et al., 2015, 2016; Abzianidze, 2015, 2016). Although logic-based approaches succeed in capturing the meanings of functional or logical words, it is difﬁcult to capture the meanings of content words or phrases us
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2081580037	ial problem in accounting for lexical relations between content words or phrases via logical inference. To solve this problem, previous logic-based approaches use knowledge databases such as WordNet (Miller, 1995) to identify lexical relations within a sentence pair. While this solution has been successful in handling word-level paraphrases, its extension to phrase-level semantic relations is still an unsolved
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2165646510	ion with word abduction. In addition, we compare our system with three purely logic-based (unsupervised) approaches: The Meaning Factory (Bjerva et al., 2014), LangPro (Abzianidze, 2015), and UTexas (Beltagy et al., 2014). We also compare our system with machine learning-based approaches: the current state-of-the-art deep learning model GRU (Yin and Schutze, 2017), a log-¨ linear regression model SemEval-2014 best (La
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2252123671	mises. The method of detecting semantic phrase alignments would be applicable to other semantic parsing formalisms and meaning representation languages such as abstract meaning representations (AMR) (Banarescu et al., 2013). Experiment results showed that our method detected various phrase alignments including noncontiguous phrases and antonym phrases. This result may contribute to previous phrase alignment approaches.
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	655477013	n 3.2) as a proof system instead of Markov Logic Networks for inference. Some research has investigated graph operations for semantic parsing (Reddy et al., 2014, 2016) and abstractive summarization (Liu et al., 2015); we contribute to these ideas by proposing a subgraph mapping algorithm that is useful for performing natural language inferences. Considerable research efforts have been focused on the identiﬁcation
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	1840435438	onal semantics, so it contains logically challenging problems involving quantiﬁers, negation, conjunction, and disjunction, as well as inferences with lexical and phrasal knowledge. The SNLI dataset (Bowman et al., 2015) contains inference problems requiring phrasal knowledge. However, it is not concerned with logically challenging expressions; the semantic relationships between a premise and a hypothesis are often l
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2143927888	orming natural language inferences. Considerable research efforts have been focused on the identiﬁcation and extraction of paraphrases. One successful technique is associated with bilingual pivoting (Bannard and Callison-Burch, 2005; Zhao et al., 2008), in which alternative phrase translations are used as paraphrases at a certain probability. However, this technique requires large bilingual parallel corpora; moreover, word align
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2165646510	her text fragment (Dagan et al., 2013). Logic-based approaches have been successful in representing the meanings of complex sentences, ultimately having a positive impact on RTE (Bjerva et al., 2014; Beltagy et al., 2014; Mineshima et al., 2015, 2016; Abzianidze, 2015, 2016). Although logic-based approaches succeed in capturing the meanings of functional or logical words, it is difﬁcult to capture the meanings of con
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2250790822	to(y 1;x 5) ^ piece(x 5))) Using this axiom, one can complete the proof of the contradiction between T0and H0. 5 Experiments 5.1 Dataset selection We use the SemEval-2014 version of the SICK dataset (Marelli et al., 2014) for evaluation. The SICK dataset is a dataset for semantic textual similarity (STS) as well as for RTE. It was originally designed for evaluating compositional distributional semantics, so it contain
2798330556	ACQUISITION OF PHRASE CORRESPONDENCES USING NATURAL DEDUCTION PROOF	2250863723	W2W+P2P combines phrase abduction with word abduction. In addition, we compare our system with three purely logic-based (unsupervised) approaches: The Meaning Factory (Bjerva et al., 2014), LangPro (Abzianidze, 2015), and UTexas (Beltagy et al., 2014). We also compare our system with machine learning-based approaches: the current state-of-the-art deep learning model GRU (Yin and Schutze, 2017), a log-¨ linear reg
2798336329	Reasoning with Sarcasm by Reading In-Between	2250539671	5 tokens are also removed. The learning optimizer used is the RMSProp with an initial learning rate of 0:001. The L2 regularization is set to 10 8. We initialize the word embedding layer with GloVe (Pennington et al., 2014). We use the GloVe model trained on 2B Tweets for the Tweets and Reddit dataset. The Glove model trained on Common Crawl is used for the Debates corpus. The size of the word embeddings is ﬁxed at d= 1
2798338181	Polyglot Semantic Role Labeling	2250539671	s it is not an argument to the indicated predicate. 3.1 Monolingual Baseline We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors (Pennington et al., 2014) from the news, web, and Wikipedia text of the Leipzig Corpora Collection (Goldhahn et al., 2012).2 We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component ana
2798338181	Polyglot Semantic Role Labeling	2397198482	ural architectures.Swayamdipta et al.(2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models.Guo et al.(2016) andRoth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition. Marcheggiani et al.(2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model
2798340137	Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?	2612953412	enerated by shared and private (task-speciﬁc) encoders to form sentence embeddings. The results can be found in table10. We compare the performance of MTL with the models trained on single tasks as inConneau et al. (2017). Table10shows that learning from multiple tasks performs better than learning from a single task. Although contextualized word vectors learned through MTL performed best on the source tasks, in secti
2798340137	Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?	2612953412	er and Schmidhuber,1997) with max pooling (BiLSTM-Max) sentence encoder as our basic building block for all multi-task learning architectures because it was found very effective in sentence encoding (Conneau et al., 2017). Formally, for a sentence with T words w= [w 1;w 2;:::;w T], we have: ! h t= LSTM(h 1;w t); (1) h t= LSTM( h t+1;w t); (2) and h t= [ ! h t;h t]; (3) where h t2R2dis the t-th hidden vectors in BiLSTM
2798340137	Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?	2738152205	ge processing (NLP) community. In recent years, many successful stories showed that the learned representations are transferable to other tasks. For example, many NLP (Zou et al.,2013;Seo et al.,2017;Lee et al., 2017;Chen et al.,2017) and computer vision applications (Venugopalan et al.,2017;Teney et al., 2017) have successfully utilized word embeddings trained on a large corpus (Mikolov et al.,2013; Pennington e
2798340137	Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?	2612953412	and the Multi-Genre NLI (Multi-NLI) (Williams et al.,2017) which consist of English sentence pairs, manually labeled with one of the three categories: entailment, contradiction and neutral. FollowingConneau et al. (2017), we also conduct experiments that combine SNLI and Multi-NLI datasets, which is denoted as AllNLI. The second task is the duplicate question detection (DupQuD) task based on a dataset of 404k questio
2798340137	Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?	1681397005	and semantic regularities within sentences range from models that compose of word embeddings (Le and Mikolov,2014; Arora et al.,2017;Wieting et al.,2016) to more complex neural network architectures (Zhao et al., 2015;Wang and Jiang,2016;Liu et al.,2016c;Lin et al.,2017). General purpose distributional sentence representations can be learned from a large collection of unlabeled text corpora. Kiros et al.(2015) pro
2798344278	SYNTACTIC PATTERNS IMPROVE INFORMATION EXTRACTION FOR MEDICAL SEARCH	2153579005	ke predictions (as above). For these embeddings, we collected 6 million PubMed abstracts (˘1.4 billion words) ﬁltering for only Human RCTs and used this to train word vectors using the Word2Vec tool (Mikolov et al., 2013a), inducing 200-dimensional vectors using the Skip-Gram model, where our vocabulary now consists of the learned n-gram patterns as single units, along with other unigrams. We then test these embeddin
2798353932	No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling	2099471712	notated stories, which is able to provide better guidance to the policy and thus improves its performances over different metrics. Comparison with GAN We here compare our method with traditional GAN (Goodfellow et al., 2014), the update rule for generator can be generally classiﬁed into two categories. We demonstrate their corresponding objectives and ours as follows: GAN1 : J = E W˘p [ logR (W)] ; GAN2 : J = E W˘p [log
2798353932	No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling	2099471712	rlap-counting methods are not able to reﬂect many semantic properties in natural language, such as coherence, expressiveness, etc. Generative Adversarial Network Generative adversarial network (GAN) (Goodfellow et al., 2014) is a very popular approach for estimating intractable probabilities, which sidestep the difﬁ- culty by alternately training two models to play a min-max two-player game: min D max G E x˘p data [logD(
2798360162	Temporal Event Knowledge Acquisition via Identifying Narratives	2147218300	arrative paragraph to have a protagonist character. Concretely, inspired byEisenberg and Finlayson(2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Person or Organization, or a gendered pronoun. Then w
2798360162	Temporal Event Knowledge Acquisition via Identifying Narratives	2096175520	arrative paragraphs but contain only 30%7 or more sentences with an actantial structure and have the longest character entity chain spanning over 20%8 of or more sentences. We choose Maximum Entropy (Berger et al., 1996) as the classiﬁer. Speciﬁcally, we use the MaxEnt model implementation in the LIBLIN540% was chosen to reﬂect that a narrative paragraph often contains a main character that is commonly mentioned acro
2798360162	Temporal Event Knowledge Acquisition via Identifying Narratives	2123442489	racter. Concretely, inspired byEisenberg and Finlayson(2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Person or Organization, or a gendered pronoun. Then we calculate the normalized length of this entity
2798360162	Temporal Event Knowledge Acquisition via Identifying Narratives	2257051837	ticated neural language models. Granroth-Wilding and Clark(2016) proposed a two layer neural network model that learns embeddings of event predicates and their arguments for predicting the next event.Pichotta and Mooney (2016) introduced a LSTM-based language model for event prediction.Wang et al.(2017) used dynamic memory as attention in LSTM for prediction. It is encouraging that by using event knowledge extracted from a
2798360162	Temporal Event Knowledge Acquisition via Identifying Narratives	2096765155	vents, therefore, we also specify a rule requiring a narrative paragraph to have a protagonist character. Concretely, inspired byEisenberg and Finlayson(2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Pe
2798362442	A Call for Clarity in Reporting BLEU Scores.	2152263452	.pl 8This paper’s observations stem in part from an early version of a research workﬂow I was using, which applied preprocessing to the reference, affecting scores by half a point. paper conﬁguration Chiang (2005) metric lc Bahdanau et al. (2014) (unclear) Luong et al. (2015b) user or metric (unclear) Jean et al. (2015) user Wu et al. (2016) user or user lc (unclear) Vaswani et al. (2017) user or user lc (uncl
2798362442	A Call for Clarity in Reporting BLEU Scores.	2101105183	asuring their outcomes. In machine translation research, the predictions are made by models whose development is the focus of the research, and the measurement, more often than not, is done via BLEU (Papineni et al., 2002). BLEU’s relative language independence, its ease of computation, and its reasonable correlation with human judgments have led to its adoption as the dominant metric for machine translation research.
2798362442	A Call for Clarity in Reporting BLEU Scores.	2133564696	ations stem in part from an early version of a research workﬂow I was using, which applied preprocessing to the reference, affecting scores by half a point. paper conﬁguration Chiang (2005) metric lc Bahdanau et al. (2014) (unclear) Luong et al. (2015b) user or metric (unclear) Jean et al. (2015) user Wu et al. (2016) user or user lc (unclear) Vaswani et al. (2017) user or user lc (unclear) Gehring et al. (2017) user,
2798362442	A Call for Clarity in Reporting BLEU Scores.	2626778328	f a point. paper conﬁguration Chiang (2005) metric lc Bahdanau et al. (2014) (unclear) Luong et al. (2015b) user or metric (unclear) Jean et al. (2015) user Wu et al. (2016) user or user lc (unclear) Vaswani et al. (2017) user or user lc (unclear) Gehring et al. (2017) user, metric Table 2: Benchmarks set by well-cited papers use different BLEU conﬁgurations (Table 1). Which one was used is often difﬁcult to determine
2798362442	A Call for Clarity in Reporting BLEU Scores.	2144600658	g scripts. Unfortunately, each of them has problems. Moses’ multi-bleu.perl cannot be used because it requires user-supplied preprocessing. The same is true of another evaluation framework, MultEval (Clark et al., 2011), which explicitly advocates for user-supplied tokenization.12 A good candidate is Moses’ mteval-v13a.pl, which makes use of metric-internal preprocessing and is used in the annual WMT evaluations. Ho
2798362442	A Call for Clarity in Reporting BLEU Scores.	2552839021	ic lc Bahdanau et al. (2014) (unclear) Luong et al. (2015b) user or metric (unclear) Jean et al. (2015) user Wu et al. (2016) user or user lc (unclear) Vaswani et al. (2017) user or user lc (unclear) Gehring et al. (2017) user, metric Table 2: Benchmarks set by well-cited papers use different BLEU conﬁgurations (Table 1). Which one was used is often difﬁcult to determine. least possible to reconstruct comparable numbe
2798362442	A Call for Clarity in Reporting BLEU Scores.	2100664567	ich applied preprocessing to the reference, affecting scores by half a point. paper conﬁguration Chiang (2005) metric lc Bahdanau et al. (2014) (unclear) Luong et al. (2015b) user or metric (unclear) Jean et al. (2015) user Wu et al. (2016) user or user lc (unclear) Vaswani et al. (2017) user or user lc (unclear) Gehring et al. (2017) user, metric Table 2: Benchmarks set by well-cited papers use different BLEU conﬁ
2798362442	A Call for Clarity in Reporting BLEU Scores.	2251926178	r with larger-scale controlled manual evaluations, BLEU has shep1https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu herded the ﬁeld through a decade and a half of quality improvements (Graham et al., 2014). This is of course not to claim there are no problems with BLEU. Its weaknesses abound, and much has been written about them (cf. CallisonBurch et al. (2006); Reiter (2018)). This paper is not, howev
2798362442	A Call for Clarity in Reporting BLEU Scores.	1816313093	out whether they report cased or case-insensitive BLEU. Allowing the user to handle pre-processing of the reference has other traps. For example, many systems (particularly before sub-word splitting (Sennrich et al., 2016) was proposed) limited the vocabulary in their attempt to deal with unknown words. It’s possible that these papers applied this same unknown-word masking to the references, too, which would artiﬁciall
2798374729	Automated essay scoring with string kernels and word embeddings	2153635508	d intersection kernel. We combine HISK and BOSWE in the dual form by summing up the two corresponding matrices. For the learning phase, we employ the dual implementation of ν-SVR available in LibSVM (Chang and Lin, 2011). We set its regularization parameter to c = 103 and ν = 10−1 in all our experiments. In-domain results. The results for the in-domain automatic essay scoring task are presented in Table 2. In our emp
2798374729	Automated essay scoring with string kernels and word embeddings	2251191978	discourse (Song et al., 2017). Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015), recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; D
2798374729	Automated essay scoring with string kernels and word embeddings	2250372169	e number of essays per prompt along with the score ranges are presented in Table 1. Since the ofﬁcial test data of the ASAP competition is not released to the public, we, as well as others before us (Phandi et al., 2015; Dong and Zhang, 2016; Dong et al., 2017; 1https://www.kaggle.com/c/asap-aes/data Tay et al., 2018), use only the training data in our experiments. Evaluation procedure. As Dong and Zhang (2016), we
2798374729	Automated essay scoring with string kernels and word embeddings	2250372169	e. when the sub-sample size is n t = 0. As evaluation metric, we use the quadratic weighted kappa (QWK). Baselines. We compare our approach with stateof-the-art methods based on handcrafted features (Phandi et al., 2015), as well as deep features (Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). We note that results for the cross-domain setting are reported only in some of these recent works (Phandi et al.
2798374729	Automated essay scoring with string kernels and word embeddings	2141362318	ic clusters of words. As in the standard bag-of-visual-words model, the clustering is done by k-means (Leung and Malik, 2001), and the formed centroids are stored in a randomized forest of k-d trees (Philbin et al., 2007) to reduce search cost. The centroid of each cluster is interpreted as a super word embedding or super word vector that embodies all the semantically related word vectors in a small region of the embe
2798374729	Automated essay scoring with string kernels and word embeddings	2158240052	as it relies on grammar as well as semantics, pragmatics and discourse (Song et al., 2017). Although traditional AES methods typically rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015), recent results indicate that state-of-the-art deep learning methods
2798374729	Automated essay scoring with string kernels and word embeddings	2250372169	rely on handcrafted features (Larkey, 1998; Foltz et al., 1999; Attali and Burstein, 2006; Dikli, 2006; Wang and Brown, 2008; Chen and He, 2013; Somasundaran et al., 2014; Yannakoudakis et al., 2014; Phandi et al., 2015), recent results indicate that state-of-the-art deep learning methods reach better performance (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016; Dong et al., 2017; Song et al.,
2798374729	Automated essay scoring with string kernels and word embeddings	2250372169	utomated Student Assessment Prize data set, in both in-domain and cross-domain settings. The empirical results indicate that our approach yields a better performance than state-of-the-art approaches (Phandi et al., 2015; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). 2 Method String kernels. Kernel functions (Shawe-Taylor and Cristianini, 2004) capture the intuitive notion of similarity between objects
2798374729	Automated essay scoring with string kernels and word embeddings	2250372169	variation introduced by randomly selecting the folds. We note that the standard deviation in all cases in below 0.2%. For the cross-domain experiments, we use the same source→target domain pairs as (Phandi et al., 2015; Dong and Zhang, 2016), namely, 1→2, 3→4, 5→6 and 7→8. All essays in the source domain are used as training data. Target domain samples are randomly divided into 5folds, where one fold is used as tes
2798376639	Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources	2401154299	on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks (Karpathy et al., 2014; Park et al., 2016). 1bigquery.cloud.google.com/dataset/fh-bigquery:reddit_posts|reddit_comments Predicted Label Probability Activation Layer (soft max) Dense Layer (100 units) Tensor Concatenation Max Pooling Layer (1D
2798376639	Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources	2250539671	text sequence sub-network with a vector representation subnetwork as shown in Figure 1. The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings (Pennington et al., 2014) followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate informationfro
2798376639	Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources	2597730742	type. Recent studies have found that 59% of bitlyURLs on Twitter are shared without ever being read (Gabielkov et al., 2016), and 73% of Reddit posts were voted on without reading the linked article (Glenski et al., 2017). Instead, users tend to rely on the commentary added to retweets or the comments section of Reddit-posts for information on the content and its credibility. Faced with this reality, we ask: what kind
2798389157	Filtering and Mining Parallel Data in a Joint Multilingual Space	1973152633	analyzing the name of WEB pages or links (Resnik and Smith,2003). Another direction of research is to use cross-lingual information retrieval, e.g. (Utiyama and Isahara,2003; Munteanu and Marcu,2005;Rauf and Schwenk, 2009). There are some works which use joint embeddings in the process of ﬁltering or mining bitexts. For instance,Gregoire and Langlais´ (2017) ﬁrst embed sentences into two separate spaces. Then, a classi
2798389157	Filtering and Mining Parallel Data in a Joint Multilingual Space	2142074148	stance in that space reﬂects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. (Hermann and Blunsom, 2014;Pham et al.,2015;Zhou et al.,2016;Chandar et al.,2013;Mogadala and Rettinger,2016). In this paper, we extend our initial approach (Schwenk and Douze,2017). The underlying idea is to use multiple sequ
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	1486723856,1905882502	ach has been derived to text-based applications, and recently some researchers investigate the potential of representing multi-modal evidence sources (e.g., texts and images) in a shared latent space [21, 31]. This research direction is particularly interesting for grounding language with common sense information extracted from images, or viceversa. In the context of computer-aided cooking, we believe tha
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2064675550	arately, and their obtained representations are then concatenated as input of a fully connected layer that maps the recipe features into the latent space. For ingredients, we use a bidirectional LSTM [17] on pretrained embeddings obtained with the word2vec algorithm [32]. With the objective to consider the different granularity levels of the instruction text, we use a hierarchical LSTM in which the wo
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2117154949,2145326909	atent space. A perfect retrieval corresponds to a set of inequalities in which the distances between the query and relevant items are smaller than the distances between the query and irrelevant items [30, 40, 42]. Each modal projection is then learned so as to minimize a loss function that measures the cost of violating each of these inequalities [21, 25]. In particular, [13, 34] consider a loss function that
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	1905882502	e distances between the query and irrelevant items [30, 40, 42]. Each modal projection is then learned so as to minimize a loss function that measures the cost of violating each of these inequalities [21, 25]. In particular, [13, 34] consider a loss function that minimizes the distance between pairs of matching cross-modal items while maximizing the distance between non-matching cross-modal pairs. However
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2092361219,2137918516	e the similarity between different modalities of data. In the Information Retrieval (IR) community, early work have circumvented this issue by annotating images to perceive their underlying semantics [20, 37]. However, these approaches generally require a supervision from users to annotate at least a small sample of images. An unsupervised solution has emerged from the deep learning community which consis
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2076434944	its improved version PWC++, described below. •PWC++, the improved version of our implementation PWC*. More particularly, we add a positive margin to the pairwise loss adopted in [34], as proposed by [19]: ℓpw++(θ,xq,x)= y d(xq,x)−αpos + + (1 −y ) αneд d xq, + (6) with y = 1 (resp. y = 0) for pos. (resp. neg.) pairs. The positive margin αpos allows matching pairs to have different representations, thu
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2117539524	nd was chosen in order to obtain comparable results to [34] by sharing a similar setup. The ResNet-50 is pretrained on the large-scale dataset of the ImageNet Large Scale Visual Recognition Challenge [33], containing 1.2 million images, and is fine-tuned with the whole architecture. This neural network is followed by a fully connected layer, which maps the outputs of the ResNet-50 into the latent spac
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	1486649854	orithm [32]. With the objective to consider the different granularity levels of the instruction text, we use a hierarchical LSTM in which the word-level is pretrained using the skip-thought technique [26] and is not fine-tuned while the sentence-level is learned from scratch. 3.2.2 Retrieval loss. The objective of the retrieval loss Lins is to learn item embeddings by constraining the latent space acc
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	2153579005	s input of a fully connected layer that maps the recipe features into the latent space. For ingredients, we use a bidirectional LSTM [17] on pretrained embeddings obtained with the word2vec algorithm [32]. With the objective to consider the different granularity levels of the instruction text, we use a hierarchical LSTM in which the word-level is pretrained using the skip-thought technique [26] and is
2798397499	Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings	12634471,1905882502,1982635469	tter understanding of each domain-specific word/image/video. In practice, a typical approach consists in aligning text and image representations in a shared latent space in which they can be compared [6, 9, 21, 22, 25, 34]. One direct application in the cooking context is to perform cross-modal retrieval where the goal is to retrieve images similar to a text recipe query or conversely text recipes similar to a image qu
2798421013	Towards Robust and Privacy-preserving Text Representations	1731081199	,b), (1) where X denotes the cross entropy function. The negative sign of the second term, referred to as the adversarial loss, can be implemented by a gradient reversal layer during backpropagation (Ganin et al., 2016). To elaborate, training is based on standard gradient backpropagation for learning the main task, but for the auxiliary task, we start with standard loss backpropagation, howevergradients arereversed
2798421013	Towards Robust and Privacy-preserving Text Representations	1982858363	ently, and many individuals may wish to keep this information private. Consider the case of the AOL search data leak, in which AOL released detailed search logs of many of their users in August 2006 (Pass et al., 2006). Although they deidentiﬁed users in the data, the log itself contained sufﬁcient personally identiﬁable information that allowed many of these individuals to be identifed (Jones et al.,2007). Other s
2798421013	Towards Robust and Privacy-preserving Text Representations	2253444830	erse, and differs according to author, their background, and personal attributes such as gender, age, education and nationality. This variation can have a substantial effectonNLPmodelslearnedfromtext(Hovy et al., 2015), leading to signiﬁcant variation in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore
2798421013	Towards Robust and Privacy-preserving Text Representations	2109426455	ten involves data masking, addition or noise, or other forms of corruption, such that formal bounds can be stated in terms of the likelihood of reconstructing the protected components of the dataset (Dwork, 2008). This often comes at the cost of an accuracy reduction for models trained on the corrupted data (Shokri and Shmatikov, 2015; Abadi et al., 2016). Another related setting is where latent representatio
2798421013	Towards Robust and Privacy-preserving Text Representations	2253444830	location classes (US, UK, Germany, Denmark, and France), which were highly skewed in the original dataset. We use the same binary representation of SEX and AGE as the POS task, following the setup in Hovy et al. (2015). To evaluate the different models, we perform 10-fold cross validation and report test performance in terms of the F1 score for the RATING task, and the accuracy of each discriminator. Note that the
2798421013	Towards Robust and Privacy-preserving Text Representations	2253444830	mplications for the authors whose text is used to train our models. Many user attributes have been shown to be easily detectable from online review data, as used extensivelyinsentimentanalysisresults(Hovy et al., 2015; Potthast et al., 2017). In this paper, we focusonthree demographic variables ofgender, age, and location. Model Sentiment is framed as a 5-class text classiﬁcation problem, which we model using Kim
2798421013	Towards Robust and Privacy-preserving Text Representations	1731081199	ns of the data with respect to speciﬁc attributes which we expect to behave as confounds in a generalisation setting. To do so, we take inspiration from adversarial learning (Goodfellow et al., 2014; Ganin et al., 2016). The architecture is illustrated in Figure 1. 2.1 Adversarial Learning A key idea of adversarial learning, following Ganin et al. (2016), is to learn a discriminator model D jointly with learning the
2798421013	Towards Robust and Privacy-preserving Text Representations	2539350388	reover, models ﬁt to language corpora often ﬁxate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, is rarely fundamental to the task of modelling language, and is better considered as a confounding inﬂuence. These auxiliary learning signals can mean the models do not adequat
2798421013	Towards Robust and Privacy-preserving Text Representations	1731081199	thm (Hitaj et al., 2017). This is true even when differential privacy mechanisms have been applied. Inspired by the above works, and recent successes of adversarial learning (Goodfellow et al., 2014; Ganin et al., 2016), we propose a novel approach for privacy-preserving learning of unbiased representations.1 Specially, we employ Ganin et al.’s approach to training deep models with adversarial learning, to explicitl
2798421013	Towards Robust and Privacy-preserving Text Representations	2253444830	vate attribute(s). We compare models trained with zero, one, or all three private attributes, denoted BASELINE, ADV-*, and ADV-all, respectively. Data We again use the TrustPilot dataset derived from Hovy et al. (2015), however now we consider the RATING score as the target variable, not POS-tag. Each review is associated with three further attributes: gender (SEX), age (AGE), and location (LOC). To ensure that LOC
2798434138	Large-scale Multi-Domain Belief Tracking with Knowledge Sharing	2120615054	backward layers are concatenated to produce hd usr;hdsys of size Lfor the user and system utterances respectively. In the second variant of the model, CNNs are used to produce these vectors (Kim,2014;Kalchbrenner et al., 2014). To detect the presence of the domain in the dialogue turn, element-wise multiplication is used as a similarity metric between the hidden states and the ontology embeddings of the domain: d k = hd ta
2798441735	A Visual Distance for WordNet.	2136930489	) applications, such word sense disambiguation, question answering, determining the structure of texts, information extraction and retrieval, etc. A lot of measures had been proposed for this purpose [1,2,3,4], mostly based on distances dened on the WordNet [5] hierarchy. WordNet [5] is a well known lexical database composed of nouns, verbs adjectives and adverbs interlinked using conceptual, semantic and
2798441735	A Visual Distance for WordNet.	2136930489	ance matrix and sections of them could give additional information about the VD. 5.2. Comparison with Distances from WordNet We can compare the VD with some of the lexical and semantic ones dened in [1], using the distance matrix proposed in x5.1. Specically, we aim to compare it with the path similarity distance, the Wu and Palmer similarity [4] and the Lin similarity [2], as all three are metrics
2798441735	A Visual Distance for WordNet.	2194775991	generate a representation of synsets in the exceptionally rich language learnt by a deep CNN. Recent studies show that features learned by a CNN for a given purpose can be reused for a dierent task [6,7,8]. This illustrates the potential generality of features learned by these models. Furthermore, results from [8,9] indicate that images belonging to the same synset will share a signicant amount of fea
2798441735	A Visual Distance for WordNet.	2136930489	happen, it would mean that the images from the two synsets are very similar, for example, the synsets goat and mountain goat. Property 2. The possible values given by the metric are within the range [0;1]. That is because the denition of the distance is based on a proportion so that it will be bounded. This property can help compare this distance with other distances and makes the value given by the
2798441735	A Visual Distance for WordNet.	2136930489	n computing the information content of words, which in turn is obtained through statistics of their use in a given corpus [3,11,2]. These types of approaches are extensively discussed and compared in [1]. In this work, we propose a similarity and distance between concepts based on the visual properties found in images tagged to the corresponding WordNet synset. This multimodal approach (i.e., picture
2798441735	A Visual Distance for WordNet.	2136930489	seudo-metric. The VD is generated upon the visual similarity, the denition of which is a proportion (see Denition 2). Accordingly, the distance will be symmetric and provide values within the range [0;1]. Thus, the VD is always non-negative and symmetric. Also, it is easy to prove using induction over the length of the representative that fulls the triangle inequality. It does not satisfy the identi
2798441907	A DATASET OF PEER REVIEWS (PEERREAD): COLLECTION, INSIGHTS AND NLP APPLICATIONS	2765203623	. respect to a group of top-tier NLP, ML and AI venues: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR and AAAI. Accepted papers. In order to assign ‘accepted’ labels, we use the dataset provided by Sutton and Gong (2017) who matched arXiv submissions to their bibliographic entries in the DBLP directory10 by comparing titles and author names using Jaccard’s distance. To improve our coverage, we also add an arXiv submi
2798441907	A DATASET OF PEER REVIEWS (PEERREAD): COLLECTION, INSIGHTS AND NLP APPLICATIONS	2765203623	ity of research work being disseminated to the larger research community. With the evergrowing rates of articles being submitted to toptier conferences in Computer Science and pre-print repositories (Sutton and Gong, 2017), there is a need to expedite the peer review process. Balachandran (2013) proposed a method for automatic analysis of conference submissions to recommend relevant reviewers. Also related to our accep
2798441907	A DATASET OF PEER REVIEWS (PEERREAD): COLLECTION, INSIGHTS AND NLP APPLICATIONS	2250251786	lachandran (2013) proposed a method for automatic analysis of conference submissions to recommend relevant reviewers. Also related to our acceptance predicting task are (Tsur and Rappoport, 2009) and Ashok et al. (2013), both of which focuses on predicting book reviews. Various automatic tools like Grammerly28 can assist reviewers in discovering grammar and spelling errors. Tools like Citeomatic29 (Bhagavatula et al
2798441907	A DATASET OF PEER REVIEWS (PEERREAD): COLLECTION, INSIGHTS AND NLP APPLICATIONS	2064675550	text only, (ii) the review text only, or (iii) both paper and review text. We use three neural architectures: convolutional neural networks (CNN, Zhang et al., 2015), recurrent neural networks (LSTM, Hochreiter and Schmidhuber, 1997), and deep averaging networks (DAN, Iyyer et al., 2015). In all three architectures, we use a linear output layer to make the ﬁnal prediction. The loss function is the mean squared error between predi
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2114451917	2001;Mannor et al.,2004; S¸ims¸ek et al.,2005;Entezari et al.,2011;Bacon, 2013). Another family of algorithms identiﬁes commonalities of policies and extracts these partial policies as useful skills (Thrun and Schwartz, 1994;Pickett and Barto,2002;Brunskill and Li, 2014). While similar in spirit to ours, these methods do not easily scale to continuous problems as in dialogue systems. More recently, researchers have propo
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2109910161	ant information and we transform these information to low dimensional representations as the initial inputs for the RNN1 instances. This is based on the causality assumption of the options framework (Sutton et al., 1999) — the agent should be able to determine the next option given all previous information, and this should not depend on information related to any later state. The low dimensional representations are o
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2594829461	cale to continuous problems as in dialogue systems. More recently, researchers have proposed deep learning models to discover subgoals in continuous-state MDPs (Bacon et al., 2017;Machado et al.,2017;Vezhnevets et al., 2017). It would be interesting to see how effective they are for dialogue management. Segmental structures are common in human languages. In the NLP community, some related research on segmentation include
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2132997613	hat arXiv:1804.07855v3 [cs.CL] 22 Sep 2018 the subgoals discovered by SDN are often human comprehensible. 2 Background A goal-oriented dialogue can be formulated as a Markov decision process, or MDP (Levin et al., 2000), in which the agent interacts with its environment over a sequence of discrete steps. At each step t2f0;1;:::g, the agent observes the current state s t of the conversation (Henderson, 2015;Mrkˇsi c
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2133458291	tezari et al.,2011;Bacon, 2013). Another family of algorithms identiﬁes commonalities of policies and extracts these partial policies as useful skills (Thrun and Schwartz, 1994;Pickett and Barto,2002;Brunskill and Li, 2014). While similar in spirit to ours, these methods do not easily scale to continuous problems as in dialogue systems. More recently, researchers have proposed deep learning models to discover subgoals i
2798451412	Subgoal Discovery for Hierarchical Dialogue Policy Learning	2109910161	tiple simpler subtasks, solve them, and combine the partial solutions into a full solution for the original task. Such an approach may be formalized as hierarchical RL (HRL) in the options framework (Sutton et al., 1999). An option can be understood as a subgoal, which consists of an initiation condition (when the subgoal can be triggered), an option policy to solve the subgoal, and a termination condition (when the
2798469938	Transfer Learning for Context-Aware Question Matching in Information-seeking Conversation Systems in E-commerce	2311783643,2410983263	del 0.214 0.194 0.221 Our Model 0.266 0.291 0.288 4 Related Work Recent research in multi-turn conversations has focused on deep learning and reinforcement learning (Shang et al.,2015;Yan et al.,2016;Li et al., 2016a,b;Sordoni et al.,2015;Wu et al.,2017; Yang et al.,2018). The recent proposed Sequential Matching Network (SMN) (Wu et al.,2017) matches a response with each utterance in the context at multiple leve
2798469938	Transfer Learning for Context-Aware Question Matching in Information-seeking Conversation Systems in E-commerce	2604986524	learning, many Neural Network (NN) based methods are proposed (Yosinski et al.,2014). A typical framework uses a shared NN to learn shared features for both source and target domains (Mou et al.,2016;Yang et al., 2017). Another approach is to use both a shared NN and domain-speciﬁc NNs to derive shared and domain-speciﬁc features (Liu et al.,2017). This is improved by some studies (Ganin et al.,2016; Taigman et al.
2798469938	Transfer Learning for Context-Aware Question Matching in Information-seeking Conversation Systems in E-commerce	2891416139	MT-hCNN) with two CNN based models ARC-I and ARC-II (Hu et al.,2014), and several advanced neural matching models: MVLSTM (Wan et al.,2016), Pyramid (Pang et al., 2016) Duet (Mitra et al.,2017), SMN (Wu et al., 2017)10, and a degenerated version of our model that removes CNN 3 from our MT-hCNN model (MT-hCNN-d). All the methods in this paper are implemented with TensorFlow and are trained with NVIDIA Tesla K40M G
2798469938	Transfer Learning for Context-Aware Question Matching in Information-seeking Conversation Systems in E-commerce	2286300105	ompared Methods: We compared our multiturn model (MT-hCNN) with two CNN based models ARC-I and ARC-II (Hu et al.,2014), and several advanced neural matching models: MVLSTM (Wan et al.,2016), Pyramid (Pang et al., 2016) Duet (Mitra et al.,2017), SMN (Wu et al., 2017)10, and a degenerated version of our model that removes CNN 3 from our MT-hCNN model (MT-hCNN-d). All the methods in this paper are implemented with Ten
2798471701	Conversations Gone Awry: Detecting Early Signs of Conversational Failure	2540646130	a “rude, insulting, or disrespectful” comment towards another user in the conversation—i.e., a personal attack. In contrast to prior work labeling antisocial comments in isolation (Sood et al. ,2012;Wulczyn et al. 2017), annotators are asked to label personal attacks in the context of the conversations in which they occur, since antisocial behavior can often be contextdependent (Cheng et al.,2017). In fact, in order
2798471701	Conversations Gone Awry: Detecting Early Signs of Conversational Failure	1654173042	her than the behavior of individuals across disparate discussions. Discourse analysis. Our present study builds on a large body of prior work in computationally modeling discourse. Both unsupervised (Ritter et al., 2010) and supervised (Zhang et al.,2017a) approaches have been used to categorize behavioral patterns on the basis of the language that ensues in a conversation, in the particular realm of online discussio
2798471701	Conversations Gone Awry: Detecting Early Signs of Conversational Failure	2138969065	g that mitigates effects which may trivialize the task. In particular, some topical contexts (such as politics and religion) are naturally more susceptible to antisocial behavior (Kittur et al. ,2009;Cheng et al. 2015). We employ techniques from causal inference (Rosenbaum,2010) to establish a controlled framework that focuses our study on topic-agnostic linguistic cues. In this controlled setting, we ﬁnd that prag
2798471701	Conversations Gone Awry: Detecting Early Signs of Conversational Failure	2138969065	hida,2006) hamper the efforts of even the best intentioned collaborators. Prior computational work has focused on characterizing and detecting content exhibiting antisocial online behavior: trolling (Cheng et al., 2015,2017), hate speech (Warner and Hirschberg, 2012;Davidson et al.,2017), harassment (Yin et al.,2009), personal attacks (Wulczyn et al., Corresponding senior author. 1“Now you see the souls of those wh
2798471701	Conversations Gone Awry: Detecting Early Signs of Conversational Failure	2065100127	ts represent signatures of conversation-starters spanning a wide range of topics and contexts which reﬂect core elements of Wikipedia, such as moderation disputes and coordination (Kittur et al.,2007;Kittur and Kraut, 2008). We assign each comment in our present dataset to one of these types.9 7We scale rows of U R and P^ to unit norm. We assign comments whose vector representation has (‘ 2) distance 1 to all cluster c
2798478419	PARSING TWEETS INTO UNIVERSAL DEPENDENCIES	655994999	and without efﬁciency sacriﬁces. Our dataset and system are publicly available at https://github.com/Oneplus/Tweebank and https://github.com/Oneplus/twpipe. 2 Annotation We ﬁrst review TWEEBANK V1 ofKong et al. (2014), the previous largest Twitter dependency annotation effort (§2.1). Then we introduce the differences in our tokenization (§2.2) and part-ofspeech (§2.3) (re)annotation withO’Connor et al. (2010) andG
2798478419	PARSING TWEETS INTO UNIVERSAL DEPENDENCIES	2301095666	F 1 98.3 97.3 POS tagging F 1 93.3 92.2 UD parsing LAS F 1 74.0 71.4 Table 7: Evaluating our pipeline against a state-ofthe-art pipeline. models that use dynamic oracles (Goldberg and Nivre,2012,2013;Kiperwasser and Goldberg, 2016;Ballesteros et al.,2016). We ﬁnd that this approach outperforms the conventional distillation model, coming in 1.5 points behind the ensemble (last line of Table6). Pipeline evaluation. Finally, we r
2798478419	PARSING TWEETS INTO UNIVERSAL DEPENDENCIES	655994999	f speech can be applied. 2.4 Universal Dependencies Applied to Tweets We adopt UD version 2 guidelines to annotate the syntax of tweets. In applying UD annotation conventions to tweets, the choices ofKong et al. (2014) must be revisited. We consider the key questions that arose in our annotation effort, and how we resolved them. Acronym abbreviations. We followKong et al. (2014) and annotate the syntax of an acrony
2798500443	Multi-lingual Common Semantic Space Construction via Cluster-Consistent Word Embedding	2118090838	state-of-the-art multi-lingual embedding learning methods. 2 Related Work Multilingual word embeddings have advanced many multilingual natural language processing tasks, such as machine translation (Zou et al., 2013;Mikolov et al.,2013;Madhyastha and Espana-Bonet˜ ,2017), dependency parsing (Guo et al.,2015;Ammar et al.,2016a), and name tagging (Zhang et al.;Tsai and Roth,2016). Using bilingual aligned words, pr
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2127978399,2129842875,2132655161	)(0#%,,.#%( &gt;&apos;&apos;# E1;,012 G701-% v r ,39)1() 9%D&apos;76393#( Figure 2: Overview of the ReMine Framework. sentences, which are then used to form entity pairs for relation tuple extraction [3, 8, 32]. Some systems further rely on dependency parsers to generate syntax parse trees to guide the relation tuple extraction [2, 10, 32]. However, these systems su‡er from error propagation as the errors i
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2167187514	= juj” (4) Phrase Mining [20, 33] makes an assumption that quality phrases can only be frequent n-grams within a corpus. To overcome the phrasal sparsity of this assumption, several NP-chunking rules [12] inTable1, areadoptedtodiscoverinfrequentbutinformativephrase candidates. In experiment 4.2, ReMine has be−er performance than AutoPhrase [33] as we consider more phrase candidates and multitype segme
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2251913848	2] utilizes open pa−ern learning and extracts pa−erns over the dependency path and part-of-speech tags. (2) ClausIE [10] adopts clause pa−erns to handle long-distance relationships. (3) StanfordOpenIE[2] learns a clause spli−er via distant training data. (4) MinIE [14] re•nes tuple extracted by ClausIE by identifying and removing parts that are considered overly speci•c. (5) ReMine-Lis a base model o
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2406945108	2007 New York Times news articles. 2Codes and datasets can be downloaded at h−ps://github.com/GentleZhu/ReMine 395 sentences are manually annotated with entities and their relationships. (2) Wiki-KBP [19]: „e training corpus contains 2.4k sentences sampled from ˘780k Wikipedia articles [19] as the training corpus and 290 manually annotated sentences as test data. (3) Twi−er [38]: consists of 1.4 milli
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1529731474	didate set was labeled by two independent annotators. A tuple is labeled as positive only if both labelers agree on its correctness. All tuples with con…icting labels results were •ltered. Similar to [10], we ignored the context of the extracted tuples during labeling. For example, both (“we”, “hate”, “it”) and (“he”, “has”, “father”) will be treated as correct as long as they meet the fact described
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2295030615	ds. For the entity phrase extraction task, NYT and Wiki-KBP are used for evaluation, since both datasets contain annotated entity mentions in test set. We adopt the sequence labeling evaluation setup [24], and compare ReMine’s entity phrase extractionmodulewithtwostate-of-the-artsequencelabelingmethods and one distantly-supervised phrase mining method on the test sets: (1) Ma &amp; Hovy [24]: adopts a
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1529731474,2129842875	e generated by relation tuple generation module as introduced in Sec 3.2, since the nearest subject of England in Fig. 3b is city. To get rid of such false tuples, current methods use textual pa−erns [10, 32] to identify it as a false extraction. In contrast, we design global cohesiveness measure using corpus-level statistics, and integrate the measure with the relation tuple generation. To capture the gl
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1934264538	entitypairsconstructedE+ p 0 andrelation tuples T, we construct a pseudo knowledge graph. Particularly, predicate ph;t may contain several relation phrases. Motivated by process of knowledge traverse [15], we average multiple relation phrases embeddings to represent the predicate i.e.vp = ˝ n i=1 vr i šn. Example 3.4 (Generating False Tuples). hParis, become capital of, Francei!h city, become capital
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2250635077	with existing knowledge base tuples, whileourproposedglobalcohesivenessrepresentationstarts from noisy extractions. „ere is another line of work trying to combining KB relations and textual relations [36] or model unstructured and structured data by universal schema [29]. However, they are all built upon on existing and speci•c relation types. Although we shared similar semantic measures as these work
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2295030615	extraction algorithms for the weakly-supervised entity phrase extraction task. Methods NYT [29] Wiki-KBP [19] F1 Prec Rec F1 Prec Rec AutoPhrase [33] 0.531 0.543 0.519 0.416 0.529 0.343 Ma &amp; Hovy [24] 0.664 0.704 0.629 0.324 0.629 0.218 Liu. et al. [22] 0.676 0.704 0.650 0.337 0.629 0.230 ReMine 0.648 0.524 0.849 0.515 0.636 0.432 Cut-o‡ ⁄reshold for Extraction Output. „e number of tuple extractio
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	277886906,1512387364	Hearst pa−erns like “NP0 such as fNP1;NP2;:::g” for hyponymy relation extraction [16]. Carlson and Mitchell et al. introduced Never-Ending Language Learning (NELL) based on freetext predicate pa−erns [7, 26]. ReVerb [12] identi•ed relational phrases via part-of-speech-based regular expressions. Besides partof-speech tags, recent works have started to use more linguistic features, such as dependency parsi
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1529731474,2251913848	hich are then used to form entity pairs for relation tuple extraction [3, 8, 32]. Some systems further rely on dependency parsers to generate syntax parse trees to guide the relation tuple extraction [2, 10, 32]. However, these systems su‡er from error propagation as the errors in prior parts of the pipeline(e.g., entity recognition) could accumulate by cascading down the pipeline(e.g., to relation tuple ext
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2150815390	i=1 P„bi+1;sijbi;F” (1) ReMine generates each segment as follows, 1. Given the start indexbi, generate the end indexbi+1 according to context-free prior P„bi+1 bi”= δjb i+1b i j, i.e. length penalty [21]. 2. Given the start and end index „bi;bi+1”of segment si, generate a word sequence si according to a multinomial distribution over all segments of the same length. P„sijbi;bi+1”= P„sijbi+1 bi” (2) 1e
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2167187514	ike “NP0 such as fNP1;NP2;:::g” for hyponymy relation extraction [16]. Carlson and Mitchell et al. introduced Never-Ending Language Learning (NELL) based on freetext predicate pa−erns [7, 26]. ReVerb [12] identi•ed relational phrases via part-of-speech-based regular expressions. Besides partof-speech tags, recent works have started to use more linguistic features, such as dependency parsing, to induce
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1529731474,2129842875	istant supervision” and adjust quality iteratively. Second, the tuple extraction module generates candidate tuples based on sentence’s language structure—it adopts widely used local structure pa−erns [10, 27, 32], including syntactic and lexical pa−erns over pos tags and dependency parsing tree. Di‡erent from previous studies, the module incorporates corpus-level information redundancy. Last, the global cohes
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2150815390	ledge base as positive samples and draws a number of phrases from unknown candidates as negative samples. Among all word sequence si, we denote unique phrase as u and P„sijbi+1 bi”as θu. Similar with [21], we use Viterbi Training [1] to •nd best segmentation boundary B and parameters θ; δiteratively. In the E-step, given and , dynamic programming is used to •nd the optimized segmentation. In the M-ste
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2283196293	ly, embedding models [5, 18, 30, 34] have been widely used to learn semantic representation for both entities and relations. By observing each relation may have di‡erent semantic meaning, Wang et al. [37] projected entity vectors to relation-speci•c hyperplane. Further research [15, 23] shows that embedding techniques can support composite query(i.e. asking about multiple relations) on knowledge graph
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2295030615	ny entities we get are correct), Recall (i.e. how many correct entities do we get), and F1-score to evaluate the performances on entity phrase extraction task, same as other sequence labeling studies [24]. For the Open IE task, since each tuple obtained by ReMine and other benchmark methods will also be assigned a con•dence score. We rank all the tuples according to their con•dence scores. Based on th
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2129842875,2167187514	un phrases. In sentence s4 of Fig. 2, “Your dry cleaner” is not a named entity, although it is the subject of this sentence and cannot be omi−ed in relation tuples extraction. „erefore, previous work [12, 32] use pre-trained NP chunkers to identify entity phrases. Positiveentityphrasepairs E+ p is a set of entity pairs that may have textual relations between them. Relation phrase r describes relation betw
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1934264538,2251363251	presentation for both entities and relations. By observing each relation may have di‡erent semantic meaning, Wang et al. [37] projected entity vectors to relation-speci•c hyperplane. Further research [15, 23] shows that embedding techniques can support composite query(i.e. asking about multiple relations) on knowledge graph. All previouos knowledge graph embedding methods start with existing knowledge bas
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1552847225	purely on local context, however, large corpus can exclude false extractions from inferred inconsistencies. KnowledgeBaseEmbeddingandCompletion. Knowledgebases (KBs), such as DBpedia [4] and Freebase [17], extract tuples from World Wide Web. Knowledge base population or completion aims at predicting whether tuples not in knowledge base are likely to be true or not. Previous works a−empted to construct
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2127978399,2129842875,2132655161	rrect. question answering systems [13, 35]. While traditional IE systems require people to pre-specify the set of relations of interest, recent studies on open-domain information extraction (Open IE) [3, 8, 32] rely on relation phrases extracted from text to represent the entity relationship, making it possible to adapt to various domains (i.e., open-domain) and di‡erent languages (i.e., language-independen
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2251913848	ths, which is typically subject, predicate and optional object with complement. Angeli et al. adopts a clause spli−er using distant training and statistically maps predicate to known relation schemas [2]. MinIE [14] removes overly-speci•c constituents and captures implicit relations in ClausIE by introducing several statistical measures like polarity, modality, a−ribution, and quantities. Compared wi
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2094728533	ts are evaluated purely on local context, however, large corpus can exclude false extractions from inferred inconsistencies. KnowledgeBaseEmbeddingandCompletion. Knowledgebases (KBs), such as DBpedia [4] and Freebase [17], extract tuples from World Wide Web. Knowledge base population or completion aims at predicting whether tuples not in knowledge base are likely to be true or not. Previous works a−e
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1646084575,2090243146	ucting false tuples from extractions, “city” occurs with relation “capitalof”inthenegativepoolmoreo–en,thenitisunlikelyfortuples with “city” and “capital of” to be correct. question answering systems [13, 35]. While traditional IE systems require people to pre-specify the set of relations of interest, recent studies on open-domain information extraction (Open IE) [3, 8, 32] rely on relation phrases extrac
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2406945108	uples is smaller than 103. Table 3: Performance comparison with state-of-the-art entity phrase extraction algorithms for the weakly-supervised entity phrase extraction task. Methods NYT [29] Wiki-KBP [19] F1 Prec Rec F1 Prec Rec AutoPhrase [33] 0.531 0.543 0.519 0.416 0.529 0.343 Ma &amp; Hovy [24] 0.664 0.704 0.629 0.324 0.629 0.218 Liu. et al. [22] 0.676 0.704 0.650 0.337 0.629 0.230 ReMine 0.648 0.
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2127426251,2184957013,2296268288	wledge base are likely to be true or not. Previous works a−empted to construct web-scale knowledge base using statistical learning and pre-de•ned rules and predicates [28]. Recently, embedding models [5, 18, 30, 34] have been widely used to learn semantic representation for both entities and relations. By observing each relation may have di‡erent semantic meaning, Wang et al. [37] projected entity vectors to rel
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	2068737686	of work, that is, pa−ern based methods or clause based methods. Pa−ern based information extraction can be as early as Hearst pa−erns like “NP0 such as fNP1;NP2;:::g” for hyponymy relation extraction [16]. Carlson and Mitchell et al. introduced Never-Ending Language Learning (NELL) based on freetext predicate pa−erns [7, 26]. ReVerb [12] identi•ed relational phrases via part-of-speech-based regular ex
2798514202	Integrating Local Context and Global Cohesiveness for Open Information Extraction	1529731474	xtraction task, we consider following Open IE baselines for comparison: (1) OLLIE [32] utilizes open pa−ern learning and extracts pa−erns over the dependency path and part-of-speech tags. (2) ClausIE [10] adopts clause pa−erns to handle long-distance relationships. (3) StanfordOpenIE[2] learns a clause spli−er via distant training data. (4) MinIE [14] re•nes tuple extracted by ClausIE by identifying a
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2194775991	: Using only the BiLSTM output vectors as the input to the FF-layer. LSTMS: Summing up the hypothesis vector and the BiLSTM output vectors as the input to the FF-layer, similar to residual networks (He et al., 2016). LSTMC: Concatenating the hypothesis vector and the BiLSTM output vectors as the input to the FF-layer. LSTMCH: Same as LSTMC except that manual cross-hypothesis features used for NCH were also add
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2313437166	ang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). To our knowledge, the work by Robichaud et al. (201
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1899794420	r for domain classiﬁcation. Figure 2 shows the overall shortlister architecture. Embedding layerIn order to capture characterlevel patterns, we construct an orthographysensitive word embedding layer (Ling et al., 2015; Ballesteros et al., 2015). Let C, W, and denote the set of characters, the set of words, and the vector concatenation operator, respectively. We represent an LSTM as a mapping ˚: Rd Rd 0!Rd that tak
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2604986524	et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 20
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2043511823	g NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al.
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	146318628	including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engines to improve response category class
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1522301498	the intent and slot models are trained on roughly 70% of the available training data. In our experiments, all the models were implemented using Dynet (Neubig et al., 2017) and were trained with Adam (Kingma and Ba, 2015). We used the initial learning rate of 4 10 4 and left all the other hyper-parameters as suggested in Kingma and Ba (2015). We also used variational dropout (Gal and Ghahramani, 2016) for regularizati
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1501568714	ious natural language processing tasks, including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engi
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1972595521	ky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). To our knowledge, the work by Robichaud et al. (2014); Crook et al. (2015); Khan et al. (2015) is most closely related to this paper. Their
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2783737271	main classiﬁcation in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain ada
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1633466834	ms, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engines to improve response category classiﬁcation for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu a
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2097584131	be re-scored. Reranking has been applied to various natural language processing tasks, including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	2212703438	re trained with Adam (Kingma and Ba, 2015). We used the initial learning rate of 4 10 4 and left all the other hyper-parameters as suggested in Kingma and Ba (2015). We also used variational dropout (Gal and Ghahramani, 2016) for regularization. 6.3Results and Discussion Table 3 summarizes the k-best classiﬁcation accuracy results for our Shortlister. With only 20 domains in the traditional IPDA setting, the accuracy is o
2798534718	A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding.	1860935423	ﬁcation. Figure 2 shows the overall shortlister architecture. Embedding layerIn order to capture characterlevel patterns, we construct an orthographysensitive word embedding layer (Ling et al., 2015; Ballesteros et al., 2015). Let C, W, and denote the set of characters, the set of words, and the vector concatenation operator, respectively. We represent an LSTM as a mapping ˚: Rd Rd 0!Rd that takes an input vector xand a s
2798539789	Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders	2306876680	hension (Du et al.,2017;Zhou et al.,2017b;Yuan et al.,2017;Subramanian et al., 2017), question answering (Qin,2015;Tang et al., 2017;Wang et al.,2017;Song et al.,2017), and visual question answering (Mostafazadeh et al., 2016). In such tasks, the answer is known and is part of the input to the generated question. Meanwhile, the generation tasks are not required to predict additional topics since all the information has bee
2798545440	Named Entities troubling your Neural Methods? Build NE-Table: A neural approach for handling Named Entities.	836999996	alized embeddings of the NEs ﬁxed as a way to handle NEs and OOV words. NE in Dialog: There has been a lot of interest in end-to-end training of dialog systems (Vinyals and Le,2015;Serban et al.,2016;Lowe et al., 2015;Kadlec et al.,2015;Shang et al.,2015;Guo et al.,2017). Among recent work,Williams and Zweig(2016) use an LSTM model that learns to interact with APIs on behalf of the user;Dhingra et al.(2017) use re
2798545440	Named Entities troubling your Neural Methods? Build NE-Table: A neural approach for handling Named Entities.	86887328	ogram that could run on databases, but those approaches are only veriﬁed on small or synthetic databases. Other papers dealing with large Knowledge Bases (KB) usually rely on entity linking techniquesCucerzan (2007);Guo et al.(2013), which links entity mentions in texts to KB queries.Yih et al.(2015);Yin et al.(2016);Yu et al.(2017) compare the text spans in questions with KB entity names at the character-level
2798545440	Named Entities troubling your Neural Methods? Build NE-Table: A neural approach for handling Named Entities.	86887328	in unexpected ways. There is another simple way in which NEs are handled in many real world systems, which is to ﬁrst recognize the NEs with either NE taggers (Finkel et al.,2005) or entity linkers (Cucerzan, 2007;Guo et al.,2013;Yang and Chang,2015), and then replace them with NE-type tags. For exarXiv:1804.09540v1 [cs.CL] 22 Apr 2018 ample, all location names could be replaced with the tag NE location. This
2798571192	A Report on the Complex Word Identification Shared Task 2018	2251814648	d appear in the BEA workshop proceedings. 1 Introduction The most common ﬁrst step in lexical simpliﬁ- cation pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identiﬁcation (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the ﬁndings of the second Comp
2798571192	A Report on the Complex Word Identification Shared Task 2018	2250539671	ls were trained using Linear Regression, Logistic Regression, Decision Trees, Gradient Boosting, Extra Trees, AdaBoost, and XGBoost classiﬁers. For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the ﬁrst approach, the resulting vector is passed on to a neural network with two ReLu laye
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	2122563357	ary labels based on the sign of the annotations’ average. YouTube dataset is an English multimodal dataset that contains 262 positive, 212 negative, and 133 neutral utterance-level clips provided by (Morency et al., 2011). We only consider the positive and negative labels during our experiments. IEMOCAP is a multimodal emotion dataset including visual, audio, and text data (Busso et al., 2008). For each sentence, we u
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	1973270182	fted features as representations (Seppi et al., 2008; Rozgic et al., 2012). Recently, deep learning structures such as CNNs and LSTMs have been used to extract highlevel features from text and audio (Eyben et al., 2010; Poria et al., 2015). However, not all parts of the text and vocal signals contribute equally to the predictions. A speciﬁc word may change the entire sentimental state of text; a different vocal del
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	2156140659	ively little work on combining text data. Early work combined human transcribed lexical features and low-level handcrafted acoustic features using feature-level fusion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004). Others used SVMs fed bag of words (BoW) and part of speech (POS) features in addition to low-level acoustic features (Seppi et al., 2008; Rozgic et al., 2012; Savran et al., 2012; Rosas et al., 2013
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	1973270182	ng was used to extract higher-level multimodal features. Bidirectional LSTMs were used to learn long-range dependencies from low-level acoustic descriptors and derivations (LLDs) and visual features (Eyben et al., 2010; Wollmer et al., 2013).¨ CNNs can extract both textual (Poria et al., 2015) and visual features (Poria et al., 2016) for multiple kernel learning of feature-fusion. Later, hierarchical LSTMs were use
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	2153579005	nterval for each word in the audio ﬁle based on the Sakoe-Chiba Band Dynamic Time Warping (DTW) algorithm (?). For the text input, we ﬁrst embedded the words into 300-dimensional vectors by word2vec (Mikolov et al., 2013), which gives us the best result compared to GloVe and LexVec. Unknown words were randomly initialized. Given a sentence Swith N words, let w i represent the ith word. We embed the words through the w
2798573901	Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment	1836465849	th. We used the rectiﬁed linear unit (ReLU) activation function and set 0.5 as the dropout rate. We also applied batch normalization functions between each layer to overcome internal covariate shift (Ioffe and Szegedy, 2015). We ﬁrst trained the text attention module and audio attention module individually. Then, we tuned the fusion network based on the word-level representation outputs from each ﬁne-tuning module. For a
2798583685	Learning Semantic Textual Similarity from Conversations.	2480068437	and mixed systems ECNU 0.847 0.810 BIT 0.829 0.809 2017) and other systems inCer et al. (2017): InferSent (Conneau et al.,2017), Sent2Vec (Pagliardini et al.,2017), SIF (Arora et al.,2017), PV-DBOW (Lau and Baldwin, 2016), C-PHRASE (Kruszewski et al.,2015), ECNU (Tian et al.,2017) and BIT (Wu et al., 2017). Note that the model trained on SNLI alone should be equivalent to the InferSent baseline, with the exception tha
2798583685	Learning Semantic Textual Similarity from Conversations.	2251861449	both models show competitive performance. Reddit+SNLI model outperforms SimBow-primary, which is ranked ﬁrst in the ofﬁcial ranking of the 2017 task. 5 Related Work The STS task was ﬁrst introduced byAgirre et al. (2012). Early methods focused on lexical semantics, surface form matching and basic syntactic similarity (Bar et al.¨ ,2012;Jimenez et al.,2012). More recently, deep learning based methods became competitiv
2798583685	Learning Semantic Textual Similarity from Conversations.	2373570000	titask learning over conversational and NLI data. 2.1 Conversational Learning Task using Input-Response Prediction We formulate the conversational learning task as response prediction given an input (Kannan et al., 2016;Henderson et al.,2017). Following prior work, the prediction task is cast as a response selection problem. As illustrated in ﬁgure2, the model P(yjx) attempts to identify the correct response yfrom K
2798583685	Learning Semantic Textual Similarity from Conversations.	2612953412	in this work. 4.3 SNLI Because the multitask model learns a shared encoder for the Reddit response prediction task and the SNLI classiﬁcation task, we also report results on the SNLI task. InferSent (Conneau et al., 2017) is used as the baseline as it served as the inspiration for the inclusion of the SNLI task in the multitask model. For comparison, we also list the results of Gumbel TreeLSTM (Williams et al., 2017)
2798589784	Modeling Naive Psychology of Characters in Simple Commonsense Stories	2040467972	categories. 2.2 Emotion Theory Among several theories of emotion, we work with the “wheel of emotions” ofPlutchik(1980), as it has been a common choice in prior literature on emotion categorization (Mohammad and Turney, 2013;Zhou et al.,2016). We use the eight basic emotional dimensions as illustrated in Figure2. 2.3 Mental State Explanations In addition to the motivation and emotion categories derived from psychology th
2798589784	Modeling Naive Psychology of Characters in Simple Commonsense Stories	2563734883	ting a model’s prediction. Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al.,2016;Yang et al.,2016;Ji et al.,2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al.,2018). Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories. 9 Conclusion We p
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	2518756966	. closed-vocab open-vocab (pure) words Mikolov et al. (2010), Sundermeyer, Schluter, and¨ Ney (2012) -impossiblewords + chars Kim et al. (2016), Ling et al. (2015) Kawakami, Dyer, and Blunsom (2017), Hwang and Sung (2017), F (pure) chars -impossible- Sutskever, Martens, and Hinton (2011) Table 4: Contextualizing this work (F) on two axes recent past. As in Hwang and Sung (2017), there is no ﬁxed vocabulary, so words t
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	2132957691	5261) when considering types. 7Baayen and Sproat (1996) argue for using only the hapax legomena (words that only appear once) to predict the behavior of rare and unknown words. The Bayesian approach (MacKay and Peto 1995; Teh 2006) is a compromise: frequent word types are also used, but they have no more inﬂuence than infrequent ones. 8Often 5–10% of held-out word tokens in language modeling datasets were never seen
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	1899794420	and compare ˙(w) to a random spelling s ˘p spell(je(w)). closed-vocab open-vocab (pure) words Mikolov et al. (2010), Sundermeyer, Schluter, and¨ Ney (2012) -impossiblewords + chars Kim et al. (2016), Ling et al. (2015) Kawakami, Dyer, and Blunsom (2017), Hwang and Sung (2017), F (pure) chars -impossible- Sutskever, Martens, and Hinton (2011) Table 4: Contextualizing this work (F) on two axes recent past. As in Hwan
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	1899794420	nd Hinton (2011) use an RNN to generate pure character-level sequences, yielding an openvocabulary language model, but one that does not make use of the existing word structure. Kim et al. (2016) and Ling et al. (2015) ﬁrst combined the two layers by deterministically constructing word embeddings from characters (training the embedding function on tokens, not types, to “get frequent words right”—ignoring the issues
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	2339995566	onal construction, and Pinter, Guthrie, and Eisenstein (2017), who prove that the spelling of a word shares information with its embedding. Finally, in the highly related ﬁeld of machine translation, Luong and Manning (2016) before the re-discovery of BPE proposed an open-vocabulary neural machine translation model in which the prediction of an UNK triggers a character-level model as a kind of “backoff.” We provide a pro
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	2518756966	rd embeddings (as has been done before by dos Santos and Zadrozny (2014)). Another line of work instead augments a character-level RNN with word-level “impulses.” Especially noteworthy is the work of Hwang and Sung (2017), who describe an architecture in which character-level and word-level models run in parallel from left to right and send vector-valued messages to each other. The word model sends its hidden state to
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	1975690018	s like the over and over again. Note that the use case that Kawakami et al. originally intended for their cache, the copying of highly infrequent words like Noriega that repeat on a very local scale (Church 2000), is not addressed in our model, so adding their cache module to our model might still be beneﬁcial. Less directly related to our approach of improving language models is the work of Bhatia, Guthrie,
2798597276	Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model	2154099718	types. 7Baayen and Sproat (1996) argue for using only the hapax legomena (words that only appear once) to predict the behavior of rare and unknown words. The Bayesian approach (MacKay and Peto 1995; Teh 2006) is a compromise: frequent word types are also used, but they have no more inﬂuence than infrequent ones. 8Often 5–10% of held-out word tokens in language modeling datasets were never seen in training
2798631125	InceptB: A CNN Based Classification Approach for Recognizing Traditional Bengali Games	1836465849	er range of applications. Inception-v3 [2] is one of the pre trained models on the TensorFlow [1]. It is a rethinking for the initial structure of computer vision after Inception-vI [5], Inception-v2 [4] in 2015. The Inception-v3 [2] model is trained on the ImageNet datasets, containing the information that can identify 1000 classes in ImageNet, the error rate of top-5 is 3.5%, the error rate of top-
2798631125	InceptB: A CNN Based Classification Approach for Recognizing Traditional Bengali Games	2601564443	p learning[5], ImageNet, CNN. A research group from china used inception-v3 [3] on Oxford17 and Oxford-102 flower dataset.Compared with any object classification methods, convolutional neural network [7],[8] use multilayer convolution to extract features and combine the features automatically. It has a higher recognition rate and a wider range of applications. Inception-v3 [2] is one of the pre train
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	98144779	“ski-in, ski-out”. Research on dialogue systems for hotel information has existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simpliﬁed the richness of the dom
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	2099542783	m (Carenini and Moore, 2000), and shown that users prefer systems whose dialogue behaviors are based on such customized content selection and presentation (Stent et al., 2002; Polifroni et al., 2003; Walker et al., 2007). These preferences (ranking on attributes) can be acquired directly from the user, or can be inferred from their past behavior. Here we try two other methods. First, in Section 3., we ask Turkers to
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	162553800	ome cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simpliﬁed the richness of the domain, and supported highly restricted versions of the hotel booking task, by limiting the information th
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	2249819665	s existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simpliﬁed the richness of the domain, and supported highly restricted versions of the hotel booking task, by limiti
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	1972639638	ted earlier in the dialogue, so one way to do this is to apply methods for inducing a ranking on the content attributes. Previous work has developed a model of user preferences to solve this problem (Carenini and Moore, 2000), and shown that users prefer systems whose dialogue behaviors are based on such customized content selection and presentation (Stent et al., 2002; Polifroni et al., 2003; Walker et al., 2007). These
2798635106	Exploring Conversational Language Generation for Rich Content about Hotels.	2085908902	tems for hotel information has existed for many years, in some cases producing shared dialogue corpora that include hotel bookings (Devillers et al., 2004; Walker et al., 2002; Rudnicky et al., 1999; Villaneau and Antoine, 2009; Bonneau-Maynard et al., 2006; Hastie et al., 2002; Lemon et al., 2006). Historically, these systems have greatly simpliﬁed the richness of the domain, and supported highly restricted versions of the
2798638375	Linguistically-Informed Self-Attention for Semantic Role Labeling	2626778328	ct syntactic dependencies with (2) multi-task learning across four related tasks. Figure1depicts the overall architecture of our model. The basis for our model is the Transformer encoder introduced byVaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked multi-head self-attention and feedforward layers (x2.1). To incorporate syntax, one self-attention head is
2798638375	Linguistically-Informed Self-Attention for Semantic Role Labeling	2158899491	tax-free models on out-of-domain data, a setting in which our technique excels. MTL (Caruana,1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011;Zhang and Weiss,2016;Hashimoto et al., 2017;Peng et al.,2017;Swayamdipta et al.,2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (S
2798638375	Linguistically-Informed Self-Attention for Semantic Role Labeling	2556468274	ting in which our technique excels. MTL (Caruana,1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011;Zhang and Weiss,2016;Hashimoto et al., 2017;Peng et al.,2017;Swayamdipta et al.,2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg,2016;Bingel and Søgaard,
2798638375	Linguistically-Informed Self-Attention for Semantic Role Labeling	2626778328	yntactic information. In response, we propose linguistically-informed self-attention (LISA): a model that combines multi-task learning (Caruana,1993) with stacked layers of multi-head self-attention (Vaswani et al., 2017); the model is trained to: (1) jointly predict parts of speech and predicates; (2) perform parsing; and (3) attend to syntactic parse parents, while (4) assigning semantic role labels. Whereas prior w
2798646872	Equipping Sequent-Based Argumentation with Defeasible Assumptions.	2156092566	orL, havingthefollowingproperties: reﬂexivity: ifφ∈S, thenS⊢φ; transitivity: ifS⊢φandS′,φ⊢ψ, then S,S′ ⊢ψ; and monotonicity: if S′ ⊢φ and S′ ⊆S, then S⊢φ. As usual in logical argumentation(see, e.g., [8, 16, 17, 22]), arguments have a speciﬁc structure based on the underlying formal language, the core logic. In the current setting arguments are represented by the well-knownproof theoretical notion of a sequent.
2798651567	Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization	2624177315	epresentation of Seq2Seq with that of autoencoder by minimizing the distance between two representations. Finally, we use adversarial learning to enhance the supervision. Following the previous work (Ma et al., 2017), We evaluate our proposed model on a Chinese social media dataset. Experimental results show that our model outperforms the state-of-theart baseline models. More speciﬁcally, our model outperforms th
2798651567	Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization	2136891251	n into a sentiment score with a sentiment classiﬁer, and evaluate the quality of the representation by means of the sentiment accuracy. We perform experiments on the Amazon Fine Foods Reviews Corpus (McAuley and Leskovec, 2013). The Amazon dataset contains users’ rating labels as well as the summary for the reviews, making it possible to train a classiﬁer to predict the sentiment labels and a seq2seq model to generate summa
2798651744	Style Transfer Through Back-Translation	2735642330	d our model with the current state-of-the-art approach introduced byHu et al.(2017);Shen et al.(2017) use this method as baseline, obtaining comparable results. We reproduced the results reported in (Hu et al., 2017) using their tasks and data. However, the same model trained on our political slant datasets (described in x3), obtained an almost random accuracy of 50.98% in style transfer. We thus omit these resul
2798651744	Style Transfer Through Back-Translation	2735642330	nd also accurate style classiﬁers. To position our method with respect to prior work, we employ a third task of sentiment transfer, which was used in two stateof-the-art approaches to style transfer (Hu et al., 2017;Shen et al.,2017). We describe the three tasks and associated dataset statistics below. The methodology that we advocate is general and can be applied to other styles, for transferring various social
2798651744	Style Transfer Through Back-Translation	2133564696	stic outputs, lexicalizing the same meaning differently, depending upon the environment. This is particularly relevant for language generation tasks such as machine translation (Sutskever et al.,2014;Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015;Xu et al.,2015), and natural language generation (Wen et al.,2017;Kiddon et al.,2016). In conversational agents (Ritter et al.,2011;Sordoni et al.,2015
2798651744	Style Transfer Through Back-Translation	2251780596	ur work is also closely-related to a problem of paraphrase generation (Madnani and Dorr,2010; Dong et al.,2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al.,2011;Ganitkevitch and Callison-Burch, 2014). More recently,Mallinson et al.(2017) and Wieting et al.(2017) showed how neural backtranslation can be used to generate paraphrases. An additional related line of research is machine translation wit
2798651744	Style Transfer Through Back-Translation	2101105183	y up to 7% in sentiment modiﬁcation. Preservation of meaning. Although we attempted to use automatics measures to evaluate how well meaning is preserved in our transformations; measures such as BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie,2011), or even cosine similarity between distributed representations of sentences do not capture this distance well. Meaning preservation in style transfer is not triv
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2463565445	4]. [8] proposed multimodal compact bilinear pooling to compactly combine the visual and textual features. [24] proposed an attentionbased model to select a region from the image based on text query. [19] introduced co-attention model, which not only employs visual attention, but also question attention. User Interaction: Incorporating interaction with users into the system has rapidly become a resear
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2064675550	460v1 [cs.CL] 27 Apr 2018 2. Related Works Visual Language: The workﬂow of extracting image features with convolutional neural network (CNN) and generating captions with long short-term memory (LSTM) [11] has been consolidated as a standard for image captioning task. [15] generated region-level descriptions by implementing alignment model of region-level CNN and bidirectional recurrent neural network
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1575833922	ageable pool of patterns. Exploiting these design characteristics, we combine the obtained pairs of questions and answers to a declarative sentence by application of rule-based transformations, as in [23, 25]. We ﬁrst rephrase the question to a declarative sentence by switching word positions, and then insert the answers to its appropriate position, mostly replacing wh-words. For example, a question “What
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2088049833	cascade from upper, ﬁne layer to lower, coarser layers of CNN, in order to better-localize the detected objects. This results in region proposals that are more contents-oriented than selective search [26] or Edge Boxes [17]. We ﬁrst extracted top 10 regions per image. Figure 2 shows an example of the regions extracted in this way. In the experiments to follow, we set the number of region proposals K a
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1933349210	deep learning techniques have succeeded in bridging the gap between vision and language in a variety of tasks, ranging from describing the image [15, 7, 28, 29] to answering questions about the image [2, 5]. Such achievements were possible under the premise that there exists a set of ground truth references that are universally applicable regardless of the target, scope, or context. In real-world settin
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2171810632	e question, which is assumed to reﬂect his interest. We then need to extract a region within the image that corresponds to the user’s response. We slightly modify the attention networks introduced in [30] in order to obtain the coordinates of the region that correspond to the user response. In [30], the question itself was fed into the network, so that the region necessary to answer that question is “
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2179022885	e VQA task, but classiﬁcation approach has been shown to outperform generative approach [1, 14]. [8] proposed multimodal compact bilinear pooling to compactly combine the visual and textual features. [24] proposed an attentionbased model to select a region from the image based on text query. [19] introduced co-attention model, which not only employs visual attention, but also question attention. User
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2254252455	en consolidated as a standard for image captioning task. [15] generated region-level descriptions by implementing alignment model of region-level CNN and bidirectional recurrent neural network (RNN). [13] proposed DenseCap that generates multiple captions from an image at region-level. [12] built SIND dataset whose image descriptions display a more casual and natural tone, involving aspects that are n
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2306876680	ent image features, we can generate multiple image-relevant questions from single image. So far, we were concerned with generating “visual” questions. We also seek to generate “non-visual” questions. [21] generated questions that a human may naturally ask and require common-sense and inference. We examined whether we can train a network to ask multiple questions of such type by visual cues. We replica
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1933349210	g the network solely with non-visual questions are shown in Table 16. Visual Question Answering: We now seek to answer the questions generated. We train the question answering system with VQA dataset [2]. Question words are sequentially encoded by LSTM as one-hot vector. Hyperbolic tangent non-linearity activation was employed, and elementwise multiplication was used to fuse the image and word featur
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2529436507	ge, not just describe certain aspects of the image. A number of different approaches have been proposed to tackle VQA task, but classiﬁcation approach has been shown to outperform generative approach [1, 14]. [8] proposed multimodal compact bilinear pooling to compactly combine the visual and textual features. [24] proposed an attentionbased model to select a region from the image based on text query. [1
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2101105183	in a highly creative way. It is also clear that conventional captioning alone will not be able to capture or mimic the semantic diversity present in them. We performed automatic evaluation with BLEU [22] with collected image narratives as ground truth annotations. Table 4: Each model’s performance on DIANE. Metric COCO SIND DenseCap Ours Diversity 2.972 2.060 3.102 3.580 Interesting 2.875 2.100 3.336
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1895577753	human-written captions with humanwritten questions, so that LSTM is trained to predict the question, rather than caption. Given an image I and a question Q = (q 0,...q N), the training proceeds as in [28]: x 1 = CNN(I);x t = W eq t;p t+1 = LSTM(x t) (1) where W e is a word embedding, x t is the input features to LSTM at t, and p t+1 is the resulting probability distribution for the entire dictionary a
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1895577753,1905882502,1931639407	ion Recent advances in visual language ﬁeld enabled by deep learning techniques have succeeded in bridging the gap between vision and language in a variety of tasks, ranging from describing the image [15, 7, 28, 29] to answering questions about the image [2, 5]. Such achievements were possible under the premise that there exists a set of ground truth references that are universally applicable regardless of the t
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2254252455	osals Baseline 2 (SIND): captions with model trained on MS SIND dataset [12], applied to both images in their entireties and the region proposals Baseline 3 (DenseCap): captions generated by DenseCap [13] at both the whole images and regions with top 5 scores using their own region extraction implementation. 5.1.2 Evaluation Automatic Evaluation: It is naturally of our interest how humans would actual
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2147347568	propose a self Q&amp;A model where questions are generated from multiple regions, and VQA is applied to answer the questions, thereby generating image-relevant contents. Region Extraction: Following [9], we ﬁrst extract region candidates from the feature map of an image, by applying linear SVM trained on annotated bounding boxes at multiple scales, and applying non-maximal suppression. The region ca
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	2306876680	question. We set the number of possible answers as 1,250. As we augmented the training data with “non-visual” questions, we also need to train the network to “answer” those non-visual answers. Since [21] provides the questions only, we collected the answers to these questions on Amazon Mechanical Turk. Since many of these questions cannot be answered without speciﬁc knowledge beyond what is seen in t
2798665231	Customized Image Narrative Generation via Interactive Visual Question Generation and Answering	1905882502	ﬂow of extracting image features with convolutional neural network (CNN) and generating captions with long short-term memory (LSTM) [11] has been consolidated as a standard for image captioning task. [15] generated region-level descriptions by implementing alignment model of region-level CNN and bidirectional recurrent neural network (RNN). [13] proposed DenseCap that generates multiple captions from
2798665661	Breaking NLI Systems with Sentences that Require Simple Lexical Inferences	2250539671	nd the new test set. We chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either ﬁne-tuned during training (RESIDUAL-STACKED-ENCODER and ESIM) or stay ﬁxed (DECOMPOSABLE ATTENTION). All models predict the label using a concatenation of features derived from the
2798665661	Breaking NLI Systems with Sentences that Require Simple Lexical Inferences	2250539671	ndeed mutually-exclusive, topicallysimilar, and interchangeable in context. We included WordNet antonyms with the same part-ofspeech and with a cosine similarity score above a threshold, using GloVe (Pennington et al., 2014). Innationalities andcountries wefocused oncountries which are related geographically (Japan, China) or culturally (Argentina, Spain). Sentence-Pairs. To avoid introducing new information not present
2798665661	Breaking NLI Systems with Sentences that Require Simple Lexical Inferences	2738015883	per we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that require lexical and world knowledge. Inspired by the work of Jia and Liang (2017) on reading comprehension, we create a new NLI test set with examplesthatcapturevariouskindsoflexicalknowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone
2798669917	hyperdoc2vec: Distributed Representations of Hypertext Documents	2101105183	. Context aware. Hyperlink contexts usually provide a summary for the target document. Therefore, the target document’s vector should be impacted by words that others use to summarize it, e.g., paperPapineni et al. (2002) and the word “BLEU” in Figure1(a). Newcomer friendly. In a hyper-document network, it is inevitable that some documents are not referred to by any hyperlink in other hyper-docs. If such “newcomers”
2798669917	hyperdoc2vec: Distributed Representations of Hypertext Documents	1888005072	ar tasks in the entity domain, e.g., Wikipedia page classiﬁcation and entity linking. We leave them for future work. Le and Mikolov(2014) employ learned d2v vectors to build different text classiﬁers.Tang et al. (2015a) apply the method in (Tang et al.,2015b) on word co-occurrence graphs for word embeddings, and average them for document vectors. For hyper-docs,Ganguly and Pudi(2017) andWang et al.(2016) target pa
2798669917	hyperdoc2vec: Distributed Representations of Hypertext Documents	1590650811	and Getoor,2003), web retrieval (Page et al.,1999), entity linking (Cucerzan,2007), and citation recommendation (He et al.,2010). To model hypertext documents, various efforts (Cohn and Hofmann,2000;Kataria et al., 2010;Perozzi et al.,2014;Zwicklbauer et al., 2016;Wang et al.,2016) have been made to depict networks of hyper-docs as well as their content. Among potential techniques, distributed representation (Mikolo
2798669917	hyperdoc2vec: Distributed Representations of Hypertext Documents	1895815892	us. It enables us to model citations and con(Zhao and Gildea, D I 2010) also evaluate siﬁer on all OUT document vectors (Papineniet al., 2002) Classifier Average Document Matrix BLEU W I W I W I D O (Zhao and Gildea, 2010)  also evaluate d exp( BLEU exp(  (Papineniet al., 2002) A citation æ@ æ á%á@ ç ç Figure 2: The hyperdoc2vec model. tents simultaneously without sacriﬁcing information on either side. Next, we descr
2798669917	hyperdoc2vec: Distributed Representations of Hypertext Documents	2131744502	Zwicklbauer et al., 2016;Wang et al.,2016) have been made to depict networks of hyper-docs as well as their content. Among potential techniques, distributed representation (Mikolov et al.,2013;Le and Mikolov, 2014) tends to be promising since its validity and effectiveness are proven for plain documents on many natural language processing (NLP) tasks. Conventional attempts on utilizing embedding techniques in h
2798672541	WHAT'S GOING ON IN NEURAL CONSTITUENCY PARSERS? AN ANALYSIS	1938755728	07;Finkel et al.,2008). Character-level models have shown promise in parsing and other NLP tasks as a way to remove the complexity of these lexical features (Ballesteros et al.,2015;Ling et al.,2015b;Kim et al., 2016;Coavoux and Crabbe´,2017;Liu and Zhang, 2017). We compare the performance of characterlevel representations and externally predicted partof-speech tags and show that these two sources of information
2798672541	WHAT'S GOING ON IN NEURAL CONSTITUENCY PARSERS? AN ANALYSIS	1869752048	l. 2016) found that selecting dependency heads independently often resulted in valid trees for their parsers (95% and 99.5% of outputs form trees, respectively). In constituency parsing, the parser ofVinyals et al. (2015), which produced linearized parses token by token, was able to output valid constituency trees for the majority of sentences (98.5%) even though it was not constrained to do so. Several other works ha
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2737638662	e (Vaswani et al., 2017), using delayed SGD updates which accumulate gradients over multiple batches. We also suggest a syntax representation which results in much shorter sequences. 1.1 Related Work Nadejde et al. (2017) perform NMT with syntax annotation in the form of Combinatory Categorial Grammar (CCG) supertags. Aharoni and Goldberg (2017) translate from source BPE into target linearized parse trees, but omit PO
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2594047108	gence on 1M WAT Ja-En, batch size 4096 Architecture Representation Dev BLEU Test BLEU Seq2seq (8-model ensemble) Best system from WAT17 (Morishita et al., 2017) - 28.4 Seq2seq + RNNG Dependency tree (Eriguchi et al., 2017) 18.6 18.8 Seq2seq Plain BPE 21.6 21.2 Linearized derivation 21.9 21.2 Transformer Plain BPE 28.0 28.9 Linearized tree 28.2 28.4 Linearized derivation 28.5 28.7 POS/BPE 28.5 29.1 Table 4: Single model
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2133564696	ical NMTwithmodels generating very long sequences. For example, linearized constituency trees may be much longerthantheequivalent plainBPErepresentation (Table 1). Long sequences are harder to train (Bahdanau et al., 2015) and increase decoding time. We address the training difﬁ- culty with an adjusted training procedure for the Transformer architecture (Vaswani et al., 2017), using delayed SGD updates which accumulate
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2594047108	improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain BPE, while plain BPE models produce shorter sequences, so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble wou
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2626778328	le 1). Long sequences are harder to train (Bahdanau et al., 2015) and increase decoding time. We address the training difﬁ- culty with an adjusted training procedure for the Transformer architecture (Vaswani et al., 2017), using delayed SGD updates which accumulate gradients over multiple batches. We also suggest a syntax representation which results in much shorter sequences. 1.1 Related Work Nadejde et al. (2017) pe
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2289899728	mit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax tokens. Eriguchi et al. (2017) apply recurrent neural network grammar (RNNG) models (Dyer et al., 2016) to NMT, combining an RNNG with an attention-based model to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previou
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2626778328	ree can be reproduced directly from it. We map words to subwords as described in Section 3. 2.1 Delayed SGDUpdate Training for Long Sequences We suggest a training strategy for the Transformer model (Vaswani et al., 2017) which gives improved performance for long sequences, like syntax representations, without requiring additional GPU memory. The Tensor2Tensor framework (Vaswani et al., 2018) deﬁnes batch size as the
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2133564696	representation of the pairs. In all cases we decode using the average of the ﬁnal 20 checkpoints. For comparison with earlier target syntax work, we also train two RNN attention-based seq2seq models (Bahdanau et al., 2015) with normal SGD to produce plain BPE sequences and linearized derivations. For these models we use embedding size 400, a single BiLSTM layer of size 750, and batch size 80. We report all experiments
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2594047108	rg (2017) translate from source BPE into target linearized parse trees, but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax tokens. Eriguchi et al. (2017) apply recurrent neural network grammar (RNNG) models (Dyer et al., 2016) to NMT, combining an RNNG with an attention-based model to produce well-formed dependency trees. Wu et al. (2017) similarly pr
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2030904529	s (Dyer et al., 2016) to NMT, combining an RNNG with an attention-based model to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representati
2798687821	Multi-representation ensembles and delayed SGD updates improve syntax-based NMT.	2737638662	structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain BPE, while plain BPE models produce shorter sequences, so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to
2798689729	Sentiment Adaptive End-to-End Dialog Systems	2101234009	e entire dataset as a corpus and created a tf-idf vector for each utterance as textual features. 5.4 Classiﬁcation results We used random forest as our classiﬁer (an implementation from scikit-learn (Pedregosa et al., 2011)), as we had limited annotated data. We separated the data to be 60% for training, 20% for validation and 20% for testing. Due to the randomness in the experiments, we ran all the experiments 20 times
2798689729	Sentiment Adaptive End-to-End Dialog Systems	2029026153	ediate rewards in RL, which is different from common practice of using task success as delayed rewards in RL training. Some previous module-based systems integrated user sentiment in dialog planning (Acosta, 2009;Acosta and Ward,2011;Pittermann et al., 2010). They all integrated user sentiment in the dialog manager with manually deﬁned rules to react to different user sentiment. In this work, we include user
2798689729	Sentiment Adaptive End-to-End Dialog Systems	1988085234	ferent from common practice of using task success as delayed rewards in RL training. Some previous module-based systems integrated user sentiment in dialog planning (Acosta, 2009;Acosta and Ward,2011;Pittermann et al., 2010). They all integrated user sentiment in the dialog manager with manually deﬁned rules to react to different user sentiment. In this work, we include user sentiment into end-to-end dialog system traini
2798693443	Exploring Emoji Usage and Prediction Through a Temporal Variation Lens.	2594184914	dierent embeddings. The Char B-LSTM takes a sequence of characters and outputs a word embedded vector as in [LLM + 15]. The Char B-LSTM output is then concatenated with the word representation as in [BBS17] and passed to the Word LSTM and Word Attention units. We use the attention mechanism introduced in [YYD + 16], which can be considered as a weighted average of the output of the Word LSTM, where the
2798693443	Exploring Emoji Usage and Prediction Through a Temporal Variation Lens.	2594184914	the following we describe our classier architecture with two variants to fuse temporal information with text. 4.1 Emoji Prediction Model We start from the state-of-the-art emoji prediction classier [BBS17], and built two dierent methods { early and late temproral signal fusion{ to incorporate the date information. The two entry points for fusing temporal information are evaluated in Section 4.2. Figur
2798693443	Exploring Emoji Usage and Prediction Through a Temporal Variation Lens.	2594184914	How Does Temporal Information Help Emoji Prediction? In this section we evaluate how temporal information can improve the accuracy of emoji prediction models. We use the same experimental settings as [BBS17], except we predict 300 emojis classes in instead of 20. We use temporal information as an input to the classier in addition to the tweet. The date is encoded as a vector of three dimensions, where t
2798716503	Knowledge-based end-to-end memory networks.	1793121960	how that incorporating this a-priori knowledge can help in improving the performance of goal-oriented endto-end dialog systems. Bordes and Weston(2016) evaluated end-to-end memory networks proposed bySukhbaatar et al. (2015) for goal-oriented dialog bAbI tasks, where the memory component effectively acts as a (dynamic) representation of the dialog context and allows for inference and reasoning over it. The system achieve
2798716503	Knowledge-based end-to-end memory networks.	1793121960	proach, our proposed solution and also explains why our approach is better suited for goal-oriented dialog tasks which involve a KB. 3.1End-to-end memory networks (memN2N) End-to-end memory Networks (Sukhbaatar et al. (2015)) are a recent class of models that have been applied to a range of natural language processing tasks. They use memory to store context and perform reasoning over it. The memory is updated iteratively
2798736886	Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora	2583105957	rrence of “cat” with “animal” and still have a valid utterance. An important insight from work on distributional methods is that the deﬁnition of context is often critical to the success of a system (Shwartz et al., 2017). Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al.,2015;Roller and Erk,2016). While both approaches f
2798736886	Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora	141602984	sPrec (Weeds et al.,2004) which captures the features of x which are included in the set of a broader term’s features, y: WeedsPrec(x;y) = P n i=1P x i 1 y i&gt;0 n i=1 x i Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. In particular, let CL(x;y) = P n i=1P min
2798736886	Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora	2583105957	VD improved recall via its matrix completion properties. We also found that the spmi model downweighted 1In addition, we also experimented with further distributional spaces and weighting schemes fromShwartz et al. (2017). We also experimented with distributional spaces using the same corpora and preprocessing as the Hearst patterns (i.e., Wikipedia and Gigaword). We found that the reported setting generally performed
2798760246	End-Task Oriented Textual Entailment via Deep Exploring Inter-Sentence Interactions	2221711388	atically inter-sentence word-toword interactions or alignments was ﬁrst studied in recurrent neural networks (Elman,1990). Rockt¨aschel et al. (2016) employ neural word-toword attention for SNLI task.Wang and Jiang (2016) propose match-LSTM, an extension of the attention mechanism in (Rockt¨aschel et al. ,2016), by more ﬁne-grained matching and accumulation. Cheng et al.(2016) present a new LSTM equipped with a memory
2798760246	End-Task Oriented Textual Entailment via Deep Exploring Inter-Sentence Interactions	2523467643	m without severe information compression. Other work following this attentive matching idea includes Bilateral MultiPerspective Matching model (Wang et al.,2017), Enhanced Sequential Inference model (Chen et al., 2016) etc. In addition, convolutional neural networks (LeCun et al.,1998), equipped with attention mechanisms, also perform competitively in textual entailment.Yin et al.(2016) implement the attention in p
2798760246	End-Task Oriented Textual Entailment via Deep Exploring Inter-Sentence Interactions	2211192759	ng model (Wang et al.,2017), Enhanced LSTM (Chen et al.,2016) etc. In addition, convolutional neural networks (LeCun et al.,1998), equipped with attention mechanisms, also perform competitively in TE.Yin et al. (2016) implement the attention in pooling phase so that important hidden states will be pooled with higher probabilities.Yin and Schutze¨ (2017a) further develop the attention idea in CNNs, so that a RNN-st
2798760675	Probabilistic FastText for Multi-Sense Word Embeddings	2171928131	cly available. 1 2 Related Work Early word embeddings which capture semantic information includeBengio et al.(2003),Col1https://github.com/benathi/multisense-prob-fasttext lobert and Weston(2008), andMikolov et al. (2011). Later,Mikolov et al.(2013a) developed the popular WORD2VEC method, which proposes a log-linear model and negative sampling approach that efﬁciently extracts rich semantics from text. Another popular
2798760675	Probabilistic FastText for Multi-Sense Word Embeddings	2129773034	the foreign-language embeddings on word similarity datasets in respective languages. We use Italian WORDSIM353 and Italian SIMLEX-999 (Leviant and Reichart,2015) for Italian models, GUR350 and GUR65 (Gurevych, 2005) for German models, and French WORDSIM353 (Finkelstein et al.,2002) for French models. For datasets GUR350 and GUR65, we use the results reported in the FASTTEXT publication (Bojanowski et al.,2016).
2798760675	Probabilistic FastText for Multi-Sense Word Embeddings	2132631284	similarity datasets, namely, SL-999 (Hill et al.,2014), WS-353 (Finkelstein et al.,2002), MEN-3k (Bruni et al.,2014), MC-30 (Miller and Charles,1991), RG-65 (Rubenstein and Goodenough ,1965), YP-130 (Yang and Powers 2006), MTurk(-287,-771) (Radinsky et al.,2011;Halawi et al.,2012), and RW-2k (Luong et al.,2013). Each dataset contains a list of word pairs with a human score of how related or similar the two words are.
2798763787	Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization	658020064	abstractive sentence. We initially experimented with techniques capitalizing on word vectors, such as k-means and hierarchical clustering based on the Euclidean distance or the Word Mover’s Distance (Kusner et al., 2015). We also tried graph-based approaches, such as community detection in a complete graph where nodes are utterances and edges are weighted based on the aforementioned distances. Best results were obtai
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2131494463	[32] proposed end-to-end memory networks (MEMN2N) which can be trained end-to-end without any intervention on which supporting fact should be used during training. Dynamic memory network proposed by [15] uses a sentence-level attention mechanism to update its internal memory during multi-hop inference. Key-value memory network [20] encodes prior knowledge by introducing a key memory structure which s
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1793121960	BD) is designed to evaluate the performance of end-to-end dialogue systems on the task of restaurant reservation. In [3], the task is formulated as a machine comprehension task by applying the MEMN2N [32] model, considering the dialogue context and last user utterance as story and question respectively, and the system response is selected from a fixed answer set. The DBD dataset is composed of five ma
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250297846	d dialogue systems. Such semantic information is critical for dialogue state update. Existing methods either extract information from predefined features (such as POS and NER tags) by heuristic rules [11], or from pretrained word embeddings by neural network encoder [21]. However, words in user utterance have different importance for updating dialogue states and predicting the next action, which is no
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250539671	dataset were pretrained on their own dialogue corpora, where there are 15,000 sessions in DMBD (3,000 per each task), 2,118 sessions in DM-DSTC and 26,827 sessions in ALDM, using the GloVe algorithm [23]. The dimensions of word embeddings, memory column vectors, and state vectors were all set to 128, and there are 8 columns in the external memory. We first pretrain our model with the heuristic loss L
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250297846	e management task. Since the process of a dialogue session naturally follows a sequence-to-sequence learning problem at the turn level, recurrent neural network (RNN) is proposed to model the process [11, 21, 36]. At each turn, RNN takes as input the structured semantic representation produced by NLU (or raw user utterance when combining NLU and DM together) and predicts system action, where the hidden state
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2129921015	el based on trainable word embeddings. The similarity score between an input and a candidate sentence is the inner product of their averaged word embeddings. The is trained with a margin ranking loss [2]. •MEMN2N: Standard end-to-end memory networks [3, 32]. It stores the dialogue history information in a memory network and chooses a response by running multi-hop reasoning upon the history. •MEMN2N(+
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	197958275,2438667436	emostlyrule-based,inwhichthestateupdate and dialogue policy process are manually defined, but these methods did not take into account the probability uncertainty in dialogue. Bayesian network methods [22, 39] formulated dialogue management as a probabilistic graphical model which models the conditional dependency between different states, and each specific state is bound with an action to be taken, but th
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250297846	a filled value for a slot2 , such as Location=north, Price=expensive and Cuisine=British. The slot-value pairs are usually regarded as the state representation in many dialogue state tracking studies [11]. During the interaction, the filled value for each slot may be provided or updated by the user, and correspondingly, the dialogue state changes. For instance, when the user says How about a British r
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2399880602	fted rules and semantic features, including NER and POS tags, to construct semantic features for user utterance. [11] proposed to use the speech recognition confidence score as an additional feature. [28, 30] used hierarchicalRNNmodels,wheretheuserutteranceisprocessedbyaword-levelRNN,andutterances are sequentially connected through an utterance-level RNN. [21] proposed to use convolutional neural network
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1948566616	gle module which can be trained end-to-end to read directly from user utterance and produce system action. The system action produced by DM will be translated into a natural language utterance by NLG [35] to interact with users. In order to decide the next action a dialogue system should take, dialogue management, particularly in task-oriented dialogue systems, should deal with the dialogue context in
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2415583245	the hand-crafted rules. Memory network provides a principled approach for modeling long-range dependency and making multi-hop reasoning, which has advanced many NLP tasks such as machine translation [33] and question answering [32]. Neural turing machines [9] was proposed to augment existing neural models with additional memory units to solve complicated tasks. It is analogous to a Turing machine but
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2129405869	icy are trained using reinforcement learning. However, the POMDP model becomes difficult to train for the domains with large state space. An improved version of POMDP - Hidden Information State (HIS) [40] is proposed to address this problem by grouping dialogue states into partitions. Another key problem in building Bayesian dialogue model is the lack of training corpus, thus user simulation [27] is e
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250297846	iented dialogue systems when combining NLU and DM together. Early methods used hand-crafted rules and semantic features, including NER and POS tags, to construct semantic features for user utterance. [11] proposed to use the speech recognition confidence score as an additional feature. [28, 30] used hierarchicalRNNmodels,wheretheuserutteranceisprocessedbyaword-levelRNN,andutterances are sequentially c
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1793121960	ity score between an input and a candidate sentence is the inner product of their averaged word embeddings. The is trained with a margin ranking loss [2]. •MEMN2N: Standard end-to-end memory networks [3, 32]. It stores the dialogue history information in a memory network and chooses a response by running multi-hop reasoning upon the history. •MEMN2N(+match): A variant of MEMN2N which included additional
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	197958275	m. Bayesian network approaches are proposed to address the issues of rule-based methods. Dialogue management was firstly formalized as a Markov decision process (MDP) [16] under the Markov assumption [22], in which the new state s t at turn t is only conditioned on the previous state s t−1 and system action a t−1. MDP models the uncertainty in dialogue and becomes more robust to the errors induced by
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2438667436	management (DM), and natural language generation (NLG). Dialogue management, which is in charge of selecting actions in response to user inputs, plays a central role in task-oriented dialogue systems [6, 39]. It takes as input the user intent which is analyzed by NLU, interacts with knowledge base, and decides the next system action. Sometimes NLU and DM can be coupled together as a single module which c
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1948566616	ng two memory networks. The slot-value memory network maintains the values of semantic slots 1The dialogue act can be translated into a natural language utterance by a language generator, as shown in [35]. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 11. Publication date: April 2018. 11:4 Z. Zhang et al. during interaction, and the external-memory augments the single state represent
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2154740693	nsive to build a rule-based system. Bayesian network approaches are proposed to address the issues of rule-based methods. Dialogue management was firstly formalized as a Markov decision process (MDP) [16] under the Markov assumption [22], in which the new state s t at turn t is only conditioned on the previous state s t−1 and system action a t−1. MDP models the uncertainty in dialogue and becomes more
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1793121960	ory network provides a principled approach for modeling long-range dependency and making multi-hop reasoning, which has advanced many NLP tasks such as machine translation [33] and question answering [32]. Neural turing machines [9] was proposed to augment existing neural models with additional memory units to solve complicated tasks. It is analogous to a Turing machine but is differentiable end-to-en
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2251058040	reparation We first evaluated our memory augmented dialogue management model on two synthetic datasets adopted from the dialog bAbI dataset[3] and the Second Dialogue State Tracking Challenge dataset [10], which are originally proposed for end-to-end dialogue systems and dialogue state tracking task. However, both of the above two datasets are small-scale. To better assess the performance of our propo
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2250297846	are RNN (Recurrent Neural Network) based which takes as input user utterance and system response at each dialogue turn, and the hidden state of RNN is utilized as the representation of dialogue state [11,38]. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 11. Publication date: April 2018. Memory-augmented Dialogue Management for Task-oriented Dialogue Systems 11:3 However, despite of the
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2415583245	rvised as shown in Latt (see Eq.26). 3.5 External Memory The external memory is used to augment the representation capacity of the single state of RNN, and it is sometimes referred to as memory state [33] in other works. Varies from the slot-value memory, external memory is not endowed with explicit semantic meaning in our framework. The ACM Transactions on Information Systems, Vol. 1, No. 1, Article
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2399880602	ser utterance are mapped to semantic slots such asrating, cuisine, price, service, and location. end-to-end models which directly take dialogue context as input and generate natural language response [17, 28, 30, 31] in open-domain conversational systems. However, due to the vanishing gradient problem and the limited ability of state representation, RNN is difficult to capture the longrange context in dialogue. H
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	2438667436	t−1 and system action a t−1. MDP models the uncertainty in dialogue and becomes more robust to the errors induced by speech recognition and NLU. Partially observable Markov decision processes (POMDP) [39] provides a more principled way in that it takes environment observation o t into consideration. On the top of this framework, state transition and dialogue policy are trained using reinforcement lear
2798766480	Memory-augmented Dialogue Management for Task-oriented Dialogue Systems.	1509171145	tate transition in dialogue, where the state represents a certain dialogue status, and the transition between states is triggered by the corresponding type of a user utterance. Slot-fillingapproaches [8] expanded the definition of dialogue state to an aggregation of slots and values. In such models, user can talk about each slot by issuing constraints and requesting the values of slots, and the dialo
2798779216	Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems	2565031282	ethods. 6 Related Works End-to-end task-oriented dialog systems train a single model directly on text transcripts of dialogs (Wen et al.,2017;Serban et al.,2016; Williams et al.,2017;Zhao et al.,2017;Seo et al., 2017;Serban et al.,2017). Here, RNNs play an important role due to their ability to create a latent representation, avoiding the need for artiﬁcial state labels. End-to-End Memory Networks (Bordes and Wes
2798779216	Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems	2403702038	other has out-of-vocabulary (OOV) entity values that does not exist in the training set. We also used dialogs extracted from the Dialog State Tracking Challenge 2 (DSTC2) with the reﬁned version fromBordes and Weston (2017), which ignores the dialog state annotations. The main difference with bAbI dialog is that this dataset is extracted from real human-bot dialogs, which is noisier and harder since the bots made mistak
2798779216	Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems	2340944142	retation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al.,2016;Wen et al., 2017;Zhao et al.,2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention
2798779216	Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems	2104544334	schedule arrangement. Traditionally, they have been built with several pipelined modules: language understanding, dialog management, knowledge query, and language generation (Williams and Young,2007;Hori et al., 2009;Lee et al.,2009;Levin et al.,2000;Young et al.,2013). Moreover, the ability to query external Knowledge Bases (KBs) is essential in taskoriented dialog systems, since the responses are guided not onl
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2026351760	1807.03491v1 [cs.CL] 10 Jul 2018 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model.
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2133564696	the components together by treating each component as a sub-task in a multitask learning setting.8 4.1 Language Model The language model is a variant of an LSTM encoder–decoder model with attention (Bahdanau et al., 2015), where the encoder encodes the preceding context (i.e. all sonnet lines before the current line) and the decoder decodes one word at a time for the current line, while attending to the preceding cont
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	1924770834	compute the weighted sum of h0 i as follows: et i = v | btanh(W h 0 i +U s t +b ) at = softmax(et) h t = X i at ih 0 i To combine s t and h t, we use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014): s0 t = GRU(s t;ht). We then feed s0 t to a linear layer with softmax activation to produce the vocabulary distribution (i.e. softmax(W outs0 t + b out), and optimise the model with standard categori
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2343635552	cross a number of fronts is: can deep learning techniques be harnessed for creative purposes? Creative applications where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016). In this paper, we focus on a creative textual task: automati
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	47006904	data might add noise to our system, and given more data it would be worthwhile to include time period as a factor in the model. 7There are a number of variations in addition to the standard pattern (Greene et al., 2010), but our model uses only the standard pattern as it is the dominant one. (a) Language model (b) Pentameter model (c) Rhyme model Figure 2: Architecture of the language, pentameter and rhyme models. C
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2115221470	e earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simpliﬁed by incorporating an attention mechanism and training at the character
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2100690397	e last vowel and all following consonants, and predict a word pair as rhyming if their extracted sequences match. The extracted sequence can be interpreted as a proxy for the last syllable of a word. Reddy and Knight (2011) propose an unsupervised model for learning rhyme schemes in poems via EM. There are two latent variables: ˚speciﬁes the distribution of rhyme schemes, and deﬁnes 230.8 is empirically found to be the
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	47006904	English poetry. A critical difference over our work is that we jointly model both poetry content and forms, and unlike previous work which use dictionaries (Ghazvininejad et al., 2016) or heuristics (Greene et al., 2010) for rhyme, we learn it automatically. 3 Sonnet Structure and Dataset The sonnet is a poem type popularised by Shakespeare, made up of 14 lines structured as 3 quatrains (4 lines) and a couplet (2 lin
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2153579005	erial. We tune the hyper-parameters of the model over the development data (optimal conﬁguration in the supplementary material). Word embeddings are initialised with pre-trained skip-gram embeddings (Mikolov et al., 2013a,b) on the BACKGROUND dataset, and are updated during training. For optimisers, we use Adagrad (Duchi et al., 2011) for the language model, and Adam (Kingma and Ba, 2014) for the pentameter and rhyme
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2609482285	during joint training. To exemplify this, we found that the pentameter model performs very poorly when we train each component separately. 9We use a single layer for all LSTMs. a selective mechanism (Zhou et al., 2017) to each h i. By deﬁning the representation of the whole ~ context h = [~h C;h 1] (where Cis the number of words in the context), the selective mechanism ﬁlters the hidden states h i using h as follow
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2115221470	language model that incorporates both character encodings and preceding context; LM -C: Similar to LM , but preceding context is encoded using convolutional networks, inspired by the poetry model of Zhang and Lapata (2014);20 LM WFST maps a sequence word to a sequence of+PM+RM: the full model, with joint training of the language, pentameter and rhyme models. Perplexity on the test partition is detailed in Table 2. Enc
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	47006904	he machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation frame
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2045135321	n that is being asked across a number of fronts is: can deep learning techniques be harnessed for creative purposes? Creative applications where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016). In this paper, we focus on a creative te
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2338312508	onts is: can deep learning techniques be harnessed for creative purposes? Creative applications where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016). In this paper, we focus on a creative textual task: automatic poetry composition
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	1522301498	rained skip-gram embeddings (Mikolov et al., 2013a,b) on the BACKGROUND dataset, and are updated during training. For optimisers, we use Adagrad (Duchi et al., 2011) for the language model, and Adam (Kingma and Ba, 2014) for the pentameter and rhyme models. We truncate backpropagation through time after 2 sonnet lines, and train using 30 epochs, resetting the network weights to the weights from the previous epoch whe
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2514713644	rd embeddings and reduce parameters further by introducing weight-sharing between output matrix W out and embedding matrix W wrd via a projection matrix W prj (Inan et al., 2016; Paulus et al., 2017; Press and Wolf, 2017): W out = tanh(W wrdW prj) 4.2 Pentameter Model This component is designed to capture the alternating iambic stress pattern. Given a sonnet line, 10We initially shared the character encodings with the
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2402172128	where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016). In this paper, we focus on a creative textual task: automatic poetry composition. A distinguishing feature of poetry is its aesthetic forms, e.g. rhyme and rhythm/meter.1 In this work, we treat the
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	47006904	and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling
2798782765	Deep-speare: A joint neural model of poetic language, meter and rhyme	2115221470	t with recurrent networks with selective encoding is more effective than convolutional networks. The full model LM sents the ten stresses of the iambic pentameter, and+PM+RM, which learns stress 20In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a ﬁxed size vector; here we use a standard convolutional network with max-pooling operatio
2798800651	Incorporating Glosses into Neural Word Sense Disambiguation	1561908597	1995) which are widely used in the knowledge-based methods. The gloss, which extensionally deﬁnes a word sense meaning, plays a key role in the well-known Lesk algorithm (Lesk, 1986). Recent studies (Banerjee and Pedersen, 2002;Basile et al.,2014) have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of Lesk algorithm. To this end, our goal is to incorporate the gloss in
2798812533	Universal Language Model Fine-tuning for Text Classification	2740582239	-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we ﬁnetune all layers until convergence at the last iteration. This is similar to ‘chain-thaw’ (Felbo et al., 2017), except that we add a layer at a time to the set of ‘thawed’ layers, rather than only training a single layer at a time. While discriminative ﬁne-tuning, slanted triangular learning rates, and gradua
2798812533	Universal Language Model Fine-tuning for Text Classification	2072715695	ang et al.,2017). Text classiﬁcation is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection (Jindal and Liu,2007;Ngai et al.,2011;Chu et al., 2012), emergency response (Caragea et al.,2011), and commercial document classiﬁcation, such as for legal discovery (Roitblat et al.,2010). 1http://nlp.fast.ai/ulmfit. ?Equal contribution. Jeremy focused o
2798812533	Universal Language Model Fine-tuning for Text Classification	2740582239	ing is not beneﬁcial. Impact of classiﬁer ﬁne-tuning We compare training from scratch, ﬁne-tuning the full model (‘Full’), only ﬁne-tuning the last layer (‘Last’) (Donahue et al.,2014), ‘Chain-thaw’ (Felbo et al., 2017), and gradual unfreezing (‘Freez’). We furthermore assess the importance of discriminative ﬁne-tuning (‘Discr’) and slanted triangular learning rates (‘Stlr’). We compare the latter to an alternative,
2798812533	Universal Language Model Fine-tuning for Text Classification	2165698076	labeled examples and achieves state-ofthe-art results also on small datasets. 3 Universal Language Model Fine-tuning We are interested in the most general inductive transfer learning setting for NLP (Pan and Yang, 2010): Given a static source task T S and any target task T T with T S 6= T T, we would like to improve performance on T T. Language modeling can be seen as the ideal source task and a counterpart of Image
2798812533	Universal Language Model Fine-tuning for Text Classification	2740582239	lr 5.00 5.69 5.38 Table 7: Validation error rates for ULMFiT with different methods to ﬁne-tune the classiﬁer. of 0:001and 0:0001for the last and all other layers respectively for ‘Chain-thaw’ as in (Felbo et al., 2017), and a learning rate of 0:001 otherwise. We show the results in Table7. Fine-tuning the classiﬁer signiﬁcantly improves over training from scratch, particularly on the small TREC-6. ‘Last’, the stand
2798812533	Universal Language Model Fine-tuning for Text Classification	2544860310	w much smaller the lowest LR is from the maximum LR  max, and  tis the learning rate at iteration t. We generally use cutfrac= 0:1, ratio= 32and  max= 0:01. STLR modiﬁes triangular learning rates (Smith, 2017) with a short increase and a long decay period, which we found key for good performance.5 In Section5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to
2798826886	Comprehensive Supersense Disambiguation of English Prepositions and Possessives	2252113711	ational work on semantic disambiguation speciﬁcally of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves,2005,2007;Litkowski, 2014;Ye and Baldwin,2007;Saint-Dizier,2006; Dahlmeier et al.,2009;Tratz and Hovy,2009; Hovy et al.,2010,2011;Tratz and Hovy,2013), and the semantic class approach (Moldovan et al., 2004;Badulescu and Mold
2798826886	Comprehensive Supersense Disambiguation of English Prepositions and Possessives	2252123671	meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g.,Palmer et al.,2005;Fillmore and Baker,2009;Oepen et al.,2016;Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al.,2003,2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our init
2798827378	Nugget Proposal Networks for Chinese Event Detection	16142220	&apos;\QDPLF 0XOWL 3RROLQJ 7RNHQ/HY HO )HDWXUH Figure 3: Token-level feature extractor, where PE is relative positional embeddings and WE is word embeddings. The concerning token is ¿#TAB#. triggers (Li et al., 2012), while words can provide more accurate and less ambiguous semantics than characters (Chen et al., 2015a). For example, character-level information can tell us that l à(kill by shooting) is a trigge
2798827378	Nugget Proposal Networks for Chinese Event Detection	2123442489	02) as the training set except 20 randomly sampled documents reserved as development set. Finally, there were 506/20/167 documents for training/development/test set. We used Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess all documents for sentence splitting and word segmentation. Adadelta update rule (Zeiler, 2012) is applied for optimization. Models are evaluated by micro-averaged Precision(P), Recall(
2798827378	Nugget Proposal Networks for Chinese Event Detection	2102563561	ated Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250575108	Die event triggered by passed away. Recently, neural network methods, which transform event detection into a word-wise classication paradigm, have achieved signicant progress in event detection (Nguyen and Grishman, 2015; &apos;LH C0 /.L;ß /).[ /4l/,A0 /.L;ß / D ,QMXUH 0HUJHB2UJDQL]DWLRQ 7UDQVIHUB2ZQHUVKLS 7KH LQMXUHG VROLGHU GLHG  6¨-× /;=/4l/:l/+e/; ( /)D/ /:= /D 7KHFRPSDQ\ DFTXLUHG DQG PHUJHG ZLWKDQXPEHU
2798827378	Nugget Proposal Networks for Chinese Event Detection	639708223	which make them difcult to be integrated into recent Deep Learning models. Recent advances have shown that neural networks can effectively capture spatial and positional information from raw inputs (Ren et al., 2015; He et al., 2017; Wang and Jiang, 2017). This paper designs Nugget Proposal Networks to capture character compositional structure of event triggers, which is more robust and more effective than previ
2798827378	Nugget Proposal Networks for Chinese Event Detection	639708223	fectively detect trigger nuggets at characters. Recent advances have presented that convolutional neural networks are effective at capturing and predicting the region information in object detection (Ren et al., 2015) and semantic segmentation (He et al., 2017), which reveals the strong ability of CNNs to learning spatial and positional information. Inspired by this, we propose a neural network based trigger nugge
2798827378	Nugget Proposal Networks for Chinese Event Detection	2516930406	grated into recent Deep Learning models. Recent advances have shown that neural networks can effectively capture spatial and positional information from raw inputs (Ren et al., 2015; He et al., 2017; Wang and Jiang, 2017). This paper designs Nugget Proposal Networks to capture character compositional structure of event triggers, which is more robust and more effective than previous hand-crafted patterns or characterle
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250999640	hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classication with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved signicant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated ar
2798827378	Nugget Proposal Networks for Chinese Event Detection	16142220	hen and Ji (2009) proposed a feature-driven BIO tagging methods at character-level sequences. Qin et al. (2010) introduced a method which can automatically expand candidate Chinese trigger set. While Li et al. (2012) and Li and Zhou (2012) denedmanuallycharactercompositionalpatterns for Chinese event triggers. However, their methods rely on hand-crafted features and patterns, which make them difcult to be integra
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250999640	HU GLHG  6¨-× /;=/4l/:l/+e/; ( /)D/ /:= /D 7KHFRPSDQ\ DFTXLUHG DQG PHUJHG ZLWKDQXPEHURIFRPSDQLHV  D E Figure 1: Examples of word-trigger mismatch. Slashes in the gure indicate word boundaries. Chen et al., 2015b; Ghaeini et al., 2016). For instance, a model will detect events in sentence Henry was injured  by successively classifying its three words into NIL, NIL and Injure. By automatically extracting fe
2798827378	Nugget Proposal Networks for Chinese Event Detection	2130714105	on is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which aut
2798827378	Nugget Proposal Networks for Chinese Event Detection	2108743083	information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards.
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250575108	ong languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classication with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved signicant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger
2798827378	Nugget Proposal Networks for Chinese Event Detection	2182946785	posed a feature-driven BIO tagging methods at character-level sequences. Qin et al. (2010) introduced a method which can automatically expand candidate Chinese trigger set. While Li et al. (2012) and Li and Zhou (2012) denedmanuallycharactercompositionalpatterns for Chinese event triggers. However, their methods rely on hand-crafted features and patterns, which make them difcult to be integrated into recent Deep Le
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250999640	relative positional embeddings and WE is word embeddings. The concerning token is ¿#TAB#. triggers (Li et al., 2012), while words can provide more accurate and less ambiguous semantics than characters (Chen et al., 2015a). For example, character-level information can tell us that l à(kill by shooting) is a trigger constructed of regular pattern manner + verb. While wordlevel sequences can provide more explicit i
2798827378	Nugget Proposal Networks for Chinese Event Detection	2250999640	resentation. 2.1 Token-level Representation Learning Two token-level neural networks are used to extract features from characters and words respectively. The network architecture is similar to DMCNN (Chen et al., 2015b). Figure 3 shows a word-level example. Given n tokens t1 ;t2 ;:::;tn in the sentence and the concerning token tc, let x i be the concatenation of the word embedding of ti and the embedding of ti&apo
2798836595	Know What You Don’t Know: Unanswerable Questions for SQuAD	2738015883	s worse than humans. We also show that our unanswerable questions are more challenging than ones created automatically, either via distant supervision (Clark and Gardner,2017) or a rule-based method (Jia and Liang, 2017). We release SQuAD 2.0 to the public as new version of SQuAD, and make it the primary benchmark on the ofﬁcial SQuAD leaderboard.2 We are optimistic that this new dataset will encourage the developmen
2798845266	Detecting Syntactic Features of Translated Chinese	2006832571	.2%) with such features shows that “function word distributions and shallow syntactic patterns” without any lexical information can already account for much of the characteristics of translated text. Volansky et al. (2013) is a very comprehensive study thatinvestigated translationese inEnglish by looking at original and translated English from 10 source languages, in a European parliament corpus. While they mainly aime
2798845266	Detecting Syntactic Features of Translated Chinese	2469195965	.3 POS n-grams(1-3) 93.9 Table 2: Results for the lexical and POS features features is much greater than the number of training texts, we perform feature selection by ﬁltering using information gain (Liu et al., 2016; Wong and Dras,2011)tochoosethemostdiscriminative features. Information gain has been shown to select highly discriminative, frequent features for similar tasks (Liu et al., 2014). We experiment with
2798845266	Detecting Syntactic Features of Translated Chinese	2123442489	.....”, change all half-width punctuations to full-width, so that our text is compatible with the Chinese Penn Treebank (Xue et al., 2005), which is the training data for the Stanford CoreNLP parser (Manning et al., 2014) used in our study. 3.3 Features Character and word n-gram features can be considered upper bound and baseline. On the one hand, they have been used extensively (see Section 2), but on the other hand,
2798845266	Detecting Syntactic Features of Translated Chinese	2006832571	based approach can distinguish translated from original texts with high accuracy for Indo-European languages such as Italian (Baroni and Bernardini, 2005), Spanish (Ilisei et al., 2010), and English (Volansky et al., 2013; Lembersky et al., 2012; Koppel and Ordan, 2011). Features used in those studies include common bag-of-words features, such as word n-grams, as well as part-of-speech (POS) n-grams, function words, e
2798845266	Detecting Syntactic Features of Translated Chinese	2104747875	d feature interpretation for phrases headed by other tags (ADJP, PP, etc.) and for speciﬁc genres. It is also desirable to improve the accuracy of constituent parsers for Chinese, along the lines of (Wang et al., 2013; Wang and Xue, 2014; Hu et al., 2017), since accurate syntactic trees are the prerequisite for accurate feature interpretation. While the parser in this study works well, better parsers will undoubte
2798845266	Detecting Syntactic Features of Translated Chinese	2148365102	d text; thus we will refrain from analyzing them here. POS n-grams also produce good results (F-measure of 93.9%), conﬁrming previous research on Indo-European languages (Baroni and Bernardini, 2005; Koppel and Ordan, 2011). Since they are not lexicalized and thus avoid a topical bias, they provide a better comparison to syntactic features. Features F (%) Unlexicalized syntactic features CFGR 90.2 subtrees: depth 2 90.9
2798845266	Detecting Syntactic Features of Translated Chinese	2252187216	ded statistically higher accuracy than simplelexical features (function words, character and POS n-grams). Other approaches have used rules of tree substitution grammar (TSG) (Post and Bergsma, 2013; Swanson and Charniak, 2012) in NLI. Swanson and Charniak (2012) compared the results of CFG rules and two variants of TSG rules and showed that TSG rules obtained through Bayesian methods reached the best results. Nevertheless,
2798845266	Detecting Syntactic Features of Translated Chinese	2252187216	e a picture together IP NP VP PU PN IP VP PU ADVP VP NP IP VP PU ADVP VP NP PN Figure2: Allsubtrees of depth 2 with root IPin the tree from Figure 1 (Post and Gildea, 2009; Sangati and Zuidema, 2011; Swanson and Charniak, 2012) in that we do not include any lexical information in order to exclude topical inﬂuence from content words. Thus no lexical rules are considered, and POS tags are considered to be the leaf nodes (Figu
2798845266	Detecting Syntactic Features of Translated Chinese	1533853787	ese sentence meaning We take a picture together IP NP VP PU PN IP VP PU ADVP VP NP IP VP PU ADVP VP NP PN Figure2: Allsubtrees of depth 2 with root IPin the tree from Figure 1 (Post and Gildea, 2009; Sangati and Zuidema, 2011; Swanson and Charniak, 2012) in that we do not include any lexical information in order to exclude topical inﬂuence from content words. Thus no lexical rules are considered, and POS tags are consider
2798845266	Detecting Syntactic Features of Translated Chinese	2164628107	inguish translated from original texts with high accuracy for Indo-European languages such as Italian (Baroni and Bernardini, 2005), Spanish (Ilisei et al., 2010), and English (Volansky et al., 2013; Lembersky et al., 2012; Koppel and Ordan, 2011). Features used in those studies include common bag-of-words features, such as word n-grams, as well as part-of-speech (POS) n-grams, function words, etc. Although such surfac
2798845266	Detecting Syntactic Features of Translated Chinese	2148365102	original texts with high accuracy for Indo-European languages such as Italian (Baroni and Bernardini, 2005), Spanish (Ilisei et al., 2010), and English (Volansky et al., 2013; Lembersky et al., 2012; Koppel and Ordan, 2011). Features used in those studies include common bag-of-words features, such as word n-grams, as well as part-of-speech (POS) n-grams, function words, etc. Although such surface features yield very hig
2798845266	Detecting Syntactic Features of Translated Chinese	2006832571	he usage of pronouns. 1 Introduction Work in translation studies has shown that translated texts differ signiﬁcantly in subtle and not so subtle ways from original, non-translated texts. For example, Volansky et al. (2013) show that the preﬁx mono- is more frequent in Greek-to-English translations because epistemologically it originates from Greek. Also, the structure of modal verb, inﬁnitive, and past participle (e.g.
2798851324	Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer	2047449974	age is a common problem of abusive behavior on online social media networks. Various work in the past have attacked this problem by using different machine learning models to detect abusive behavior (Xiang et al., 2012;Warner and Hirschberg,2012;Kwok and Wang,2013;Wang et al.,2014;Nobata et al.,2016; Burnap and Williams,2015;Davidson et al.,2017; Founta et al.,2018). Most of these work follow the assumption that it
2798851324	Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer	2099471712	s using non-parallel data have been proposed recently. Many of these methods borrow the idea of using an adversarial discriminator/classiﬁer from the Generative Adversarial Networks (GANs) framework (Goodfellow et al., 2014) and/or use a cycle consistency loss.Zhu et al.(2017) proposed the pioneering use of the cycle consistency loss in GANs to perform image style transfer from non-parallel data. In the NLP area, some re
2798852526	Assessing Language Models with Scaling Properties.	2064675550	d the previous hidden state h t 1 to incorporate past 7 information eectively: h t= ( x t;h t 1): (7) The function depends on the recurrent architecture of the network. This article focuses on LSTM (Hochreiter and Schmidhuber 1997), because the dierence in performance is insignicant among architectures such as the gated recurrent unit (GRU) (Cho et al. 2014) and other LSTM variants (Chung et al. 2014; Gre et al. 2017; Melis e
2798852526	Assessing Language Models with Scaling Properties.	2154099718	e would generate a language model, like the two-stage model proposed in (Goldwater et al. 2011). Here, we consider a more advanced model proposed as the hierarchical Pitman-Yor language model (HPYLM)(Teh 2006), which integrates the Pitman-Yor process into an n-gram model. 4.4 Neural Language Models The predictive performance of state-of-the-art language models improved radically with neural language models
2798852526	Assessing Language Models with Scaling Properties.	1999965501	gram model. 4.4 Neural Language Models The predictive performance of state-of-the-art language models improved radically with neural language models. The majority of promising neural language models (Mikolov and Zweig 2012; Melis et al. 2018; Merity et al. 2018; Yang et al. 2018) adopt recurrent neural networks (RNNs). The RNNs compute a hidden state h t from the input x t and the previous hidden state h t 1 to incorpo
2798852526	Assessing Language Models with Scaling Properties.	2154099718	roblems of sparseness and unknown words. The Chinese restaurant process, as a kind of PY process (Pitman 2006), was introduced partly because it satises Zipf’s and Heaps’ law (Goldwater et al. 2011; Teh 2006). In the recent works on neural language models, application of the two laws was considered to improve the architecture. Jozefowicz et al. (2016) evaluated models in terms of word frequencies and conc
2798896439	Multitask Parsing Across Semantic Representations	2604593109	.7 27.1 UD 73.7⋆ 72.6⋆ 73.2⋆ 61.8 24.9⋆ 35.5⋆ Table 4: Labeled precision, recall and F1 (in %) for primary and remote edges, on the 20K test sets. ⋆indicates signiﬁ- cantly better than Single. HAR17: Hershcovich et al. (2017). tively small number of remote edges (about 2% of all edges), none of the differences is signiﬁcant. Note that our baseline single-task model (Single) is slightly better than the current state-of-the
2798896439	Multitask Parsing Across Semantic Representations	2344508595	(McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg,2016),including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has shown gre
2798896439	Multitask Parsing Across Semantic Representations	2301095666	2014) between MLP layers, and recurrent dropout (Gal and Ghahramani, 2016) between BiLSTM layers, both with p = 0.4. We also use word (α =0.2), tag (α =0.2) and dependency relation (α = 0.5) dropout (Kiperwasser and Goldberg, 2016).14 In addition, we use a novel form of 13http://dynet.io 14In training, the embedding for a feature value w is replaced with a zero vector with a probability of α #(w)+α , where #(w)is the number of
2798896439	Multitask Parsing Across Semantic Representations	2604593109	8 54.8 51.2 All 75.6⋆ 73.1 74.4⋆ 50.9 53.2 52 Table 3: Labeled precision, recall and F1 (in %) for primary and remote edges, on the Wiki test set. ⋆indicates signiﬁ- cantly better than Single. HAR17: Hershcovich et al. (2017). dropout, node dropout: with a probability of 0.1 at each step, all features associated with a single node inthe parser state are replaced withzero vectors. For optimization we use a minibatch size o
2798896439	Multitask Parsing Across Semantic Representations	2151170651	ce analyses, i.e., produce a graph covering all (content) words in the text, or the lexical concepts they evoke. This contrasts with “shallow” semantic parsing, primarily semantic role labeling (SRL; Gildea and Jurafsky, 2002; Palmer et al.,2005),whichtargets argument structure phenomena using ﬂat structures. We consider four formalisms: UCCA, AMR, SDP and Universal Dependencies. Figure 1 presents one sentence annotated i
2798896439	Multitask Parsing Across Semantic Representations	2052816566	ective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state deﬁned by a buffer B of remaining tok
2798896439	Multitask Parsing Across Semantic Representations	2604593109	es considered in this work exhibit reentrancy and discontinuity (or non-projectivity), to varying degrees. In addition, UCCA and AMR contain non-terminal nodes. To parse these graphs, we extend TUPA (Hershcovich et al., 2017), a transition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’stransition systemcanyieldany labeled DAG whose terminals are anchored in the text token
2798896439	Multitask Parsing Across Semantic Representations	2610308073	Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transit
2798896439	Multitask Parsing Across Semantic Representations	2604593109	h primary edges form a DAG. Figure 3 shows examplesforconverted graphs. ConvertingUCCA into the uniﬁed format consists simply of removinglinkagenodesandedges(seeFigure3a),which were also discarded by Hershcovich et al. (2017). 3See Supplementary Material for a full listingof features. 4http://spacy.io 5http://fasttext.cc 6 Toimplement thisconstraint, we deﬁne aswap index for each node, assigned when the node is created. A
2798896439	Multitask Parsing Across Semantic Representations	2250623140	h structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016;
2798896439	Multitask Parsing Across Semantic Representations	2134612861	hem jointly. 2 Related Work MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classiﬁcation of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adapta
2798896439	Multitask Parsing Across Semantic Representations	2529228643	ion (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling form
2798896439	Multitask Parsing Across Semantic Representations	2301095666	labels when training on AMR. 4.2 Transition Classiﬁer To predict the next transition at each step, we use a BiLSTM with embeddings as inputs, followed by an MLP and a softmax layer for classiﬁcation (Kiperwasser and Goldberg, 2016). The model is illustrated in Figure 2. Inference is performed greedily, and training is done with an oracle that yields the set of all optimal transitions at a given state (those that leadtoastate fr
2798896439	Multitask Parsing Across Semantic Representations	2137132216	o CoNLL shared tasks (Surdeanu et al., 2008; Hajicˇ et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and Ma`rquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharin
2798896439	Multitask Parsing Across Semantic Representations	2515003191	on-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Semantic Dependency Parsing. SDP uses a set
2798896439	Multitask Parsing Across Semantic Representations	2252123671	ow for a unit to participate in several super-ordinate relations. Primary edges form a tree in each layer, whereas remote edges enable reentrancy, forming a DAG. Abstract Meaning Representation. AMR (Banarescu et al., 2013) is a semantic representation that encodes information about named entities, argument structure, semantic roles, word sense and co-reference. AMRs are rooted directed graphs, in which both nodes and e
2798896439	Multitask Parsing Across Semantic Representations	2251495238	rdeanu et al., 2008; Hajicˇ et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and Ma`rquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal st
2798896439	Multitask Parsing Across Semantic Representations	2604593109	rser state S , B John moved to Paris . G After L graduation P H Classiﬁer BiLSTM Embeddings After graduation ... to Paris MLP transition softmax Figure 2: Illustration of the TUPA model, adapted from Hershcovich et al. (2017). Top: parser state. Bottom: BiLTSMarchitecture. vious work (Hershcovich et al., 2017) and discard implicitunitsfromthetraining andevaluation, and so do not include transitions for creating them. In A
2798896439	Multitask Parsing Across Semantic Representations	2252123671	his shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on dif
2798896439	Multitask Parsing Across Semantic Representations	98423455	in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg,2016),including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo
2798896439	Multitask Parsing Across Semantic Representations	2158899491	tant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (S
2798896439	Multitask Parsing Across Semantic Representations	2341311217	or UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016;Søgaard and Goldberg, Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving taskspeciﬁc sub-networks as well. Concretely, w
2798896439	Multitask Parsing Across Semantic Representations	1523189118	UCCA, we use v1.2 of the English Wikipedia corpus (Wiki; Abend and Rappoport, 2013), with the standard train/dev/test split (see Table 1), and the Twenty Thousand Leagues Under the Sea corpora (20K; Sulem et al., 2015), annotated in English, French and German.8 For English and French we use 20K v1.0, a small parallel corpus comprising the ﬁrst ﬁve chapters of thebook. Asinprevious work(Hershcovich et al., 8http://g
2798902062	Automatic Article Commenting: the Task and Dataset	2557508245	deas of the article; instead it is often desirable for a comment to carry additional information not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017;Li et al.,2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. I
2798902062	Automatic Article Commenting: the Task and Dataset	2251913848	ed deep generative models (Goodfellow et al.,2016;Hu et al.,2018) that incorporate effective reading comprehension modules (Rajpurkar et al.,2016;Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015;Hu et al.,2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al.,2009), upvotes prediction (Rizos et al.,2016), and article headline gen
2798902062	Automatic Article Commenting: the Task and Dataset	2154652894	hough variations do exist. 4 Quality Weighted Automatic Metrics Automatic metrics, especially the reference-based metrics such as BLEU (Papineni et al.,2002), METEOR (Banerjee and Lavie,2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al.,2015), are widely used in text generation evaluations. These metrics have assumed all references are of equal golden qualities. However, in the task of article commenting, the
2798902062	Automatic Article Commenting: the Task and Dataset	2123301721	rpus with a human-annotated subset for scientiﬁc research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al.,2002) and METEOR (Banerjee and Lavie, 2005), to better ﬁt the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar a
2798904449	Building Language Models for Text with Named Entities	1990365157	oil is the type for named entities like olive oil, canola oil, grape oil, etc.1 Such type information is even more prevalent for source code corpus written in statically typed programming languages (Bruce, 1993), since all the variables are by construct associated with types like integer, ﬂoat, string, etc. Our model exploits such deterministic type information of the named entities and learns the probabilit
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	1840435438	and backward encoders, resulting in v and u (in R2d) for the context and hypothesis.5 We follow the common practice of feeding the concatenation (v,u) ∈ R4d to a classiﬁer (Rocktäschel et al., 2016; Bowman et al., 2015; Mou et al., 2016; Liu et al., 2016; Cheng et al., 2016; Munkhdalai and Yu, 2017). Sentencepairrepresentations arefedintoaclassiﬁerwithasoftmax layerthatmapsontothenumber of labels. Experiments with
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2737504179	data We use four distinct datasets to train classiﬁers: MultiNLI (Williams et al., 2017), a recent expansion of SNLI containing a broad array of domains that was used in the 2017 RepEval shared task (Nangia et al., 2017), and three recast NLI datasets from The JHU Decompositional Semantics Initiative (Decomp)6 released by White et al. (2017). Sentence-pairs and labels were recast, i.e. automatically converted, from e
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2118463056	den state from the forward and backward encoders, resulting in v and u (in R2d) for the context and hypothesis.5 We follow the common practice of feeding the concatenation (v,u) ∈ R4d to a classiﬁer (Rocktäschel et al., 2016; Bowman et al., 2015; Mou et al., 2016; Liu et al., 2016; Cheng et al., 2016; Munkhdalai and Yu, 2017). Sentencepairrepresentations arefedintoaclassiﬁerwithasoftmax layerthatmapsontothenumber of labe
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2613904329	dy how wellNMTencoders captureothersemanticphenomena, possibly by recasting other datasets. Comparing how semantic phenomena are represented in different NMT architectures, e.g. purely convolutional (Gehring et al., 2017) or attentionbased (Vaswani et al., 2017), may shed light on whether different architectures may better capturesemantic phenomena.Finally,investigating how multilingual systems learn semantics can bri
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2737504179	indicated by Table 5, the representations perform noticeably better than amajority baseline. However, our results are not competitive with state-of-the-art systems trained speciﬁcally for Multi-NLI (Nangia et al., 2017). 6. Related Work In concurrent work, Poliak et al. (2018) explore whether NLI datasets contain statistical irregularities by training a model with access to only hypotheses. Their model signiﬁcantly
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2130942839	ith non-equivalence of words in the source and target languages (Baker, 2018). 3. Methodology We use NMT models based on bidirectional long short-term memory (Bi-LSTM) encoderdecoders with attention (Sutskever et al., 2014; Bahdanau et al., 2015), trained on a parallel corpus. Given an NLI context-hypothesis pair, we pass each sentence independently through a trained NMT encoder to extract their respective vector repre
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	1840435438	nference, 3) and complex anaphora resolution. Additionally, we evaluate the NMT sentence representations on 4) Multi-NLI, a recent extension of the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) that includes multiple genres and domains (Williams et al., 2017). 2Sometimes referred to as recognizing textual entailment (Dagan et al., 2006, 2013). We contextualize our results with a standard ne
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2139183784	ntics and machine translation. MEANT and its extension XMEANT evaluate MT systems based on semantics (Lo and Wu, 2011; Lo et al., 2014). Others have focused on incorporating semantics directly in MT. Chan et al. (2007) use word sense disambiguation to help statistical MT, Gao and Vogel (2011) add semantic-roles to improve phrase-based MT, and Carpuat et al. (2017) demonstrate how ﬁltering parallel sentences that ar
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2157281038	on these recast datasets. Prior work has focused on the relationship betweensemantics and machine translation. MEANT and its extension XMEANT evaluate MT systems based on semantics (Lo and Wu, 2011; Lo et al., 2014). Others have focused on incorporating semantics directly in MT. Chan et al. (2007) use word sense disambiguation to help statistical MT, Gao and Vogel (2011) add semantic-roles to improve phrase-base
2798905273	On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference	2304113845	roduction What do neural machine translation (NMT) models learn about semantics? Many researchers suggest that state-of-the-art NMT models learn representations that capture the meaning of sentences (Gu et al., 2016; Johnson et al., 2017;Zhou et al.,2017;Andreas and Klein,2017; Neubig, 2017; Koehn, 2017). However, there is limitedunderstanding ofhowspeciﬁcsemanticphenomena are captured in NMT representations bey
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	2158847908	) (ﬂuent f from the KBhas value v at time step s in the story time line, where v may be true or false) and sthpd(a,v,s) (action a from the KBwas observed to have occurred if v 3 The PropBank project (Palmer et al. 2005) provides a corpus of text annotated with information about basic semantic propositions. 14 Inclezan et al. is true, or not if v is false, at time step s in the story time line). In addition to the ob
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	109398974	ands that Nicole hasstopped csubact2 immediately after starting it because she realized that its goal is already fulﬁlled. Note that approaches that do notviewcharactersasgoal-drivenagents(including (Gabaldon 2009; Ng and Mooney 1992; Mueller 2007)) face substantial diﬃculties or simply cannot handle serendipitous scenarios. All answer sets of program Π(2) ∪{q1,q2}contain the additional atoms answer(occur(leav
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	618874514	he customer always wants to become satiated). Gabaldon (2009) performed activity recognition using a simpler theory of intentions by Baral and Gelfond (2005) that did not consider goal-driven agents. Nieves et al. (2013) proposed an argumentation-based approach for activity recognition, applied to activities deﬁned as pairs of a motive and a set of goal-directed actions; in contrast, in Blount et al.’s work, basic ac
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	1600627406	we denote by TI the ASP translation of the ALencoding. Additionally, Blount et al. developed an agent architecture AIAfor an intentional agent, implemented in CR-Prolog (Balduccini and Gelfond 2003; Balduccini 2007) – an extension of ASP. Blount (2013) adapted the agent loop proposed by Balduccini and Gelfond (2008) and outlined the control loop that governs the behavior of an intentional agent, which we reprodu
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	8446309	goals, which are always the same for each role in our case (e.g., the customer always wants to become satiated). Gabaldon (2009) performed activity recognition using a simpler theory of intentions by Baral and Gelfond (2005) that did not consider goal-driven agents. Nieves et al. (2013) proposed an argumentation-based approach for activity recognition, applied to activities deﬁned as pairs of a motive and a set of goal-d
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	8446309	ns. 3 Preliminary: Theory of Intentions Blount et al. (Blount 2013; Blount et al. 2015) developed a theory about the intentions of a goal-driven agent by substantially elaborating on previous work by Baral and Gelfond (2005). In their theory, each sequence of actions (i.e., plan) of an agent was associated with a goal that it was meant to achieve, and the combination of the two was called an activity. Activities could ha
2798921072	An ASP Methodology for Understanding Narratives about Stereotypical Activities	109398974	single stereotypical activity (restaurant dining) does not require identifying agents’ goals, which are always the same for each role in our case (e.g., the customer always wants to become satiated). Gabaldon (2009) performed activity recognition using a simpler theory of intentions by Baral and Gelfond (2005) that did not consider goal-driven agents. Nieves et al. (2013) proposed an argumentation-based approach
2798949492	Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling	2229639163	g methods for coreference (Lee et al.,2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al.,2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al.,2018). arXiv:1805.04787v2 [cs.CL] 13 Aug 2018 2 Model We consider the space of possible predicates to be all the tokens in the input sentence, and the space of argument
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2301095666	on 6.4) with a focus on machine translation. 6.1 Auxiliary Tasks We use dependency parsing and part-of-speech tagging as auxiliary tasks. Our method utilizes BiLSTM features for syntax as proposed by Kiperwasser and Goldberg (2016) and attention proposed by Dozat and Manning (2016), however ours does not impose any tree structure constraints since it is the architecture for translation described in Section 2. The model does not
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133564696	8) p(y jjy &lt;j;x) ˇsoftmax(W outu j+b out): (9) 3 Many Tasks One Sequence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different t
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1902237438	al recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspi
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1753482797	ble improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the im
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2130942839	Dec 2 d j+W Att 2 c j) (8) p(y jjy &lt;j;x) ˇsoftmax(W outu j+b out): (9) 3 Many Tasks One Sequence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2554915555	dict the sequence of distances to head as described in Section 3. This is below the best results achieved by state-of-the-art parsers, that are already around 95 for English (Dozat and Manning, 2016; Kuncoro et al., 2017), and around 90 for the same German dataset (Andor et al., 2016; Kuncoro et al., 2016; Bohnet and Nivre, 2012). As a side product of our research, we show that dependency parsing can be approached via
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133564696	ectors through an embedding matrix WS, the vector of the i-th word is WSx i. We get the representations of the i-th word by summarizing the information of neighboring words using bidirectional LSTMs (Bahdanau et al., 2014), hF i = LSTM F(hF i 1;W Sx i) (1) hB i = LSTM B(hB i+1;W Sx i): (2) The forward and backward representation are concatenated to get the bi-directional encoder representation of word ias h i= [hF i ;h
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2130942839	endency arc connecting it with its matching head word. We, therefore, learn to translate between the original sentence and the resulting sequence of dependency labels. MachineTranslation Similarly to Sutskever et al. (2014) and Bahdanau et al. (2014), we use sequence to sequence to translate between a sentence written in a source language and a sentence written in a target language. 4 Scheduled Multi-Task Learning In or
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2301095666	eof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2626792426	et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2158899491	ing the representation to accommodate multiple tasks can result in a drop in accuracy compared to the accuracy of each task learned separately (Caruana, 1997; Bingel and Søgaard, 2017). Pre-Training (Collobert et al., 2011) is a widely used approach (Goldberg, 2017) which initializes the parameters with the parameters used to solve a somewhat related task. Similarly, Fine-Tuning uses a small annotated in-domain corpus a
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133564696	ing strategies show interesting differences in performance both in the lowresource and standard setups. 2 Sequence to Sequence with Attention Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) directly models the conditional probability p(yjx) of the target sequence of words y=&lt;y 1;:::;y T &gt;given a source sequence x=&lt;x 1;:::;x S&gt;. In this paper, we base our neural architecture
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2605717780	re input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly st
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2289899728	ion. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (D
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2344508595	and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspired by Ammar et al. (2016). All tasks share a common output vocabulary and generate terms according to (3). We learn multiple tasks simultaneously by prepending a special task embedding vector to the target. The task vector sy
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1902237438	ni-batch size to 5000 words. Based on the scheduler we sample, the dataset to draw training examples from, and add it to the mini-batch until the word limit is reached. Incontrastto other approaches (Luong et al., 2015a; Zoph and Knight, 2016), our minibatch is not separated by tasks and often contains examples from multiple tasks. We shufﬂe each dataset at the beginning of the training, and after the model has bee
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133512280	onent Scheduler). Given that the examples in Table 2 suggest that the SMTL models may be doing a better job at avoiding dropping words we complement our BLEU scores with the METEOR evaluation metric (Lavie and Agarwal, 2007) which is more sensitive to recall. We report METEOR (and fragmentation penalty that captures how well the system produces the correct order of the words) for the models with highest BLEU scores in ea
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1843891098	out): (9) 3 Many Tasks One Sequence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of th
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2130942839	p. Our different scheduling strategies show interesting differences in performance both in the lowresource and standard setups. 2 Sequence to Sequence with Attention Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) directly models the conditional probability p(yjx) of the target sequence of words y=&lt;y 1;:::;y T &gt;given a source sequence x=&lt;x 1;:::;x S&gt;. In this paper, we base
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133564696	rge parallel corpus (WMT14 English to German) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extra
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2554915555	rrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneﬁcial, a
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2740215900	rs able to capture the training data when employing LSTM (RNN) models. This applies to sequence to sequence models with attention. Each set of parameters provides a different level of generalization (Reimers and Gurevych, 2017). As suggested by Dyer (2017), representations learned by the network do not capture the linguistic properties, and they are biased to make overly strong linguistic generalizations. Providing “guidanc
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2130942839	U score on relatively large parallel corpus (WMT14 English to German) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1902237438	showing superior results (on average) on the WIT German to English translation task. Many approaches have been employing multitask learning in order to inject linguistic knowledge with great success (Luong et al., 2015b; Niehues and Cho, 2017; Mart´ınez Alonso and Plank, 2017, inter-alia). The ﬁnal representation is then adapted to solve multiple tasks, however continuing to ﬁnetune on solely the main task might re
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2222949842	shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspired by Ammar et al. (201
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1902237438	we study the impact of our method with a single (and shared across tasks) encoder and the architecture of separate decoders which has already proven to be a very effective multi-task learning scheme (Luong et al., 2015a; Niehues and Cho, 2017). In the latter, each of the decoders is responsible for a different task (i.e. syntax, parts-of-speech, translation, etc.) using a single representation generated by the shar
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1522301498	t to 500 (instead of 1000), in order to enable quick convergence and thereby examine our approach in many different combinations. The weight updates were determined using the unbiased Adam algorithm (Kingma and Ba, 2014). We used 0.5 as the scheduler’s slope () (see Section 4) for all our experiments. We use beam search decoding (of size 5) when decoding the test results. For all tasks (including dependency parsing
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2149933564	te parameters. We ﬁrst learn using the large out-of-domain corpus and once that is ﬁnished, we continue learning (ﬁne-tuning) on the in-domain corpus. This is a common approach for transfer learning (Yosinski et al., 2014). A related approach is to start with a pre-trained neural network model and ﬁne-tune only the ﬁnal layers in order to keep the coarse features detected for the previous task (Hinton and Salakhutdinov
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2138857742	ted approach is to start with a pre-trained neural network model and ﬁne-tune only the ﬁnal layers in order to keep the coarse features detected for the previous task (Hinton and Salakhutdinov, 2006; Erhan et al., 2010). Both approaches, facilitate encoding useful information from related tasks (Pre-training) or data-sets (Fine-tuning) without demanding that the representation accommodate both tasks, and can be view
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1816313093	th languages. We use the same datasets as in the parsing task. Note that both for part-of-speech tagging and dependency parsing, our models are trained with bytepair encoding (BPE) in the input side (Sennrich et al., 2015), meaning that there are usually more tokens in the input than in the output (which has exactly one label or a token representing the distance to head per word). For the single-task models we also use
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2133564696	th its matching head word. We, therefore, learn to translate between the original sentence and the resulting sequence of dependency labels. MachineTranslation Similarly to Sutskever et al. (2014) and Bahdanau et al. (2014), we use sequence to sequence to translate between a sentence written in a source language and a sentence written in a target language. 4 Scheduled Multi-Task Learning In order to produce accurate tra
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2594047108	tions. Providing explicit linguistic information (Dyer et al., Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneﬁcial, achieving higher results in language modeling and machine translation. Mul
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2410082850	trong linguistic generalizations. Providing explicit linguistic information (Dyer et al., Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneﬁcial, achieving higher results in language modeling and m
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2153579005	ture. 7 Discussion Scheduled Multi-Task Learning is complementary to other transfer learning methods like pre-training and ﬁne-tuning. It is common to use pre-training in the form of word embeddings (Mikolov et al., 2013; Goldberg, 2017). One advantage of pre-trained word embeddings is the representation of out-ofvocabulary (OOV) words. Through pre-training, OOV words are commonly trained using an earlystopping metho
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1869752048	uence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al.,
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2101105183	we wish to focus on. 5 Experimental Setup We evaluate the effectiveness of our models for a low-resource setting and a standard setting. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002). We report translation quality using tokenized1 BLEU comparable with existing Neural Machine Translation papers. Our experiments are centered around the translation task. We aim to determine whether
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	1902237438	he next word in the sequence. This is based on the words decoded so far represented by the decoder state (d j), and the encoder representation of the sentence (h i). Concretely, we use dot attention (Luong et al., 2015b) to calculate the attention weights. More formally, iis calculated as follows: e i= d&gt; jh i (4) i= exp(e i) P k exp(e k) : (5) A vector representation (c j) capturing the information relevant to
2798957007	Scheduled Multi-Task Learning: From Syntax to Translation	2222949842	words. Based on the scheduler we sample, the dataset to draw training examples from, and add it to the mini-batch until the word limit is reached. Incontrastto other approaches (Luong et al., 2015a; Zoph and Knight, 2016), our minibatch is not separated by tasks and often contains examples from multiple tasks. We shufﬂe each dataset at the beginning of the training, and after the model has been trained on all the sour
2798966187	Generative Stock Question Answering.	1895577753	(Goldberg et al., 1994 ;Dale et al. ,2003 Reiter et al. 2005). More recently, automatically generating text from series or structured data is widely studied by exploiting neural network based models (Vinyals et al., 2015). We take a further step in this direction by generating interpretations based on structured data as well as text (i.e., user questions). Large-scale numerical data is commonly available in various in
2798966187	Generative Stock Question Answering.	1793121960	l.,2008;Berant et al.,2013;Yao et al., 2014;Bordes et al.,2015;Xu et al.,2016;Yin et al.,2016). Knowledge base contains facts expressed in various forms, such as factual sentences (Weston et al.,2015;Sukhbaatar et al., 2015), logical form (Berant et al.,2013), and relation triples (Yin et al.,2016;Miller et al.,2016). In this work, we focus on numerical knowledge base, which consists of ﬂoat numbers in different scales,
2798966187	Generative Stock Question Answering.	2101105183	n, and all tokens for BPE. Evaluation. We randomly extracted 1000 sentence pairs as the validation set, and another 500 sentence pairs as the ﬁnal test set. We used an automatic metric – 2-gram BLEU (Papineni et al., 2002), to select the best trained model on the validation set, and used human evaluation on the test set to report convincing results. For human evaluation, we asked two annotators to label the answers usi
2798966187	Generative Stock Question Answering.	2126776599	previous QA tasks, the answers are typically selected from a knowledge base (e.g., entities) (Berant et al.,2013;Yin et al.,2016) or question-answer pairs collected from web forums (e.g., sentences) (Bian et al., 2008;Cong et al.,2008). In contrast, due to the dynamic nature of stock related QA, we generate answers based on the question and the knowledge of the referred stock. In our task, the answers to a given q
2798966187	Generative Stock Question Answering.	1521413921	ta to Text Generation In recent years, there has been a growing interest in generating text to describe structured data, such as weather forecasts(Goldberg et al. ,1994 ;Reiter et al. 2005 Belz ,2007;Angeli et al. 2010) and image captioning(Vinyals et al.,2015). One traditional method is using hand-crafted rules(Goldberg et al., 1994 ;Dale et al. ,2003 Reiter et al. 2005). More recently, automatically generating tex
2798970679	Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation	1591801644	tions. We also include the results for VAE with continuous latent variables reported on the same PTB (Zhao et al.,2017). Additionally, we report the perplexity from a standard GRU-RNN language model (Zaremba et al., 2014). The evaluation metrics include reconstruction perplexity (PPL), KL(q(z)kp(z)) and the mutual information between input data and latent variables I(x;z). Intuitively a good model should achieve low p
2798981442	TYPESQL: KNOWLEDGE-BASED TYPE-AWARE NEURAL TEXT-TO-SQL GENERATION	2252123671	nd MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney,1996;Zettlemoyer and Collins,2005;Wong and Mooney,2007;Das et al.,2010;Liang et al.,2011;Banarescu et al., 2013;Artzi and Zettlemoyer,2013;Reddy et al., 2014;Berant and Liang,2014;Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a
2798981442	TYPESQL: KNOWLEDGE-BASED TYPE-AWARE NEURAL TEXT-TO-SQL GENERATION	2101964891	s (Zelle and Mooney,1996;Zettlemoyer and Collins,2005;Wong and Mooney,2007;Das et al.,2010;Liang et al.,2011;Banarescu et al., 2013;Artzi and Zettlemoyer,2013;Reddy et al., 2014;Berant and Liang,2014;Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al.,2015; Ling et al.,
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2427527485	2017)3. Evaluation Datasets We mainly focus on two large-scale machine comprehension datasets to train and evaluate our model. One is the SQuAD and the other is the recently released TriviaQA. SQuAD (Rajpurkar et al. 2016) is a machine comprehension dataset, totally containing more than 100K queries manually annotated by crowdsourcing workers on a set of Wikipedia articles. TriviaQA (Joshi et al. 2017) is a newly avail
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2516930406	ach which aims at directly optimizing the evaluation metric of MC task. Supervised Learning with Boundary Detecting The moset widely used training method for MC task is the boundary detecting method (Wang and Jiang 2017), which minimizes the sum of negative log probabilities of the true start and end position based on predicted distributions: JMLE() = XN i=1 logpL s(y s i) + logp L e(y e) (9) where ys i and e i are
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1544827683	atural language processing and artiﬁcial intelligence. Beneﬁting from the rapid development of deep learning techniques (Goodfellow, Bengio, and Courville 2016) and large-scale MC benchmark datasets (Hermann et al. 2015; Hill et al. 2016; Rajpurkar et al. 2016; Joshi et al. 2017), end-to-end neural networks have achieved promising results on MC tasks (Wang et al. 2016; Xiong, Zhong, and Socher 2017; Seo et al. 2017;
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2508728158	e wrong span. Besides, most of previous models use the boundary detecting method proposed by (Wang and Jiang 2017) to train their model, which is equivalent to optimizing the Exact Match (EM) metric (Norouzi et al. 2016). However, this optimization strategy may fail when the answer boundary is fuzzy or too long, such as the answer of the “why” query. In this paper, we propose the Reinforced Mnemonic Reader for MC tas
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2410983263	essfully used to solve a wide variety of problems in the ﬁeld of NLP, including abstractive summarization (Paulus, Xiong, and Socher 2017), question generation (Yuan et al. 2017) and dialogue system (Li et al. 2016). The basic idea is to consider the evaluation metrics (like BLEU or ROUGE) which are not differentiable as the reward, and apply REINFORCE algorithm (Williams 1992) to maximize the expected reward. P
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2119717200	et al. 2017) and dialogue system (Li et al. 2016). The basic idea is to consider the evaluation metrics (like BLEU or ROUGE) which are not differentiable as the reward, and apply REINFORCE algorithm (Williams 1992) to maximize the expected reward. Previous models for MC only utilize maximum-likelihood estimation to optimize the EM metric, and they may fail when the answer span is too long or fuzzy. However, Rei
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2521709538	ion mechanism. Reasoning mechanism. Inspired by the phenomenon that human increase their understanding by reread the context and the query, multi-hop reasoning models have been proposed for MC tasks (Shen et al. 2016; Xiong, Zhong, and state which incorporates the current information of reasoning with the previous information in the memory, by following the framework of Memory Networks (Sukhbaatar et al. 2015). R
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2516930406	ist several candidate answers. “One-hop” prediction may fail to fully understand the query and point to the wrong span. Besides, most of previous models use the boundary detecting method proposed by (Wang and Jiang 2017) to train their model, which is equivalent to optimizing the Exact Match (EM) metric (Norouzi et al. 2016). However, this optimization strategy may fail when the answer boundary is fuzzy or too long,
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1522301498	ith reinforcement learning (RL) by optimizing Eq. 11, until the F1 score on the development set no longer improves. We use a = 0:01 for the M-Reader+RL during RL training. We use the Adam optimizer (Kingma and Ba 2014) with an initial learning rate of 0.0008 for MLE training, which is halved whenever meeting a bad iteration. We use the SGD optimizer with a learning rate of 0.001 for RL training. The batch size is s
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2427527485	iviaQA and SQuAD datasets. Model Overview For machine comprehension (MC) task, a query Q and a context C are given, our task is to predict an answer A, which is constrained as a segment of text of C (Rajpurkar et al. 2016; Joshi et al. 2017). We design the Reinforced Mnemonic Reader to model the probability distribution p(AjC;Q), where is the set of all trainable parameters. Our model consists of three basic modules
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2427527485	l intelligence. Beneﬁting from the rapid development of deep learning techniques (Goodfellow, Bengio, and Courville 2016) and large-scale MC benchmark datasets (Hermann et al. 2015; Hill et al. 2016; Rajpurkar et al. 2016; Joshi et al. 2017), end-to-end neural networks have achieved promising results on MC tasks (Wang et al. 2016; Xiong, Zhong, and Socher 2017; Seo et al. 2017; Wang et al. 2017). A common trait of the
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1544827683	lated Work Reading comprehension. The signiﬁcant advance on reading comprehension has largely beneﬁted from the availability of large-scale datasets. Large cloze-style datasets such as CNN/DailyMail (Hermann et al. 2015) and Childrens Book Test (Hill et al. 2016) were ﬁrst released, make it possible to solve MC tasks with deep neural architectures. The SQuAD (Rajpurkar et al. 2016) and the TriviaQA (Joshi et al. 2017
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1924770834	ly capture the long-distance contextual interaction between parts of the context, by only using long short-term memory network (LSTM) (Hochreiter and Schmidhuber 1997) or gated recurrent units (GRU) (Chung et al. 2014) for encoding and modeling. 3. In the pointer layer, previous models (Weissenborn, Wiese, and Seiffe 2017; Chen et al. 2017) use the pointer network to calculate two probability distributions for the
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1793121960	MC tasks (Shen et al. 2016; Xiong, Zhong, and state which incorporates the current information of reasoning with the previous information in the memory, by following the framework of Memory Networks (Sukhbaatar et al. 2015). ReasoNet (Shen et al. 2016) utilizes reinforcement learning to dynamically determine when to stop reading. In contrast to their model, Reinforced Mnemonic Reader contains a memory-based answer point
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	1544827683	n end-to-end neural networks for machine comprehension (MC). Unlike the original attention mechanism (Bahdanau, Cho, and Bengio 2014) that uses a summary vector of the query to attend to the context (Hermann et al. 2015), the coattention is computed as an alignment matrix corresponding to all pairs of context words and query words, which can model complex interaction between the query and the context (Cui et al. 2016
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2119717200	ore measures the overlap between the predicted answer and the ground-truth answer, serving as a “soft” metric compared to the “hard” EM. Taking the F1 score as reward, we use the REINFORCE algorithm (Williams 1992) to maximize the model’s expected reward. For each sampled answer A^, we deﬁne the loss as: JRL() = EA^˘p (AjC;Q) [R(A;A^ )] (10) where p is the policy to be learned, and R(A;A^ ) is the reward fun
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2427527485	style datasets such as CNN/DailyMail (Hermann et al. 2015) and Childrens Book Test (Hill et al. 2016) were ﬁrst released, make it possible to solve MC tasks with deep neural architectures. The SQuAD (Rajpurkar et al. 2016) and the TriviaQA (Joshi et al. 2017) are more recently released datasets, which take a segment of text instead of a single entity as the answer, and contain substantial syntactic and lexical variabil
2798999921	Reinforced Mnemonic Reader for Machine Reading Comprehension	2508728158	tch (EM) metric, since the log-likelihood objective is equivalent to a KL divergence between a delta distribution (AjA) and the model distribution p(AjC;Q), where (AjA) = 1 at A= A and 0 at A6= A (Norouzi et al. 2016), and A denotes the ground-truth answer. However, directly determining the exact boundary may be difﬁ- cult in some situations where the answer boundary is fuzzy or too long. For example, it is quite
2799005354	Are BLEU and Meaning Representation in Opposition	1861492603	) for the remaining tasks. 4.2 Paraphrases We also evaluate the representation of paraphrases. We use two paraphrase sources for this purpose: COCO and HyTER Networks. COCO (Common Objects in Context;Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image. We extracted the captions from its validation set to form a set of 5 5k = 25k sentences grouped by the sou
2799005354	Are BLEU and Meaning Representation in Opposition	2626778328	attention), and hence it may not be useful for a given head to look for a speciﬁc syntactic or semantic role. 7 Conclusion We presented a novel variation of attentive NMT models (Bahdanau et al.,2014;Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sen0.0 0.1 0.2 0.0 0.1 0.2 0.0 0.1 0.2 0.0 0.5 1.0 0.0 0.1 0.2 Figure 5: Attention weight by relative positio
2799005354	Are BLEU and Meaning Representation in Opposition	2133564696	tial advances in translation quality and it is thus natural to ask how these improvements affect the learned representations. One of the key technological changes was the introduction of “attention” (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al.,2017). Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on th
2799008727	TutorialBank: Using a Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation	2343487696	g or teaching the concept. Liang et al.(2017) experiment with prerequisite chains on education data but focus on the recovery of a concept graph rather than on predicting unseen course relations as inLiu et al. (2016). They introduce both a synthetic dataset as well as one scraped from 11 universities which includes course prerequisites as well as conceptprerequisite labels. Concept graphs are also used in (Gordon
2799010330	Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding	2583010282	y Hu et al. (2016a), which is a general framework for distilling knowledge from FOL rules into NN (+hu16). Besides, since we consider few-short learning, we also include the memory module proposed by Kaiser et al. (2017), which performs well in various few-shot datasets (+mem)5. Finally, the state-of-art model on the ATIS dataset is also included (L&amp;L16), which jointly models the intent detection and slot ﬁlling
2799052853	Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages	2250525911	nderresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein,2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al.,2013;Hermann and Blunsom,2014;Artetxe et al.,2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingu
2799052853	Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages	1828724394	s no parallel data. It does, however, require large amounts of unlabeled data. Bilingual Embedding Methods: Recently proposed bilingual embedding methods (Hermann and Blunsom,2014;Chandar et al.,2014;Gouws et al., 2015) offer a natural way to bridge the language gap. These particular approaches to bilingual embeddings, however, require large parallel corpora in order to build the bilingual space, which are not avail
2799052853	Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages	2252065932	A at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al.,2002; Wiegand et al.,2010;Zhu et al.,2014;Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recog
2799054028	GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.	1840435438	andard test set, for which we obtained private labels from the authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) sections. We also use and recommend the SNLI corpus (Bowman et al., 2015) as 550k examples of auxiliary training data. QNLI The Stanford Question Answering Dataset (Rajpurkar et al.2016) is a questionanswering dataset consisting of questionparagraph pairs, where one of the
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	1593050888	Challenge (Schu¨ller 2014, Bailey et al. 2015). In the context of controlled natural language processing, answer set programming has been used as a prototype of a rule system in the Attempto project (Kuhn 2007, Fuchs et al. 2008), as a target language for biomedical queries related to drug discovery(ErdemandYeniterzi2009),as a sourcelanguageforgeneratingexplanationsforbiomedical queries (Erdem and Oztok 20
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	1903938722	eclarative programming and has its roots in logic programming, logic-based knowledge representation and reasoning, deductive databases, and constraint solving (Gelfond and Lifschitz 1988, Baral 2003, Lifschitz 2008, Gebser et al. 2017).Themainideabehindanswersetprogrammingis torepresentacomputationalproblemas a(non-monotonic)logicprogramwhoseresultingmodels(=answersets)correspondtosolutions of the problem. Answ
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	2252198809	parsing in the context of the Combinatory Categorial Grammar formalism (Lieler and Schu¨ller 2012, Schu¨ller 2013), for semantic parsing and representation of textual information (Baral et al. 2011, Nguyen et al. 2015), and for natural language understanding in the context of question answering (Baral and Tari 2006)and the WinogradSchema Challenge (Schu¨ller 2014, Bailey et al. 2015). In the context of controlled n
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	1973180461	phrase (more on this in Section 6). 5 Implementation of the Bi-directional Grammar To implement our bi-directional grammar, we use the deﬁnite clause grammar (DCG) formalism (Pereira and Warren 1980, Pereira and Shieber 1987). This formalism gives us a convenient notation to express feature structures and the resulting grammar can easily be transformed into another notation via term expansion, if an alternative parsing st
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	2135380458	s, controlled natural languages, answer set programming, sentence planning, executable speciﬁcations 1 Introduction There exist a number of controlled natural languages (Sowa 2004, Clark et al. 2005, Fuchs et al. 2008, Guy and Schwitter 2017) that have been designed as high-level interface languages to knowledge systems. However, none of the underlying grammars of these controlled natural languages is bi-direction
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	2135380458	Schu¨ller 2014, Bailey et al. 2015). In the context of controlled natural language processing, answer set programming has been used as a prototype of a rule system in the Attempto project (Kuhn 2007, Fuchs et al. 2008), as a target language for biomedical queries related to drug discovery(ErdemandYeniterzi2009),as a sourcelanguageforgeneratingexplanationsforbiomedical queries (Erdem and Oztok 2015), as a framework
2799070212	Specifying and Verbalising Answer Set Programs in Controlled Natural Language	1575728221	swer set program, this leads to a form of round-tripping that preserves semantic equivalence but not syntactic equivalence. This work has certain similarities with our previous 2 Rolf Schwitter work (Schwitter 2008) where a bi-directional grammar is used to translate an experimental controlled natural language into syntactically annotated TPTP1 formulas; these annotated formulas are then stored and serve as temp
2799071926	Stylistic Variation in Social Media Part-of-Speech Tagging	1014449310	cKeown,2011),gender(EckertandMcConnellGinet,2003), race (Green,2002), and geography (Trudgill,1974), thanks to the phenomenon of homophily , also known as assortative mixing (McPherson et al.,2001;Al Zamal et al., 2012). These demographic variables are in turn closely linked to language variation in American English (Wolfram and Schilling-Estes,2005), and have been shown to improve some document classicationtasks(Ho
2799071926	Stylistic Variation in Social Media Part-of-Speech Tagging	2107474859	s network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are rened by performing label propagation over social relations.Tan et al.(2011) andHu et al. (2013)leveragesocialrelationsforsentimentanalysis by exploiting a factor graph model and the graphLaplaciantechniquerespectively,sothatthe tweets belonging to social connected users share similar label dist
2799074487	Character-Level Models versus Morphology in Semantic Role Labeling	1899794420	h rich resources and less morphological complexity like English and Chinese. Character-level Models: Character-level models have proven themselves useful for many NLP tasks such as language modeling (Ling et al., 2015;Kim et al.,2016), POS tagging (Santos and Zadrozny,2014;Plank et al.,2016), dependency parsing (Dozat et al.,2017) and machine translation (Lee et al.,2017). However the number of comparative studies
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	2612953412	We also compare our approach against other models in the same class—in this case, models that encode sentences individually and do not allow attention across the two sentences.1 We include InferSent (Conneau et al., 2017), which also makes use of a BiLSTM-Max sentence encoder. In addition, we include a setting where we combine not two, but six different embedding types, adding FastText wiki-news embeddings2, English-G
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	1662133657	shed new insight into the usage of word embeddings in NLP systems. 1 Introduction It is no exaggeration to say that word embeddings have revolutionized NLP. From early distributional semantic models (Turney and Pantel, 2010;Erk,2012;Clark,2015) to deep learningbased word embeddings (Bengio et al.,2003;Collobert and Weston,2008;Mikolov et al.,2013; Pennington et al.,2014;Bojanowski et al.,2016), word-level meaning repres
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	2112184938	ngs efﬁciently, in interpretable ways. 6 Image-Caption Retrieval An advantage of the proposed approach is that it is inherently capable of dealing with multi-modal information. Multi-modal semantics (Bruni et al., 2014) often combines linguistic and visual representations via concatenation with a global weight , i.e., v = [v ling;(1 )v vis]. In DME we instead learn to combine embeddings dynamically, optionally base
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	2153579005	nts to highlight the general usefulness of our technique and how it can lead to new insights. 2 Context-Attentive Embeddings Commonly, NLP systems use a single type of word embedding, e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al.,2014) or FastText (Bojanowski et al.,2016). We propose giving networks access to multiple types of embeddings, and allowing them to select which embeddings they prefer by as
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	2145056192	ombining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014;Lazaridou et al.,2015), seeBaltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [
2799080176	Context-Attentive Embeddings for Improved Sentence Representations	2252211741	are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg,2014b), evaluating their performance (Milajevs et al.,2014;Schnabel et al., 2015;Bakarov,2017), specializing them for certain tasks (Maas et al.,2011;Faruqui et al.,2014; Kiela et al.,2015;Mrksiˇ ´c et al. ,2016;Vuli´c and Mrksiˇ ´c,2017), learning sub-word level representations
2799100448	A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification	2250539671	is then fed into a multi-gram (i.e. different window sizes) convolution layer to capture different local character chunk information. For word-level embedding, we use pretrained word vectors, GloVe (Pennington et al., 2014), to map each word to a low-dimensional vector space. Finally, each word is represented as a concatenation of the character-level embedding and word-level embedding. This is performed on the context w
2799100448	A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification	1903029394	mantic information in character chunk. Speciﬁcally, we ﬁrst input onehot-encoding character sequences to a 1 1 convolution layer to enhance the semantic nonlinear representation ability of our model (Long et al., 2015), and the output is then fed into a multi-gram (i.e. different window sizes) convolution layer to capture different local character chunk information. For word-level embedding, we use pretrained word
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2563734883	4) incorporating the SenticNetexternal knowledge base (Cambria et al., 2016). We additionally implement a bi-directional EntNetwith the same hyper-parameter settings and GloVeembeddings as our model (Henaff et al., 2017). In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following Saeidi et al.
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2536725458	aspect-speciﬁc word and sentence representations (Tang et al., 2016a). Despite these successes, keeping track of multiple entity–aspect pairs remains a difﬁcult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classiﬁcation, compared to a simple logistic regression model with n-gram features. Intuitively, we would ex
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2536725458	capture any entities.5 4http://nlp.stanford.edu/data/glove.42B.300d.zip 5In line with the ﬁndings of Henaff et al. (2017) that tying key vectors damages model performance, we observed Consistent with Saeidi et al. (2016), we tackle the data unbalanced problem (none ≫positive + negative) bysamplingthesamenumberoftraining instances withinabatchrandomlyfromeachclass. Evaluation. We benchmark against baseline systems pre
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2563734883	delay recurrence → dj i is deﬁned as: → h˜ j i= φ( → Uh i−1 + Vk j + −→ Ww) (3) → dj i = −−→ GRU( → ˜h j i, → dj i−1 ) (4) 3While →g j i could instead be a vector for ﬁner-grained control, following Henaff et al. (2017), we use a scalar for simplicity. where → h˜ j i is the new candidate memory vector to be incorporated into the existing memory → hj i−1 to form the new memory → hj i , φ is the parametric ReLUactivat
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2563734883	e other 4 chains with free key embeddings which are updated during training, and therefore free to capture any entities.5 4http://nlp.stanford.edu/data/glove.42B.300d.zip 5In line with the ﬁndings of Henaff et al. (2017) that tying key vectors damages model performance, we observed Consistent with Saeidi et al. (2016), we tackle the data unbalanced problem (none ≫positive + negative) bysamplingthesamenumberoftraining
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2536725458	et al., 2017). In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT-LOCATION, and SAFETY) andemploy the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, a
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	1924770834	h the input, the “content” term triggers the activation when the content of the entities (hj i−1 ) matches the input. The delay term models how and when the gate was turned on in the past with a GRU (Chung et al., 2014) and how past activations should inﬂuence the current one. Moreformally,witharrowsdenoting processing direction, the update gate is deﬁned as: →g j i = σ(w i· → h j −1 +w i·k + →v ·dj) (2) where →g j
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2074694452	ing training: Pennington et al. (2014)) 4 and pre-process the corpus with tokenisation using NLTK(Bird et al., 2009) and case folding. Training is carried out over 800 epochs with the FTRL optimiser (McMahan et al., 2013) and a batch size of 128 and learning rate of 0.05. We use the following hyper-parameters for weight matrices in both directions: R ∈ R300×3, H, U, V, W are all matrices of size R300×300, v ∈R300, and
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2126209950	ks: on the Children’s Book Test corpus (CBT), for example, competitive models take as input a window of text, centred around candidate entities, with crucial information contained within that window (Hill et al., 2015; Henaff et al., 2017). In TABSA, given the ﬁne-grained nature of the task, it is common practice for models to operate at the word- rather than chunk/sentence-level. It is not uncommon to see example
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2536725458	multiple targets. Each sentence is annotated with a list of tuples {(t,a,y)}with each identifying the sentiment polarity y towards a speciﬁc aspect a of Model Aspect Sentiment Acc. F1 AUC Acc. AUC LR(Saeidi et al., 2016) — 39.3 92.4 87.5 90.5 LSTM-Final(Saeidi et al., 2016) — 68.9 89.8 82.0 85.4 LSTM-Loc(Saeidi et al., 2016) — 69.3 89.7 81.9 83.9 LSTM+TA+SA(Ma et al., 2018) 66.4 76.7 — 86.8 — SenticLSTM(Ma et al., 20
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2250539671	pect pair. A detailed description of the task is presented in Section 2. Model conﬁguration. We initialise our model with GloVe (300-D, trained on 42B tokens, 1.9M vocab, not updated during training: Pennington et al. (2014)) 4 and pre-process the corpus with tokenisation using NLTK(Bird et al., 2009) and case folding. Training is carried out over 800 epochs with the FTRL optimiser (McMahan et al., 2013) and a batch size
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2212703438	R300, and hidden size of the GRU in Equation (4) is 300. Dropout is applied to the output of φ in the ﬁnal classiﬁer (Equation (8)) with a rate of 0.2. Moreover, we employ the technique introduced by Gal and Ghahramani (2016) where the same dropout mask is applied to the input w i at every step with a rate of 0.2. Lastly, to curb overﬁtting, we regularise the last layer (Equation (8)) with an L2 penalty on its weights: λk
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2252057809	ring (Wagner et al., 2014; 1Codeavailableathttps://github.com/liufly/delayed-memory-update-entnet. 2Note that in our dataset, all entity mentions have been pre-nomalised to LOCn, where n is an index. Kiritchenko et al., 2014), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-speciﬁc word and sentence representations (Tang et al., 2016a). Despite these successes, keepi
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2536725458	sists of a sequence of words: {w1,...,w i,...,w m}where w i denotes words interleaved with one or more targets (t), which we assume to be pre-identiﬁed as with LOC1 and LOC2 in Example (1). Following Saeidi et al. (2016), we frame the task as a 3-class classiﬁcation problem: given a sentence s, a preidentiﬁed set of target entities T and ﬁxed set of aspects A, predict the sentiment polarity y ∈{positive,negative,none
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2563734883	softmax(Rφ(Hu+a)) (8) Training iscarried out based oncross entropy loss. L= CrossEntropy( y, ˆ) (9) Comparision with EntNet. While our model is largely inspired by Recurrent Entity Networks (EntNets: Henaff et al. (2017)), it differs in three main respects. First, we explicitly model the delay of activation of the update gates gj with the GRU in Equations (2) and (4) as opposed to making hj i implicitly assume the sa
2799110529	RECURRENT ENTITY NETWORKS WITH DELAYED MEMORY UPDATE FOR TARGETED ASPECT-BASED SENTIMENT ANALYSIS	2563734883	upies a memoryslotand isthen accessed viaattention independently, recent advances in machine reading suggest that processing inputs sequentially is beneﬁcial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks: on the Children’s Book Test corpus (C
2799119017	Dating Documents using Graph Convolution Networks	2112251443	the (unknown) document date in a document (Mirza and Tonelli,2016;Chambers et al.,2014). While methods to perform reasoning over such structures exist (Verhagen et al., 2007,2010;UzZaman et al.,2013;Llorens et al., 2015;Pustejovsky et al.,2003), none of them have exploited advances in deep learning (Krizhevsky et al.,2012;Hinton et al.,2012;Goodfellow et al., 2016). In particular, recently proposed Graph Convolution
2799122342	Attention Based Natural Language Grounding by Navigating Virtual Environment.	2138804539	esearchers in different domains. From the robotics domain : (Guadarrama et al.(2013),Tellex et al.(2011)) focus on grounding verbs in navigational instructions like go , pick up , move , follow etc. (Chao et al. (2011),Lemaignan et al.(2012)) ground various concepts through human-robot interaction. (Artzi &amp; Zettlemoyer (2013),Misra et al.(2016)) focus on grounding natural language instructions by mapping instru
2799124508	What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties	2780932362	2 layers of 512 hidden units (˘4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (˘12M parameters). We use pre-trained fastText word embeddings of size 300 (Mikolov et al., 2018) without ﬁne-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets. Training task performance and further details are in Appendix. task source target Aut
2799124508	What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties	2130942839	p that you cannot cram the meaning of a whole %&amp;!$# sentence into a single $&amp;!#* vector, sentence embedding methods have achieved impressive results in tasks ranging from machine translation (Sutskever et al., 2014;Cho et al.,2014) to entailment detection (Williams et al.,2018), spurring the quest for “universal embeddings” trained once and used in a variety of applications (e.g.,Kiros et al.,2015;Conneau et al
2799124508	What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties	2250790822	STS, SICK-R). This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (Marelli et al., 2014). It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar). Remarkably, TREC question type classiﬁcation is the downs
2799124508	What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties	2612953412	task, a sentence encoder is trained to encode two sentences, which are fed to a classiﬁer and whose role is to distinguish whether the sentences are contradictory, neutral or entailed. Finally, as inConneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings. 3.3 Training details BiLSTM encoders use 2 layers of 512 hidden units (˘4M par
2799126217	Neural Argument Generation Augmented with Externally Retrieved Evidence	2212703438	1,892 OPs). 6.2 Training Setup For all models, we use a two-layer biLSTM as encoder and a two-layer unidirectional LSTM as decoder, with 200-dimensional hidden states in each layer. We apply dropout (Gal and Ghahramani, 2016) on RNN cells with a keep probability of 0.8. We use Adam (Kingma and Ba,2015) with an initial learning rate of 0.001 to optimize the cross-entropy loss. Gradient clipping is also applied with the max
2799126217	Neural Argument Generation Augmented with Externally Retrieved Evidence	2609722168	text planning. 9 Related Work There is a growing interest in argumentation mining from the natural language processing research community (Park and Cardie,2014;Ghosh et al., 2014;Palau and Moens,2009;Niculae et al., 2017;Eger et al.,2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investiga
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2397198482	d to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classiﬁcation (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classiﬁcation has been a longstanding research topic (Liu, 2012; Pang et al., 2008; Chen et al., 2017; Moraes et al., 2
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2103471677	model cannot learn sentiment-aware embeddings across multiple domains. Some works have been proposed to learn word representations considering multiple domains (Yang et al., 2017; Bach et al., 2016; Bollegala et al., 2015). Most of them learn separate embeddings of the same word for different domains. Then they choose pivot words according to frequency-based statistical measures to bridge the semantics of individual em
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2103471677	omains p and q, we use the vector Uc w for both domains. Otherwise, w is represented by Up w and U q w for p and q respectively. In traditional cross-domain word embedding methods (Yang et al., 2017; Bollegala et al., 2015, 2016), each word is represented by different vectors in different domains without differentiation of domain-common and domain-speciﬁc words. In contrast tothesemethods, foreachwordw,weuse a latent v
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2158899491	s,which has led to the current popularity of word embeddings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classiﬁcation (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classiﬁcation has b
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2103471677	tes a negative opinion describing movies that do not invoke deep thoughts among the audience. This observation motivates the study of learning domainsensitive word representations (Yang et al., 2017; Bollegala et al., 2015, 2014). They basically learn separate embeddings of the same word for differentdomains. Tobridge thesemantics ofindividual embedding spaces, they select a subset of words that are likely to be domain
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2518202280	urrent popularity of word embeddings. Word embedding models have been applied to many tasks, such as named entity recognition (Turian et al., 2010), word sense disambiguation (Collobert et al., 2011; Iacobacci et al., 2016; Zhang and Hasan, 2017; Dave et al., 2018), parsing (Roth and Lapata, 2016), and document classiﬁcation (Tang et al., 2014a,b; Shi et al., 2017). Sentiment classiﬁcation has been a longstanding resea
2799129705	Learning Domain-Sensitive and Sentiment-Aware Word Embeddings	2072128103	word embeddings. Several techniques for generating such representations have been investigated. For example, Bengio et al. propose a neural network architecture for this purpose (Bengio et al., 2003; Bengio, 2009). Later, Mikolov et al. (2013) propose two methods that are considerably more efﬁcient, namely skip-gram and CBOW. This work has made it possible to learnwordembeddings fromlargedatasets,which has led
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2337601653	(Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language learning through pure textual dialogues. However, in these works (He et al., 2016; Weston, 2016; Li et al., 2017), a set of candidate sequences is provid
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2579486552	016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language learning through pure textual dialogues. However, in these works (He et al., 2016; Weston, 2016; Li et al., 2017), a set of candidate sequences is provided and the action is
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2337601653	acher. This makes our game distinct from other seemingly relevant games, in which the agent cannot speak (Wang et al., 2016) or “speaks” by selecting a candidate from a provided set (He et al., 2016; Weston, 2016; Li et al., 2017) rather than generating sentences by itself, or games mainly focus on slow learning (Das et al., 2017; Strub et al., 2017) and falls short on one-shot learning. In this game, session
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2121863487	aking actions, the policy pS  () is adjusted by maximizing expected future reward as represented by LR  . As a non-differentiable sampling operation is involved in Eqn.(4), policy gradient theorem (Sutton and Barto, 1998) is used to derive the gradient for updating pS  () in the reinforce module: r L R  = E pS  P t A tr logpS  (a tjct) ; (6) where At=V(ht I ;c t) r t+1 V(h +1 I ;c t+1) is the advantage (Sutton an
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2176263492	and Bangalore, 2014). Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	889023230	d Language Learning. Deep neural network-based language learning has seen great success on many applications, including machine translation (Cho et al., 2014b), dialogue generation (Wen et al., 2015; Serban et al., 2016), image captioning and visual question answering (?Antol et al., 2015). For training, a large amount of labeled data is needed, requiring signiﬁcant efforts to collect. Moreover, this setting essentia
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2176263492	e At=V(ht I ;c t) r t+1 V(h +1 I ;c t+1) is the advantage (Sutton and Barto, 1998) estimated using a value network V(). The imitation module contributes by implementing LI  with a crossentropy loss (Ranzato et al., 2016) and minimizing it with respect to the parameters in pI  (), which are shared with pS  (). The training signal from imitation takes the shortcut connection without going through the controller. More
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2591957724	e basic ability to generate sensible sentences. As learning is done by observing the teacher’s behaviors during conversion, the agent essentially imitates the teacher from a third-person perspective (Stadie et al., 2017) rather than imitating an expert agent who is conversing with the teacher (Das et al., 2017; Strub et al., 2017). During conversations, the agent perceives sentences and images without any explicit la
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2465628802	e-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language learning through pure textual dialogues. However, in these works (He et al., 2016; Weston, 2016; Li et
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	1933349210	en great success on many applications, including machine translation (Cho et al., 2014b), dialogue generation (Wen et al., 2015; Serban et al., 2016), image captioning and visual question answering (?Antol et al., 2015). For training, a large amount of labeled data is needed, requiring signiﬁcant efforts to collect. Moreover, this setting essentially captures the statistics of training data and does not respect the
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2401823607	f data (Borovsky et al., 2003). From even just one example, children seem to be able to make inferences and draw plausible boundaries between concepts, demonstrating the ability of one-shot learning (Lake et al., 2011). The language acquisition process and the oneshot learning ability of human beings are both impressive as a manifestation of human intelligence, and are inspiring for designing novel settings and alg
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2465628802	g taught once. teacher. This makes our game distinct from other seemingly relevant games, in which the agent cannot speak (Wang et al., 2016) or “speaks” by selecting a candidate from a provided set (He et al., 2016; Weston, 2016; Li et al., 2017) rather than generating sentences by itself, or games mainly focus on slow learning (Das et al., 2017; Strub et al., 2017) and falls short on one-shot learning. In this
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	889023230	ing methods are compared: Reinforce: a baseline model with the same network structure as the proposed model and trained using RL only, i.e. minimizing LR  ; Imitation: a recurrent encoder decoder (Serban et al., 2016) model with the same structure as ours and trained via imitation (minimizing LI  ); Imitation+Gaussian-RL: a joint imitation and reinforcement method using a Gaussian policy (Duan et al., 2016) in t
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2579486552	kes our game distinct from other seemingly relevant games, in which the agent cannot speak (Wang et al., 2016) or “speaks” by selecting a candidate from a provided set (He et al., 2016; Weston, 2016; Li et al., 2017) rather than generating sentences by itself, or games mainly focus on slow learning (Das et al., 2017; Strub et al., 2017) and falls short on one-shot learning. In this game, sessions (S l) are random
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2401823607	lp the guesser achieve the ﬁnal goal, while we focus on transferable speaking and one-shot ability. One-shot Learning and Active Learning. Oneshot learning has been investigated in some recent works (Lake et al., 2011; Santoro et al., 2016; Woodward and Finn, 2016). The memoryaugmented network (Santoro et al., 2016) stores visual representations mixed with ground truth class labels in an external memory for one-sh
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2564324149	one from a provided candidate set. Communication and Emergence of Language. Recent studies have examined learning to communicate (Foerster et al., 2016; Sukhbaatar et al., 2016) and invent language (Lazaridou et al., 2017; Mordatch and Abbeel, 2018). The emerged language needs to be interpreted by humans via postprocessing (Mordatch and Abbeel, 2018). We, however, aim to achieve language learning from the dual perspec
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2342662072	r (Serban et al., 2016) model with the same structure as ours and trained via imitation (minimizing LI  ); Imitation+Gaussian-RL: a joint imitation and reinforcement method using a Gaussian policy (Duan et al., 2016) in the latent space of the control vector ct(Zhang et al., 2017). The policy is changed by modifying the control vector ctthe action policy depends upon. Training Details. The training algorithm is i
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2487501366	Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which ex
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2402402867	a sequence action rather than to simply select one from a provided candidate set. Communication and Emergence of Language. Recent studies have examined learning to communicate (Foerster et al., 2016; Sukhbaatar et al., 2016) and invent language (Lazaridou et al., 2017; Mordatch and Abbeel, 2018). The emerged language needs to be interpreted by humans via postprocessing (Mordatch and Abbeel, 2018). We, however, aim to ach
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2410983263	for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language l
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	2410540304	t statement (“this is cherry”) for another instance of cherry after only being taught once. teacher. This makes our game distinct from other seemingly relevant games, in which the agent cannot speak (Wang et al., 2016) or “speaks” by selecting a candidate from a provided set (He et al., 2016; Weston, 2016; Li et al., 2017) rather than generating sentences by itself, or games mainly focus on slow learning (Das et al
2799176896	Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game	1948566616	ted Work Supervised Language Learning. Deep neural network-based language learning has seen great success on many applications, including machine translation (Cho et al., 2014b), dialogue generation (Wen et al., 2015; Serban et al., 2016), image captioning and visual question answering (?Antol et al., 2015). For training, a large amount of labeled data is needed, requiring signiﬁcant efforts to collect. Moreover,
2799190973	AMR dependency parsing with a typed semantic algebra	2149837184	2013) and two successful SemEval Challenges (May,2016; May and Priyadarshi,2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014,2016) distinguishes concept identiﬁcation (assigning graph fragments to words) from relation identiﬁcation (adding graph edges which connect these fragments), and solves the former with a supertaggin
2799208956	Automatic Estimation of Simultaneous Interpreter Performance	2021618504	sed to the interpreter, maximizing the quality of interpreter output. As a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems (Specia et al., 2010,2015), which takes in the source sentence and MT-generated outputs and estimates a measure of quality. In doing so, we arrive at two natural research questions: 1.Do existing methods for performing Q
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	102233799	.9%. This is only 1.5% below training with all of WSJTRAIN and QBANKTRAIN. The resulting system improves slightly on WSJTEST getting 94.38%. On the more difﬁcult GENIA corpus of biomedical abstracts (Tateisi et al., 2005), we see a similar, if somewhat less dramatic, trend. See Table 5. With 50 annotated sentences, performance on GENIADEV jumps from 79.5% to 86.2%, outperforming all but one parser from David McClosky’
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	65617392	015) focuses on dependency parsing. (Li et al., 2016) provides a good overview. Here we highlight three important highlevel strategies. The ﬁrst is “complete-then-train” (Mirroshandel and Nasr, 2011; Majidi and Crane, 2013), which “completes” every partially annotated dependency parse by ﬁnding the most likely parse (according to an already trained parser model) that respects the constraints of the partial annotations.
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2138382875,2163568299	06; Blitzer et al., 2006; Daum´e, 2007; Finkel and Manning, 2009). In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-traini
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2105677720	2013), which shows how to train a part-of-speech tagger with a minimal amount of annotation effort. 6.2 Learning from Partial Annotation Most literature on training parsers from partial annotations (Sassano and Kurohashi, 2010; Spreyer et al., 2010; Flannery et al., 2011; Flannery and Mori, 2015; Mielens et al., 2015) focuses on dependency parsing. (Li et al., 2016) provides a good overview. Here we highlight three importa
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2251100697	agger with a minimal amount of annotation effort. 6.2 Learning from Partial Annotation Most literature on training parsers from partial annotations (Sassano and Kurohashi, 2010; Spreyer et al., 2010; Flannery et al., 2011; Flannery and Mori, 2015; Mielens et al., 2015) focuses on dependency parsing. (Li et al., 2016) provides a good overview. Here we highlight three important highlevel strategies. The ﬁrst is “complet
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	93558233	ain a part-of-speech tagger with a minimal amount of annotation effort. 6.2 Learning from Partial Annotation Most literature on training parsers from partial annotations (Sassano and Kurohashi, 2010; Spreyer et al., 2010; Flannery et al., 2011; Flannery and Mori, 2015; Mielens et al., 2015) focuses on dependency parsing. (Li et al., 2016) provides a good overview. Here we highlight three important highlevel strategie
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2158108973	al annotation, each have a long tradition in natural language processing. 6.1 Domain Adaptation Domain adaptation has been recognized as a major NLP problem for over a decade (Ben-David et al., 2006; Blitzer et al., 2006; Daum´e, 2007; Finkel and Manning, 2009). In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b;
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2138382875,2163568299	e on Brown verticals. MSP refers to the Minimal Span Parser (Stern et al., 2017a). Charniak refers to the Charniak parser with reranking and self-training (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006a). MSP + Stanford POS tags refers to MSP trained and tested using part-of-speech tags predicted by the Stanford tagger (Toutanova et al., 2003). Training Data Rec. Prec. F 1 WSJ QBANK 40k 0 91.07 88.
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2048679005	eichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-training (Blum and Mitchell, 1998) or self-training to augment a small domain corpus, or how to best combine models to perform well on a particular domain. In this work, we focus on the direct impact that just a few dozen partially an
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2250564385	en the loss function only records loss when that span is classiﬁed as a non-constituent (i.e. any label is ok). 5 Experiments 5.1 Geometry Questions We took the publicly available training data from (Seo et al., 2015), split the data into sentences, and then annotated each sentence as in Figure 1. Next, we randomly split these sentences into GEOTRAIN and GEODEV7. After removing duplicate sentences spanning both se
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2250564385	which no gold parses exist) with a couple hours of effort. We explore this possibility in the next section. 4 Rapid Parser Extension To create a parser for their geometry question answering system, (Seo et al., 2015) did the following: Designed regular expressions to identify mathematical expressions. Replaced the identiﬁed expressions with dummy words. Parsed the resulting sentences. FRAG . . NP 24 and QS = 1
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	1983582289	ikely parse (according to an already trained parser model) that respects the constraints of the partial annotations. These “completed” parses are then used to train a new parser. The second strategy (Nivre et al., 2014; Li et al., 2016) is similar to “complete-then-train,” but integrates parse completion into the training process. At each iteration, new “complete” parses are created using the parser model from the
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2610748790	instead of parse trees. This results in a parser that can be trained, with no adjustments to the training regime, from partial sentence bracketings. 2. The use of contextualized word representations (Peters et al., 2017; McCann et al., 2017) greatly reduces the amount of data needed to train linguistic models. Contextualized word representations, which encode tokens conditioned on their context in a sentence, have b
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2138433202	ither the training algorithm or the training data. While the bulk of the literature on training from partial annotations focuses on dependency parsing, the earliest papers (Pereira and Schabes, 1992; Hwa, 1999) focus on constituency parsing. These leverage an adapted version of the inside-outside algorithm for estimating the parameters of a probabilistic context-free grammar (PCFG). Our work is not tied to
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2104936489	ition in natural language processing. 6.1 Domain Adaptation Domain adaptation has been recognized as a major NLP problem for over a decade (Ben-David et al., 2006; Blitzer et al., 2006; Daum´e, 2007; Finkel and Manning, 2009). In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and T
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2131953535	and learning from partial annotation, each have a long tradition in natural language processing. 6.1 Domain Adaptation Domain adaptation has been recognized as a major NLP problem for over a decade (Ben-David et al., 2006; Blitzer et al., 2006; Daum´e, 2007; Finkel and Manning, 2009). In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (Mc
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2163899016	Mori, 2015; Mielens et al., 2015) focuses on dependency parsing. (Li et al., 2016) provides a good overview. Here we highlight three important highlevel strategies. The ﬁrst is “complete-then-train” (Mirroshandel and Nasr, 2011; Majidi and Crane, 2013), which “completes” every partially annotated dependency parse by ﬁnding the most likely parse (according to an already trained parser model) that respects the constraints of
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2250407797	n,” but integrates parse completion into the training process. At each iteration, new “complete” parses are created using the parser model from the most recent training iteration. The third strategy (Li et al., 2014, 2016) transforms each partial annotation into a forest of parses that encodes all fully-speciﬁed parses permitted by the partial annotation. Then, the training objective is modiﬁed to support optimi
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	1745817670	nd Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-training (Blum and Mitchell, 1998) or self-training to augment a small domain corpus, or how to best combine models to perform
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2124772738	ning, 2009). In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-training (Blum and Mitchell, 1998) or self-training to augmen
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2289899728	o 0:4 . Unlike the model it is based on, our model uses word embeddings of length 1124. These result from concatenating a 100 dimension learned word embedding, with a 1024 diParser Rec Prec F 1 RNNG (Dyer et al., 2016) - - 91.7 MSP (Stern et al., 2017a) 90.6 93.0 91.8 (Stern et al., 2017b) 92.6 92.6 92.6 RSP 93.8 94.8 94.3 Table 1: Parsing performance on WSJTEST, along with the results of other recent single-model
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2138382875,2163568299	parsers outside of the newswire domain. When (Kummerfeld et al., 2012) parsed the various Brown verticals with the (then state-of-the-art) Charniak parser (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006a), it achieved F 1 scores between 83% and 86%, even though its F 1 score on WSJTEST was 92.1%. In Table 3, we discover that RSP does not suffer nearly as much degradation, with an average F 1-score o
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	168425931	to part-of-speech taggers. 3.2 Beyond Newswire The Brown Corpus The Brown corpus (Marcus et al., 1993) is a standard benchmark used to assess WSJ-trained parsers outside of the newswire domain. When (Kummerfeld et al., 2012) parsed the various Brown verticals with the (then state-of-the-art) Charniak parser (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006a), it achieved F 1 scores between 83% and 86%, e
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2168199177	when using a particular neural model with contextualized word representations. Co-training, self-training, and model combination are orthogonal to our approach. Our work is a spiritual successor to (Garrette and Baldridge, 2013), which shows how to train a part-of-speech tagger with a minimal amount of annotation effort. 6.2 Learning from Partial Annotation Most literature on training parsers from partial annotations (Sassan
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	98423455	sers (Plank, 2011; Ma and Xia, 2013) has received considerable attention. Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-training (Blum and Mitchell, 1998) or self-training to augment a small domain corpus, or how to best combine mode
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2512597464	tried performed equivalently well. 2.2 Classiﬁcation Model For our span classiﬁcation model ˙(l j x;s), we use the model from (Stern et al., 2017a), which leverages a method for encoding spans from (Wang and Chang, 2016; Cross and Huang, 2016). First, it creates a sentence encoding by running a two-layer bidirectional LSTM over the sentence to obtain forward and backward encodings for each position i, denoted by f i
2799222909	Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples	2138433202	y asserts the probability of a full parse tree) and the annotation task (which asserts the correctness of some subcomponent, like a constituent span or a dependency arc). There is a body of research (Hwa, 1999; Li et al., 2016) that discusses how to bridge this gap by modifying the training data, training algorithm, or the training objective. Alternatively, we could just better align the model with the ann
2799236921	Analogical Reasoning on Chinese Morphological and Semantic Relations	2294970769	r semantic regularities (Mikolov et al.,2013). Analogical reasoning has become a reliable evaluation method for word embeddings. In addition, It can be used in inducing morphological transformations (Soricut and Och, 2015), detecting semantic relations (Herdagdelen and Baroni,2009), and translating unknown words (Langlais and Patry,2007). It is well known that linguistic regularities vary a lot among different language
2799250213	AN ANNOTATED CORPUS FOR MACHINE READING OF INSTRUCTIONS IN WET LAB PROTOCOLS	2252225847	efforts that have initiated natural language processing research in new directions, for example political framing (Card et al., 2015), question answering (Rajpurkar et al.,2016) and cooking recipes (Jermsurawong and Habash, 2015). Although mapping natural language instructions to machine readable representations is an important direction with many practical applications, we believe current research in this area is hampered by
2799250213	AN ANNOTATED CORPUS FOR MACHINE READING OF INSTRUCTIONS IN WET LAB PROTOCOLS	2251617662	hine learning approaches. There have been many recent data collection and annotation efforts that have initiated natural language processing research in new directions, for example political framing (Card et al., 2015), question answering (Rajpurkar et al.,2016) and cooking recipes (Jermsurawong and Habash, 2015). Although mapping natural language instructions to machine readable representations is an important dir
2799264621	Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification	2427527485	ion on single passage (Wang and Jiang,2016;Seo et al.,2016;Pan et al.,2017). A signiﬁcant milestone is that several MRC models have exceeded the performance of human annotators on the SQuAD dataset1 (Rajpurkar et al., 2016). However, this success on single Wikipedia passage is still not adequate, considering the ultimate goal of reading the whole web. Therefore, several latest datasets (Nguyen et al.,2016;He et al.,2017
2799328256	Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog	2412899141	(2015) achieved human-level performance in Atari games based on deep neural networks. Deep RL was then used to jointly learn the dialog state tracking and policy optimization in an end-to-end manner (Zhao and Eskenazi, 2016). In our framework, we use a DQN to learn the higher level policy for question selection or image guessing.Van Hasselt et al.(2016) proposed a double DQN to overcome the overestimation problem in the
2799328256	Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog	2109910161	related to hierarchical reinforcement learning (HRL) which often decomposes the problem into several sub-problems and achieves better learning convergence rate and generalization compared to ﬂat RL (Sutton et al., 1999;Dietterich,2000). HRL has been applied to dialog management (Lemon et al.,2006; Cuay´ahuitl et al. ,2010;Budzianowski et al.,2017) which decomposes the dialog policy with respect to system goals or d
2799328256	Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog	2201581102	work, we use a DQN to learn the higher level policy for question selection or image guessing.Van Hasselt et al.(2016) proposed a double DQN to overcome the overestimation problem in the Q-Learning andSchaul et al. (2015) suggested prioritized experience replay to improve the data sampling efﬁciency for training DQN. We apply both techniques in our implementation. One limitation of DQNs is that they cannot handle unbo
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	126426901	may be used to convey rapport in competitive settings and (Poesio and Rieser, 2010) mentioned them as signs of coordination and alignment. However, in a similar set up to the one used in this study, (Bull and Aylett, 1998) found that the complexity and the lack of familiarity with the tasks could result in longer gaps. Uncertainty display is another mechanism that indicates that the dialogue might approaching a point w
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	2107163554	for efﬁciency purposes. The fact that the corpus we are releasing is multi-modal could beneﬁt an integrated approach to improve the understanding of dialogue utterances such as the one presented in (Kennington et al., 2013). But these mechanisms and efforts are not exclusively linguistic. (Nenkova et al., 2008) found that higher degrees of entrainment are associated with more overlaps and fewer interruptions. (Oviatt et
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	2100355644	press the button to show the solution. These choice of topics followed an hypothesis that participants behaviour would change between topics as the graph in Figure 2 shows for participation equality (Lai et al., 2013). We also automatically extracted all the speech overlaps found in the data and we annotated those which corresponded to ﬂoor changes as interruptions or not. Among overlaps, we considered interruptio
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	2142970219	s dialogue is still a very challenging problem since its structure is often difﬁcult to represent, unlike task-oriented dialogues which could easily be represented by a ﬂow chart. Therefore, DeVault (DeVault, 2008) compared task-oriented dialogue to assembling furniture. On the other hand, non task-oriented dialogue may be compared to dancing. To be able to dance one needs to learn the steps, but this might not
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	2182998842	both in task and non task-oriented dialogues. For instance, studies by (Garrod and Anderson, 1987) and (Brennan and Clark, 1996) have focused on participants’ coordination in terms of lexical items. (Reitter and Moore, 2007) showed that for task solving in dialogue, lexical and syntactic repetition is a reliable predictor of task success given the ﬁrst ﬁve minutes of task oriented dialogue. (Friedberg et al., 2012) found
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	2035500771	w that the domain experts differed in the kind of interruptions they made from nondomain experts. (Goldberg, 1990) stated that interruptions may be used to convey rapport in competitive settings and (Poesio and Rieser, 2010) mentioned them as signs of coordination and alignment. However, in a similar set up to the one used in this study, (Bull and Aylett, 1998) found that the complexity and the lack of familiarity with t
2799416969	The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions.	1608177573	y display is another mechanism that indicates that the dialogue might approaching a point where some recovery strategy might be needed. It has already been studied in the scope of tutoring dialogues (Liscombe et al., 2005), but also in spontaneous speech (Schrank and Schuppler, 2015). Although tutoring dialogues can be seen as a collaborative dialogue, there is no short term goal, and therefore we hypothesize that the
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2407080277	]. Training audio samples were randomly perturbed in speed and volume during the training process. This approach is commonly called audio augmentation and is known to be benet for speech recognition [5]. 3.2 Language model Two approaches were used, both aiming at rescoring lattices. The rst one is a N-gram model of order 4 trained with the pocolm toolkit5, which was pruned to 10 million N-grams. We
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	1524333225	e. For each talk, we built a sphere audio le, and its corresponding transcript in stm format. The text from each .stm le was automatically aligned to the corresponding .sph le using the Kaldi toolkit [9]. This consists in the adaptation of existing scripts 3, meant to rst decode the audio les with a biased language model, and then align the obtained .ctm le with the reference transcript. To maximize
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2193413348	oduce words, since it emits sequences of characters. 4.1 Model architecture The fully end-to-end architecture used in this study is similar to the Deep Speech 2 neural ASR system proposed by Baidu in [1]. This architecture is composed of ncconvolution layers (CNN), followed by nruni or bidirectional recurrent layers, a lookahead convolution layer [13], and one fully connected layer just before the so
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2407080277	paces to delimit words); { the Greedy+augmentation one, in red, which is similar to the Greedy one, but in which each training audio samples is randomly perturbed in gain and tempo for each iteration [5]; { the Beam+augmentation one, in brown, got by applying a language model through a beam search decoding on the top of the neural network hypotheses using the Greedy+augmentation conguration. This la
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	1524333225	peaker adaptive trained (SAT) AM. The SAT AM was trained using GMM-derived (GMMD) features [14], adapted with maximum a posteriori adaptation (MAP) algorithm [2]. The Kaldi speech recognition toolkit [9] was used for these experiments. Both AMs have TDNN-LSTM topology described in [7], and dier only in the input features. For the SI AM, 40-dimensional Mel-frequency cepstral coecients (MFCCs) withou
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2251321385	from the TED conference videos which were since widely used by the ASR community for research purposes. These corpora were called TED-LIUM, release 1 and release 2, presented respectively in [10] and [11]. Ubiqus joined these eorts to pursue the improvements both from an increased data standpoint, as well as from a technical achievement one. We believe that this corpus has become a reference and will
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2251321385	text from the original alignments of release 2 to this corpus. In total, the textual corpus used to train language models contains approximately 255 million words. These source data are described in [11]. 3.3 Experimental results In this section, we present the recent development on Automatic Speech Recognition (ASR) systems that can be compared with the two previous releases of the TED-LIUM Corpus f
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2193413348	tion [2], in order to predict a sequence of characters from the input audio. In our experiments, we used two CNN layers and six bidirectional recurrent layers with batch normalization as mentioned in [1]. Given an utterance x iand label y sampled from a training set X= (x1;y1);(x2;y2);:::, the RNN architecture has to train to convert an input sequence xi into a nal transcription yis. For notational c
2799473636	TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation	2293634267	tter-based features and importance sampling [15], coupled with a pruned approach to lattice-rescoring [14]. The RNNLM we retained was a mixture of three TDNN layers with two interspersed LSTMP layers [12] containing around 10 million parameters. The latter helps to reduce the word error rate drastically. We used the same corpus and vocabulary in both methods, which are those released along with TED-LI
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	1505544955	2 represents the general SlugNERDS pipeline. Our Name Entity Recognition consists of two standard modules, Entity Segmentation and Entity Classiﬁcation (Ritter et al., 2011; Collins and Singer, 1999; Downey et al., 2007). Subsequently we perform Entity Linking on the recognized entity. We will examine this process with the following example utterance: ”I think my favorite star wars is revenge of the sith”. Please not
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2056894934	conversations with the system. We perform an extensive analysis of our system and the corpora to identify important areas of future work. NER and NEL have been actively researched topics for decades (Finkel and Manning, 2009; Ratinov and Roth, 2009; Ritter et al., 2011; Derczynski et al., 2015; Nitish Gupta and Roth, 2017). However, the resulting entity classiﬁcation is often coarse and does not encode an ontology. For e
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2094728533	us data streams necessary to support open domain conversation(Bowden et al., 2017a). While Ling and Weld (2012) attempt to address this by using 112 ﬁne-grained entity types consistent with Freebase (Bollacker et al., 2008), Freebase is no longer maintained and recent inspection has shown it to be signiﬁcantly incomplete (Dong et al., 2014). While the accuracy of these state of the art NER systems can be quite high, ran
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2016753842	ddress this by using 112 ﬁne-grained entity types consistent with Freebase (Bollacker et al., 2008), Freebase is no longer maintained and recent inspection has shown it to be signiﬁcantly incomplete (Dong et al., 2014). While the accuracy of these state of the art NER systems can be quite high, ranging between 80-90% on long text, on short informal text, such as tweets, accuracies drop to between 30-50% (Derczynski
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2123442489	entity classiﬁcation is often coarse and does not encode an ontology. For example, Stanford NER features only a small number of abstract entity types such as PERSON, LOCATION, ORGANIZATION, and MISC (Manning et al., 2014; Finkel and Manning, 2009); these categories don’t provide enough information for dialogue interpretation and generalization. Although other resources such as that from Ratinov and Roth (2009) utiliz
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2081580037	ies in the corpus are colored to match the associated entity. Currently the verbs used in this corpus have been hand annotated. These seed verbs are then expanded using synonym relations from Wordnet(Miller, 1995) and other lexical resources. Using these verbs in addition to prepositional phrases we automatically generate a list of short phrases associated with speciﬁc entities, such as arrive at for the City
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2139243841	a known entity to existing resources on the web while discourse linking is focused on linking each mention of the entity within the input to the same discourse entity in our internal representation (Brennan et al., 1987; Walker et al., 1997). As mentioned in Section 3.1., the Google Knowledge Graph query returns a Wikipedia article associated with the entity. We can further increase our web based linking by utilizin
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2123442489	l evaluation we have concluded that Slugbot missing an entity can be more detrimental to a conversation than over-classifying entities. First, we construct a constituency tree using Stanford CoreNLP (Manning et al., 2014) and build our candidate pool by collapsing each of the noun phrases, verb phrases, and sentences in the tree. A sample constituency tree can be seen in Figure 3. Additionally, we collapse sequential
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2004763266	and MISC (Manning et al., 2014; Finkel and Manning, 2009); these categories don’t provide enough information for dialogue interpretation and generalization. Although other resources such as that from Ratinov and Roth (2009) utilize additional external knowledge by extracting 30 gazetteers from both the web and Wikipedia, the entity types are still not as varied as we need, and the framework lacks a clear ontology. Furth
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	102708294	on, we note that pairing the entity type with the precise entity name as provided in the query will allow for easy subsequent queries to large databases such as YAGO (Rebele et al., 2016) or DBpedia (Auer et al., 2007). Finally, our tool uses an augmented version of the Stanford Coreference Annotator (Manning et al., 2014) to perform Discourse Linking. 4. Evaluation 4.1. NER Results Our SlugNERDS tool was originall
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2123442489	ow for easy subsequent queries to large databases such as YAGO (Rebele et al., 2016) or DBpedia (Auer et al., 2007). Finally, our tool uses an augmented version of the Stanford Coreference Annotator (Manning et al., 2014) to perform Discourse Linking. 4. Evaluation 4.1. NER Results Our SlugNERDS tool was originally developed and utilized in the 2017 Alexa Prize Competition for SlugBot, which scored in the top 25% of c
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2406945108	ses between systems can be inconsistent as there is no universally shared taxonomy between them and the various data streams necessary to support open domain conversation(Bowden et al., 2017a). While Ling and Weld (2012) attempt to address this by using 112 ﬁne-grained entity types consistent with Freebase (Bollacker et al., 2008), Freebase is no longer maintained and recent inspection has shown it to be signiﬁcantly
2799522139	SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue Systems.	2004763266	tem. We perform an extensive analysis of our system and the corpora to identify important areas of future work. NER and NEL have been actively researched topics for decades (Finkel and Manning, 2009; Ratinov and Roth, 2009; Ritter et al., 2011; Derczynski et al., 2015; Nitish Gupta and Roth, 2017). However, the resulting entity classiﬁcation is often coarse and does not encode an ontology. For example, Stanford NER fea
2799586518	AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples	1840435438	n found to easily break neural systems on other linguistic tasks such as reading comprehension (Jia and Liang,2017). A key contributor to this brittleness is the use of speciﬁc datasets such as SNLI (Bowman et al., 2015) and SQuAD (Rajpurkar et al.,2016) to drive model development. While large and challenging, these datasets also tend to be homogeneous. E.g., SNLI was created by asking crowd-source workers to generat
2799586518	AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples	2099471712	se can be used with any entailment model without constraining model architecture. We also introduce the ﬁrst approach to train a robust entailment model using a Generative Adversarial Network or GAN (Goodfellow et al., 2014) style framework. We iteratively improve both the entailment system (the discriminator) and the dierentiable part of the data-augmenter (speciﬁcally the neural generator), by training the generator b
2799755555	Towards Inference-Oriented Reading Comprehension: ParallelQA	2738015883	nces which have high lexical overlap with the question. We observe that this accounts for the largest chunk of errors across all models (example 2). Our observations are consistent with the ﬁndings ofJia and Liang (2017). The models often simply resolve the referential expression in the question to its corresponding entity. In example 1, the models resolve “organisation” in the question to “The UN” due to high lexica
2799836808	Multimodal Machine Translation with Reinforcement Learning.	648786980	assign end evaluation metrics (which are typically not log-likelihood) as the reinforcement learning training reward, Second, traditional machine translation models was known to have an exposure bias [2] between training and testing phases. At testing phase, the trained model conditions its future outputs on its own past guesses. When its own guess goes wrong, any future predictions are prone to the
2799836808	Multimodal Machine Translation with Reinforcement Learning.	2509283656	hine translation shared task [ 11 ], Calixto et al. from DCU proposed an attention-based neural machine translation model [ 3]. Guasch et al. presented a bidirectional RNN model with double embedding [5]. However, to the best of our knowledge, none of these models use reinforcement learning. 1 http://www.statmt.org/wmt16/multimodal-task.html 2 3 Methods 3.1 Problem Formulation We formulate the proble
2799920282	NEURAL MACHINE TRANSLATION FOR BILINGUALLY SCARCE SCENARIOS: A DEEP MULTI-TASK LEARNING APPROACH	2227523508	2SEQ transduction problem (Konstas et al., 2017). 5.3 Models and Baselines We have implemented the proposed multi-task learning architecture in C++ using DyNet (Neubig et al.,2017), on top of Mantis (Cohn et al., 2016) which is an implementation of the attentional SEQ2SEQ NMT model in (?). In our multitask architecture, we do partial sharing of parameters, where the parameters of the top 2 stacked layers are shared
2799920282	NEURAL MACHINE TRANSLATION FOR BILINGUALLY SCARCE SCENARIOS: A DEEP MULTI-TASK LEARNING APPROACH	2743555600	gate syntax/semantics phenomena learned as a byproduct of SEQ2SEQ NMT training. We, in turn, investigate the effect of injecting syntax/semantic on learning NMT using MTL. The closet work to ours is (Niehues and Cho, 2017), which has made use of part-of-speech tagging and named-entity recognition tasks to improve NMT. They have used the attentional encoder-decoder with a shallow architecture, and share different parts
2799920282	NEURAL MACHINE TRANSLATION FOR BILINGUALLY SCARCE SCENARIOS: A DEEP MULTI-TASK LEARNING APPROACH	2610308073	sentation (AMR) corpus Release 2.0 (LDC2017T10), which pairs English sentences AMR meaning graphs. We linearise the AMR graphs, in order to convert semantic parsing as a SEQ2SEQ transduction problem (Konstas et al., 2017). 5.3 Models and Baselines We have implemented the proposed multi-task learning architecture in C++ using DyNet (Neubig et al.,2017), on top of Mantis (Cohn et al., 2016) which is an implementation of
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2737711067	1 i 1 ) s1 i = g 1 y;2 (c i;s 0 i ) Like the encoders, the decoder recurrent unit function gl y at each layer lcan be instantiated by LSTM or GRU. 2.3.2. Deep Transition RNN The deep transition RNN (Miceli-Barone et al., 2017) involves a number of layers within a time step j through which an input word is fed, as illustrated in Figure 2. The recurrent unit function of each layer lis deﬁned as a transition, which outputs an
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	1816313093	2005) trained on the Chinese Penn Treebank (CTB) segmentation standard. To alleviate the effect of rare words in NMT, we fragmented words to sub-words through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 59,500 merge operations. All our training sentences are lowercased. Figure 2: An illustration of a deep transition RNN model (Miceli-Barone et al., 2017) with 4 encoder transitions (L x= 4) and
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	1522301498	del progresses by updating the model parameters at each mini-batch of 40 sentence pairs to minimize the negative log-likelihood loss function on the parallel training data. We use the Adam algorithm (Kingma and Ba, 2015) with learning rate of 0:0001. At each update, we clip the gradient norm to 1:0. We apply layer normalization (Ba et al., 2016) on the model parameters for faster convergence and tie the target-side e
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2514713644	e gradient norm to 1:0. We apply layer normalization (Ba et al., 2016) on the model parameters for faster convergence and tie the target-side embedding with the transpose of the output weight matrix (Press and Wolf, 2017). Model parameters are saved at every checkpoint of 10,000 update iterations. At this stage, the negative log-likelihood loss function on the development set is checked. Training stops when there has
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2740433069	et is checked. Training stops when there has been no improvement over the lowest loss function value on the development set for 10 consecutive checkpoints. The main difference between our system and (Sennrich et al., 2017a) is that while they only built NMT models with GRU, we also made use of LSTM. Another difference is in the usage of the larger monolingual English text. They built a synthetic Chinese-English parall
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2527845440	experiments on the United Nations Parallel Corpus (Ziemski et al., 2016), following (JunczysDowmunt et al., 2016). We used the pre-deﬁned training, development, and test sets of the corpus following (Junczys-Dowmunt et al., 2016) and conducted NMT experiments accordingly. We pre-processed our parallel training corpora by segmenting Chinese sentences, which originally have no spaces to demarcate words, and tokenizing English s
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2101105183	ge model to re-rank the k-best translation outputs produced by our NMT system. 4. Experimental Results For experiments using the LDC corpora, translation quality is measured by case-insensitive BLEU (Papineni et al., 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)6. Statistical significance testing between systems is conducted by bootstrap resampling (Koehn, 2004). As shown
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2740433069	i;s 0) where the input to the LSTM or GRU function for g y;1 is the embedding of the previous word E[y i w 1] and the input for g y;2 is the context vector c i. 2.3. Deep Recurrent Layers Following (Sennrich et al., 2017a), we adopt deep RNN models for encoding and decoding. There are two alternatives to achieve this, namely the deep stacked RNN and deep transition RNN. The deep stacked RNN passes the whole input seq
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	1526659760	important in research. In speech recognition, for instance, there has been work on building a virtual machine as a means of collaboratively building a state-of-the-art system for speech recognition (Metze et al., 2013), aiming at realizing a standardized state-of-theart system in a collaborative manner. While we do not provide any virtual machines, we have a similar intention of making available a state-of-the-art
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2426188458	l 1, for 1 &lt;lD y The ﬁrst RNN stack layer, s 1 i = g att (E[y i 1];s1i 1 ;c i), is a composition of two recurrent unit functions like in EquaFigure 1: An illustration of a deep stacked RNN model (Zhou et al., 2016) with encoder stack depth (D x) of 4 and decoder stack depth (D y) of 4. tions 8 and 9, i.e., s1 i= g 1 att (E[y 1];s 1 i 1 ;c i) s0 i= g 1 y;1 (E[y 1];s 1 i 1 ) s1 i = g 1 y;2 (c i;s 0 i ) Like the e
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2527845440	LEU, and IBM-BLEU with IBM-BLEU9. For UN Parallel Corpus experiments, translation quality is measured by case-insensitive BLEU using the script provided by Moses10, following the evaluation setup in (Junczys-Dowmunt et al., 2016)11. As shown in Table 3, our best result is obtained by an ensemble of 4 independent models with k-best output reranking using N-gram LM trained on the whole English side of the UN Parallel Corpus. Ou
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2740433069	nceptually a soft alignment model. To compute the decoding hidden state s iin Equation 4, we adopt an approach that incorporates the context c i from the attention mechanism by using two transitions (Sennrich et al., 2017b). The decoder hidden state function g y(E[y i 1];s i 1;c i) in Equation 4 ﬁrst passes the embedding E[y i 1] of the input word y i 1 to the ﬁrst recurrent unit function, resulting in an intermediate
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2785522575	nces, which originally have no spaces to demarcate words, and tokenizing English sentences to split punctuation symbols from words. Chinese word segmentation was performed by a maximum entropy model (Low et al., 2005) trained on the Chinese Penn Treebank (CTB) segmentation standard. To alleviate the effect of rare words in NMT, we fragmented words to sub-words through the byte pair encoding (BPE) algorithm (Sennri
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2725082186	ng a standardized state-of-theart system in a collaborative manner. While we do not provide any virtual machines, we have a similar intention of making available a state-of-the-art MT system. On NMT, Denkowski and Neubig (2017) argued that experiments should be based on a strong baseline system to ensure that a newly proposed approach indeed improves over the best prior published approaches. They performed their experiments
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2426188458	NN We adopt the deep stacked RNN that involves residual connection, summing the output of the previous RNN layer with the computed hidden state of the current RNN layer, and alternation of direction (Zhou et al., 2016), as illustrated in Figure 1. In this model, for each layer land time step j, we need to distinguish between the computed hidden state of the current RNN unit function without residual connection, i.e
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2527845440	re-ranking, and the improvement is statistically signiﬁcant (p&lt;0:05). Both of our systems achieve higher BLEU scores than the best published result for Chinese-to-English translation reported in (Junczys-Dowmunt et al., 2016). 7CL journal, TACL, ACL, COLING, EMNLP, NAACL, SSST, WMT, AAAI, and IJCAI. 8In one paper out of 403 papers (Huang et al., 2013), testing was performed on 4 test subsets: the news subset and the web s
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2737711067	rough the byte pair encoding (BPE) algorithm (Sennrich et al., 2016) with 59,500 merge operations. All our training sentences are lowercased. Figure 2: An illustration of a deep transition RNN model (Miceli-Barone et al., 2017) with 4 encoder transitions (L x= 4) and 4 decoder transitions (L y= 4). 3.1.1. LDC Corpora We divide the LDC corpora we used into older corpora3 and newer corpora4. Due to the dominant older corpora,
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2740433069	s. There is only one reference translation in the development and test sets of the UN Parallel Corpus. 3.2. NMT Model Parameters We built our neural machine translation (NMT) system by using Nematus (Sennrich et al., 2017b), an open-source NMT toolkit which implements the encoder-decoder NMT architecture with attention mechanism. Our system is based on the NMT system in (Sennrich et al., 2017a). We built an 5LDC2010T1
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2527845440	t p&lt;0:01). Published Ours (4mod-ens) no reranking with reranking 53.1 55.0 55.3 Table 3: Experimental results in BLEU (%) on the test set of the UN Parallel Corpus of the best published result in (Junczys-Dowmunt et al., 2016) and our system, without and with k-best re-ranking with N-gram language model. Statistical signiﬁcance testing shows the comparison between our 4-model system with re-ranking and without reranking (:
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	1816313093	T vocabulary size is limited. To cope with the limitation of the vocabulary size, we adopt fragmentation of words into sub-words of character sequences through the byte pair encoding (BPE) algorithm (Sennrich et al., 2016). This algorithm ﬁnds the N most frequent character sequences of variable length, through Ncharacter merge operations, and splits less frequent words based on this list of character sub-sequences. 2.2
2799925531	Upping the Ante: Towards a Better Benchmark for Chinese-to-English Machine Translation.	2133564696	tion-evaluation 2catalog.ldc.upenn.edu the conclusion. 2. Neural Machine Translation We built a neural machine translation (NMT) system based on the encoder-decoder approach with attention mechanism (Bahdanau et al., 2015). This NMT approach encodes an input sentence into a continuous representation by an encoder recurrent neural network (RNN) and produces translation output by a decoder RNN. The decoder RNN, through a
2800016038	Various Approaches to Aspect-based Sentiment Analysis.	2064675550	. We shall also explore why one worked at another did not. Both methods are built upon on the LSTM [11] and Attention [12] concepts. • Attention LSTM-RNN: The ﬁrst model we tried was vanilla LSTM-RNN [10] with an attention layer to learn the weights of context words with respect to the aspect. This approach did not work well as a single attention layer is not suﬃcient to learn the abstract features be
2800016038	Various Approaches to Aspect-based Sentiment Analysis.	2252057809	r Machines (SVM): Support vector machines have shown to be extremely capable of handling aspect-based sentiment analysis in domains such asbut not limited to customerreviewson laptops and restaurants [6]. Unsurprisingly, SVMs performed the best in the classical machine learning methods we tried. We shall see those results later. • Random Forest Classiﬁer: Random forest classiﬁer is a meta-estimator t
2800016038	Various Approaches to Aspect-based Sentiment Analysis.	2465978385	it the train and test dataset, saved the trained model, and loaded it back up for testing. III. RESULTS The dataset used for our experiments is a modiﬁed version of the SemEval 2016: Task 4 challenge [15]. We apply the algorithms on the Tech Reviews and Food Reviews domains. In our experiments, MemNet worked best with an overall accuracy of 0.713 and 0.7866 on tech reviews and food reviews dataset res
2800043509	Argument Harvesting Using Chatbots.	2104126268	g attitudes of women to participation in sport. Keywords. argument harvesting, chatbots, value-based argumentation, behaviour change 1. Introduction Abstract argument graphs, such as proposed by Dung [10], are an important formalism in computational models of argument. However, the issue of acquiring the graphs tends to be omitted. In order to construct graphs using real arguments as opposed to theore
2800043509	Argument Harvesting Using Chatbots.	52290235,117549802,1563666749	in this paper, we are interested in the notion of values and their relevance to argumentationin behaviourchange. None of the papers that apply value-based argumentation frameworks to speciﬁc examples [4,13,12] explain where the values come from or according to what principles they should be chosen. We therefore need to deﬁne our own notion of values for our purpose. In a dialogue when someone posits an arg
2800043509	Argument Harvesting Using Chatbots.	1671158715	recognised that the parties within a debate will have different perspectives on what is important to pursue, according to their subjective aspirations and preferences [5]. In value-based argumentation[7,8] argumentspromotespeciﬁc values whichaccountforthe social interests of debate participants. Values are assigned to an argument when constructing argument graphs. They providean explanationas to why it
2800043509	Argument Harvesting Using Chatbots.	2081580037	stering algorithm that could take advantage of domain speciﬁc knowledge. We describe the algorithm below and the pseudo-codecan be foundin AppendixJ [1]. First, we create a synonym list using WordNet [11]. This list contains lists of all the words in a given corpus that are synonyms of each other. Then the arguments are normalised by deleting stopwords and punctuation, and setting the case to low. We
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2340954483	/reportinghate.eu/contact2017/ http://likestiltnorden2017.regjeringen.no/language/en/ nordic-hate-speech-conference/ online content, and on most social platforms it is heavily moderated. For example, Nobata et al. (2016) report that in their corpus of comments on Yahoo! articles collected between April 2014 and April 2015, the percentage of abusive comments is around 3.4% on Finance articles and 10.7% on News. Since
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2340954483	as been done in the area of automatic detection using a variety of techniques, from lists of prominent keywords (Warner and Hirschberg, 2012) to regression classiﬁers as seen in the previous section (Nobata et al., 2016; Waseem and Hovy, 2016), naive Bayes, decision trees, random forests, and linear SVMs (Davidson et al., 2017), as well as deep learning models with convolutional neural networks (Gamb¨ack and Utpal,
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2127267264	eatures: Zaman et al. (2013) predict the total number of retweets a given amount of time after posting, using a Bayesian model based on features of early retweet times and follower graphs. Similarly, Hong et al. (2011) predict the number of retweets, using binary and multi-class classiﬁers. They use a more varied set of features, and aside from temporal features, they use content, topical and graph features, as wel
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2127267264	eetability either way. 3.3. Popularity prediction Features. We use an large set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015). We divide our features into three groups: Tweet features (metadata about the the tweet itself), user features (metadata a
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2473555522	erformed are ephemeral, which makes replication of results very difﬁcult. In this paper, we focus on the latter two points. We consider a particular hate speech corpus – a Twitter corpus collected by Waseem and Hovy (2016), which has been gaining traction as a resource for training hate speech detection models (Waseem and Hovy, 2016; Gamback and Utpal, 2017; Park¨ and Fung, 2017) – and analyse it critically to better u
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2595653137	es and 10.7% on News. Since the phenomenon is elusive, researchers often use lists of offensive terms to collect datasets with the aim to increase the likelihood of catching instances of hate speech (Davidson et al., 2017; Waseem and Hovy, 2016). This ﬁltering process, however, has the risk of producing corpora with a variety of biases, which may go undetected. 3.Finally, hate speech is present in user-generated conte
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2303217074	being featured on LREC2018’s list of hot topics. However, hate speech research is still in its infancy. In part, this is due to the following challenges: 1.The term hate speech is difﬁcult to deﬁne. Silva et al. (2016) say that “hate speech lies in a complex nexus with freedom of expression, group rights, as well as concepts of dignity, liberty, and equality. For this reason, any objective deﬁnition (i.e., that can
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2473555522	ince the phenomenon is elusive, researchers often use lists of offensive terms to collect datasets with the aim to increase the likelihood of catching instances of hate speech (Davidson et al., 2017; Waseem and Hovy, 2016). This ﬁltering process, however, has the risk of producing corpora with a variety of biases, which may go undetected. 3.Finally, hate speech is present in user-generated content that is not under the
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2473555522	ne, due to the lack of comprehensive documentation. More speciﬁcally, there are two important aspects of the pipeline that present us with this problem: the algorithm and the features. The algorithm. Waseem and Hovy (2016) state that they use a logistic regression classiﬁer for their hate speech prediction task. What is not mentioned is which implementation of the algorithm is used, how the model was ﬁt to the data, wh
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2595653137	ords (Warner and Hirschberg, 2012) to regression classiﬁers as seen in the previous section (Nobata et al., 2016; Waseem and Hovy, 2016), naive Bayes, decision trees, random forests, and linear SVMs (Davidson et al., 2017), as well as deep learning models with convolutional neural networks (Gamb¨ack and Utpal, 2017; Park and Fung, 2017). Our intent in this section is to explore hate speech beyond just detection, using
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2026318959	peech or not, does not impact its retweetability either way. 3.3. Popularity prediction Features. We use an large set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015). We divide our features into three groups: Tweet features (metadata about the the t
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	1996263819	prediction Features. We use an large set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015). We divide our features into three groups: Tweet features (metadata about the the tweet itself), user features (metadata about the author of a tweet) and content
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2026318959	rk relating tweet popularity with hate speech. However, there is a signiﬁcant body of work dealing with tweet popularity modeling and prediction. Many papers explore features that lead to retweeting. Suh et al. (2010) perform an extensive analysis of features that affect retweetability, singling out two groups of features: content and contextual features. Similarly, Zhang et al. (2012) train a model to predict the
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2473555522	rstand its usefulness as a hate speech resource. In particular, we make the following contributions:  We report the outcome of a reproduction experiment, where we attempt to replicate the results by Waseem and Hovy (2016) on hate speech detection using their Twitter corpus.  We use the corpus to study a novel aspect related to hate speech: the popularity of tweets containing hate speech. arXiv:1805.04661v1 [cs.CL] 12
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2054885337	set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015). We divide our features into three groups: Tweet features (metadata about the the tweet itself), user features (metadata about the author of a tweet) and content features (features derived from the c
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2473555522	ss the generality of the results obtained for the hate speech detection and popularity tasks. 2. Replication: hate speech detection results We aim to replicate the results on hate speech detection by Waseem and Hovy (2016) using the hate speech Twitter corpus created by the authors.2 The dataset is a useful resource as it is one of few freely available corpora for hate speech research; it is manually annotated and dist
2800170985	Examining a Hate Speech Corpus for Hate Speech Detection and Popularity Prediction	2171410332	We use an large set of features inspired by related work (Waseem and Hovy, 2016; Sutton et al., 2015; Suh et al., 2010; Zaman et al., 2013; Hong et al., 2011; Zhang et al., 2012; Cheng et al., 2014; Ma et al., 2013; Zhao et al., 2015). We divide our features into three groups: Tweet features (metadata about the the tweet itself), user features (metadata about the author of a tweet) and content features (feature
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1924770834	ation of forward RNN hidden state ! h j and backward RNN hidden state h j, and ! h j can be computed as follows: ! h j = f( ! h j 1;x j) (1) where function f is deﬁned as a Gated Recurrent Unit (GRU) [17]. The decoder is also an RNN that predicts the next word y t given the context vector c t, the hidden state of the decoder s t and the previous predicted word y t 1, which is computed by: p(y tjy &lt;
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2101105183	c Data Consortium (LDC) 15 parallel corpora, such as LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, LDC2004T08, and LDC2005T06. We tune the models with NIST0616 as development data using BLEU metric [51], and use NIST08 Chinese-English parallel corpus as testing data with four references. For the baseline model RNNSearch*, in order to effectively train the model, we limit the maximum sentence length
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1916559533,2157331557	ching of 1st NMT Workshop by Google 8, in addition to the traditional WMT workshops 9. NMT models treat MT task as encoder-decoder work-ﬂow which is much different from the conventional SMT structure [15,30]. The encoder applies in the source language side learning the sentences into vector representations, while the decoder applies in the target language side generating the words from the target side ve
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2133564696	drawbacks in the NMT models e.g. lack of alignment information between source and target side, and less transparency, etc. To address these, attention mechanism was introduced to the decoder ﬁrst by [2] to pay interests to part information of the source sentence selectively, instead of the whole sentence always, when the model is doing translation. This idea is similar like alignment functions in SM
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2251131401,2608068008	e combination of them and traditional words/characters perform in the NMT systems. There are some published works about the investigation of Chinese radicals embedding for other tasks of NLP, such as [55,39] explored the radical usage for word segmentation and text categorization. Some MT researchers explored the word composition knowledge into the systems, especially on the western languages. For instan
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2153653739	e launching of IBM mathematical models proposed in 1990s [7]. Representative SMT works include the word alignment models [50], introducing of Minimum Error Rate Training (MERT) [49], phrase-based SMT [35], hierarchical structure models [13], and large parallel data development, e.g. [29], etc. Meanwhile, many research groups developed their open source tools to advance the MT technology, such as Moses
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2101105183	e of state of the art MT evaluation metrics, which are developed in recent years, to do a more comprehensive evaluation, including hLEPOR [22], CharacTER [60], BEER [56], in addition to BLEU and NIST [51]. The model hLEPOR is a tunable translation evaluation metric yielding higher correlation with human judgments by adding n-gram position difference penalty factor into the traditional F-measures. Char
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2294699749	g distance rate metric. BEER uses permutation trees and character n-grams integrating many features such as paraphrase and syntax. They have shown top performances in recent years’ WMT17 shared tasks [42,41,21,6]. Both CharacTER and BEER metrics achieved the parallel top performance in correlation scores with human judgment on Chinese-to-English MT evaluation in WMT-17 shared tasks [5] . While LEPOR metric se
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1816313093	Han   Fig.2: Radical can not be independent character. 2 Related Work MT models have been developed by utilizing smaller units, i.e. phrase-level to wordlevel, sub-word level and character-level [31,54,16]. However, for Chinese language, sub-character level or radical level is also quite interesting topic since the Chinese radicals carry somehow essential meanings of the Chinese characters that they ar
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2141399712	lignment functions in SMT and what the human translators usually perform when they undertake the translation task. Earlier, attention mechanisms were applied in neural nets for image processing tasks [36,18]. Recently, Attention based models have appeared in most of the NMT projects, such as the the investigation of global attention-based architectures [40] and target information 4 translate.google.com 5
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2152263452	ls proposed in 1990s [7]. Representative SMT works include the word alignment models [50], introducing of Minimum Error Rate Training (MERT) [49], phrase-based SMT [35], hierarchical structure models [13], and large parallel data development, e.g. [29], etc. Meanwhile, many research groups developed their open source tools to advance the MT technology, such as Moses featuring statistical phrase-based
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2133564696	n rules of source and target languages to the machine, to example based MT (EBMT), statistical MT (SMT), Hybrid MT (e.g. the combination of RBMT and SMT) and then recent years’ Neural MT (NMT) models [48,11,33,2]. MT gained much more attention from researchers after the launching of IBM mathematical models proposed in 1990s [7]. Representative SMT works include the word alignment models [50], introducing of M
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2183748887	n source tools to advance the MT technology, such as Moses featuring statistical phrase-based MT [32], Joshua featuring parsing-based translations [38], Phrasal incorporating arbitrary model features [12], CDEC favoring ﬁnite-state and context-free translation Models [19], and NiuTrans featuring syntax-based models [62], etc.; and some advanced information technology (IT) companies also built theirs,
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2100664567	ndle different levels of input from their combinations. For Chinese character decomposition, e.g. the radicals generation, we use the HanziJS open source toolkit 14. On the usage of target vocabulary [26], we choose 30,000 as the volume size. 4 Experiments In this section, we introduce our experiment settings and the evaluation of the designed models. 4.1 Experiments Setting We used 1.25 million paral
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2113104171	ng parsing-based translations [38], Phrasal incorporating arbitrary model features [12], CDEC favoring ﬁnite-state and context-free translation Models [19], and NiuTrans featuring syntax-based models [62], etc.; and some advanced information technology (IT) companies also built theirs, such as the machine translators by Google 4, Baidu 5, Yandex 6, and Microsoft Bing 7, etc. Thanks to the work of word
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1489525520	ply Chinese Radicals Into Neural Machine Translation 7 ation to get the character representation and radical representation of the word, i.e. z j and r j can be computed as follows: z j = Xm k=1 z jk (10) r j = Xn k=1 r jk (11) Each word can be decomposed into different numbers of character and radical, and, by addition operations, we can generate a ﬁxed length representation. In principle? our model
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2513263213,2553522418	ry to address the unseen words or outof-vocabulary (OOV) words issue and improve the adequacy level by exploring the Chinese radicals into NMT. There are some other advanced topics such as multimodal [20,25,8], multilingual [9,27] and syntactic [3,1,37] NMT, but not in this work’s scope. For Chinese radical knowledge, let’s see two examples about their construction in the corresponding characters. This Fig
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2133564696	section introduces the baseline attention-based NMT model and our model. 3.1 Attention-based NMT Typically, as mentioned before, neural machine translation (NMT) builds on an encoderdecoder framework [2,57] based on recurrent neural networks (RNN). In this paper, we take the NMT architecture proposed by [2]. In NMT system, the encoder apples a Apply Chinese Radicals Into Neural Machine Translation 5 bid
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1753482797,2172140247	from the target side vectors. Recurrent Neural Networks (RNN) models are usually used for both encoder and decoder, though there are some researchers employing convolutions neural networks (CNN) like [14,28]. The hidden layers in the neural nets are designed to learn and transfer the information [47]. There were some drawbacks in the NMT models e.g. lack of alignment information between source and target
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	1753482797,2133564696,2172140247	theirs, such as the machine translators by Google 4, Baidu 5, Yandex 6, and Microsoft Bing 7, etc. Thanks to the work of word to vector embedding from [45], the NMT was available to be introduced in [28,14,2] by utilizing both deep learning (DL) and word representation (WR) approaches. Earlier, NMT structure [46] did not work out which may be due to the limitations of computational power of machines and t
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2133564696	tutorial 11 as our baseline and call it RNNSearch*12. 10 github.com/nyu-dl/dl4mt-tutorial/tree/master/ session2 11 github.com/nyu-dl/dl4mt-tutorial 12 To distinguish it from RNNSearch as in the paper [2] 6 Kuang and Han D Í ? ç D 5 D 6 S 5 S 6 S Í V 5 V 6 V Í N 5 N 6 N Í encoder D Í O ç?5 U ç?5 O ç U ç Fig.3: Architecture of NMT with multi-embedding. 3.2 Our model Traditional NMT model usually us
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2609011624	ulary (OOV) words issue and improve the adequacy level by exploring the Chinese radicals into NMT. There are some other advanced topics such as multimodal [20,25,8], multilingual [9,27] and syntactic [3,1,37] NMT, but not in this work’s scope. For Chinese radical knowledge, let’s see two examples about their construction in the corresponding characters. This Figure 1 shows three Chinese characters (forest
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2550821151	words or outof-vocabulary (OOV) words issue and improve the adequacy level by exploring the Chinese radicals into NMT. There are some other advanced topics such as multimodal [20,25,8], multilingual [9,27] and syntactic [3,1,37] NMT, but not in this work’s scope. For Chinese radical knowledge, let’s see two examples about their construction in the corresponding characters. This Figure 1 shows three Chi
2800936852	Apply Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level.	2172140247,2511945356	r ﬂuent output, however, the adequacy is lower sometimes compared with the conventional SMT, e.g. some meaning from the source sentences will be lost in the translation side when the sentence is long [58,59,34,47,14]. One kind of reason of this phenomenon could be due to the unseen words problem, except for the un-clear learning procedure of the neural nets. With this assumption, we try to address the unseen word
2800958914	Zero-Shot Dialog Generation with Cross-Domain Latent Actions	2101105183	sed in (Zhao et al., 2017) that employ four metrics to quantify the performance of a task-oriented dialog model. BLEU is the corpus-level BLEU-4 between the generated response and the reference ones (Papineni et al., 2002). Entity F 1 checks if a generated response contains the correct entities (slots) in the reference response. Act F 1 measures whether the generated responses reﬂect the dialog acts in the reference re
2801213160	Cross-lingual Candidate Search for Biomedical Concept Normalization.	2133564696	es are processed by a bidirectional recurrent neural network, in particular a bidirectional GRU (Chunget al., 2014), to produce ﬁnal encoder states. A 2-layer recurrent neural network with attention (Bahdanau et al., 2015) on the encoder states subsequently produces the translation character by character. For more in-depth, technical details we refer the reader to (Lee et al., 2016). The model is trained on mini-batche
2801454967	Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach	2605287558	f-the-art systems, especially in content preservation. 2 Related Work Style transfer in computer vision has been studied (Johnson et al., 2016; Gatys et al., 2016; Liao et al., 2017; Li et al., 2017; Zhu et al., 2017). The main idea is to learn the mapping between two image domains by capturing shared representations or correspondences of higher-level structures. There have been some studies on unpaired language s
2801760385	Polite Dialogue Generation Without Parallel Data	1601924930,2311783643	, and Spearman’s rank-order correlation is 0:9276 (p = 0:0077). Next, for BLEU scores, although these scores (as percentages) are very low (consistent with the observation in Ritter et al. (2011) and Li et al. (2016b)), their relative system-ranking still roughly agrees with that of human judgments — we found reasonably high correlation between human Dialogue Quality and BLEU (based on the six scores in Table 3
2801760385	Polite Dialogue Generation Without Parallel Data	194577561	); we use Xavier initializer (Glorot and Bengio, 2010) for out-ofvocabulary words. Pretraining Following Serban et al. (2016), we pretrained the Seq2seq base model for 4epochs with Q-A SubTle corpus (Ameixa et al., 2014), which contains around 5:5M movie subtitle Q&amp;A pairs. Implementation Details We used 300-dim embeddings, the AdamOptimizer (Kingma and Ba, 2015) with a learning rate of 0:001, and a dropout rate
2801760385	Polite Dialogue Generation Without Parallel Data	2176263492	;:::;y s ngbe the sampled response, then the reinforce part of the loss is: L RL = (R R b) Xn t=1 logp(ys tjy s 1;:::;y s t ;x) (4) where R b is a baseline that helps reduce variance during training (Ranzato et al., 2016). Note that we can invert the classiﬁer scores or reward (by ﬂipping the ﬁrst minus sign in Eq. 4), if we want to encourage rudeness as the style, instead of politeness. This also shows that an advant
2801760385	Polite Dialogue Generation Without Parallel Data	2157163421	17. The two retrieval baselines and the continuous version arXiv:1805.03162v1 [cs.CL] 8 May 2018 alogue responses, using data from separate style and dialogue domains: the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al., 2013) with Wikipedia and StackExchange requests, and the MovieTriples Dialogue Corpus (Serban et al., 2016) with IMSDB movie scripts, respectively. Each of our three models is based on a state-of-the-art p
2801760385	Polite Dialogue Generation Without Parallel Data	2592480533	accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et a
2801760385	Polite Dialogue Generation Without Parallel Data	2064675550	accuracy. They also visualized what features the CNN model was learning and discovered some new features along the way. Our classiﬁer mainly extends their work by adding a bi-directional LSTM layer (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) before the CNN layer to capture long-distance relationships in the sentence, which leads to higher cross-domain performance. A related early work in personality-based dia
2801760385	Polite Dialogue Generation Without Parallel Data	889023230	ass equally consisting of 1,089 requests for the Wikipedia domain and 1,651 requests for the Stack Exchange domain. For the content (dialogue) domain, we use the popular MovieTriples dialogue corpus (Serban et al., 2016), which contains 245K conversations extracted from IMSDB movie scripts in X-Y-X triplet-utterance format, where X and Y correspond to two movie characters (and the model’s task is to generate the last
2801760385	Polite Dialogue Generation Without Parallel Data	2284660317	d to the style of the target-speaker. This way of incorporating MTL into Seq2seq learning was ﬁrst investigated by Dong et al. (2015) and Luong et al. (2016) to achieve multilingual NMT. In addition, Sennrich et al. (2016b) also employed MTL to improve NMT models with monolingual (non-parallel) data. These approaches are related to our Fusion model, because we use our classiﬁer to obtain noisy polite target sequences
2801760385	Polite Dialogue Generation Without Parallel Data	2251743902	decoder parameters of these two models so that the generated responses can be adapted to the style of the target-speaker. This way of incorporating MTL into Seq2seq learning was ﬁrst investigated by Dong et al. (2015) and Luong et al. (2016) to achieve multilingual NMT. In addition, Sennrich et al. (2016b) also employed MTL to improve NMT models with monolingual (non-parallel) data. These approaches are related to
2801760385	Polite Dialogue Generation Without Parallel Data	2605287558	el data (by using a classiﬁer with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et
2801760385	Polite Dialogue Generation Without Parallel Data	2605246398	et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training s
2801760385	Polite Dialogue Generation Without Parallel Data	2735574368	et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honoriﬁcs). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated their text with meta-data and keywords/POS tags based heuristics, while Ghosh et al. (2017) also adopted keyword spotting based on a dictionary of emotional words. The basic ideas of their
2801760385	Polite Dialogue Generation Without Parallel Data	1601924930,2311783643	at in the last example, while LFT and Polite-RL seem to provide a relevant compliment, they are actually complimenting the wrong person. This kind of issue motivates us toward creating persona-based (Li et al., 2016c) politeness models for future work. 7.3 Visualization of Polite-RL Reward Using derivative saliency (Simonyan et al., 2013; Li et al., 2016a; Aubakirova and Bansal, 2016), we also visualize how much
2801760385	Polite Dialogue Generation Without Parallel Data	2131774270	at features the CNN model was learning and discovered some new features along the way. Our classiﬁer mainly extends their work by adding a bi-directional LSTM layer (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) before the CNN layer to capture long-distance relationships in the sentence, which leads to higher cross-domain performance. A related early work in personality-based dialogue is Mairesse and Walker
2801760385	Polite Dialogue Generation Without Parallel Data	889023230	g data from separate style and dialogue domains: the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al., 2013) with Wikipedia and StackExchange requests, and the MovieTriples Dialogue Corpus (Serban et al., 2016) with IMSDB movie scripts, respectively. Each of our three models is based on a state-of-the-art politeness classiﬁer and a sequence-to-sequence dialogue model. The ﬁrst model (Fusion) employs a late
2801760385	Polite Dialogue Generation Without Parallel Data	836999996	with the given label. We will explore this extension in future work. mance: the ﬁrst with oracle-level ﬂuency, the second with additional oracle-level politeness. Classiﬁer-based Retrieval Following Lowe et al. (2015), for a [X;Y;X 2] triple, our retrieval model treats the context (X 1;Y) and each response (X 2) as two documents and convert them to their TF-IDF based vectors (Ramos, 2003) to check for similarity.
2801760385	Polite Dialogue Generation Without Parallel Data	2284660317	introvert/extrovert personality language based on templated content and sentence planning (via personality dimensions such as hedges, tag questions, negations, subject implicitness, etc.). Relatedly, Sennrich et al. (2016a) use an English to German translation task to present a model that can generate target sequences that are either formal or informal, specifically based on honoriﬁcs-related verbs and pronouns. Our t
2801760385	Polite Dialogue Generation Without Parallel Data	2250616809	iple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task
2801760385	Polite Dialogue Generation Without Parallel Data	2259472270	and only keeping those with politeness scores great than a certain threshold (set to 0:8in our experiments, as will be discussed in Section 4.5). The polite-LM model is a two-layer LSTM-RNN based on Jozefowicz et al. (2016). During inference time, we used the language &lt;label&gt; S 1S 2 S 3 &lt;start&gt; G 1 G 2 G 3 T 2 3 &lt;end&gt; Input Generated Response Politeness Classifier Target politeness score Figure 3: Labe
2801760385	Polite Dialogue Generation Without Parallel Data	889023230	except with more layers (similar to Shao et al. (2017)). Our base dialogue model achieves perplexity and word error rate results on par with those reported for the popular hierarchical HRED models in Serban et al. (2016), thus serving as a good base model to incorporate style into. Details will be discussed in Section 6. 4.2 Fusion Model Inspired by the ‘late fusion’ approach in Venugopalan et al. (2016), our Fusion
2801760385	Polite Dialogue Generation Without Parallel Data	2617566453	le transfer models that rely on the latent representation of text and use variational auto-encoders or cross-alignment to disentangle the representation of content and style in text (Hu et al., 2017; Shen et al., 2017; Zhao et al., 2017; Fu et al., 2018). During inference time, the latent style representation is combined with new content to generate stylized, content-preserving text. Although both fall into the ca
2801760385	Polite Dialogue Generation Without Parallel Data	2598581049	lel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are
2801760385	Polite Dialogue Generation Without Parallel Data	2172589779	these two models so that the generated responses can be adapted to the style of the target-speaker. This way of incorporating MTL into Seq2seq learning was ﬁrst investigated by Dong et al. (2015) and Luong et al. (2016) to achieve multilingual NMT. In addition, Sennrich et al. (2016b) also employed MTL to improve NMT models with monolingual (non-parallel) data. These approaches are related to our Fusion model, becau
2801760385	Polite Dialogue Generation Without Parallel Data	1601924930,2311783643	n this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not in order to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rath
2801760385	Polite Dialogue Generation Without Parallel Data	889023230	ns such as MovieTriples. 6.2 Base Dialogue Model Results Next, in Table 2, we show that our starting point, base dialogue model is comparable in quality to a popular, representative previous model of Serban et al. (2016), trained on the same corpora with similar model architectures. We use their Perplexity (PPL) and Word Error Rate (WER) metrics. In order to have a meaningful perplexity (i.e., the probability of rege
2801760385	Polite Dialogue Generation Without Parallel Data	2101105183	ns, we only use automatic evaluation metrics as complementary and trend-veriﬁcation information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not in order to draw any
2801760385	Polite Dialogue Generation Without Parallel Data	2158899491	on-linear function and bis a bias term. Every feature map c 2Rn u+1 is applied to each window, so that c = [c 1;:::;c n u+1]. The output of the convolutional layer is then fed to a max-pooling layer (Collobert et al., 2011) which gives C = maxfcgfor the ﬁlter. Filters of various sizes are used to obtain multiple features. The result is then passed to a fully-connected softmax layer that outputs probabilities over two la
2801760385	Polite Dialogue Generation Without Parallel Data	2141766660	order to calculate inter-rater agreement, for which we employ Cohens Kappa (Cohen, 1968), a score that measures the level of inter-rater agreement between two annotators on a classiﬁcation problem (Artstein and Poesio, 2008). For both dialogue quality and 9We opted for dialogue quality rather than several separated, ﬁne-grained metrics such as relevance, speciﬁcity, informativeness because Lowe et al. (2017) found that l
2801760385	Polite Dialogue Generation Without Parallel Data	2284660317	outputs are probabilities ranging from 0:0to 1:0. This allows us to interpret the outputs as continuous politeness scores. 4Note that the position of the label did not affect the results much (e.g., Sennrich et al. (2016a) appended the label at the end of the input sequence). Moreover, our models use a bidirectional encoder, which does not distinguish between the beginning and end of the source sequence. 4.4 Polite R
2801760385	Polite Dialogue Generation Without Parallel Data	2140054881	perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not in order to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue q
2801760385	Polite Dialogue Generation Without Parallel Data	10957333	to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not in order to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human
2801760385	Polite Dialogue Generation Without Parallel Data	2284660317	produce polite, neutral and rude responses depending on the prepended label, similar to recent multilabel, multi-space, and zero-shot machine translation work using language identity or style labels (Sennrich et al., 2016a; Johnson et al., 2017; Ghosh et al., 2017). Intuitively, this prepended label serves as the prior for the intended style of the generated response sequence, while the source utterance serves as the
2801760385	Polite Dialogue Generation Without Parallel Data	2605246398	that rely on the latent representation of text and use variational auto-encoders or cross-alignment to disentangle the representation of content and style in text (Hu et al., 2017; Shen et al., 2017; Zhao et al., 2017; Fu et al., 2018). During inference time, the latent style representation is combined with new content to generate stylized, content-preserving text. Although both fall into the category of style tra
2801760385	Polite Dialogue Generation Without Parallel Data	2560313346	reproducibility details and more analysis examples in a post-publication supplement on our webpage. 12https://code.google.com/archive/p/ word2vec/ 13We also tried using a self-critical baseline as in Rennie et al. (2017), but found that our way of setting the constant-based baseline led to better responses. We speculate that this is because a self-critical approach tries to make an utterance as polite as possible, wh
2801760385	Polite Dialogue Generation Without Parallel Data	2157163421	s probabilities over two labels, namely Polite and Rude. Our classiﬁcation model achieves comparable in-domain accuracy and improved crossdomain accuracy over the state-of-the-art results reported in Danescu-Niculescu-Mizil et al. (2013) and Aubakirova and Bansal (2016). We will discuss these results in detail in Section 6. 4 Polite-Style Dialogue Models In this section, we ﬁrst describe our base dialogue model, i.e., the core (backb
2801760385	Polite Dialogue Generation Without Parallel Data	2732863878	s can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been propo
2801760385	Polite Dialogue Generation Without Parallel Data	1522301498	seq base model for 4epochs with Q-A SubTle corpus (Ameixa et al., 2014), which contains around 5:5M movie subtitle Q&amp;A pairs. Implementation Details We used 300-dim embeddings, the AdamOptimizer (Kingma and Ba, 2015) with a learning rate of 0:001, and a dropout rate of 0:2. All models were trained with a minibatch of size 96. The classiﬁer was trained for 3 epochs, and the three proposed stylistic models were eac
2801760385	Polite Dialogue Generation Without Parallel Data	2580723344	a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates
2801760385	Polite Dialogue Generation Without Parallel Data	2284660317	style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related ke
2801760385	Polite Dialogue Generation Without Parallel Data	10957333	t. signiﬁcance p= 0:0422), and Spearman’s rank-order correlation is 0:9276 (p = 0:0077). Next, for BLEU scores, although these scores (as percentages) are very low (consistent with the observation in Ritter et al. (2011) and Li et al. (2016b)), their relative system-ranking still roughly agrees with that of human judgments — we found reasonably high correlation between human Dialogue Quality and BLEU (based on the si
2801760385	Polite Dialogue Generation Without Parallel Data	2157163421	ting generated responses to different politeness styles and optimizing two subtasks’ (namely response and politeness generation) loss functions (related to a multi-task setup). 2.3 Politeness Studies Danescu-Niculescu-Mizil et al. (2013) created the Stanford Politeness Corpus and trained an SVM classiﬁer using a list of useful linguistic features based on strategies from Brown and Levinson’s theory of politeness (Brown and Levinson,
2801760385	Polite Dialogue Generation Without Parallel Data	2157163421	ur politeness classiﬁer (Sec. 3) and base dialogue model (Sec. 4.1) results, and then focus on the stylisticdialogue results (retrieval and generative). 6.1 Politeness Classiﬁcation Results Following Danescu-Niculescu-Mizil et al. (2013), we use accuracy (i.e., percentage of correctly labeled messages for binary polite/rude labels) to evaluate our politeness classiﬁer’s generalization ability. Speciﬁcally, we used data from the train
2801760385	Polite Dialogue Generation Without Parallel Data	889023230	with word2vec trained on Google News dataset (about 100 billion words)12 (Mikolov et al., 2013); we use Xavier initializer (Glorot and Bengio, 2010) for out-ofvocabulary words. Pretraining Following Serban et al. (2016), we pretrained the Seq2seq base model for 4epochs with Q-A SubTle corpus (Ameixa et al., 2014), which contains around 5:5M movie subtitle Q&amp;A pairs. Implementation Details We used 300-dim embeddi
2801760385	Polite Dialogue Generation Without Parallel Data	2133564696	yer LSTM-RNN decoder to generate the response. Additive attention from the output of the encoder is applied to the last layer of the decoder. This architecture is almost identical to that proposed by Bahdanau et al. (2015), except with more layers (similar to Shao et al. (2017)). Our base dialogue model achieves perplexity and word error rate results on par with those reported for the popular hierarchical HRED models i
2801930304	Construction of the Literature Graph in Semantic Scholar	2120354757	005 (e.g., Lample et al., arXiv:1805.02262v1 [cs.CL] 6 May 2018 2016), and assume that entity types in the test set match those labeled in the training set (including work on domain adaptation, e.g., Daumé, 2007). These assumptions, while useful for developing and benchmarking new methods, are unrealistic for many domains and applications. The paper also serves as an overview of the approach we adopt at www.s
2801930304	Construction of the Literature Graph in Semantic Scholar	1713614699	and g(e) and the intersection-based scores into an ane transformation followed by a sigmoid nonlinearity to compute the ﬁnal score for the pair (m, e). Results. We use the Bag of Concepts F1 metric (Ling et al., 2015) for comparison. Table 4 compares the performance of the most-frequent-entity baseline and our neural model described above. 5 Other Research Problems In the previous sections, we discussed how we con
2801930304	Construction of the Literature Graph in Semantic Scholar	2158899491	k ;g k ]; h! k = LSTM(g k;h ! k1 );h k = [h ! k ;g k ] where W is a weight matrix, g k and h k are deﬁned similarly to g! k and h ! k but process token sequences in the opposite direction. Following Collobert et al. (2011), we feed the output of the second layer h k into a dense layer to predict unnormalized label weights for each token and learn label bigram feature weights (often described as a conditional random ﬁel
2801930304	Construction of the Literature Graph in Semantic Scholar	2610748790	a linear project layer. While training the LM parameters, lm! k is used to predict t k+1 and lm k is used to predict t k1. We ﬁx the LM parameters during training of the entity extraction model. See Peters et al. (2017) and Ammar et al. (2017) for more details. Given the x k and lm k embeddings for each token k 2f1;:::;Ng, we use a two-layer bidirectional LSTM to encode the sequence with x k and lm k feeding into th
2801930304	Construction of the Literature Graph in Semantic Scholar	2145870108	LM 49.9 With LM 54.1 Avg. of 15 models with LM 55.2 Table 3: Results of the entity extraction model on the development set of SemEval-2017 task 10. BC5CDR (Li et al., 2016) and the CHEMDNER datasets (Krallinger et al., 2015) to extract key entity mentions in the biomedical domain such as diseases, drugs and chemical compounds. The third instance is trained on mention labels induced from Wikipedia articles in the computer
2801930304	Construction of the Literature Graph in Semantic Scholar	2398606196	n or an entity name in the KB to associated entity IDs, along with the frequency this token is associated with that entity. This is similar to the index used in previous entity linking systems (e.g., Bhagavatula et al., 2015) to estimate the probability that a given mention refers to an entity. At train and test time, we use this index to ﬁnd candidate entities for a given mention by looking up the tokens in the mention.
2801930304	Construction of the Literature Graph in Semantic Scholar	1496319844	people often share the same name. Moreover, dierent venues and sources use dierent conventions in reporting the author names, e.g., “ﬁrst initial, last name” vs. “last name, ﬁrst name”. Inspired by Culotta et al. (2007), we train a supervised binary classiﬁer for merging pairs of author instances and use it to incrementally create author clusters. We only consider merging two author instances if they have the same l
2801930304	Construction of the Literature Graph in Semantic Scholar	2107598941	same reasons, we also need to augment the relations imported from the KB with relations extracted from text. Our approach to address both entity and relation coverage is based on distant supervision (Mintz et al., 2009). In short, we train two models for identifying entity deﬁnitions and relations expressed in natural language in scientiﬁc documents, and automatically generate labeled data for training these models
2801930304	Construction of the Literature Graph in Semantic Scholar	2611818442	s the eects of Ranibizumab on the Retina? In this paper, we focus on the problem of extracting structured data from scientiﬁc documents, which can later be used in natural language interfaces (e.g., Iyer et al., 2017) or to improve ranking of results in academic search (e.g., Xiong et al., Figure 1: Part of the literature graph. 2017). We describe methods used in a scalable deployed production system for extractin
2801958725	An Attention-based BI-GRU-CapsNet Model for Hypernymy Detection between Compound Entities	6908809	.e. entity capsules and classication capsules) with two iterations (i.e., r= 2), and all the routing logits are initialized to zero. The model is trained by an adaptive learning rate method AdaDelta [29] to minimize the margin loss and the batch size is 128. Both the selection of margin loss function and the number of iterations r= 2 are determined based on the experimental results in Section 5.1 and
2801958725	An Attention-based BI-GRU-CapsNet Model for Hypernymy Detection between Compound Entities	1924770834	. For the recurrent layer, we employ gated recurrent units (GRUs) [23] as the basic recurrent units, as they has similar functionalities but fewer parameters compared to long short-term memory (LSTM) [24]. Afterwards, an attention mechanism is utilized to obtain the feature vector (also considered as a capsule) of each entity. Finally, Capsule network (CapsNet) is used to decide whether the two compou
2801958725	An Attention-based BI-GRU-CapsNet Model for Hypernymy Detection between Compound Entities	2133564696	ss as our loss function in our model. 16 5.3 Eectiveness of the attention mechanism Attention mechanism is a very eective method for solving NLP tasks. It was originally proposed by Bahdanau et al. [30] to deal with neural machine translation, where a softmax function is employed to calculate the attention weights. This section is devoted to investigating the eectiveness of our attention mechanism.
2802105481	Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems.	2611755161	DeepMoji (Felbo et al.,2017), Skip thoughts (Kiros et al.,2015)). The lexicon features were often derived from the NRC emotion and sentiment lexicons (Mohammad and Turney,2013;Kiritchenko et al.,2014;Mohammad, 2018), AFINN (Nielsen,2011), and Bing Liu Lexicon (Hu and Liu,2004). 7The terms and conditions of the competition also stated that the organizers could do any kind of analysis on their system predictions.
2802105481	Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems.	1648303880	lukbasi et al.,2016;Caliskan et al.,2017). In general, any predictive model built on historical data may inadvertently inherit human biases based on gender, ethnicity, race, or religion (Sweeney,2013;Datta et al., 2015). Discrimination-aware data mining focuses on measuring discrimination in data as well as on evaluating performance of discrimination-aware predictive models (Zliobaite,2015;Pedreshi et al., 2008;Haji
2802105481	Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems.	2026019770	y,2013;Datta et al., 2015). Discrimination-aware data mining focuses on measuring discrimination in data as well as on evaluating performance of discrimination-aware predictive models (Zliobaite,2015;Pedreshi et al., 2008;Hajian and Domingo-Ferrer,2013;Goh et al.,2016). In NLP, the attention so far has been primarily on word embeddings—a popular and powerful framework to represent words as low-dimensional dense vector
2802327808	Divide and Generate: Neural Generation of Complex Sentences	2010835028	We used CaboCha (Kudo and Matsumoto, 2002) for dependency analysis, and MeCab (Kudo et al.
2802327808	Divide and Generate: Neural Generation of Complex Sentences	165283731	We used CaboCha (Kudo and Matsumoto, 2002) for dependency analysis, and MeCab (Kudo et al., 2004) + IPAdic 2 for morphological analysis.
2802327808	Divide and Generate: Neural Generation of Complex Sentences	2133564696	As our generator model for the complex sentences, we use the Encoder-Decoder with Attention (Bahdanau et al., 2015).
2802327808	Divide and Generate: Neural Generation of Complex Sentences	2198844692	There are also other studies that delve into the topic of complex sentences (Derr and McKeown, 1984; Sato, 1980).
2802327808	Divide and Generate: Neural Generation of Complex Sentences	295828404	A typical chat dialogue system selects an appropriate response from a database as an output for a user’s utterance input to it (Ji et al., 2014).
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	1481820510	. Recent models of learning from noisy instances of perceptually grounded reference, for example, often leverage associationist or connectionist frameworks (Fazly et al., 2010; Chrupała et al., 2015; Yu and Ballard, 2004), while studies of abstract rule learning about syntax and object category structure often rely on hierarchical Bayesian inference (Kemp et al., 2007; Perfors et al., 2011). A productive line of work
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	2189089430	d meaning inference process for novel words within the CCG framework. It leverages the existing knowledge represented in the CCG lexicon to incrementally track beliefs about the abstract relation5See Artzi and Zettlemoyer (2013) for algorithm details. Figure 4: The probabilistic model used to predict the semantic properties of a word given its syntactic properties and surface form. Shaded circles are observed variables and e
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	2111742432	ends on the particular lexical entries it draws on for each word. These lexical scores are combined in a simple log-linear model, which can be efﬁciently and exactly computed via dynamic programming (Zettlemoyer and Collins, 2007): P(L u;T u ju)µexp 0 @ å (wj;sj;mj;qj)2Tu q j 1 A (1) Validation-based lexical induction and learning While our CCG model is built to perform inferences over possible logical forms L u, its only sour
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	2189089430	nction V(L u) which returns the referent(s) denoted by L u in the scene in which the sentence uwas uttered.4 Using this validation function, we implement a distantly supervised learning process after Artzi and Zettlemoyer (2013), described below. For each example utterance u and referent R, we ﬁrst update the lexicon so that the sentence can be successfully parsed (see section “Lexical induction”). Using this augmented lexic
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	2189089430	retations of the novel word(s) in question and permanently add them to the lexicon. Learning The parameters of the lexicon L are optimized with a stochastic online learning algorithm, as described in Artzi and Zettlemoyer (2013). We perform a perceptron update on the lexicon weights in order to maximally separate the computed scores of parses which yield the correct referent R from those which yield the incorrect referent.5
2802817436	Word learning and the acquisition of syntactic-semantic overhypotheses.	1983685815	speaking children ranging from age 1;9 to 3;11 (Bohannon III and Marquis, 1977; Clark, 1978; Demetras, 1986; Morisset et al., 1990; Nelson, 1989), with syntactic annotations automatically produced by Sagae et al. (2007). For each corpus, we collected all instances of simple prenominal modiﬁcation: phrases with the part-ofspeech pattern &lt;adjective&gt;&lt;noun&gt;, in which the adjective is in a direct dependency r
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2134750673,2148614760	ial and big data set for it. Most previous works typically used a public set consists of legitimate or \ham&quot; emails1 and another public set of phishing emails2 for their classication evaluation [11, 7, 6, 15, 38]. Other works used private but small data sets[8, 4]. In addition, the ratio between phishing and legitimate emails in these data sets was typically balanced. This is not the case in the real-world sc
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2133564696	ing has been shown to achieve state-of-the-art performance for many natural language processing tasks, including text categorization [12, 19], information extraction [28, 27, 29], machine translation [5], among others, we expect that it would also help to build eective systems for phishing email detection. We present a new deep learning model to solve the problem of email phishing prediction using h
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2002964284	a public set consists of legitimate or \ham&quot; emails1 and another public set of phishing emails2 for their classication evaluation [11, 7, 6, 15, 38]. Other works used private but small data sets[8, 4]. In addition, the ratio between phishing and legitimate emails in these data sets was typically balanced. This is not the case in the real-world scenario where the number of legitimate emails is much
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2131906261	remains the most common attack used by cybercriminals [2] due to its eectiveness. Phishing attacks exploit users’ inability to distinguish between legitimate information from fake ones sent to them [9, 33, 34, 32]. In an email phishing campaign, attackers send emails appearing to be from wellknown enterprises or organizations directly to their victims or by spoofed emails [35]. These emails try to lure victims
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2133564696	s are passed to LSTM models in the sentence level to generate a representation vector for the entire email. The outputs of the LSTM models in the two levels are combined using the attention mechanism [5] that assigns contribution weights to the words and sentences in the emails. A header network is also integrated to model the headers of the emails if they are available. In addition, we propose a nov
2802914762	A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing.	2002964284,2134750673	t makes is huge. In the US alone, the estimated cost of phishing emails to business is half a billion dollars per year [3]. Numerous methods have been proposed to automatically detect phishing emails [7, 11, 4, 14]. Chandrasekaran et. al proposed to use structural properties of emails and Support Vector Machines (SVM) to classify phishing emails [8]. In [4], Abu-Nimeh et. al evaluated six machine learning class
2803113447	On Learning Associations of Faces and Voices.	2544224704	e correlation between audio and visual signals in applications such as: improving sound classication [1] by combining images and their concurrent sound signals in videos; scene and place recognition [2] by transferring knowledge from visual to auditory information; vision-sound cross modal retrieval [28,29,37]; and sound source localization in visual scenes [31]. These works focus on the fact that v
2803113447	On Learning Associations of Faces and Voices.	1834627138	hich presumably is one of the residual signals beyond demographic attributes and aects the performance, among many possible factors. We trained SVM on our representation to predict CelebA attributes [21] and show in Table5several attributes suggesting correlation. 12 C. Kim et al. lower higher softer louder lower higher softer louder (a) Face; pitch (b) Face; loudness (c) Voice; pitch (d) Voice; loud
2803113447	On Learning Associations of Faces and Voices.	2544224704	on. 4.1 Network Architecture The overall architecture is based on the triplet network [12], which is widely used for metric learning. As subnetworks for two modalities, we use VGG16 [33] and SoundNet [2], which have shown sucient model capacities while allowing for stable training in a variety of applications. In particular, SoundNet was devised in the context of transfer learning between visual and
2803113447	On Learning Associations of Faces and Voices.	2593768305	tion is not dense enough in the regions where most non-celebrity faces are distributed. This could be alleviated by additional ne-tuning on the new dataset or by domain adaptation (e.g., Tzeng et al. [41]), which is left to future work. Accent and regional cues. It is worth noting that cultural or regional cues, such as accent or the subject’s appearance, can play a role in the face-voice matching tas
2803115315	A web-scale system for scientific knowledge exploration	2100071287	ncept hierarchy construction approaches extract concepts from unstructured documents, select representative terms to denote a concept, and build the hierarchy on top of them (Sanderson and Croft,1999;Liu et al., 2012). The concepts extracted this way not only lack authoritative deﬁnition, but also arXiv:1805.12216v1 [cs.CL] 30 May 2018 Concept Concept Hierarchy discovery tagging building Main scalability / trustwo
2803125506	VISUAL REFERRING EXPRESSION RECOGNITION: WHAT DO OUR SYSTEMS ACTUALLY LEARN?	1933349210	al scene with natural language (Chen et al.,2015), describing or localizing speciﬁc objects in a scene (Kazemzadeh et al.,2014;Mao et al.,2016), answering natural language questions about the scenes (Antol et al., 2015), and performing visually grounded dialogue (Das et al.,2016). Here, we focus on referring expression recognition (RER) – the task of identifying the object in an image that is referred to by a natura
2803125506	VISUAL REFERRING EXPRESSION RECOGNITION: WHAT DO OUR SYSTEMS ACTUALLY LEARN?	2505639562	d categories (Section3.3), and the ﬁnal one analyzing potential biases in the dataset (Section3.4). 3.1 Analysis Methodology To perform our analysis, we take two state-of-theart systems CNN+LSTM-MIL (Nagaraja et al., 2016) and CMN (Hu et al.,2017) and train them from scratch with perturbed referring expressions. We note that the perturbation experiments explained in next subsections are performed on all train and test
2803125506	VISUAL REFERRING EXPRESSION RECOGNITION: WHAT DO OUR SYSTEMS ACTUALLY LEARN?	1861492603	nnington et al.,2014) and ﬁnetuned during training. We extracted features for bounding boxes using the fc7 layer output of Faster-RCNN VGG-16 network (Ren et al., 2015) pre-trained on MSCOCO dataset (Lin et al., 2014). Hyperparameters such as hidden layer size of LSTM networks were picked based on the best validation score. For perturbation experiments, we did not perform any grid search for hyperparameters. We us
2803125506	VISUAL REFERRING EXPRESSION RECOGNITION: WHAT DO OUR SYSTEMS ACTUALLY LEARN?	2144960104	user interfaces (Chai et al.,2004), human-robot interaction (Fang et al.,2012;Chai et al.,2014;Williams et al.,2016), and situated dialogue (Kennington and Schlangen, 2017).Kazemzadeh et al.(2014) andMao et al. (2016) introduce two benchmark datasets for referring expression recognition. Several models that leverage linguistic structure have been proposed. Nagaraja et al.(2016) propose a model where the target and
2803125506	VISUAL REFERRING EXPRESSION RECOGNITION: WHAT DO OUR SYSTEMS ACTUALLY LEARN?	639708223	Word embeddings were initialized using GloVe (Pennington et al.,2014) and ﬁnetuned during training. We extracted features for bounding boxes using the fc7 layer output of Faster-RCNN VGG-16 network (Ren et al., 2015) pre-trained on MSCOCO dataset (Lin et al., 2014). Hyperparameters such as hidden layer size of LSTM networks were picked based on the best validation score. For perturbation experiments, we did not p
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2603266952	. (2017), which does not. Within the ML and CV communities, recent research on conversational agents combined with Deep Learning techniques has yielded interesting results on visually grounded tasks (Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017). In this line of research, the focus is mostly on improving model performance by investigating new machine learning paradigms (like reinforcement l
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2603266952	al., 2016; Li et al., 2016a; Li et al., 2016b) — augmented with visual features. This community has mostly focused on model learning paradigms. Initial models, proposed by de Vries et al. (2017) and Das et al. (2017a), use supervised learning (SL): the Questioner and the Answerer are trained to generate utterances (by word sampling) that are similar to the human gold standard. To account for the intuition that d
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2130942839	al scene by asking yes-no questions (more details are provided in the next section). Research on visually-grounded dialogue within the Computer Vision community exploits encoderdecoder architectures (Sutskever et al., 2014) — which have shown some promise for modelling chatbotstyle dialogue (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a; Li et al., 2016b) — augmented with visual featu
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2399301052	ches to design incremental dialogue policies for how and when to act. Two common approaches are the use of rules parametrised by thresholds that are optimised with human-human data (Buß et al., 2010; Ghigi et al., 2014; Paetzel et al., 2015; Kennington and Schlangen, 2016) and the use of reinforcement learning (Kim et al., 2014; Khouzaimi et al., 2015; Manuvinakurike et al., 2017). For example, Paetzel et al. (2015
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2583186419	d CV communities, recent research on conversational agents combined with Deep Learning techniques has yielded interesting results on visually grounded tasks (Das et al., 2017a; de Vries et al., 2017; Mostafazadeh et al., 2017). In this line of research, the focus is mostly on improving model performance by investigating new machine learning paradigms (like reinforcement learning or adversarial learning) in end-toend settin
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2603266952	d dialogue has experienced a boost in recent years, in part thanks to the construction of large visual human-human dialogue datasets built by the Computer Vision community (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). These datasets include two participants, a Questioner and an Answerer, who ask and answer questions about an image. For example, in the GuessWhat!? dataset developed by de V
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1598875788	d different approaches to design incremental dialogue policies for how and when to act. Two common approaches are the use of rules parametrised by thresholds that are optimised with human-human data (Buß et al., 2010; Ghigi et al., 2014; Paetzel et al., 2015; Kennington and Schlangen, 2016) and the use of reinforcement learning (Kim et al., 2014; Khouzaimi et al., 2015; Manuvinakurike et al., 2017). For example,
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2251235149	in dialogue systems, such as dialogue acts (Paetzel et al., 2015; Manuvinakurike et al., 2017; Kennington and Schlangen, 2016), segment labels (Manuvinakurike et al., 2016), dialogue state features (Williams et al., 2013; Young et al., 2013; Kim et al., 2014), or logical formulas (Yu et al., 2016). 3 Dataset To develop our model and perform our analyses, we use the GuessWhat?! dataset,3 a dataset of approximately 155
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1518951372	dialogue within the Computer Vision community exploits encoderdecoder architectures (Sutskever et al., 2014) — which have shown some promise for modelling chatbotstyle dialogue (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a; Li et al., 2016b) — augmented with visual features. This community has mostly focused on model learning paradigms. Initial models, proposed by de Vries et al.
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1975244201	ed part of dialogue management and have been the focus of attention in dialogue systems research within the CL/NLP community (Larsson and Traum, 2000; Williams et al., 2008; Bohus and Rudnicky, 2009; Young et al., 2013). In this paper, we thus take a step back: instead of focusing on learning paradigms, we focus on the system architecture. We argue that the time is ripe for exploring how the abilities brought in by
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2111151330	em architecture includes several components – mainly, a language interpreter, a dialogue manager, and a response generator – as discrete modules that operate in a pipeline (Jurafsky and Martin, 2009; Jokinen and McTear, 2009) or in a cascading incremental manner (Schlangen and Skantze, 2009; Dethlefs et al., 2012). The dialogue manager is the core component of a dialogue agent: it integrates the semantic content produced
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2583186419	formation. Visually-grounded dialogue has experienced a boost in recent years, in part thanks to the construction of large visual human-human dialogue datasets built by the Computer Vision community (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). These datasets include two participants, a Questioner and an Answerer, who ask and answer questions about an image. For example, in the GuessWhat!? dataset
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2110633879	ger, and a response generator – as discrete modules that operate in a pipeline (Jurafsky and Martin, 2009; Jokinen and McTear, 2009) or in a cascading incremental manner (Schlangen and Skantze, 2009; Dethlefs et al., 2012). The dialogue manager is the core component of a dialogue agent: it integrates the semantic content produced by the interpretation module into the agent’s representation of the context (the dialogue
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1861492603	given, but the Questioner can leave the game incomplete (viz. not try to guess). The set of images and target objects has been built from the training and validation sections of the MS-COCO dataset (Lin et al., 2014) by only keeping images that contain at least three and at most twenty objects and by only considering target objects whose area is big enough to be located well by humans (area &gt; 500px2). Further
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2127838323	ies are typically considered part of dialogue management and have been the focus of attention in dialogue systems research within the CL/NLP community (Larsson and Traum, 2000; Williams et al., 2008; Bohus and Rudnicky, 2009; Young et al., 2013). In this paper, we thus take a step back: instead of focusing on learning paradigms, we focus on the system architecture. We argue that the time is ripe for exploring how the abi
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1600735390	k at hand. These abilities are typically considered part of dialogue management and have been the focus of attention in dialogue systems research within the CL/NLP community (Larsson and Traum, 2000; Williams et al., 2008; Bohus and Rudnicky, 2009; Young et al., 2013). In this paper, we thus take a step back: instead of focusing on learning paradigms, we focus on the system architecture. We argue that the time is ripe
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2622980782	machine learning paradigms (like reinforcement learning or adversarial learning) in end-toend settings, where the model learns directly from raw data without symbolic annotations (Strub et al., 2017; Lu et al., 2017; Wu et al., 2017). Task accuracy, however, is not the only criterion by which a conversational agent should be judged. Crucially, the dialogue should be coherent, with no unnatural repetitions nor un
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2250891614	resholds that are optimised with human-human data (Buß et al., 2010; Ghigi et al., 2014; Paetzel et al., 2015; Kennington and Schlangen, 2016) and the use of reinforcement learning (Kim et al., 2014; Khouzaimi et al., 2015; Manuvinakurike et al., 2017). For example, Paetzel et al. (2015) implement an agent that aims to identify a target image out of a set of images given descriptive content by its dialogue partner. Dec
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	2410983263	ty exploits encoderdecoder architectures (Sutskever et al., 2014) — which have shown some promise for modelling chatbotstyle dialogue (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a; Li et al., 2016b) — augmented with visual features. This community has mostly focused on model learning paradigms. Initial models, proposed by de Vries et al. (2017) and Das et al. (2017a), use sup
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1975244201	uch as dialogue acts (Paetzel et al., 2015; Manuvinakurike et al., 2017; Kennington and Schlangen, 2016), segment labels (Manuvinakurike et al., 2016), dialogue state features (Williams et al., 2013; Young et al., 2013; Kim et al., 2014), or logical formulas (Yu et al., 2016). 3 Dataset To develop our model and perform our analyses, we use the GuessWhat?! dataset,3 a dataset of approximately 155k human-human dialog
2803189511	Ask No More: Deciding when to guess in referential visual dialogue	1591706642	on visually-grounded dialogue within the Computer Vision community exploits encoderdecoder architectures (Sutskever et al., 2014) — which have shown some promise for modelling chatbotstyle dialogue (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a; Li et al., 2016b) — augmented with visual features. This community has mostly focused on model learning paradigms. Initial models, propos
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2512549881	. We use Canonical Correlation Analysis (CCA) (Hotelling, 1936) to perform this fusion. CCA captures maximal information between two views and creates a combined representation (Hardoon et al., 2004; Benton et al., 2016). In the event of having more than two views, fusion can be performed using an extension of CCA called Generalized CCA (see Supplementary). Canonical Correlation Analysis: Let us consider the learnt s
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2099653665	g prosodic and spectral cues. Carvalho et al. (2009) use linguistic features like positive predicates, interjections and gestural clues such as emoticons, quotation marks, etc. Davidov et al. (2010), Tsur et al. (2010) use syntactic patterns to construct classiﬁers. Gonzalez-Ib´ ´anez et al. (2011) also study the use of emoticons, mainly amongst tweets. Riloff et al. (2013) assert sarcasm to be a contrast to positi
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2153579005	h vector is represented by a column in matrix D∈Rds×Nu and W s ∈Rds×SV S, respectively. Here, d s is the embedding size and SVSrepresents the size of the vocabulary. Continuous-bag-of-words approach (Mikolov et al., 2013) is then performed where a target word is predicted given the word vectors from its context-window. The key idea here is to use the document vector of the associated document as part of the context wo
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2126631960	their own idiolect and authorship styles, which is reﬂected in their writing. These styles are generally affected by attributes such as gender, diction, syntactic inﬂuences, etc. (Cheng et al., 2011; Stamatatos, 2009) and present behavioral patterns which aid sarcasm detection (Rajadesingan et al., 2015). We use this motivation to learn stylometric features of the users by consolidating their online comments into
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2100235303	mbedding for each user. We use Canonical Correlation Analysis (CCA) (Hotelling, 1936) to perform this fusion. CCA captures maximal information between two views and creates a combined representation (Hardoon et al., 2004; Benton et al., 2016). In the event of having more than two views, fusion can be performed using an extension of CCA called Generalized CCA (see Supplementary). Canonical Correlation Analysis: Let us
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2165044314	n dialogue systems using prosodic and spectral cues. Carvalho et al. (2009) use linguistic features like positive predicates, interjections and gestural clues such as emoticons, quotation marks, etc. Davidov et al. (2010), Tsur et al. (2010) use syntactic patterns to construct classiﬁers. Gonzalez-Ib´ ´anez et al. (2011) also study the use of emoticons, mainly amongst tweets. Riloff et al. (2013) assert sarcasm to be
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2142112646	ntain information which is highly temporal and contextual. In such scenarios, mining linguistic information becomes relatively inefﬁcient and need arises for additional clues (Carvalho et al., 2009). Wallace et al. (2014) demonstrate this need by showing how traditional classiﬁers fail in instances where humans require additional context. They also indicate the importance of speaker and/or topical information associat
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2250489604	positive predicates, interjections and gestural clues such as emoticons, quotation marks, etc. Davidov et al. (2010), Tsur et al. (2010) use syntactic patterns to construct classiﬁers. Gonzalez-Ib´ ´anez et al. (2011) also study the use of emoticons, mainly amongst tweets. Riloff et al. (2013) assert sarcasm to be a contrast to positive sentiment words and negative situations. Joshi et al. (2015) use multiple feat
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2252381721	s primarily focused on lexical, pragmatic cues found in sentences (Kreuz and Caucci, 2007). Interjections, punctuations, sentimental shifts, etc., have been considered as major indicators of sarcasm (Joshi et al., 2017). When such lexical cues are present in sentences, sarcasm detection can achieve high accuracy. However, sarcasm is also expressed implicitly, i.e., without the use of any explicit lexical cues. Such
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2250247764	tional and personality representations of the input text. Previous works have mainly used historical posts of users to understand sarcastic tendencies (Rajadesingan et al., 2015; Zhang et al., 2016). Khattri et al. (2015) try to ﬁnd users’ sentiments towards entities in their histories to ﬁnd contrasting evidence. Wallace et al. (2015) utilize sentiments and noun phrases used within a forum to gather context typical t
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2131744502	uments. We ﬁrst gather all the comments by a user and create a document by appending them using a special delimiter &lt;END&gt;. An unsupervised representation learning method ParagraphVector (Le and Mikolov, 2014) is then applied on this document. This method generates a ﬁxed-sized vector for each user by performing the auxiliary task of predicting the words within the documents. The choice of ParagraphVector
2803424915	CASCADE: Contextual Sarcasm Detection in Online Discussion Forums	2097998348	which is useful for contextual modeling. 4.2 Training details We hold out 10% of the training data for validation. Hyper-parameter tuning is performed using this validation set through RandomSearch (Bergstra and Bengio, 2012). To optimize the parameters, Adam optimizer (Kingma and Ba, 2014) is used, starting with an initial learning rate of 1e−4. The learnable parameters in the network consists of ={U d;D;W [1 ;2;o;s ];F
2803457824	Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge	2107901333	mprove the prediction of correct answers in a strong single-pass system. See Figure1for illustration. 2.1 Knowledge Retrieval In our experiments we use knowledge from the Open Mind Common Sense (OMCS,Singh et al. (2002)) part of ConceptNet, a crowd-sourced resource of commonsenseknowledgewith a total of ˘630k facts. Each fact f i is represented as a triple f i=(subject, relation, object), where subject and object ca
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	1843891098	using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This ﬁnal vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ˘14K words (Version 0.92) and their binary
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	2469055036	en a slight variation in the composition might turn them away. Vonnegut (1981) discussed the potential of plotting emotions in stories on the “Beginning-End” and the “Ill Fortune-Great Fortune” axes. Reagan et al. (2016) used mathematical tools like Singular Value Decomposition, agglomerative clustering, and Self Organizing Maps (Kohonen et al., 2001) to generate basic shapes of stories. They found that stories are d
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	2469055036	hes to Rags (Tragedy) Figure 6: Emotion ﬂow for four cluster centroids in the dataset. The two curves on top match the “Fall to Rise” shape and the two at the bottom match the “Tragedy” one deﬁned in Reagan et al. (2016). Emotion Shapes: We visualize the prominent emotion ﬂow shapes in the dataset using K-means clustering algorithm. We took the average joy across 50 chunks for all books and clustered them into 100 di
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	2133564696	ion of 20 each. The motivation behind using the standard deviation as a feature is to capture the dispersion of emotions within a chunk. Model: We then use bidirectional Gated Recurrent Units (GRUs) (Bahdanau et al., 2014) to summaFigure 2: Multitask Emotion Flow Model. rize the contextual emotion ﬂow information from both directions. The forward and backward GRUs will read the sequence from x 1 to x n, and from x n to
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	1522301498	ndom selection of different values for the hyperparameters. We tuned the weight initialization (Glorot Uniform (Glorot and Bengio, 2010), LeCun Uniform (LeCun et al., 1998)), learning rate with Adam (Kingma and Ba, 2015) f1e-4,...,1e1g, dropout rates f0.2,0.4,0.5g, attention and recurrent units f32, 64g, and batch-size f1, 4, 8g with early stopping criteria. 4 Results Book Content 1000 sents All Methods Chunks ST MT
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	1894439685	ng Maps (Kohonen et al., 2001) to generate basic shapes of stories. They found that stories are dominated by six different shapes. They even correlated these different shapes to the success of books. Mohammad (2011) visualized emotion densities across books of different genres. He found that the progression of emotions varies with the genre. For example, there is a stronger progression into darkness in horror st
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	1894439685	r) from the Hourglass of emotions model with polarity (positive and negative) (Mohammad and Turney, 2013, 2010). These lexicons have been shown to be effective in tracking emotions in literary texts (Mohammad, 2011). Inputs: Let Xbe a collection of books, where each book x 2Xis represented by a sequence of nchunk emotion vectors, x= (x 1;x 2;:::;x n), where x i is the aggregated emotion vector for chunk i, as sh
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	2411480514	single book vector using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This ﬁnal vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ˘14K words (Version 0.9
2803465626	LETTING EMOTIONS FLOW: SUCCESS PREDICTION BY MODELING THE FLOW OF EMOTIONS IN BOOKS	2250251786	uccess. In addition, we show that using the entire content of the book yields better results. Considering only a fragment, as done in earlier work that focuses mainly on style (Maharjan et al., 2017; Ashok et al., 2013), disregards important emotional changes. Similar to Maharjan et al. (2017), we also ﬁnd that adding genre as an auxiliary task improves success prediction. 2 Methodology We extract emotion vectors fr
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	2250539671	= (wa 1,...,w m a )and wb = (wa 1,...,wbn) be the input sentence pair consisting of m and n tokens, respectively. Each word vector w i ∈Rd is initialized with pretrained ddimensional word embedding (Pennington et al., 2014; Wieting et al., 2015, 2016), then encoded with word context and sequence order through bidirectional LSTMs: →− h i =LSTM f(w i, → h i−1) (1) ←− h i =LSTMb(w i, ←− h i+1) (2) ←→ h i =[ → h i, ←− h i]
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	2153702313	ation: Twitter URL (Lan et al., 2017) was collected from tweets sharing the same URL with major news outlets such as @CNN. This dataset keeps a balance between formal and informal language. PIT-2015 (Xu et al., 2014, 2015)comes fromtheTask1ofSemeval2015 and was collected from tweets under the same trending topic, which contains varied topics and language styles. MSRP (Dolan and Brockett, 2005) was derived from c
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	2338266296	ch as machine translation (Luong et al., 2015; Costa-jussa` and Fonollosa, 2016), language modeling (Ling et al., 2015; Vania and Lopez, 2017), and sequence labeling (dos Santos and Guimara˜es, 2015; Plank et al., 2016), they are not systematically studied in the tasks that concern pairs of sentences. Unlike in modeling individual sentences, subword representations have impacts not only on the out-of-vocabulary word
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	560371024	ing identical words and similar surface context features. Moreover, pretrained word embeddings generally have poor coverage in social media domain where out-of-vocabulary rate often reaches over 20% (Baldwin et al., 2013). We investigated the effectiveness of subword units, such as characters and character n-grams, in place of words for vector representations in sentence pair modeling. Though it is well-known that sub
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	2250539671	models into a single PyTorch framework.1 We followed the setups in (He and Lin, 2016) and (Lan et al., 2017) for the pairwise word interaction model, and used the 200-dimensional GloVe word vectors (Pennington et al., 2014), trained on 27 billion words from Twitter (vocabulary size of 1.2 milion words) for social media datasets, and 300- dimensional GloVe vectors, trained on 840 billion words (vocabulary size of 2.2 mil
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	2417736714	rd and Subword Representations In addition, we experimented with combining the pretrained word embeddings and subword models with various strategies: concatenation, weighted average, adaptive models (Miyamoto and Cho, 2016) and attention models (Rei et al., ). The weighted average outperformed all others but only showed slight improvement over word-based models in social media datasets; other combination strategies coul
2803469432	Character-Based Neural Networks for Sentence Pair Modeling	1840435438	work models proposed for sentence pair modeling tasks, including semantic similarity (Agirre et al., 2015), paraphrase identiﬁcation (Dolan et al., 2004; Xu et al., 2015), natural language inference (Bowman et al., 2015), etc. Most, if not all, of these state-of-the-art neural models (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Tomar et al., 2017; Shen et al., 2017) have achieved the best performances fo
2803502351	All-in-one: Multi-task Learning for Rumour Verification	2742330194	mation can cause to society in critical situations (Lewandowsky et al., 2012) has led to an increased interest within the scientiﬁc community to develop tools to verify information from social media (Shu et al., 2017). Likewise, social media platforms themselves, such as Facebook,1 are investing signiﬁcant effort in mitigating the problems caused by misinformation. One of the features that characterises social med
2803502351	All-in-one: Multi-task Learning for Rumour Verification	2151433832	s been shown that rumours attracting higher levels of skepticism in the form of denials and questioning responses are more likely to be proven false later (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014). Previous work on stance classiﬁcation (Lukasik et al., 2016; Kochkina et al., 2017; Zubiaga et al., 2017; Zubiaga et al., 2018b) has explored the use of sequential classiﬁers, treating the task as o
2803502351	All-in-one: Multi-task Learning for Rumour Verification	1638051351	s work; stance classiﬁcation could be used as a feature in a system for veracity classiﬁcation, as has indeed been the case in previous studies, which have shown a relationship between the two tasks. Zhao et al. (2015) and Enayet and El-Beltagy (2017) have created successful models using this premise. However these studies assume access to stance and veracity labels for the same data, which does not apply in our ca
2803502351	All-in-one: Multi-task Learning for Rumour Verification	1638051351	that these sequential classiﬁers substantially outperform standard, non-sequential classiﬁers. Rumour detection. Work on rumour detection is more scarce. One of the ﬁrst approaches was introduced by Zhao et al. (2015), who built a rule-based approach to identify skepticism (e.g. is this true?) and therefore determine that the associated information is a rumour. The limitations of this approach consist in having to
2803502351	All-in-one: Multi-task Learning for Rumour Verification	2050619059	the veracity of a rumour, furthermore, it has been shown that rumours attracting higher levels of skepticism in the form of denials and questioning responses are more likely to be proven false later (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014). Previous work on stance classiﬁcation (Lukasik et al., 2016; Kochkina et al., 2017; Zubiaga et al., 2017; Zubiaga et al., 2018b) has explored the use
2803502351	All-in-one: Multi-task Learning for Rumour Verification	2121911157	и набора данных и результатами исполь- зования моделей с многозадачным обучением. 1 Introduction Social media have gained popularity as platforms that enable users to follow events and breaking news (Hu et al., 2012). However, not all information that spreads on social media during such events is accurate. The serious harm that inaccurate information can cause to society in critical situations (Lewandowsky et al.
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	1591706642	osethreeRNN-basedarchitecturesforconversationalagents, called Stateful Model, Stateful-Decoder, Context-Prepro. ey extend the Neural Conversational Model , i.e., Sequence-to-Sequence (seq2seq) model [14], and the encoder-decoder architecture [13]. Stateful Model: Previous architectures have not considered information from preceding turns in a conversation, but simply returns the best predicted answer
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	10957333	rethoseusingarepository of predened responses, while systems in category (2) consist of those that are oen based on machine translation methods. is work mainly falls in category (2). Rier et al . [10] proposed one of the rst approaches to treat the response generation problem as a statistical machine translation problem, but the approach was not sensitive to the context of the conversation. e wo
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	1518951372	rst approaches to treat the response generation problem as a statistical machine translation problem, but the approach was not sensitive to the context of the conversation. e work by Sordoni et al . [12] had, on the other hand, a beer success in making the translation model contextsensitive by incorporating previous turns using RNN-based Language Model. Further, the eort on seq2seq model for machin
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	1591706642	tences by adding a socalled aention mechanism to the Encoder-Decoder approach, thus making it possible for the decoder to decide which parts of the source sentence to pay aention to. Vinyals and Le [14] employed the architecture described in [ 13 ] for building a model to generate responses in conversational systems, with the aim to show the ability to produce natural conversations. However, their m
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	2133564696	text and well-dened evaluation methods, e.g., BLEU [ 9]. In line with this, Sutskever et al . [13] proposed an RNN Encoder-Decoder approach to solve the machine translation problem. Bahdanau et al . [1] addressed weakness of the previous architectures in handling long sentences by adding a socalled aention mechanism to the Encoder-Decoder approach, thus making it possible for the decoder to decide
2803522252	Context-Aware Sequence-to-Sequence Models for Conversational Systems.	2159640018	ural conversations. However, their model did not take the context of a conversation into account, thus resulting in generating possibly contradicting replies within a dialogue. Finally, Shang et al . [11] proposed a Neural Responding Machine, employing the Encoder-Decoder framework using GRU-cells to address the response generation problem. Unlike the previously mentioned approaches, they focused on S
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282,2196897294,2293344577	[11], German and French for image retrieval [12], Japanese for cross-lingual document retrieval [16] and image captioning [10], [17], and Chinese for image captioning [14], [18], [19].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282,1895577753	We adopt the Show and Tell network [38], originally developed for English caption generation and later found to be effective for Chinese caption generation [14], [18], [19].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2293344577	Bilingual caption [11] 2016 MS-COCO German / English 1,000 1,000 no machine translation / image captioning
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282	The datasets closely related to ours in terms of the target language are Flickr8k-CN [18], Flickr30k-CN [14] and AIC-ICC [19], all focusing on Chinese.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2345720230	Depending on their applications, the target languages of these datasets differ, including German for multi-modal machine translation [9], ar X iv :1 80 5.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2293344577,2196897294	It thus differs from previous cross-lingual efforts on MS-COCO that target Japanese [10], [17], German [11] or French [12].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2406165856	We employ the BosonNLP toolkit [35] for sentence segmentation and part-of-speech tagging.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2007972815,2108598243	Exemplars are ImageNet [3] for visual class recognition, NUS-WIDE [4] for image tagging, MS-COCO [5] for image captioning, and more recently Twitter100k [6] for cross-media retrieval, to name a few.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	1510632636	As Flickr data is known to be noisy [1], [23], [33], we de-noise with the following rule of thumb.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282	Flickr30k-CN [14] 2017 Flickr30k Chinese / English 1,000 5,000 no image captioning
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282,2185175083	As for Flickr30kCN [14], its manual annotation covers only the test set of Flickr30k [30], by collectively translating 5,000 test sentences from English to Chinese.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2896234464,2133564696	For GRU based vectorization, W2VV uses the hidden vector at the last time step, which is now known to be over compact to represent the entire sentence [15], [39].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2896234464	Although the importance of multimedia annotation and retrieval in a cross-lingual setting has been recognized early [7], only recently has the topic gained increasing attention [8]– [15].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2119775030	Japanese Pascal sentences [16] 2015 Pascal sentences [29] Japanese / English 1,000 5,000 no cross-lingual document retrieval
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2196897294	MIC test data [12] 2016 MS-COCO French / German / English 1,000 5,000 no image retrieval
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2345720230,2185175083	Multi30k [9] 2016 Flickr30k [30] German / English 31,014 186,084 no machine translation / image captioning
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2007972815	To prevent dataset bias, we repeated the human study on another test set of 100 images randomly sampled from NUSWIDE [4], which is independent of all the training sets, i.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	1510632636	While much progress has been made for image auto-tagging [23]–[25] and caption retrieval [26]–[28], whether these techniques can be used to assist dataset construction is unexplored.
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2738919465	propose a neural model to learn multilingual multi-modal representations with image as pivot [22].
2803583314	COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval	2749708282	Works on cross-lingual image captioning consistently report that for training an image captioning model for a target language, machine-translated sentences are less effective than manually written sentences [10], [14], [17], [18].
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2121227244	alization properties of the ASR engine are preserved by using a statistical n-gram LM [21] allowing to mix parts of the training queries to create new ones, and by using class-based language modeling [9] where the value of each entity may be replaced by any other. We now detail the resulting LM construction strategy. The ﬁrst step in building the LM is the creation of patterns abstracting the type of
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	371426616	the built-in entities extractor. The problem of data sparsity is addressed by integrating features based on word clusters [27]. More speciﬁcally, we use Brown clusters [9] released by the authors of [37], as well as word clusters built 6BILOU is a standard acronym referring to the possible positions of a symbol in a sequence: Beginning, Inside, Last, Outside, Unit. 13 from word2vec embeddings [32] us
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2121227244	cy, thanks to the robustness of the built-in entities extractor. The problem of data sparsity is addressed by integrating features based on word clusters [27]. More speciﬁcally, we use Brown clusters [9] released by the authors of [37], as well as word clusters built 6BILOU is a standard acronym referring to the possible positions of a symbol in a sequence: Beginning, Inside, Last, Outside, Unit. 13
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1970381522	the data generation pipeline. 5.1.1 Crowdsourcing Crowdsourcing tasks were originally submitted to Amazon Mechanical Turk9, a widely used platform in non-expert annotations for natural language tasks [51]. While a sufﬁcient number of Englishspeaking contributors can be reached easily, other languages such as French, German or Japanese suffer from a comparatively smaller available crowd. Local crowdsou
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2312434537	different architecture variations presented in this section were chosen for the sake of comparison and demonstration. This experimental comparison, along with optional layer factorization (similar to [44]) or weight quantization are carried out for each target hardware setting, but this analysis is out of the scope of this paper. 3 Language Modeling We now turn to the description of the language model
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2129456397	e-art speech recognition engines reaching human level in English [56]. This achievement unlocked many practical applications of voice assistants which are now used in many ﬁelds from customer support [6, 47], to autonomous cars [41], or smart homes [16, 26]. In particular, smart speaker adoption by the public is on the rise, with a recent study showing that nearly 20% of U.S. adults reported having a sma
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2147687736	e at least two out of three new contributors must conﬁrm its formulation, spelling, and intent. Majority voting is indeed a simple and straightforward approach for quality assessment in crowdsourcing [19]. A custom dashboard hosted on our servers has been developed to optimize the contributor’s workﬂow, with clear descriptions of the task. The dashboard also prevents a contributor from submitting a qu
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2160042006	ery. The choice of CRFs for the slot-ﬁlling step results from careful considerations and experiments. They are indeed a standard approach for this task, and are known to have low generalization error [54, 45]. Recently, more computationally demanding approaches based on deep learning models have been proposed [31, 30]. Our experiments however showed that these approaches do not yield any signiﬁcant gain i
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2398131491	evel in English [56]. This achievement unlocked many practical applications of voice assistants which are now used in many ﬁelds from customer support [6, 47], to autonomous cars [41], or smart homes [16, 26]. In particular, smart speaker adoption by the public is on the rise, with a recent study showing that nearly 20% of U.S. adults reported having a smart speaker at home1. These recent developments how
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1993882792	ient, high-quality training data without compromising user privacy. 1 Introduction Over the last years, thanks in part to steady improvements brought by deep learning approaches to speech recognition [33, 17, 14, 5], voice interfaces have greatly evolved from spotting limited and predetermined keywords to understanding arbitrary formulations of a given intention. They also became much more reliable, with state-o
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2403440562	ility. 3.2.3 On-device personalization Using contextual information in ASR is a promising approach to improving the recognition results by biasing the language model towards a user-speciﬁc vocabulary [1]. A straightforward way of customizing the LM previously described is to update the list of values each entity can take. For instance, if we consider an assistant dedicated to making phone calls (“cal
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1988584482	the lines are obtained through a linear regression. The conﬁdence score is correlated with the word error rate. On the other hand, crowdsourcing – widely used in Natural Language Processing research [46] – ensures diversity in formulation by sampling queries from a large number of demographically diverse contributors. However, the accuracy of intent and slot supervision decreases as soon as humans ar
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2113738519	the living roomÓ User Intent: SwitchLightOn Slots: room: living room brightness: None Device Action/Feedback Figure 1: Interaction ﬂow 1.2 A private-by-design embedded platform The Privacy by Design [10, 25] principle sets privacy as the default standard in the design and engineering of a system. In the context of voice assistants, that can be deployed anywhere including users’ homes, this principle call
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1524333225	using a method close to that presented in [22]. 2.2 Model training Acoustic models are hybrid NN/HMM models. More speciﬁcally, they are a custom version of the s5 training recipe of the Kaldi toolkit [42]. 40 MFCC features are extracted from the audio signal with windows of size 25ms every 10ms. Models with a variable number of layers and neurons can be trained, which will impact their accuracy and co
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1494198834	owever in order to assess the quality of the acoustic model in a more general setting, the evaluation of this section is carried out in a large vocabulary setup, on the LibriSpeech evaluation dataset [38], chosen because it is freely available and widely used in state-of-the-art comparisons. The language model for the large-vocabulary evaluation is also freely available online5. It is a pruned trigram
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2007261869,2160042006	us providing a richer output than the 1-best decoding hypothesis. In particular, confusion networks in conjunction with NLU systems typically improve end-to-end performance in speech-to-meaning tasks [15, 54, 53]. In the following, we restrict our use of confusion networks to a greedy decoder that outputs, for each speech segment, the most probable decoded word along with its probability. In this context, our
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	2594610113	rrectly decoded. Conﬁdence scoring is a notoriously hard problem in speech recognition [20, 59]. Our approach is based on the so-called “confusion network” representation of the hypotheses of the ASR [28, 57] (see Figure 5). A confusion network is a graph encoding, for each speech segment in an utterance, the competing decoding hypotheses along with their posterior probability, thus providing a richer out
2803609229	Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.	1524333225	tes, allowing the composition and the inference to run faster. More detailed deﬁnitions of the previous classical transducers are beyond the scope of this paper, and we refer the interested reader to [34, 42] and references therein. In the following, we focus on the construction of the G transducer, encoding the LM, from the domain-speciﬁc dataset presented above. 3.2.1 Language Model Adaptation As explai
2803691791	Native Language Cognate Effects on Second Language Lexical Choice	52446197	(Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker,2013;Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al.,2017). Our study is also corpus-based; but it stands out as it focu
2803691791	Native Language Cognate Effects on Second Language Lexical Choice	2252012503	, identiﬁcation of the mother tongue of English learners (Koppel et al.,2005;Tetreault et al.,2013;Tsvetkov et al., 2013;Malmasi et al.,2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker,2013;Berzak et al., 2014).
2803691791	Native Language Cognate Effects on Second Language Lexical Choice	2764337833	r exploitation of etymological clues in the study of non-native language, but their results were very inconclusive. In contrast to the learner corpora that dominate studies in this ﬁeld (Granger,2003;Geertzen et al., 2013;Blanchard et al.,2013), our corpus contains spontaneous productions of advanced, highly proﬁ- cient non-native speakers, spanning over 80K topical threads, by 45K distinct users from 50 countries (wi
2803814120	Language Expansion In Text-Based Games.	2250539671	ata and modern architectures and algorithms, end-to-end supervised training is the dominating strategy. However, Natural Language Processing (NLP) is one ﬁeld in which pre-trained word embeddings[6], [10] have been successfully used in several downstream techniques. While language modeling is the common task for learning word representations, human beings learn the language mainly by interaction. Inte
2803814120	Language Expansion In Text-Based Games.	1934909785	earning word representations, human beings learn the language mainly by interaction. Interaction based language learning has been recently explored in the context of learning to play text-based games [8] and dialogue based language learning [17]. [8] proposed a Q-learning agent called LSTM-DQN which learnt to play simple Multi-User Dungeon (MUD) games by applying reinforcement learning. [8] also show
2803814120	Language Expansion In Text-Based Games.	2153579005	led data and modern architectures and algorithms, end-to-end supervised training is the dominating strategy. However, Natural Language Processing (NLP) is one ﬁeld in which pre-trained word embeddings[6], [10] have been successfully used in several downstream techniques. While language modeling is the common task for learning word representations, human beings learn the language mainly by interaction
2803814120	Language Expansion In Text-Based Games.	1934909785	radicting state dynamics which will shed some light on the effect of policy distillation. 3 Multi-task Distillation Agent for learning representations from multiple sources LSTM-DQN agent proposed in [8] differs from the standard DQN agent [7] in two ways. Firstly, since the state is a sequence of words in the case of text-based games, LSTM-DQN uses an LSTM layer for state representation instead of a
2803814120	Language Expansion In Text-Based Games.	1934909785	s. We also provide deeper insights into how policy distillation functions using heat maps of the representation and control modules of the agent. arXiv:1805.07274v1 [cs.CL] 17 May 2018 2 Related Work [8] proposed a simple LSTM-DQN architecture for learning text based games. The action space in this architecture was restricted to be of one action word and one object word (e.g. go east). [2] considered
2803814120	Language Expansion In Text-Based Games.	1934909785	set of textual descriptions among which one is randomly provided to the agent on entering that room. The quests (tasks given to the agent) and room descriptions are structured in a similar fashion to [8] in order to constrain an agent to understand the underlying information from the state so as to achieve higher rewards. Figure 2: Different game layouts Each of the 5 worlds consists of four rooms –
2803814120	Language Expansion In Text-Based Games.	2110798204	ss of deep learning methods can be mainly attributed to their ability to learn meaningful task speciﬁc representations. While the initial success of deep learning was due to unsupervised pre-training [1], [4], with the availability of large amount of labeled data and modern architectures and algorithms, end-to-end supervised training is the dominating strategy. However, Natural Language Processing (N
2803814120	Language Expansion In Text-Based Games.	1934909785	xplain the text-based game environment used in the experiments before reporting our ﬁndings. 4.1 Game Environment We conduct experiments on 5 worlds which we have built upon the Home World created by [8]1. For our vocabulary expansion experiments, we have created multiple worlds, such that no two of the worlds have identical vocabulary. Out of the 5 worlds we created, game-4 and game-5 have a differe
2803828341	A Corpus for Multilingual Document Classification in Eight Languages.	2607106700	ions A second direction of research is to directly learn multilingual sentence representations. In this paper, we evaluate a recentlyproposedtechniqueto learnjoint multilingualsentencerepresentations(Schwenk and Douze, 2017). Theunderlying idea is to use multiple sequence encoders and decodersand to train them with alignedcorporafromthe machine translation community. The goal is that all encoders sharethe same sentencere
2803828341	A Corpus for Multilingual Document Classification in Eight Languages.	2150102617	wordorsentenceembeddingsrespectively. By these means, we hope to deﬁne a clear evaluation environment for highly multilingual documentclassiﬁcation. 2. Corpus description The Reuters Corpus Volume 2 (Lewis et al., 2004), in short RCV21, is a multilingual corpus with a collection of 487,000 news stories. Each news story was manually classiﬁed into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics
2803863433	Extrapolation in NLP	2153579005	he following section, we investigate a global symmetry within the representation of words. 4 Global Structure in Word Embeddings Word embeddings, such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013), have been enormously effective as input representations for downstream tasks such as question answering or natural language inference. One well known application istheking = queen−woman+man example,
2803863433	Extrapolation in NLP	2250539671	ion between sentences in the task. In the following section, we investigate a global symmetry within the representation of words. 4 Global Structure in Word Embeddings Word embeddings, such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013), have been enormously effective as input representations for downstream tasks such as question answering or natural language inference. One well known application
2803863433	Extrapolation in NLP	1840435438	mmetries. In the next section, we examine how global symmetries can be exploited in an inference task. 3 Global Symmetries in Natural Language Inference The Stanford Natural Language Inference (SNLI, Bowman et al., 2015) dataset attempts to provide training and evaluation data for the task of categorising the logical relationship between a pair of sentences. Systems must identify whether each hypothesis stands in a r
2803878944	Estimating the Rating of Reviewers Based on the Text.	1545467275	-Gain 55 43 Sentiment 52 43 rating as we expected, but compared to TF-IDF are still among the top features. Comparing the result of this paper with other papers, and especially with De Albornozs work [7] (83 percent for SVM), we showed that the result of this research is almost comparable with the other works in this area. One important note here is that reviewed works leveraged various types of feat
2803878944	Estimating the Rating of Reviewers Based on the Text.	2005110322	al word to our features to expand and improve the prediction models. Hashtags were used in previous study in social media analysis for topic modeling [38], sentiment analysis [34], and opinion mining [23]. Numbers of research leveraged social media information to predict a movies success [21]. However, the research on combining these two user-generated texts is not well explored in the area of rating
2803878944	Estimating the Rating of Reviewers Based on the Text.	1859957297,2047756776,2145955806,2160660844	ating analysis. Other areas such 1 as opinion extraction and sentiment analysis, building recommendation systems and summarizing the texts are among domains that are extensively explored in this area [30, 4, 24, 25, 16, 18, 22] . Variousnumbersofstudies focused onpredicting ratingsofdiﬀerent products on Amazon (please refer to section 2 for more information). In this paper, we focus on predicting the ratings of ﬁlms (docume
2803878944	Estimating the Rating of Reviewers Based on the Text.	2022204871	considered a lexicon based approach which leverages a predeﬁned lexicon consisting words as well as their polarity (as a tag or ratio). To get the sentimental words, we used MPQA subjectivity lexicon [37], and extracted and tagged the words in the reviews. In total 2,055 words were extracted. After extracting the words of each feature set, we created the feature vectors using python scikit-learn libra
2803878944	Estimating the Rating of Reviewers Based on the Text.	2087294982	essfully identiﬁed the most helpful reviews to the users. The most helpful reviews can be displayed on top to improveusersre-viewingexperienceonelectronicmarketplaces[3, 12,28, 27, 29]. Kim et al. in [20] suggested an automated assessment for the review helpfulness by considering the length of the review as the most useful feature compared to the other ones. The main contribution of this paper is: a)
2803878944	Estimating the Rating of Reviewers Based on the Text.	1545467275	f that product (e.g. picture quality of a camera), aspect-based sentiment analysis is introduced to ﬁnd the opinions related to each attribute on the sentence level. In a research, De Albornoz et al. [7] used both topic and sentiment of the reviews on sentence level to assess the impact of text-driven information in predicting the rating of the reviews in recommendation systems. This article aims to
2803878944	Estimating the Rating of Reviewers Based on the Text.	2098173428	iews. As reported, this model successfully identiﬁed the most helpful reviews to the users. The most helpful reviews can be displayed on top to improveusersre-viewingexperienceonelectronicmarketplaces[3, 12,28, 27, 29]. Kim et al. in [20] suggested an automated assessment for the review helpfulness by considering the length of the review as the most useful feature compared to the other ones. The main contribution o
2803878944	Estimating the Rating of Reviewers Based on the Text.	2133990480	mly selected 90 percent of the data for training the classiﬁers. The rest of the data will be used for testing the classiﬁers with the highest accuracy and the most eﬃcient feature sets. We used WEKA [14] to implement the two algorithms. Table 4 shows the results of the selected features and the values of average accuracy, precision, recall, and f-score of two classiﬁers, SVM and NB. Based on the resu
2803878944	Estimating the Rating of Reviewers Based on the Text.	2101234009	nd extracted and tagged the words in the reviews. In total 2,055 words were extracted. After extracting the words of each feature set, we created the feature vectors using python scikit-learn library [31]. Table 2 shows the list of features that were used in this study. In addition, top 10 words of each feature set, as Info-Gain, TF-IDF and sentiment are listed in Table 3. 5 Table 3: List of the top w
2803878944	Estimating the Rating of Reviewers Based on the Text.	2099934438	nt spelling errors in the reviews and (3) the presence of neutral sentences which do not express any opinion but are nec2 essarily classied as positives or negatives. In another research, Ghose et al [11] conducted a study to create a dataset of products from the Amazon website. The dataset consisted of product-speciﬁc characteristics and the details of the product review. They generated a training se
2803878944	Estimating the Rating of Reviewers Based on the Text.	1545467275,2022204871,2087294982,2134353060	ompared to others. Due to their importance, business owners and academic scholars have studied user generated reviews to ﬁnd eﬃcient techniques for estimating ratings based on the content of the text [1, 37, 20, 7, 6]. The research in the area of review mining is very vast and is not just limited to ﬁnding rating analysis. Other areas such 1 as opinion extraction and sentiment analysis, building recommendation sys
2803878944	Estimating the Rating of Reviewers Based on the Text.	2594568420	prediction. Using the insights of this paper, for data parallel processing of big data, more sophisticated algorithms based on MapReduce can be used for speeding up the processing time, e.g. look at [36, 39, 5, 40, 19]. 4 Data We used the reviews of eight ﬁlms from Amazon . Table 1 shows the names and the number of the reviews for each ﬁlm. Around 65 percent of the reviews are 5 3 Table 1: Number of reviews of each
2803878944	Estimating the Rating of Reviewers Based on the Text.	2022204871	t eﬃcient algorithms in opinion mining and sentiment analysis of the reviews. Supervised and unsupervised approaches are employed to extract the sentiment of the reviews on sentence or document level [37]. Since a positive or negative opinion about a product (e.g. Camera) does not show the feeling of the opinion holder about every speciﬁc feature of that product (e.g. picture quality of a camera), asp
2803878944	Estimating the Rating of Reviewers Based on the Text.	2112744748	tailed analysis of features showed that length of the reviews, unigrams, and product rating were the most helpful ones in the prediction and structural and syntactic features had no signiﬁcant impact [42]. Rezapour and Diesner in [33] studied the movie reviews from a new perspective that, as claimed by the authors, is new in the area of review and opinion mining. In this work, the authors captured the
2803878944	Estimating the Rating of Reviewers Based on the Text.	2121392694,2145955806,2160409620	ted Works As mentioned earlier prior work on review mining is very vast. Researchers in this area have tried to ﬁnd eﬃcient algorithms for predicting rating, helpfulness, and sentiment of the reviews [13, 9, 15, 35, 32, 25, 26, 17]. In this section, we explore the most related work in the area of review mining and analyze the papers that used text of the reviews to extract information. Bing and Zhang (2012) reviewed the most eﬃ
2803878944	Estimating the Rating of Reviewers Based on the Text.	2067344105	ﬁlms and add them as the sentimental and/or topical word to our features to expand and improve the prediction models. Hashtags were used in previous study in social media analysis for topic modeling [38], sentiment analysis [34], and opinion mining [23]. Numbers of research leveraged social media information to predict a movies success [21]. However, the research on combining these two user-generated
2803960965	A Manually Annotated Chinese Corpus for Non-task-oriented Dialogue Systems.	1958706068	es can be used for multiple prompts, we consider them acceptable instead of good, so as to avoid chatbots from yielding “one size ﬁts all” replies, which is a major drawback of the existing chatbots (Li et al., 2016;Xing et al.,2017) due to the prominence of general responses on social media. For example, [S 6] is an example of versatile responses, which can be used to reply to diverse prompts, such as “I’m so h
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	2401527985	a context is important. 2.2. Modelling Approaches Lexical, Prosodic, and Syntactic Cues: Many studies have been carried out to ﬁnd out the lexical, prosodic and syntactic cues (Stolcke et al., 2000; Surendran and Levow, 2006; O’Shea et al., 2012; Yang et al., 2014). For the SwDA corpus, the state-of-the-art baseline result was 71% for more than a decade using a standard Hidden Markov Model (HMM) with language features su
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1526096287	corpus, our model achieved an accu1Available at https://github.com/cgpotts/swda racy of 77.3% compared to 73.9% as state of the art, where the context-based learning is used for the DA classiﬁcation (Kalchbrenner and Blunsom, 2013). - Beneﬁts of using context arise from using only a few preceding utterances making the model suitable for dialogue system in real time, in contrast to feeding the whole conversation, which can achie
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1503312748	f dialogue. Corpus Insight: We have investigated the annotation method for two corpora: Switchboard (SWBD) (Godfrey et al., 1992; Jurafsky et al., 1997) and ICSI Meeting Recorder Dialogue Act (MRDA) (Shriberg et al., 2004). They are annotated with the DAMSL tag set. The annotation includes not only the utterance-level but also the segmentedutterance labelling. The DAMSL tag set provides very ﬁne-grained and detailed DA
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	2110485445	guage model representation s t are fed to the RNN with the preceding utterances (s t 1, s t 2) being the context. We use the RNN, which gets the input s t, and stores the hidden vector h t at time t (Elman, 1990), which is calculated as: h t = f (W h h t 1 +I s t +b) (1) where f() is a sigmoid function, W h and I are recurrent and input weight matrices respectively and bis a bias vector learned during trainin
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1526096287	all learning cases, we minimise the categorical cross-entropy. 3.3. Results We follow the same data split of 1115 training and 19 test conversations as in the baseline approach (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013). Table 3 shows the results of the proposed model with several setups, ﬁrst without the context, then with one, two, and so on preceding utterances in the context. We examined different values for the
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1522301498	let the network ﬁnd the change in the speaker’s turn. The speaker id ’A’ is represented by [1,0] and id ’B’ by [0,1] and it is concatenated with the corresponding utterances s t. The Adam optimiser (Kingma and Ba, 2014) was used with a learning rate 1e 4, which decays to zero during training, and clipping gradients at norm 1. Early stopping was used to avoid over-ﬁtting of the network, 20% of training samples were u
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1526096287	logue system where one can only perceive the preceding utterance as a context but does not know the upcoming utterances. Hence, we use a context-based learning approach and regard the 73.9% accuracy (Kalchbrenner and Blunsom, 2013) on the SwDA corpus as a current state of the art for this task. 3. Our Approach Our approach takes care of discourse compositionality while recognising dialogue acts. The DA class of the current utte
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1526096287	times Table 3: Accuracy of the dialogue act identiﬁcation with the context-learning approach. Model setup Acc.(%) SD Baseline Most common class 31.50 Related previous work Stolcke et al. (2000) 71.00 Kalchbrenner and Blunsom (2013) 73.90 Our work Our baseline (without context) 73.96 0.26 RNN (1 utt. in context w. SpeakerID) 76.48 0.33 RNN (1 utt. in context) 76.57 0.28 RNN (2 utts. in context) 76.81 0.24 RNN (3 utts. in context
2804103719	A Context-based Approach for Dialogue Act Recognition using Simple Recurrent Neural Networks.	1526096287	utterances share similar lexical and syntactic cues (words and phrases) like the backchannel, yes-answer and accept/agree classes. Some researchers proposed an utterance-dependent learning approach (Kalchbrenner and Blunsom, 2013; Ji et al., ; Kumar et al., 2017; Tran et al., 2017; Liu et al., 2017; Ortega and Vu, 2017; Meng et al., 2017). Kalchbrenner and Blunsom (2013) and Ortega and Vu (2017) have proposed contextbased lea
2804107505	DIVERSE FEW-SHOT TEXT CLASSIFICATION WITH MULTIPLE METRICS	344512287	trics from the previous learning experience. The key of the proposed FSL framework is the task clustering algorithm. Previous works (Kumar and Daume III,2012;Kang et al.,2011;Crammer and Mansour,2012;Barzilai and Crammer, 2015) mainly focused on convex objectives, and assumed the number of classes is the same across different tasks (e.g. binary classiﬁcation is often considered). To make task clustering (i) compatible with
2804147802	Reinforced Extractive Summarization with Question-Focused Rewards	2150869743	nt and concatenated to form a summary (Nenkova and McKeown,2011). Existing supervised approaches to extractive summarization frequently use human abstracts to create annotations for extraction units (Gillick and Favre, 2009;Li et al.,2013;Cheng and Lapata,2016). E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise. Despite the usefulness, there are two issues with this scheme. First, a vast major
2804147802	Reinforced Extractive Summarization with Question-Focused Rewards	2427527485	to produce generic summaries that are capable of answering all questions. Cloze questions have been used in reading comprehension (Richardson et al.,2013;Weston et al., 2016;Mostafazadeh et al.,2016;Rajpurkar et al., 2016) to test the system’s ability to perform reasoning and language understanding. Hermann et al. (2015) describe an approach to extract (context, question, answer) triples from news articles. Our work dr
2804147802	Reinforced Extractive Summarization with Question-Focused Rewards	1544827683	sed in reading comprehension (Richardson et al.,2013;Weston et al., 2016;Mostafazadeh et al.,2016;Rajpurkar et al., 2016) to test the system’s ability to perform reasoning and language understanding. Hermann et al. (2015) describe an approach to extract (context, question, answer) triples from news articles. Our work draws on this approach to automatically create questions from human abstracts. Reinforcement learning
2804147802	Reinforced Extractive Summarization with Question-Focused Rewards	2440159969	the summary (y^ t) thus can be decided based on hD t . However, we also want to accommodate the previous t-1 sampling decisions (y^ 1:t 1) to improve the ﬂuency of the extractive summary. Following (Lei et al., 2016), we introduce a single-direction LSTM encoder whose hidden state s t tracks the sampling decisions up to time step t(Eq.8). It represents the semantic meaning encoded in the current summary. To sampl
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2232826555	Czech project AI&amp;Reasoning CZ.02.1.01/0.0/0.0/15 003/0000466 and the European Regional Development Fund. 3 https://www.degruyter.com/view/j/forma arXiv:1805.06502v1 [cs.CL] 10 May 2018 Previously [12,14], we have built and trained on the smaller aligned corpora custom translation systems based on probabilistic grammars, enhanced with semantic pruning methods such as type-checking. Here we for the rst
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2101105183	diculty of generating correct words in a sentence, and the BLEU rate gives a score on the quality of the overall translation. Details explaining perplexity and the BLEU rate can be found in [18] and [20], respectively. Due to the abundance of hyperparameters, we decided to do our experiments progressively, by rst comparing a few basic hyperparameters, xing the best choices and then comparing the othe
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2130942839	e-art articial neural networks. It has been shown recently that given enough data, state-of-the-art neural architectures can learn to a high degree the syntactic correspondence between two languages [21]. We are interested to see to what extent the neural methods can achieve meaningful translation by training on aligned informal-formal pairs of mathematical statements. The neural machine translation
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2130942839,2157331557	l network architectures have been invented, and easy-to-use neural frameworks such as Tensor ow [1] have been built. We are particularly interested in the sequence-to-sequence (seq2seq) architectures [21,6] which have achieved tremendous successes in natural language translation as well as related tasks. In particular, we have chosen Luong’s NMT framework [16] that encapsulates the Tensor ow API gracefu
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2232826555	of LATEX-written mathematical texts to a formal and veriable mathematical language { in this case the Mizar language. This is the next step in our project to automatically learn formal understanding [12,13,14] of mathematics and exact sciences using large corpora of alignments between informal and formal statements. Such machine learning and statistical translation methods can additionally integrate strong
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2295728669	lation from informal to formal mathematics. The approach that we have used so far for experimenting with non-neural translation methods is to take a large formal corpus such as Flyspeck [10] or Mizar [3] and apply various informalization (ambiguation) [12,14] transformations to the formal sentences to obtain their less formal counterparts. Such transformations include e.g. forgetting which overloaded
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2232826555	oach that we have used so far for experimenting with non-neural translation methods is to take a large formal corpus such as Flyspeck [10] or Mizar [3] and apply various informalization (ambiguation) [12,14] transformations to the formal sentences to obtain their less formal counterparts. Such transformations include e.g. forgetting which overloaded variants and types of the mathematical symbols are used
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2232826555	rge corpora that would align many pairs of human-written informal LATEX formulas with their corresponding formalization, we obtain the rst corpus for the experiments presented here by informalization [12]. This is in general a process in which a formal text is turned into (more) informal one. In our previous work over Flyspeck and Mizar [12,14] the main informalization method was to forget which overl
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2232826555	ure work thus includes a full semantic evaluation, i.e., using translation to MPTP/TPTP and ATP systems to check if the resulting formal statements are equivalent to their aligned counterparts. As in [12,14], this will likely also show that the translator can produce semantically dierent, but still provable statements and conjectures. Another line of research opened by these results is an extension of t
2804320211	First Experiments with Neural Translation of Informal to Formal Mathematics	2592864539	ures the diculty of generating correct words in a sentence, and the BLEU rate gives a score on the quality of the overall translation. Details explaining perplexity and the BLEU rate can be found in [18] and [20], respectively. Due to the abundance of hyperparameters, we decided to do our experiments progressively, by rst comparing a few basic hyperparameters, xing the best choices and then comparing
2804323159	Robust Handling of Polysemy via Sparse Representations.	2141599568	apitals .872 .957 .941 city-in-state .657 .972 .955 currency .030 .037 .122 nationality .515 .615 .655 world capitals .472 .789 .668 family .617 .217 .306 Table 6: Performance on Analogy classes from Mikolov et al. (2013). The ﬁrst two columns are derived from the same corpus, whereas the last column reports numbers on the data we will release. For category builder, we used ρ =3,n =100 of country-based factual coverag
2804323159	Robust Handling of Polysemy via Sparse Representations.	1662133657	corresponding algorithms that perform set expansion in Category Builder (CB). 4.1 Sparse Representations for Expansion We use the traditional word representation that distributional similarity uses (Turney and Pantel, 2010), and that is commonly used in ﬁelds such as context sensitive spelling correction and grammatical correction (Golding and Roth, 1999; Rozovskaya and Roth, 2014); namely,words are associated with some
2804323159	Robust Handling of Polysemy via Sparse Representations.	2141599568	and Disney characters, and then attempt to cluster the expansion into meaningful clusters. 3.2 Analogies Solving analogy problems usually refers to proportional analogies, such as hand:glove::foot:?. Mikolov et al. (2013) showed how word embeddings such as Word2Vec capture linguistic regularities and thereby solve this. Turney (2012) used a pair of similarity functions (one for function and one for domain) to address
2804323159	Robust Handling of Polysemy via Sparse Representations.	2295058825	ext mickey should determine what is considered ‘similar’ to pluto, rather than being biased by the more dominant sense of pluto, to determine that neptune is similar to it. Earlier approaches such as Rong et al. (2016) approach this problem differently: they expand to both planets and Disney characters, and then attempt to cluster the expansion into meaningful clusters. 3.2 Analogies Solving analogy problems usuall
2804323159	Robust Handling of Polysemy via Sparse Representations.	2141599568	ow our ability to do these analogies is central to cognition. The current work aims to tackle these nonproportional analogies and in fact performs better than Word2Vec on some analogy classes used by Mikolov et al. (2013), despite being shown one fewer term. The approach is rather close to that used by Turney (2012) for a different problem: word compounds. Understanding what a dog house is can be phrased as “What is t
2804323159	Robust Handling of Polysemy via Sparse Representations.	2141599568	xpansion examples using Category Builder so as to illustrate its ability to deal with Polysemy. † For these examples, ρ =5 6 Analogies 6.1 Experimental Setup We evaluated the analogy examples used by Mikolov et al. (2013). Category Builder evaluation were done by expanding using syntactic and sentence-based-cooccurrence contexts as detailed in Section 4.6 and scoring items according to Equation 9. For evaluating using
2804377263	Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators.	2735574368	vation and style transfer of news headlines and product review sentences (Fu et al.,2018), multiple automatically extracted style attributes along with sentiment and sentence theme for movie reviews (Ficler and Goldberg, 2017), sentiment, ﬂuency and semantic equivalence (Shen et al.,2017), utterance length and topic (Fan et al.,2017), and the personality of customer care utterances in dialogue (Herzig et al., 2017). Howeve
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2420948438	articularly challenging, because the worlds they describe are largely implicit and changing. While there are few large datasets in this genre, two exceptions are bAbI (Weston et al., 2015) and SCoNE (Long et al., 2016), described earlier2. bAbI has helped advance methods for reasoning over text, such as memory network architectures (Weston et al., 2014), but has also been criticized for using machine-generated text
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2411480514	ating the output. We ﬁrst create h ev by concatenating the contextual embedding of the participant and verb.7 We then use a bilinear similarity function sim(h i;h ev) = (hT i B h ev) + b, similar to (Chen et al., 2016), to compute attention weights A i over each word w i in the sentence. For state change type prediction, the words between the verb and participant may be important, while for the location tagging, co
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2420948438	bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains, and arXiv:1805.06975v1 [cs.CL] 17 May 2018 Figure 2: A (simpliﬁed) annotated paragraph from ProPara. E
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2252016937	lobal consistency constraints: e.g., it does not make sense to create an already-existing entity, or destroy a non-existent entity. Global constraints were found useful in the earlier ProRead system (Berant et al., 2014). Data augmentation through weak supervision: additional training data can be generated by applying existing models of state change, e.g., from VerbNet (Kipper et al., 2008), to new sentences to creat
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2252136820	Is the location of e known?” (yes/no) Q3.“Where is e?” (span) The template Q1 is applied to all participants. Q2 9This approach has been adopted previously for questions with multiple answers (e.g., (Berant et al., 2013)). For questions with only one answer, F 1 is equivalent to accuracy. 10https://github.com/siddk/entity-network and https://github.com/uwnlp/qrn will only be present in the training data if Q1 is “yes
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2427527485	network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced? (A: in the leaf)1 *Bha
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	1544827683	odel of the initial state is given for each task. However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (Hermann et al., 2015; Winograd, 1972). In this work, we create a new dataset, ProPara (Process Paragraphs), containing 488 humanauthored paragraphs of procedural text, along with 81k annotations about the changing states
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2427527485	odels that learn to infer and propagate entity states in novel ways, and outperform existing methods on this dataset. 2 Related Work Datasets: Large-scale reading comprehension datasets, e.g., SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), have successfully driven progress in question answering, but largely targeting explicitly stated facts. Often, the resulting systems can be fooled (Jia and Liang, 2017
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2738015883	on, e.g., (Seo et al., 2017a; Clark and Gardner, 2017), enabled by well-designed datasets and modern neural network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, mi
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2252016937	particular entities in the text, to encourage the memories to record information about those entities. Similarly, Query Reduction Networks (QRN) (Seo et al., 2017b) tracks state in 2The ProcessBank (Berant et al., 2014) dataset is smaller and does not address state change, instead containing 585 questions about event ordering and event arguments. a paragraph, represented as a hidden vector h. QRN performs gated prop
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2738015883	urkar et al., 2016), TriviaQA (Joshi et al., 2017), have successfully driven progress in question answering, but largely targeting explicitly stated facts. Often, the resulting systems can be fooled (Jia and Liang, 2017), prompting eorts to create harder datasets where a deeper understanding of the text appears necessary (Welbl et al., 2017; Araki et al., 2016). Procedural text is a genre that is particularly challe
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2250539671	utput layers. We give the detail of these layers below. Input Encoding: Each word w i in the input sentence is encoded with a vector x i = [v w : v e : v v], the concatenation of a pre-trained GloVe (Pennington et al., 2014) word embedding v w, indicator variables v e on whether w i is the speciﬁed participant and v v on whether w i is a verb (via a POS tagger). Context Encoding: A biLSTM is used to contextualize the wor
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2064675550	on is viewed as a SQuAD-style surface-level QA task with the goal to ﬁnd a location span in the input sentence. The design of this model is depicted in Figure 3(a), which adapts a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) recurrent neural network architecture (biLSTM) with attention for input encoding. The prediction tasks are handled by two dierent output layers. We give the detail of these layers below. Input Encod
2804447833	TRACKING STATE CHANGES IN PROCEDURAL TEXT: A CHALLENGE DATASET AND MODELS FOR PROCESS PARAGRAPH COMPREHENSION	2252016937	world to generate data. Models: For answering questions about procedural text, early systems attempted to extract a process structure (events, arguments, relations) from the paragraph, e.g., ProRead (Berant et al., 2014) and for newswire (Caselli et al., 2017). This allowed questions about event ordering to be answered, but not about state changes, unmodelled by these approaches. More recently, several neural systems
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2108373063	1 s(u) returns 1 iff the condition holds for all centers. corpus and the outputs of six recent simpliﬁcation systems for these sentences:5 (1) TSM (Zhu et al., 2010) using Tree-Based SMT, (2) RevILP (Woodsend and Lapata, 2011) using Quasi-Synchronous Grammars, (3) PBMTR (Wubben et al., 2012) using Phrase-Based SMT, (4) Hybrid (Narayan and Gardent, 2014), a supervised system using DRS, (5) UNSUP (Narayan and Gardent, 2016),
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2161004347	14) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4- gramsarefound) and SRL(based onsemantic role labeling). Using 280 pairs of a source sentence and a simpli
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2511538013	and 3 (“yes”). Following Glavas and Stajnerˇ (2013), we used a 3 point Likert scale, which has recently been shown to be preferable over a 5 point scale through human studies on sentence compression (Toutanova et al., 2016). Question Qd was accompanied by a negative example 7 showing acaseoflexical simpliﬁcation, whereacomplexwordisreplaced byasimpleone. A positive example was not included so as not to bias the annotato
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2471690557	ANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions likenominalargumentstructures arenotaddressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, ta
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2604593109	c resources. One is a semantic annotation (UCCA in our experiments) of the source side, which can be carried out either manually or automatically, using the TUPA parser3 (Transitionbased UCCA parser; Hershcovich et al., 2017) for UCCA.UCCAdecomposes each sentence sinto a set of Scenes {sc1,sc2,..,sc n}, where each scene 3https://github.com/danielhers/tupa sc i contains a main relation mr i (sub-span of sc i) and a set of
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2471690557	cture (Abend and Rappoport, 2013). UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for machine translation evaluation (Birch et al., 2016) (Section 2). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR (Banarescu et al., 2013) or Discourse Representation
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2101105183	d out with a small number of sentences (18 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simpliﬁed sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive c
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2252001469	deﬁnition of SAMSA. Xu et al. (2016) were the ﬁrst to propose two evaluation measures tailored for simpliﬁcation, focusing on lexical simpliﬁcation. The ﬁrst metric is FKBLEU, a combination of iBLEU (Sun and Zhou, 2012), originally proposed for evaluating paraphrase generation by comparing the output both to the reference and to the input, and of the Flesch-Kincaid Index (FK), a measure of the readability of the tex
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2145882814	dRephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simpliﬁcation in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metricsobt
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2471690557	several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Unlike AMR, UCCA semantic units are directly anchored in the text (Abend and Rappoport, 2017; Birch et al., 2016), which allows easy inclusion of a word-toword alignment in the metric model (Section 4). UCCA Scenes. A Scene, which is the most basic notion of the foundational layer of UCCAconsidered here, describ
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2108373063	evelopment of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2511538013	h involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metricsobtain higher correlation withhumanevaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not signiﬁcantly outperform bi
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2163986298	the other hand uses linguistic annotation only on the source side, with semantic structures instead of syntactic ones. Semantic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the sourc
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2145882814	higher correlation withhumanevaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not signiﬁcantly outperform bigram-based metrics on any parameter. Both Clarke and Lapata (2006) and Toutanova et al. (2016) use reference-based metricsthatusesyntactic structure onboththeoutput and the references. SAMSA on the other hand uses linguistic annotation only on the source side, with
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2143017621	idelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al.,2017). Sentencesegmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, weused thealigner ofSultan et al.(2014).9 7 Correlation with Human Evaluation We compare the system rankings obtained by SAMSA and by the four human parameters. We ﬁnd that the
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2116492146	mplicity, but no correlation for adequacy. Stajner et al.ˇ (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4- gramsarefound) and SRL(based onsemantic role labeli
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2108373063	n. In most of the work investigating the structural operations involved in text simpliﬁcation, both in rule-based systems (Siddharthan and Angrosh, 2014) and in statistical systems (Zhu et al., 2010; Woodsend and Lapata, 2011), the structures that were considered were syntactic. Narayan and Gardent (2014, 2016) proposed to use semantic structures in the simpliﬁcation model, in particular in order to avoid splits and deleti
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2739007515	were previously proposed for the evaluation of structural simpliﬁ- cation. The need for such a measure is pressing, given recent interest in structural simpliﬁcation, e.g.,intheSplitandRephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simpliﬁcation in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that
2804491852	SEMANTIC STRUCTURAL EVALUATION FOR TEXT SIMPLIFICATION	2157281038	tic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions likenominalargumentstructures arenotaddressed by the
2804491889	NATURAL LANGUAGE GENERATION BY HIERARCHICAL DECODING WITH LINGUISTIC PATTERNS	179875071	emplates is time-consuming and the scalability issue, data-driven approaches have been investigated for open-domain NLG tasks. Recent advances in recurrent neural networkbased language model (RNNLM) (Mikolov et al., 2010,2011) have demonstrated the capability of modeling long-term dependency by leveraging RNN structure. Previous work proposed an RNNLM-based NLG (Wen et al.,2015) that can be trained on any corpus of d
2804531841	Self-Attention-Based Message-Relevant Response Generation for Neural Conversation Model.	2626778328	tep. duced in the next subsection. 3.4 Self-Attention-Based Response Generation Self-attention is a special case of the attention mechanism, which is modeled to learn dependencies in a word sequence (Vaswani et al., 2017; Shen et al., 2017a). Such a self-attention is usually used for sentence representation which abstractssentence-level meanings. Speciﬁcally, multiplicative self-attention isan attention mechanism to
2804581867	Annotating Electronic Medical Records for Question Answering.	2171278097	context (i.e. panel within summarization or note ID of last informative note reviewed). To ensure that the questions generated would be representative of the topics most of interest to physicians, we (1) provide to the annotators a list of the general types of questions asked by physicians on EMRs, and (2) include standard questions on common topics that were asked on every patient EMR (Table 1). Tab
2804581867	Annotating Electronic Medical Records for Question Answering.	2252136820	EMR, and then discuss the inter-annotator reliability and factors leading to annotator disagreement. (3) We present an analysis of the dataset created, as well as some limitations of our methodology. (4) Our methodology is replicable, can be conducted by medical students as annotators, and results in high inter-annotator agreement (0.71 Cohen’s κ). (5) Eleven medical students created a question answe
2804581867	Annotating Electronic Medical Records for Question Answering.	2251673953	ided into two stages i.e. question generation and answer generation. Question Generation The question generation process is designed to generate questions that (1) minimize bias in question language, (2) are representative of what physicians would ask of a patient’s EMR in a clinical setting, and (3) cover most common topics of interest to physicians. One important concern is that the language in the
2804581867	Annotating Electronic Medical Records for Question Answering.	2252136820	ike “Does the patient have any neurological symptoms?” may require information from both the EMR as well as external medical resources to provide a complete answer. We do not generate such questions. (4) Brat allows us to only view a single clinical note at a time. Thus, it does not support the annotation of cross-document relations and restricts the marking of related answer spans to a single note.
2804581867	Annotating Electronic Medical Records for Question Answering.	2250225488	as well as some limitations of our methodology. (4) Our methodology is replicable, can be conducted by medical students as annotators, and results in high inter-annotator agreement (0.71 Cohen’s κ). (5) Eleven medical students created a question answering dataset of 5696 questions over 71 patient records following our annotation methodology. Of these questions, 1747 have corresponding answers genera
2804581867	Annotating Electronic Medical Records for Question Answering.	2251673953	he questions generated would be representative of the topics most of interest to physicians, we (1) provide to the annotators a list of the general types of questions asked by physicians on EMRs, and (2) include standard questions on common topics that were asked on every patient EMR (Table 1). Table 1. List of common question topics and their counts within the VA Dataset of 976 questions, along with
2804581867	Annotating Electronic Medical Records for Question Answering.	2171278097	a real setting. We try to minimize this bias by not allowing the annotators to read through the entire EMR during question generation. Instead, they are presented with the following starting points: (1) The Watson Electronic Medical Record Analysis (EMRA) summarization user interface (Figure 1). This is a dashboard that provides a view of automatically generated problem lists, structured medications
2804581867	Annotating Electronic Medical Records for Question Answering.	2171278097	setting, and we also defined a process for annotating corresponding answers from both structured and unstructured parts of the patient&apos;s EMR. The main contributions of this paper are as follows: (1) We outline a replicable annotation process, with minimal annotator bias, for generating questions similar to what a physician may want to ask about a patient. (2) We also outline a replicable annotat
2804581867	Annotating Electronic Medical Records for Question Answering.	2251673953	With this as our starting point, we found in our earlier pilots that there were essentially two types of disagreements: (1) actual disagreements due to differences in what is considered an answer and (2) apparent disagreements that were simply answers overlooked by one of the two annotators due to differences in their method of searching and navigating through EMRs. Two examples of the first type of
2804581867	Annotating Electronic Medical Records for Question Answering.	2171278097	tion, an individual browses and/or searches through the EMR to find the answer. With this as our starting point, we found in our earlier pilots that there were essentially two types of disagreements: (1) actual disagreements due to differences in what is considered an answer and (2) apparent disagreements that were simply answers overlooked by one of the two annotators due to differences in their met
2804581867	Annotating Electronic Medical Records for Question Answering.	2171278097	training. The annotation process is divided into two stages i.e. question generation and answer generation. Question Generation The question generation process is designed to generate questions that (1) minimize bias in question language, (2) are representative of what physicians would ask of a patient’s EMR in a clinical setting, and (3) cover most common topics of interest to physicians. One impor
2804581867	Annotating Electronic Medical Records for Question Answering.	2250225488	ument relations and restricts the marking of related answer spans to a single note. While there may be ways to work around this, annotating relations across a longitudinal EMR is a very tedious task. (5) Simulated clinical setting. Medical student annotators simulate the process of a physician asking patientspecific questions in a real setting where he is treating a patient. In this simulated setting
2804581867	Annotating Electronic Medical Records for Question Answering.	2251673953	on user interface (Figure 1). This is a dashboard that provides a view of automatically generated problem lists, structured medications, labs, procedures, and a view of clinical notes over time.20,21 (2) The latest clinical note or any well formed clinical note (that has a reason for visit, assessment and plan). Figure 1. Screenshot of Watson EMRA summarization UI. During review of Watson EMRA summar
2804581867	Annotating Electronic Medical Records for Question Answering.	2251673953	utions of this paper are as follows: (1) We outline a replicable annotation process, with minimal annotator bias, for generating questions similar to what a physician may want to ask about a patient. (2) We also outline a replicable annotation process for marking answers to the generated questions in the EMR, and then discuss the inter-annotator reliability and factors leading to annotator disagreeme
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2767224889	assiﬁcation and translation. 2 Unsupervised Learning of the Speech Embedding Space Recently, there is an increasing interest in learning the semantics of a language directly, and only from raw speech [24, 27, 28]. Assuming utterances in a speech corpus are already pre-segmented into audio segments corresponding to words using word boundaries obtained by forced alignment, existing approaches aim to represent e
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2296681920,2336585117	as another form of language, is rarely considered as a source for learning semantics, compared to text. Although there is work that explores the concept of learning vector representations from speech [18, 19, 20, 21, 22, 23], they are primarily based on acoustic-phonetic similarity, and aim to represent the way a word sounds rather than its meaning. Recently, the Speech2Vec [24] model was developed to be capable of repre
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2493916176	ic gradient descent (SGD) with a ﬁxed learning rate of 10−3. The text embeddings were obtained by training Word2Vec on the transcriptions using the fastText implementation without subword information [3]. The dimension of both speech and text embeddingsis 50.1 Fortheadversarialtraining,thediscriminatorwasatwo-layerneuralnetworkofsize512withReLU as the activation function. Both the discriminatorand W
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2130942839	le-length input 2 and output sequences of acoustic features, Speech2Vec replaces the two fully-connected layers in the Word2Vec model with a pair of RNNs, one as an Encoder and the other as a Decoder [25, 26]. When training Speech2Vec with Skip-grams, the Encoder RNN takes the audio segment (corresponding to the current word) as input and encodes it into a ﬁxed dimensional embedding z(n) that represents t
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2143612262	ors of many points in high-dimensional spaces). Subsequently, we apply Equation 1 on this generated dictionaryto reﬁne W. 4 4 Spoken Word Classiﬁcation and Translation Conventional hybrid ASR systems [40] and recent end-to-end ASR models [41, 42, 43, 44] rely on a large amount of parallel audio-text data for training. However, most languages spoken across the worldlackparalleldata,so it is nosurpriset
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2102113734	spaces). Subsequently, we apply Equation 1 on this generated dictionaryto reﬁne W. 4 4 Spoken Word Classiﬁcation and Translation Conventional hybrid ASR systems [40] and recent end-to-end ASR models [41, 42, 43, 44] rely on a large amount of parallel audio-text data for training. However, most languages spoken across the worldlackparalleldata,so it is nosurprisethat onlyveryfewlanguagessupportASR. It is thesame
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2493916176	troduction Word embeddings—continuous-valued vector representations of words—are almost ubiquitous in recent natural language processing research. Most successful methods for learning word embeddings [1, 2, 3] rely on the distributionalhypothesis[4], i.e., wordsoccurringin similar contextstend to have similar meanings. Exploiting word co-occurrence statistics in a text corpus leads to word vectorsthat reﬂe
2804648901	Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	2150622104	udio-text data for training. However, most languages spoken across the worldlackparalleldata,so it is nosurprisethat onlyveryfewlanguagessupportASR. It is thesame story for speech-to-text translation [45], which typically pipelines ASR and machine translation, and could be even more challenging to develop as it requires both components to be well trained. Compared to parallel audio-text data, the cost
2804753883	Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data.	2134738818	has been applied to analyze people’s reactions to advertisements, thus creating better neuromarketing campaigns [2]. It has also gained in popularity amongst various other domains such as healthcare [3], customer service, or gaming. However, effective emotion recognition still remains a challenging task, due to the sheer complexity of generalizing human emotions. For example, individuals express and
2804753883	Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data.	175750906	ata with high-dimensional feature spaces like images, similar efforts to capture complex feature space of emotional data have also shown promising results with several emotion databases such as EmoDB [8] or IEMOCAP [9]. Unfortunately, human emotion in real-life is often expressed through complex combination of multiple modes of expression, and a lot of information is lost by employing unimodal analys
2804753883	Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data.	2038256537	le’s actions and thoughts, but also is a fundamental part of human communication. As such, emotion recognition technology has become growingly important in improving how humans interact with machines [1]. For instance, emotion recognition has been applied to analyze people’s reactions to advertisements, thus creating better neuromarketing campaigns [2]. It has also gained in popularity amongst variou
2804753883	Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data.	2134738818	nnotations consist of six emotion indexes: sadness (2843), angry (6794), happy (10028), disgust(1845), surprise(349), fear(817) with value ranges of [0,4.6], and sentiment label with a value range of [-3,3]. The dataset is organized byvideo ID sand corresponding segmentwith six emotion and sentiment labels. Video IDs are then furthersplit intosegments. The training set consists 3303 video ID and 23453 s
2804753883	Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data.	2610961739	rakis et al. uses deep residual networks to extract features from facial expressions, convolutional neural networks to extract features from speech, and concatenates them to input into a LSTM network [10]. Work of Ranganathan et al. uses deep believe networks on facial expressions, body expressions, vocal expressions, and physiological signals [11]. Inspired by these approaches, we suggest a new appro
2804771341	A Study on Dialog Act Recognition Using Character-Level Tokenization	1888026739	3]. The article by Kral and Cerisara [10] provides an interesting overview of most of those approaches on the task. However, recently, similarly to many other Natural Language Processing (NLP) tasks [13,4], most approaches on dialog act recognition take advantage of dierent DNN architectures. To our knowledge, the rst of those approaches was that by Kalchbrenner and Blunsom [8]. They used a Convolutio
2804771341	A Study on Dialog Act Recognition Using Character-Level Tokenization	2128970689	English, with variable domain, containing 223,606 segments. The set is partitioned into a training set of 1,115 conversations, a test set of 19 conversations, and a future use set of 21 conversations [22]. In our experiments, we used the latter as a validation set. In Character-Level Dialog Act Recognition 5 terms of dialog act annotations, we used the most used version of its tag set, which features
2804771341	A Study on Dialog Act Recognition Using Character-Level Tokenization	1539256681	hat has been widely explored over the years, using multiple machine learning approaches, from Hidden Markov Models (HMMs) [22] to Support Vector Machines (SVMs) [3]. The article by Kral and Cerisara [10] provides an interesting overview of most of those approaches on the task. However, recently, similarly to many other Natural Language Processing (NLP) tasks [13,4], most approaches on dialog act reco
2804771341	A Study on Dialog Act Recognition Using Character-Level Tokenization	2128970689	s for future work. 2 Related Work Automatic dialog act recognition is a task that has been widely explored over the years, using multiple machine learning approaches, from Hidden Markov Models (HMMs) [22] to Support Vector Machines (SVMs) [3]. The article by Kral and Cerisara [10] provides an interesting overview of most of those approaches on the task. However, recently, similarly to many other Natu
2804818790	SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings.	2608256743	e deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBCcorpus (Han et al.,2013). Moreover, in order to perform word sense induction and disambiguation, the word embedding could be transformed t
2804818790	SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings.	2072692647	language processing (NLP) tasks, especially those semantically intensive ones aiming for inference and reasoning with generalization capability, such as question answering (Harabagiu and Hickl, 2006; Yahya et al., 2013) and textual entailment (Dagan et al., 2013; Roller and Erk, 2016), can beneﬁt from identifying semantic relations between words beyond synonymy. The hypernym discovery task (Camacho-Collados et al.,
2804818790	SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings.	2340486923	lters. RCNN Since some input terms are phrases, whose member words share different weights. In RCNN, an adaptive gated decay mechanism is used to weight the words in the convolution layer. Following (Lei et al., 2016), we introduce neural gatessimilarλtoLSTMstospecifywhenandhow to average the observed signals. The resulting architecture integrates recurrent networks with nonconsecutive convolutions: λ =σ(Wλxt +Uλh
2804818790	SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings.	2112644606	n Table 1. Various natural language processing (NLP) tasks, especially those semantically intensive ones aiming for inference and reasoning with generalization capability, such as question answering (Harabagiu and Hickl, 2006; Yahya et al., 2013) and textual entailment (Dagan et al., 2013; Roller and Erk, 2016), can beneﬁt from identifying semantic relations between words beyond synonymy. The hypernym discovery task (Cama
2804818790	SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings.	2436788615	rank the hypernym candidates for given terms. 2.1 Embedding To use deep neural networks, symbolic data needs to be transformed into distributed representations(Wang et al., 2016a; Qin et al., 2016b; Cai and Zhao, 2016; Zhang et al., 2016; Wang et al., 2016b, 2015; Cai et al., 2017). We use Glove toolkit to train the word embeddings using UMBCcorpus (Han et al.,2013). Moreover, in order to perform word sense induct
2804862876	Style Obfuscation by Invariance	2130942839	al representations of the base architecture. 3.1 Architecture Components In addition to the architecture described above, we introduce a few extra components: Gradient Reversal Layer ( GRL ) The GRL (Sutskever et al., 2014) is applied on top of the encoder output. During the forward pass the GRL is the identity function which feeds input to a shallow MultiLayer Perceptron (MLP) style classier. However, during back-propa
2804862876	Style Obfuscation by Invariance	2033696334	ated studies can be found in (Caliskan-Islam et al., 2015; Le et al., 2015). Brennan et al. (2012) explicitly frame obfuscation as an adversarial task and use MT (round-trip translation), similar to (Caliskan and Greenstadt, 2012). Rule-based perturbations (Juola and Vescovi, 2011) and mixtures of both (Karadzhov et al., 2017) have also been applied for fully automatic obfuscation. Closest to our approach is recent work by She
2804862876	Style Obfuscation by Invariance	9292421	attribute often play a signicant role in the accuracy of a potential adversary. There is ample evidence for this phenomenon in age and gender classication work (Koppel et al., 2002; Rao et al., 2010; Burger et al., 2011; Sap et al., 2014, inter alia). Taking examples from Sap et al. (2014) specically, features with strong coefcient weights for gender include e.g. boxers, shaved, girlfriend, beard, ghtin for males, a
2804862876	Style Obfuscation by Invariance	2084708974	corpus of styles (be it attributes or authors) would perform obfuscation-by-transfer. Moreover, it would likely largely preserve the original meaning as translation is a meaning-preserving operation (Ide et al., 2002; Dyvik, 2004). However, such parallel corpora are generally not available and have very high associated compilation costs, as it would require large amounts of identical information (ideally on sente
2804862876	Style Obfuscation by Invariance	2017729405	d to a particular attribute often play a signicant role in the accuracy of a potential adversary. There is ample evidence for this phenomenon in age and gender classication work (Koppel et al., 2002; Rao et al., 2010; Burger et al., 2011; Sap et al., 2014, inter alia). Taking examples from Sap et al. (2014) specically, features with strong coefcient weights for gender include e.g. boxers, shaved, girlfriend, bear
2804862876	Style Obfuscation by Invariance	2144364794	demonstrated that such stylometric features can be used to accurately infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work
2804862876	Style Obfuscation by Invariance	2250238316	infer an extensive set of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-rep
2804862876	Style Obfuscation by Invariance	2735456298	itly frame obfuscation as an adversarial task and use MT (round-trip translation), similar to (Caliskan and Greenstadt, 2012). Rule-based perturbations (Juola and Vescovi, 2011) and mixtures of both (Karadzhov et al., 2017) have also been applied for fully automatic obfuscation. Closest to our approach is recent work by Shetty et al. (2017), who pursue the task of learning obfuscation-by-transfer using a Generative Adve
2804862876	Style Obfuscation by Invariance	2095705004	ll model parameters are optimized with Adam (Kingma and Ba, 2014) in mini-batches of 50 examples and a learning rate of 0.001 which is decreased by 0.75 after each epoch. To avoid overtting, dropout (Srivastava et al., 2014) is applied to the input of each LSTM layer with a dropping probability of 0.25 and we stop training when loss stops decreasing for 3 epochs. During test time, we approximate the model best output seq
2804862876	Style Obfuscation by Invariance	2134414769	metry might be used to compromise anonymity was rst explored by Raoetal.(2000). Theysawpotentialtoconcealstyleinformationinmachinetranslation(MT),butnoted that it was not powerful enough at the time. Kacmarcik and Gamon (2006) continued the proposed line of work by informing users regarding characteristic features and deeper linguistic cues in their writing style. Recent related studies can be found in (Caliskan-Islam et a
2804862876	Style Obfuscation by Invariance	2101105183	ontent words for their contrastive variant ( wife to husband ,school to wedding ). Whensuchvariantsarealsocloseinsemanticspacesthatsequencemodels make use of, any reconstruction metricssuch as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), embedding distances, etc.might become an inaccurate indication of the change in meaning. Our contributions In this work we propose a different approach to automat
2804862876	Style Obfuscation by Invariance	2101105183	per/bible_databases 4 http://www.openbible.info/labs/cross-references/ 5 https://spacy.io/ 6 https://github.com/facebookresearch/fastText MT Metrics ( BLEU , METEOR ) We calculate BLEU -4 and METEOR (Papineni et al., 2002; Banerjee and Lavie, 2005) using nlg-eval 7 . Given that this is not a standard MT task, we provide these scores between the generated sentence and the source sentence ( ), as well as the generated s
2804862876	Style Obfuscation by Invariance	2130942839	performance for a number of language tasks, including generation. These results bode well for its application to obfuscation-by-invariance. 3 Models Our base architecture is a neural encoder-decoder (Sutskever et al., 2014) model similar to that of (Wu et al., 2016), implemented in PyTorch (Paszke et al., 2017). Given an input sequence of one-hot encoded words, the encoder rst embeds the words into dense vectors which a
2804862876	Style Obfuscation by Invariance	1427965619	the proposed line of work by informing users regarding characteristic features and deeper linguistic cues in their writing style. Recent related studies can be found in (Caliskan-Islam et al., 2015; Le et al., 2015). Brennan et al. (2012) explicitly frame obfuscation as an adversarial task and use MT (round-trip translation), similar to (Caliskan and Greenstadt, 2012). Rule-based perturbations (Juola and Vescovi
2804862876	Style Obfuscation by Invariance	1882958252	ranslation between many different versions of the English Bible. 2.2 Gradient Reversal The use of a Gradient Reversal Layer (GRL) for learning domain invariant feature representations was proposed by Ganin and Lempitsky (2015), who demonstrated its viability for learning lightningcondition invariance in computer vision. Since then, it has been applied to several language tasks: e.g. textual feature extraction (Pryzant et a
2804862876	Style Obfuscation by Invariance	2251812186	a signicant role in the accuracy of a potential adversary. There is ample evidence for this phenomenon in age and gender classication work (Koppel et al., 2002; Rao et al., 2010; Burger et al., 2011; Sap et al., 2014, inter alia). Taking examples from Sap et al. (2014) specically, features with strong coefcient weights for gender include e.g. boxers, shaved, girlfriend, beard, ghtin for males, and purse, blueberr
2804862876	Style Obfuscation by Invariance	658020064	strongly decrease when adding the GRL. Word Mover&apos;s Distance (WMD) To assess the word embedding distance of the obfuscated sentence to the original, we take the Word Mover&apos;s Distance (WMD) (Kusner et al., 2015), based on the English fastText embeddings for Wikipedia (Bojanowski et al., 2016). WMD takes the distance between two sentences in a weighted point cloud of embedded words as the minimum cumulative d
2804862876	Style Obfuscation by Invariance	2466778245	t of personal information, such as age, gender, education, socio-economic status, and mental health issues (Eisenstein et al., 2011; Alowibdi et al., 2013; Volkova et al., 2014; Plank and Hovy, 2015; Volkova and Bachrach, 2016). Traditionally, these techniques relied on expensive human-labelled examples; however, more recent work has demonstrated near equal accuracy when only relying on self-reports as a distant supervision
2804862876	Style Obfuscation by Invariance	105812634	tant supervision signal (Beller et al., 2014; Emmery et al., 2017; Yates et al., 2017). While these efforts have been greatly benecial to various research elds such as computational sociolinguistics (Daelemans, 2013), the resulting techniques potentially expose users of such media to directed attacks where this information can be abused unbeknownst to them. This is particularly harmful to individuals in a vulnera
2804862876	Style Obfuscation by Invariance	2165431734	in terms of meaning preservation and grammaticality. 1 Introduction The fact that writing style uniquely characterizes a person, and can be leveraged for automatic author identication (Holmes, 1998; Stamatatos et al., 2000), has been well-studied in the eld of (computational) stylometry (Neal et al., 2017). Similarly, work on author proling (Koppel et al., 2002) has demonstrated that such stylometric features can be use
2804862876	Style Obfuscation by Invariance	1882958252	test our hypothesis in several experimental conditions. 2 The main component in our encoder-decoder architecture to achieve a style-invariant encoding of the input is a Gradient Reversal Layer (GRL) (Ganin and Lempitsky, 2015) inserted between the input sentence embedding and the style classier. We investigate the effect of this module in isolation, as well as in a style-invariant soft transfer setting by using a condition
2804862876	Style Obfuscation by Invariance	2123301721	ttp://www.openbible.info/labs/cross-references/ 5 https://spacy.io/ 6 https://github.com/facebookresearch/fastText MT Metrics ( BLEU , METEOR ) We calculate BLEU -4 and METEOR (Papineni et al., 2002; Banerjee and Lavie, 2005) using nlg-eval 7 . Given that this is not a standard MT task, we provide these scores between the generated sentence and the source sentence ( ), as well as the generated sentence and the target sent
2804862876	Style Obfuscation by Invariance	2735456298	variable. The main challenge is to preserve the original meaning of the input, whilst hiding the act of obfuscation (Potthast et al., 2016). Recent work on automatic obfuscation (Shetty et al., 2017; Karadzhov et al., 2017) shows promising results in minimizing performance of the adversary; however, these models (and as noted by the authors) struggle with correctly maintaining the semantic content of theinput. Toillustr
2804862876	Style Obfuscation by Invariance	2123301721	ve variant ( wife to husband ,school to wedding ). Whensuchvariantsarealsocloseinsemanticspacesthatsequencemodels make use of, any reconstruction metricssuch as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), embedding distances, etc.might become an inaccurate indication of the change in meaning. Our contributions In this work we propose a different approach to automatic obfuscation that we hypothesize
2804862876	Style Obfuscation by Invariance	2611862713	computer vision. Since then, it has been applied to several language tasks: e.g. textual feature extraction (Pryzant et al., 2017), POS tagging (Kim et al., 2017; Gui et al., 2017), image captioning (Chen et al., 2017), and document classication (Liu et al., 2017; Xu and Yang, 2017). Most importantly, Xie et al. (2017) demonstrate the GRL module can be used to implement an adversarial setting, and to improve perfor
2804862876	Style Obfuscation by Invariance	2119804197	of work by informing users regarding characteristic features and deeper linguistic cues in their writing style. Recent related studies can be found in (Caliskan-Islam et al., 2015; Le et al., 2015). Brennan et al. (2012) explicitly frame obfuscation as an adversarial task and use MT (round-trip translation), similar to (Caliskan and Greenstadt, 2012). Rule-based perturbations (Juola and Vescovi, 2011) and mixtures of
2805039358	Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task.	2123946480	complex non-factoid questions such as deﬁnition questions inTREC2003 (Voorhees,2003)and “other” questions in TREC 2007 (Dang et al., 2007), manual assessment becomes moredifﬁcult. “Nugget pyramids” (Nenkova et al., 2007) are employed for scoring, which prefer answers coveraging more key points (nuggets). The nuggets are annotated and weighted by human assessors, which is labor-intensive. Breck et al. (2000) proposed
2805039358	Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task.	2123301721	GE (Dang et al., 2007) are most widely-used. In general, BLEUfocuses moreon n-gramprecision and ROUGE is recall-oriented. Later work has made adaptations on these metrics from different perspectives (Banerjee and Lavie, 2005; Liu and Liu, 2008). In this paper, our adaptations are aimed at increasing their correlation to real human judgment on yes-no and entity question answering, which are proved to be practical. 3 Metho
2805039358	Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task.	2125436846	l., 2016; Wang et al., 2017). Although MRC model architectures have been intensively studied, the evaluation metrics for them are rarely discussed. For early cloze-style and multiple choice datasets (Richardson et al., 2013; Hermann et al., 2015), this may not be problematic. However, considering the trend that the model is required to generate answers and question type is becoming more variable and closer to real cases
2805039358	Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task.	2516930406	passages and manually annotated answers for each question is more close to real application. Based on these resources, end-to-end neural MRC model architectures are implemented, including match-LSTM (Wang and Jiang, 2016), BiDAF (Seo et al.,2016),DCN(Xiong et al.,2016) and rnet (Wang et al.,2017). With the objective of lexical overlap based evaluation metrics, these models focus more on text matching to references, wh
2805039358	Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task.	2516930406	Withtherelease ofseveral large-scale datasets like SQuAD (Rajpurkar et al., 2016), MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017), many MRC models have been proposed in previous works (Wang and Jiang, 2016; Seo et al., 2016; Wang et al., 2017). Although MRC model architectures have been intensively studied, the evaluation metrics for them are rarely discussed. For early cloze-style and multiple choice
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2139645402	ahiye’. This sentence contains words in English such as ‘Dear’, ‘sir’ and words in Hindi such as ‘hai’, ‘bina’, etc. which are transliterated to English. Several code-mixed datasets have been created [5],[6] for dierent NLP tasks but no opinion mining experiment has been performed on English-Hindi codemixed data. Therefore, we aim to provide an English-Hindi code-mixed dataset and perform an experim
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2347127863	We can say that the author if this tweet is most likely to be against the target demonetisation. There have been several experiments in the eld of opinion mining on social media and online texts [2],[3]. Opinion mining can provide a lot of information about the texts present in social media and can benet many other tasks such as information retrieval, text summarization, etc. arXiv:1805.11868v1 [cs
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2347127863	en treated as a separate token. 6 3.2 Features Character N-grams Character n-gram refers to presence or absence of contiguous sequence of n characters in the tweet. It can be seen from previous works [3],[11] that character level features have a signicant eect on stance detection. We extract character n-grams for all values of n between 1 and 3. Including all the n-grams increases the size of featu
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2469195965	the language tags and consider them in the feature vector. The threshold value for scores and number of occurrences has been decided after empirical ne tuning. 3.3 Feature Selection Previous studies [10],[11] have shown that feature selection algorithms improve eciency and accuracy of classication systems. We use chi square feature selection algorithm which uses chi-squared statistic to evaluate in
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2584256393	N-grams Word n-gram refers to presence or absence of contiguous sequence of n words or tokens in the tweet. Word n-grams have proven to be important features for stance detection in previous studies [2],[3],[11]. We extract word n-grams for all values of n between 1 and 5. We include only those n-grams in our feature vector which occur at least 10 times in the dataset. Stance Indicative Tokens This
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2584256393	ne’. We can say that the author if this tweet is most likely to be against the target demonetisation. There have been several experiments in the eld of opinion mining on social media and online texts [2],[3]. Opinion mining can provide a lot of information about the texts present in social media and can benet many other tasks such as information retrieval, text summarization, etc. arXiv:1805.11868v1
2805163984	An English-Hindi Code-Mixed Corpus: Stance Annotation and Baseline System.	2347127863	rams Word n-gram refers to presence or absence of contiguous sequence of n words or tokens in the tweet. Word n-grams have proven to be important features for stance detection in previous studies [2],[3],[11]. We extract word n-grams for all values of n between 1 and 5. We include only those n-grams in our feature vector which occur at least 10 times in the dataset. Stance Indicative Tokens This feat
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	1580375566	attention for alignment in NMT systems. Similar mechanisms have recently been applied in other NLP tasks like machine reading and parsing [2, 30]. Early active learning for sequence labeling research [7, 27] employ least confidence (LC) sampling strategies. Settles and Craven made a thorough analysis over other strategies and proposed their entropy based strategies in [28]. However, the sampling strategy
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2064675550	the best possible tag decision, attribute values are extracted and compared with ground truth. 5.3 Performance: Attribute Value Extraction Baselines.The first baseline we consider is the BiLSTM model [10]. The second one is the state-of-the-art sequence tagging model for named entity recognition (NER) tasks using BiLSTM and CRF [11, 1Note that we do not do any hyper-parameter tuning. Most default para
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2250539671	BOW) features, word embeddings capture both syntactic and semantic information with low-dimensional and dense word representations. The most popular tools for this purpose are Word2Vec [19] and GloVe [22], which are trained over large unlabeled corpus. Pretrained embeddings have a single representation for each token. This does not serve our purpose as the same word can have a different representation
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2626778328	on of its decisions. Bahdanau et al. [1] successfully applied attention for alignment in NMT systems. Similar mechanisms have recently been applied in other NLP tasks like machine reading and parsing [2, 30]. Early active learning for sequence labeling research [7, 27] employ least confidence (LC) sampling strategies. Settles and Craven made a thorough analysis over other strategies and proposed their en
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2133564696	e in “low resolution” and then adjusting the focal point over time. In the Natural Language Processing domain, attention mechanism has been used with great success in Neural Machine Translation (NMT) [1]. NMT systems comprise of a sequence-to-sequence encoder and decoder. Semantics of a sentence is mapped into a fixed-length vector representation by an encoder, and then the translation is generated b
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	1522301498	ed to support variable length input. We set the hidden size of LSTM to 100, which generates a 200 dimensional output vector for BiLSTM after concatenation. The dropout rate is set to 0.4. We use Adam [12] for parameter optimization with a batch size of 32. We train the models for 500 epochs, and report the averaged evaluation measures for the last 20 epochs. 5.2 Data Sets We perform experiments in 3 d
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2158899491	hand-crafted features, and uses active learning to reduce burden of annotation. Early attempts include [9, 23], which apply feed-forward neural networks (FFNN) and LSTM to NER tasks. Collobert et al. [5] combine deep FFNN and word embedding [19] to explore many NLP tasks including POS tagging, chunking and NER. Character-level CNNs were integrated [26] to augment feature representation, and their mod
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	172829878,1969221592	ize), “Fillet Mignon” (flavor) and “Porterhouse Steak” (flavor). Challenges. This problem presents the following challenges. Open World Assumption (OWA). Previous works for attribute value extraction [8, 16, 24, 25] work with a closed world assumption which uses a limited and pre-defined vocabulary of attribute values. Therefore, these cannot discover emerging attribute values (e.g., new brands) of newly launche
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	70399244	ke use of domain-specific vocabulary or dictionary to spot key phrases and attributes. These suffer from limited coverage and closed world assumptions. Similarly, rule-based and linguistic approaches [3, 18] leveraging syntactic structure of sentences to extract dependency relations do not work well on irregular structures like titles. An NER system was built [25] to annotate brands in product listings o
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2250539671	low, where some basic layers are brought from Keras.1. We run our experiments within docker containers on a 72-core machine powered by Ubuntu Linux. Weuse100-dimensionalpre-trainedwordvectorsfromGloVe[22] for initializing our word embeddings that are optimized during training. Embeddings for words not in GloVe are randomly initialized and re-trained. Masking is adopted to support variable length input
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	70399244	re often multi-word phrases like “Fillet Mignon” and “Porterhouse Steak”. Lack of regular grammatical structure renders NLP tools like parsers, part-of-speech (POS) taggers, and rule-based annotators [3, 18] less useful. Additionally, they also have a very sparse context. For instance, over 75% of product titles in our dataset contain fewer than 15 words whereas over 60% bullets in descriptions contain f
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2516936824	roducts. Comparing results of SVM, MaxEnt, and CRF, they found CRF to perform the best. They used seed dictionaries containing over 6,000known brands for bootstrapping. A similar NER system was built [20] to tag brands in product titles leveraging existing brand values. In contrast to these, we do not use any dictionaries for bootstrapping, and can discover new values. There has been quite a few works
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	172829878	rule-based and linguistic approaches [3, 18] leveraging syntactic structure of sentences to extract dependency relations do not work well on irregular structures like titles. An NER system was built [25] to annotate brands in product listings of apparel products. Comparing results of SVM, MaxEnt, and CRF, they found CRF to perform the best. They used seed dictionaries containing over 6,000known brand
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2171671120	to select the most informative instances to improve the active learner. As our baseline strategy, we consider the method of least confidence (LC) [6] which is shown to perform quite well in practice [28]. It selects the sample for which the classifier is least confident. In our sequence tagging task, the confidence of the CRF in tagging an input sequence is given by the conditional probability in Equ
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2171671120	r sequence labeling research [7, 27] employ least confidence (LC) sampling strategies. Settles and Craven made a thorough analysis over other strategies and proposed their entropy based strategies in [28]. However, the sampling strategy of OpenTag is different from them. 7 CONCLUSIONS We presented OpenTag — an end-to-end tagging model leveraging BiLSTM, CRF and Attention — for imputation of missing at
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2133564696	STM-CRF models as above is state-of-the-art for NER. Unlike prior works, OpenTag uses attention to improve feature representation and gives interpretable explanation of its decisions. Bahdanau et al. [1] successfully applied attention for alignment in NMT systems. Similar mechanisms have recently been applied in other NLP tasks like machine reading and parsing [2, 30]. Early active learning for seque
2805173585	OpenTag: Open Attribute Value Extraction from Product Profiles	2064675550	traditionally used to identify attributes like names of persons, organizations, and locations from unstructured text. We leverage recurrent neural networks like Long Short Term Memory Networks (LSTM) [10] to capture the semanticsandcontext of attributes through distributed word representations. LSTM’s are a natural fit to this problem because of their ability to handle sparse context and sequential na
2805389643	LexNLP: Natural Language Processing and Information Extraction For Legal and Regulatory Texts	2131744502	/consulting, and underwriting agreements. These models have been available since October 2017. ‹ doc2vec contract models: LexNLP has been used to produce large doc2vec models from SEC EDGAR material ([16]). These models are scheduled for release and distribution in the 0.1.10 release, along with a forthcoming academic article. ‹ doc2vec opinion models: LexNLP has been used to produce large doc2vec mod
2805389643	LexNLP: Natural Language Processing and Information Extraction For Legal and Regulatory Texts	2101234009	ations in languages such as Java, Python, and R that stand on the shoulders of comprehensive, well-tested libraries like Stanford NLP ([1]), OpenNLP ([2]), NLTK ([3]), spaCy ([4]), scikit-learn ([5], [6]), Weka ([7]), and gensim ([8]). Consequently, for most domains, the rate of research has increased and the cost of application development has decreased. For some specialized areas like medicine and
2805389643	LexNLP: Natural Language Processing and Information Extraction For Legal and Regulatory Texts	2133990480	nguages such as Java, Python, and R that stand on the shoulders of comprehensive, well-tested libraries like Stanford NLP ([1]), OpenNLP ([2]), NLTK ([3]), spaCy ([4]), scikit-learn ([5], [6]), Weka ([7]), and gensim ([8]). Consequently, for most domains, the rate of research has increased and the cost of application development has decreased. For some specialized areas like medicine and marketing, t
2805389643	LexNLP: Natural Language Processing and Information Extraction For Legal and Regulatory Texts	168564468	va, Python, and R that stand on the shoulders of comprehensive, well-tested libraries like Stanford NLP ([1]), OpenNLP ([2]), NLTK ([3]), spaCy ([4]), scikit-learn ([5], [6]), Weka ([7]), and gensim ([8]). Consequently, for most domains, the rate of research has increased and the cost of application development has decreased. For some specialized areas like medicine and marketing, there are focused l
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2750588180	, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be eithe
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2099471712	.4 Adversarial Domain Adaptation and Domain Generation Generative adversarial networks are a class of artiﬁcial intelligence algorithms used in unsupervised machine learning, which are introduced by (Goodfellow et al., 2014). Adversarial methods have become popular in domain adaptation (Ganin et al., 2016), which minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2133622676	&apos;1$++%/-&apos;3-$./8 Figure 1: Overview of domain adaptation for NMT. 2016; Marie and Fujita, 2017). Model centric methods interpolate in-domain and out-of-domain models in either a model level (Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016) or an instance level (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2010; Rousseau et al., 2011; Zhou et al., 2015). However, due to the d
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2130942839	per (Section 7). 2 Neural Machine Translation NMT is an end-to-end approach for translating from one language to another, which relies on deep learning to train a translation model (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). The encoder-decoder model with attention (Bahdanau et al., 2015) is the most commonly used NMT architecture. This model is also known as RNNsearch. Figure 2 describes the RNN
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2550821151	6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as crosslingual transfer learning, which transfers NMT model parameters among multiple languages. It is known that a multilingual model, which relies on parameter sharing, hel
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2395579298	7); however, they focus on general NMT and more diverse topics. Domain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the ﬁrst comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT, we categorize domain
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2100664567	al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et al., 2015). For instancelevel interpolation, the most related method is to assign a weight in NMT objective function (Chen et al., 2017a; Wang et al., 2017b). However, the model structures of SMT and NMT are qu
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2111362445	al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They ﬁrstly score each instance/domain by using rules or statistical method
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2592864539	ategorizes all approaches. As such a study will greatly beneﬁt the community, we present in this paper a survey of all prominent domain adaptation techniques for NMT. There are survey papers for NMT (Neubig, 2017; Koehn, 2017); however, they focus on general NMT and more diverse topics. Domain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1905522558	to two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This wor
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2147262247	a centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed unde
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2579496717	Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and thus it is important to leverage the spoken language domain data with the patent domain data (Chu et al., 2017). Furthermore, there are monolingual corpora containing millions of sentences for the spoken language domain, which can also be leveraged (Sennrich et al., 2016b). There are many studies of domain ada
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1731081199	re a class of artiﬁcial intelligence algorithms used in unsupervised machine learning, which are introduced by (Goodfellow et al., 2014). Adversarial methods have become popular in domain adaptation (Ganin et al., 2016), which minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator (Tzeng et al., 2017). They have been applied to domain adaptation ta
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2136156618	core sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; W
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2261339088	is crucial for good translation. To address this problem, a common method in SMT is to ﬁrstly classify the domains and then translate input sentences in classiﬁed domains using corresponding models (Huck et al., 2015). Xu et al. (2007) perform domain classiﬁcation for a Chinese-English translation task. The classiﬁers operate on whole documents rather than on individual sentences, using LM interpolation and vocabu
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1905522558	data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sente
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2340221426	domain. Banerjee et al. (2010) build a support vector machine classiﬁer using tf-idf features over bigrams of stemmed content words. Classiﬁcation is carried out on the level of individual sentences. Wang et al. (2012) rely on averaged perceptron classiﬁers with various phrase-based features. For NMT, Kobus et al. (2016) propose an NMT domain control method, by appending either domain tags or features to the word e
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2133622676	and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain speciﬁc translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain speciﬁc machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2116652448	e in-domain and out-of-domain models in either a model level (Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016) or an instance level (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2010; Rousseau et al., 2011; Zhou et al., 2015). However, due to the different characteristics of SMT and NMT, many methods developed for SMT cannot be applied to NMT directly. Domain adaptation for NMT i
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2604275745	etic data generation is very effective for domain adaptation using either the target side monolingual data (Sennrich et al., 2016c), the source side monolingual data (Zhang and Zong, 2016b), or both (Park et al., 2017). 4.1.3 Using Out-of-Domain Parallel Corpora With both in-domain and out-of-domain parallel corpora, it is ideal to train a mixed domain MT system that can improve in-domain translation while do not d
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2130942839	f domain adaptation for MT in the input domain unknown scenario. 6 Future Directions 6.1 Domain Adaptation for State-of-the-art NMT Architectures Since the success of RNN based NMT (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), other architectures of NMT have been developed. One representative architecture is CNN based NMT (Gehring et al., 2017). Compared to RNN based models, CNN based models can be
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2118434577	hur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain speciﬁc dictionary for terminology translation, but they simply apply the unknown word replacement method proposed by Luong et al. (2015), which suffers from noisy attention. 6.3 Multilingual and Multi-Domain Adaptation It may not always be possible to use an out-of-domain parallel corpus in the same language pair and thus it is import
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1848260265	ic methods interpolate in-domain and out-of-domain models in either a model level (Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016) or an instance level (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2010; Rousseau et al., 2011; Zhou et al., 2015). However, due to the different characteristics of SMT and NMT, many methods developed for SMT cannot be applied to NMT directly. Domain a
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2410217169	ictionaries, it will signiﬁcantly promote the practical use of MT. There are some studies that try to use dictionaries for NMT, but the usage is limited to help low frequent or rare word translation (Arthur et al., 2016; Zhang and Zong, 2016a). Arcan and Buitelaar (2017) use a domain speciﬁc dictionary for terminology translation, but they simply apply the unknown word replacement method proposed by Luong et al. (20
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2223152534	ied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They ﬁrstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the cor
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2165698076	ig, 2017; Koehn, 2017); however, they focus on general NMT and more diverse topics. Domain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the ﬁrst comprehensive survey of domain adaptation for NMT. In this paper, similar to SMT,
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2604275745	an be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models tha
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1848260265	Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They ﬁrstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each insta
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2121870595	and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014) and parallel phrase pairs (Chu, 2015; Wang et al., 2016). Most of the data centric-based methods in SMT can be directly applied to NMT. However, most of these methods adopt the criteria of data selec
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2115410424	jita, 2017). Model centric methods interpolate in-domain and out-of-domain models in either a model level (Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016) or an instance level (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2010; Rousseau et al., 2011; Zhou et al., 2015). However, due to the different characteristics of SMT and NMT, many methods developed for SMT cannot be applied to N
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1905522558	k concerning instance weighting in NMT (Wang et al., 2017b). They set a weight for the objective function, and this weight is learned from the cross-entropy by an indomain LM and an out-of-domain LM (Axelrod et al., 2011) (Figure 5). Instead of instance weighting, Chen et al. (2017a) modify the NMT cost function with a domain classiﬁer. The output probability of the domain classiﬁer is transferred into the domain weig
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2121870595	l (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ arXiv:1806.00258v1 [cs.CL]
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2147262247	of language pairs and domains, only few or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain speciﬁc translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain speciﬁc machine translation (MT) systems are in high demand whereas general purpose MT has limited application
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	1821462560	main translation after ﬁne tuning on in-domain data, Dakwale and Monz (2017) propose an extension of ﬁne tuning that keeps the distribution of the out-of-domain model based on knowledge distillation (Hinton et al., 2015). Mixed Fine Tuning This method is a combination of the multi-domain and ﬁne tuning methods (Figure 6). The training procedure is as follows: 1.Train an NMT model on out-of-domain data until convergen
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2117278770	an be mainly divided into two categories: data centric and model centric. Data centric methods focus on either selecting training data from out-of-domain parallel corpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015;
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2130942839	mainspeciﬁc translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are characte
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2166098990	models (Chen et al., 2016) can be used to score sentences. ii) When there are not enough parallel corpora, there are also studies that generate pseudo-parallel sentences using information retrieval (Utiyama and Isahara, 2003), self-enhancing (Lambert et al., 2011) or parallel word embeddings (Marie and Fujita, 2017). Besides sentence generation, there are also studies that generate monolingual n-grams (Wang et al., 2014)
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2133622676	models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2422843715	nd NMT multitask learning. Zhang and Zong (2016b) use source side monolingual data to strengthen the NMT encoder via multitask learning for predicting both translation and reordered source sentences. Cheng et al. (2016) use both source and target monolingual data for NMT through reconstructing the monolingual data by using NMT as an autoencoder. 4.1.2 Synthetic Parallel Corpora Generation As NMT itself has the abili
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2147262247	ned from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can be used to score sentences. ii) When ther
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2116652448	nstance/domain by using rules or statistical methods as a weight, and then train SMT models by giving each instance/domain the weight. An alternative way is to weight the corpora by data re-sampling (Shah et al., 2010; Rousseau et al., 2011). For NMT, several methods have been proposed to interpolate model/data like SMT does. For modellevel interpolation, the most related NMT technique is model ensemble (Jean et a
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2422843715	ntric. The data centric category focuses on the data being used rather than specialized models for domain adaptation. The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2132001515	on. Several SMT models, such as LMs, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Foster and Kuhn, 2007; Bisazza et al., 2011; Niehues and Waibel, 2012; Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to sever
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2625092622	ora are available. It has been known that both vanilla SMT and NMT perform poorly for domain speciﬁc translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain speciﬁc machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translation systems usually perform
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2579496717	os;&amp;25&gt;&amp;2=)4&apos;&apos;&amp;-2 *13%$)4*12?$-09= @$9-?&amp;3#4-.&amp;52 ($953$:3%$)4*1, @$9-?&amp;3#4-.&amp;5 (*13%$)4*1, Figure 6: Mixed ﬁne tuning with domain tags for domain adaptation (Chu et al., 2017). The section in the dotted rectangle denotes the multi-domain method . system on a resource rich out-of-domain corpus is trained until convergence, and then its parameters are ﬁne tuned on a resource
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2337363174	or no parallel corpora are available. It has been known that both vanilla SMT and NMT perform poorly for domain speciﬁc translation in low resource scenarios (Duh et al., 2013; Sennrich et al., 2013; Zoph et al., 2016; Koehn and Knowles, 2017). High quality domain speciﬁc machine translation (MT) systems are in high demand whereas general purpose MT has limited applications. In addition, general purpose translatio
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2579496717	ra (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017), synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017), or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017). On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptatio
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2750588180	resource languages especially when the target language is the same (Zoph et al., 2016). There are studies where either multilingual (Firat et al., 2016; Johnson et al., 2017) or multi-domain models (Sajjad et al., 2017) are trained, but none that attempt to package multiple language pairs and multiple domains into a single translation system. Even if out-of-domain data in the same language pair exists, it is possibl
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2750588180	the respective corpora. This primes the NMT decoder to generate sentences for the speciﬁc domain. Oversampling the smaller corpus so that the training procedure pays equal attention to each domain. Sajjad et al. (2017) further compare different methods for training a multi-domain system. In particular, they compare concatenation that simply concatenates the multi-domain corpora, staking that iteratively trains the
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2166098990	rpora based a language model (LM) (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014; Durrani et al., 2015; Chen et al., 2016) or generating pseudo parallel data (Utiyama and Isahara, 2003; Wang et al., 2014; Chu, 2015; Wang et al., This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ arXiv:1
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2293111166	rpora to improve indomain translation is known as domain adaptation for MT (Wang et al., 2016; Chu et al., 2018). For example, the Chinese-English patent domain parallel corpus has 1M sentence pairs (Goto et al., 2013), but for the spoken language domain parallel corpus there are only 200k sentences available (Cettolo et al., 2015). MT typically performs poorly in a resource poor or domain mismatching scenario and
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2579496717	rts as multi-domain methods, which have been successfully developed for NMT. In addition, the idea of data selection from SMT also have been developed for NMT. Multi-Domain The multi-domain method in Chu et al. (2017) is originally motivated by Sennrich et al. (2016a), which uses tags to control the politeness of NMT. The overview of this method is shown in Sentence -1 Sentence -2 Y Sentence -i Y Sentence -N Sente
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2117278770	to score the outdomain data using models trained from the in-domain and out-of-domain data and select training data from the out-of-domain data using a cut-off threshold on the resulting scores. LMs (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013), as well as joint models (Hoang and Sima’an, 2014; Durrani et al., 2015), and more recently convolutional neural network (CNN) models (Chen et al., 2016) can
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2613904329	Since the success of RNN based NMT (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), other architectures of NMT have been developed. One representative architecture is CNN based NMT (Gehring et al., 2017). Compared to RNN based models, CNN based models can be computed fully parallel during training and are much easier to optimize. Another representative architecture is the Transformer, which is based
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2115410424	a and Sumita, 2016). ii) Instance level interpolation. Instance weighting has been applied to several natural language processing (NLP) domain adaptation tasks (Jiang and Zhai, 2007), especially SMT (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012; Zhou et al., 2015). They ﬁrstly score each instance/domain by using rules or statistical methods as a weight, and then train SMT models
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2590953969	here are survey papers for NMT (Neubig, 2017; Koehn, 2017); however, they focus on general NMT and more diverse topics. Domain adaptation surveys have been done in the perspective of computer vision (Csurka, 2017) and machine learning (Pan and Yang, 2010; Weiss et al., 2016). However, such survey has not been done for NMT. To the best of our knowledge, this is the ﬁrst comprehensive survey of domain adaptation
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2223152534	ther a model level (Sennrich et al., 2013; Durrani et al., 2015; Imamura and Sumita, 2016) or an instance level (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2010; Rousseau et al., 2011; Zhou et al., 2015). However, due to the different characteristics of SMT and NMT, many methods developed for SMT cannot be applied to NMT directly. Domain adaptation for NMT is rather new and has attracted plenty of at
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2593768305	thods have become popular in domain adaptation (Ganin et al., 2016), which minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator (Tzeng et al., 2017). They have been applied to domain adaptation tasks in computer vision and machine learning (Tzeng et al., 2017; Motiian et al., 2017; Volpi et al., 2017; Zhao et al., 2017; Pei et al., 2018). Recentl
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2185701500	ting methods focus on adapting from a general domain into a speciﬁc domain. In the real scenario, training data and test data have different distributions and the target domains are sometimes unseen. Irvine et al. (2013) analyze the translation errors in such scenarios. Domain generalization aims to apply knowledge gained from labeled source domains to unseen target domains (Li et al., 2018). It provides a way to mat
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2579496717	he training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017), the NMT architecture (Kobus et al., 2016; Gulc¸ehre et al., 2015; Britz et al., 2017) or the decoding algorithm (G¨ ulc¸ehre et al., 2015; Dakwale and¨ Monz, 2017; Khayr
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2525778437	the Transformer, which is based on attention only (Vaswani et al., 2017). It has been shown that CNN based NMT and the Transformer signiﬁcantly outperform the state-of-the-art RNN based NMT model of Wu et al. (2016) in both the translation quality and speed perspectives. However, currently, most of the domain adaptation studies for NMT are based on the RNN based model (Bahdanau et al., 2015). The research of dom
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2157435188	in translation after ﬁne tuning on in-domain data. Neural Lattice Search Khayrallah et al. (2017) propose a stack-based decoding algorithm over word lattices, while the lattices are generated by SMT (Dyer et al., 2008). In their domain adaptation experiments, they show that stack-based decoding is better than conventional decoding. 5 Domain Adaptation in Real-World Scenarios A domain adaptation method should be ado
2805394970	A Survey of Domain Adaptation for Neural Machine Translation	2337363174	uages. It is known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). There are studies where either multilingual (Firat et al., 2016; Johnson et al., 2017) or multi-domain models (Sajjad et al., 2017) are trained, but none that attempt to package multiple language pa
2805425103	Improving Dialogue Act Classification for Spontaneous Arabic Speech and Instant Messages at Utterance Level.	1643441707	classification task (Sridhara et al., 2009; Eugenio et al., 2010). Moreover, there is a strong relationship between the speaker’s dialogue act and the surface utterances expressing that dialogue act (Andernach, 1996; Kang et al., 2013; Choi et al., 2005). For instance, the speaker utters a sentence, which most well expresses his/her intention (act) so that the hearer can easily understand what the speaker’s dial
2805430026	OpenNMT: Neural Machine Translation Toolkit	2130942839	(Chung et al., 2014) which help the model learn long-term features. (b) Translation requires relatively large, stacked RNNs, which consist of several vertical layers (2-16) of RNNs at each time step (Sutskever et al., 2014). (c) Input feeding, where the previous attention vector is fed back into the input as well as the predicted word, has been shown to be quite helpful for machine translation (Luong et al., 2015a). (d)
2805430026	OpenNMT: Neural Machine Translation Toolkit	1902237438	(Gehring et al., 2017) and the attention-only transformer network (Vaswani et al., 2017). Finally, the translation code allows for user customization. In addition to out-of-vocabulary (OOV) handling (Luong et al., 2015b), OpenNMT also allows beam search with various normalizations including length and attention coverage normalization (Wu et al., 2016), and dynamic dictionary support for copy/pointer networks. We al
2805430026	OpenNMT: Neural Machine Translation Toolkit	2064675550	. In practice, there are also many other important aspects that improve the effectiveness of the base model. Here we brieﬂy mention four areas: (a) It is important to use a gated RNN such as an LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) which help the model learn long-term features. (b) Translation requires relatively large, stacked RNNs, which consist of several vertical layers (2-16) of RNNs at each tim
2805430026	OpenNMT: Neural Machine Translation Toolkit	1902237438	(Sutskever et al., 2014). (c) Input feeding, where the previous attention vector is fed back into the input as well as the predicted word, has been shown to be quite helpful for machine translation (Luong et al., 2015a). (d) Test-time decoding is done through beam search where multiple hypothesis target predictions are considered at each time step. Implementing these correctly can be difﬁcult, which motivates thei
2805430026	OpenNMT: Neural Machine Translation Toolkit	1902237438	6; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization. As NMT approaches
2805430026	OpenNMT: Neural Machine Translation Toolkit	2168231600	aining using data parallelism. Each GPU has a replica of the master parameters and processes independent batches during training phase. Two modes are available: synchronous and asynchronous training (Dean et al., 2012). Experiments with 8 GPUs show a 6 speed up in per epoch, but a slight loss in training efﬁciency. When training to similar loss, it gives a 3.5 total speed-up to training. C/Mobile/GPU Translation Tr
2805430026	OpenNMT: Neural Machine Translation Toolkit	1902237438	ary implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules such as the copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which is widely used in summarization and gen
2805430026	OpenNMT: Neural Machine Translation Toolkit	1816313093	eur et al., 2017), to name a few of many applications. Additional Tools OpenNMT packages several additional tools, including: 1) reversible tokenizer, which can also perform Byte Pair Encoding (BPE) (Sennrich et al., 2015); 2) loading and exporting word embeddings; 3) translation server which enables showcase results remotely; and 4) visualization tools for debugging or understanding, such as beam search visualization,
2805430026	OpenNMT: Neural Machine Translation Toolkit	2133564696	n a source sentence x 1:S as p(w 1:T jx) = T 1 p(w tjw 1:t 1;x;) where the distribution is parameterized with . This distribution is estimated using an attention-based encoder-decoder architecture (Bahdanau et al., 2014). A source encoder recurrent neural network (RNN) maps each source word to a word vector, and processes these to a sequence of hidden vectors h 1;:::;h S. The target decoder combines an RNN hidden rep
2805430026	OpenNMT: Neural Machine Translation Toolkit	2770394828	on, and the output network to produce multiple conditionally independent predictions. We have seen instances of this use in published research. In addition to machine translation (Levin et al., 2017; Ha et al., 2017; Ma et al., 2017), researchers have employed OpenNMT for parsing (van Noord and Bos, 2017), document summarization (Ling and Rush, 2017), data-toSystem newstest14 newstest15 GNMT 4 layers 23.7 26.5 G
2805430026	OpenNMT: Neural Machine Translation Toolkit	1902237438	rity Due to the deliberate modularity of our code, OpenNMT is readily extensible for novel feature development. As one example, by substituting the attention module, we can implement local attention (Luong et al., 2015a), sparse-max attention (Martins and Astudillo, 2016) and structured attention (Kim et al., 2017) with minimal change of code. As another example, in order to get feature-based factored neural transl
2805430026	OpenNMT: Neural Machine Translation Toolkit	1855892484	as speech signals, and multi-dimensional data such as images. To support these different input modalities the library implements image encoder (Xu et al., 2015; Deng et al., 2017) and audio encoders (Chan et al., 2015). OpenNMT implements various attention types including general, dot product, and concatenation (Luong et al., 2015a; Britz et al., 2017). This also includes recent extensions to these standard modules
2805430026	OpenNMT: Neural Machine Translation Toolkit	2133564696	systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization
2805430026	OpenNMT: Neural Machine Translation Toolkit	2130942839	terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015a), NMT has now become a widely-applied technique for machine translation, as well as an
2805436235	Anaphora and Coreference Resolution: A Review.	2124214517	del. Given the wide range of applications of CR, practical tools to tackle this issue are a necessity. These tools may deploy a specic approach like (Mitkov et al,2002) and or others like Reconcile (Stoyanov et al, 2010) could be a combination of many research methodologies. The GuiTAR tool (Poesio and Kabadjov,2004) aimed at making an open source tool available for researchers mainly interested in applying AR to NLP
2805436235	Anaphora and Coreference Resolution: A Review.	2154407881	he term suggests refers to words or phrases referring to a single unique entity in the world. Anaphoric and co-referent entities themselves form a subset of the broader term \discourse parsing&quot; (Soricut and Marcu, 2003), which is crucial for full text understanding. In spite of having a rich research history in the NLP community, anaphora resolution is one of the subelds of NLP which has seen the slowest progress th
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2271245358	of the analysis in terms of persuasion strategies can be generalized across different text genres and contexts. In our study, we used a dataset collected from the ChangeMyView platform introduced by Tan et al. (2016), where only the replies by the root challenger are considered, deﬁning all the replies by the root challenger in a path as the rooted path-unit. For each rooted path-unit that wins a ∆, they select a
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2012012028	ators is an expensive process, weusetheexpertannotated dataasasmalllabeledtraining data(totalof980instances; Section 52). Due to this small annotated data scenario, we develop a self-training method (Clark et al., 2003;Mihalcea,2004),whichusestheremaining unannotated 70%oftheCMVcorpus (section 6.2) as unannoted data. From our annotation studies it is clear that all the datasets are highly unbalanced (the size of th
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2251931307	their class (Biran and McKeown, 2015). 6.1 Feature Description We use linguistically-motivated features inspired by research on discourse relation identiﬁcation and persuasive argument identiﬁcation (Stab and Gurevych, 2014; Ghosh et al., 2016). A brief description of the features is given below. • bag-of-words: We selected the entire CMV dataset used by Tan et al. (2016) to extract the bag-of-words features (e.g., unig
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2271245358	enient to observe the function of concessions in different datasets, the lack of explicit signals of persuasion prevents us to carry out such a comparison. We use the ChangeMyView dataset released by Tan et al. (2016). Our task faces two challenges. First, in order for concessions to increase persuasion, they need to be part of an argument. However, not every concession is argumentative (Grote et al.,1997): The pr
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2012012028	also evaluate the classiﬁer on the dev set to assess the quality of the new data that is added to the training set L (Table 3). For details of the self-training procedure, please see Mihalcea (2004); Clark et al. (2003). We use the Support Vector Machines classiﬁer with RBF kernel (we use Scikit-learn tool (Pedregosa et al., 2011)). The class weights are inversely proportional to the number of instances in the categ
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2101234009	g set L (Table 3). For details of the self-training procedure, please see Mihalcea (2004); Clark et al. (2003). We use the Support Vector Machines classiﬁer with RBF kernel (we use Scikit-learn tool (Pedregosa et al., 2011)). The class weights are inversely proportional to the number of instances in the categories. As a ﬁnal classiﬁer we used a system combination: if any dev/test instance contains a lexical pattern obta
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2012012028	hat I see what you I see where you I think you are correct Table 2: Examples of manually ﬁltered lexicon 6.2 Self-training method for identifying concessions Self-training algorithms (Mihalcea, 2004; Clark et al., 2003) start with a small subset of annotated training data and attempt to increase the amount of training data by using a large set of unannotated data. We adopt the approach of Mihalcea (2004) that used s
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2271245358	ion and persuasive argument identiﬁcation (Stab and Gurevych, 2014; Ghosh et al., 2016). A brief description of the features is given below. • bag-of-words: We selected the entire CMV dataset used by Tan et al. (2016) to extract the bag-of-words features (e.g., unigrams and bigrams). We consider every sentence that contains the four candidate discourse markers (i.e., “but”, “while”, “however”, and “though”) as can
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2109462987	l/argumentative relations is bound to the performance of state of the art discourse parsers trained on the Penn Discourse Treebank, the largest annotated corpus so far available (Prasad et al. 2009). Pitler et al. (2009) showed that syntactic feautures play a crucial role in disambiguating explicit connectives. Drawing from their work, Lin et al. (2014) built a four-steps pipeline for the identiﬁcation of implicit an
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2166957049	ons through discourse markers is difﬁcult due to the polysemy with contrast and other discourse relations (Prasad et al., 2014). State of the art discourse parsers trained on Penn Discourse Treebank (Prasad et al., 2008) achieve low accuracy in the identiﬁcation of concessive uses of discourse connectives in general due to the small number of instances in the training data; this result is reﬂected also on our task of
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2271245358	from four sets of hostage negotiation transcriptions annotated as to persuasion features. Using supervised learning algorithms they show that persuasion tactics constitute machine-learnable features. Tan et al. (2016) analyzed shallow linguistic and interactional features which happen to be persuasive in ChangeMyView, a subreddit where users exchange opinions and assign a Delta point to the user that managed to ch
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2160660844	sions. We use (a) the MPQA Lexicon (Wilson et al., 2005) of over 8,000 positive, negative, and neutral sentiment words, (b) an opinion lexicon with around 6,800 positive and negative sentiment words (Hu and Liu, 2004) to see whether training instances contain sentiment words. Apart from the above features, we also retrieved lexical patterns that could be indicators of argumentative concessive uses of the discourse
2805667322	ChangeMyView Through Concessions: Do Concessions Increase Persuasion?	2271245358	in verb (e.g., “I think it is not worth buying it”). The use of hedges is common in argumentative concessions since they contribute to avoid a potentially face-threatening act of abrupt disagreement. Tan et al. (2016) argue that depending on the context, hedges can make an argument weaker or easier to accept by softening its tone. Based on their research and also on Hanauer et al. (2012), we collect a set of candi
2805707570	Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.	2584992898	ask of automatic video description [9], which is the generation of natural language descriptions of videos (e.g., a sentence that summarizes an input video). To enhance video description performance, [6] introduced an attention-based multimodal fusion approach that selectively attends to different input 1http://workshop.colips.org/dstc7/call.html modalities such as audio and video features. Visual Di
2805707570	Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.	1933349210	ch that selectively attends to different input 1http://workshop.colips.org/dstc7/call.html modalities such as audio and video features. Visual Dialog [2, 3, 4] extends visual question answering (VQA) [1] from simple single-turn question answering to multi-turn dialog, in which the utterances in each turn may reference information from previous turns of the dialog. In the AVSD challenge, we further ex
2805707570	Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.	2337252826	cting text-based human dialog data for videos from human action recognition datasets such as CHARADES3 and Kinetics4. We have already collected text-based dialog data about short videos from CHARADES [8], which contains untrimmed and multi-action videos, along with video descriptions. The data collection paradigm for dialogs was similar to that described in [2], in which for each image, two different
2805707570	Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.	2558809543,2603266952	oduced an attention-based multimodal fusion approach that selectively attends to different input 1http://workshop.colips.org/dstc7/call.html modalities such as audio and video features. Visual Dialog [2, 3, 4] extends visual question answering (VQA) [1] from simple single-turn question answering to multi-turn dialog, in which the utterances in each turn may reference information from previous turns of the
2805707570	Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7.	2136036867	velopments in related areas such as video description and Visual Dialog. Encoder-decoder networks developed for image captioning have recently been extended to the task of automatic video description [9], which is the generation of natural language descriptions of videos (e.g., a sentence that summarizes an input video). To enhance video description performance, [6] introduced an attention-based mult
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2609138599	additional tasks of language reconstruction or multi-label word prediction. In contrast, other methods use a single encoding scheme such as seq2seq or simply mean pooling the features. The authors of [24] use a model similar to our Two-Wings model but they need additional annotated data to learn the language encoding. In [17], they use an implicit ensemble by building a decoder deﬁned by a linear mix
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2101105183	aluation metrics most often used in the current literature for natural language tasks, shown here in inverse chronological order of their publication date: CIDEr [30], METEOR [4], ROUGE [20] and BLEU [22]. Models in the pool: Our ﬁnal consensus network works over a pool of 16 models based on the 4 main architectures, differing in the visual, audio and video features used and the number of layers of de
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2507365558	aph generator. The sentence decoder has an attention mechanism to focus on video features while exploiting spatial attention. Methods for selecting captions from multiple models have been proposed in [10,29]. Unlike our work, they learn a compatibility score between a single sentence and a given video, without taking in consideration the whole group of output sentences. The authors of [5] use latent topi
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2123301721	art we use four of the evaluation metrics most often used in the current literature for natural language tasks, shown here in inverse chronological order of their publication date: CIDEr [30], METEOR [4], ROUGE [20] and BLEU [22]. Models in the pool: Our ﬁnal consensus network works over a pool of 16 models based on the 4 main architectures, differing in the visual, audio and video features used and
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	1957740064	captioning using RNNs perform average feature pooling over the video and bring the task closer to image captioning. The strategy works well for short videos, in which a single major event takes place [38]. For longer videos, different video encoding schemes are proposed. These schemes use either a recurrent encoder [8,32] or an attention model [36]. In [38] authors use a hierarchical RNN model, with a
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2425121537	ces. 3) Re-rank the top C using the Oracle Net and output the top scored sentence. 4 Experimental analysis We trained our models on the challenging MSR-VTT 2016 video captioning dataset and benchmark [35]. This is the main dataset used for experimental testing in recent literature. It contains 10k videos with diverse visual content. Each clip is 10 to 30 seconds long and is annotated with 20 sentences
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2599772929	difference between the distributions at training vs. testing time, an issue called exposure bias. To tackle it, reinforcement learning approaches have been studied in the context of image captioning [6,21,27]. An already trained model is improved by a policy gradient method that works on whole output sentences, guided by a non-differentiable reward, given by the language metrics. Recently this approach ha
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2609138599	DUT¸A˘ et al.: MINING FOR MEANING Cider Meteor Rouge Bleu 4 VideoLAB [26] 44.1 27.7 60.6 39.1 v2t navig [16] 44.8 28.2 60.9 40.8 Aalto [29] 45.7 26.9 59.8 39.8 ruc-uva [9] 45.9 26.9 58.7 38.7 MT-Ent [24] 47.1 28.8 60.2 40.8 HRL [34] 48.0 28.7 61.7 41.3 dense [28] 48.9 28.3 61.1 41.4 CIDEnt-RL [23] 51.7 28.4 61.4 40.5 TGM [17] 52.9 29.7 - 45.4 Ours 53.8 29.7 63.0 44.2 Table 2: Comparison with the top
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2507365558	e, Natsev, Toderici, Varadarajan, and Vijayanarasimhan} 20168 I. DUT¸A˘ et al.: MINING FOR MEANING Cider Meteor Rouge Bleu 4 VideoLAB [26] 44.1 27.7 60.6 39.1 v2t navig [16] 44.8 28.2 60.9 40.8 Aalto [29] 45.7 26.9 59.8 39.8 ruc-uva [9] 45.9 26.9 58.7 38.7 MT-Ent [24] 47.1 28.8 60.2 40.8 HRL [34] 48.0 28.7 61.7 41.3 dense [28] 48.9 28.3 61.1 41.4 CIDEnt-RL [23] 51.7 28.4 61.4 40.5 TGM [17] 52.9 29.7 -
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	1836465849	g, in order to reduce the temporal dimension of input from N t to 1. Each block has 2 dilated convolutional layers [37]. Each applies several 1x3 ﬁlters with Relu nonlinearity and batch normalization [15]. The dilation rate is increased with the depth, in order to compute over different scales. Between 2 successive levels, there are residual connections [12]. Batch normalization is applied to ease the
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2194775991	h Relu nonlinearity and batch normalization [15]. The dilation rate is increased with the depth, in order to compute over different scales. Between 2 successive levels, there are residual connections [12]. Batch normalization is applied to ease the optimization process. Based on this architecture, we trained several models, varying the network depth, the size of the ﬁlters and the dilation rate. 3 Mul
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	1586939924	ort videos, in which a single major event takes place [38]. For longer videos, different video encoding schemes are proposed. These schemes use either a recurrent encoder [8,32] or an attention model [36]. In [38] authors use a hierarchical RNN model, with a sentence generator and a separate paragraph generator. The sentence decoder has an attention mechanism to focus on video features while exploitin
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2742943414	a policy gradient method that works on whole output sentences, guided by a non-differentiable reward, given by the language metrics. Recently this approach has been applied also for video captioning [23,34]. Main contributions. The main contributions of our approach are: 1) We describe videos in sentences by ﬁnding a consensus among multiple encoder-decoder networks. While the individual encoder-decoder
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2609138599	s, one for each topic. The number of parameters is reduced by a 3-way factorization [19] of the mixture of all topic parameters. External data can be used to enlarge the linguistic knowledge [33]. In [24] the authors use additional tasks for improving the learning process: an unsupervised video prediction and a language entailment generation task. The usual way of predicting the next word given the pr
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	1947481528,2139501017	he strategy works well for short videos, in which a single major event takes place [38]. For longer videos, different video encoding schemes are proposed. These schemes use either a recurrent encoder [8,32] or an attention model [36]. In [38] authors use a hierarchical RNN model, with a sentence generator and a separate paragraph generator. The sentence decoder has an attention mechanism to focus on vid
2805711188	Mining for meaning: from vision to language through multiple networks consensus.	2742943414	v2t navig [16] 44.8 28.2 60.9 40.8 Aalto [29] 45.7 26.9 59.8 39.8 ruc-uva [9] 45.9 26.9 58.7 38.7 MT-Ent [24] 47.1 28.8 60.2 40.8 HRL [34] 48.0 28.7 61.7 41.3 dense [28] 48.9 28.3 61.1 41.4 CIDEnt-RL [23] 51.7 28.4 61.4 40.5 TGM [17] 52.9 29.7 - 45.4 Ours 53.8 29.7 63.0 44.2 Table 2: Comparison with the top models on MSRVTT 2016 test dataset. We obtain state of the art results on three evaluation metr
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	1862888253	aph based techniques to identify boundaries [9, 28]. The TextTiling [11] document segmentation algorithm uses shifts in vocabulary patterns to mark segment boundaries. Works such as Riedl and Biemann [25] adapt the TextTiling algorithm to work on topics generated by Latent Dirichlet Allocation. Glavis et al.[9] use a graph based representation of documents based on semantic relatedness of sentences to
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	887185921,2100626830,2128709346	arning objective generation. Document chunking: Broadly, most methods for chunking/segmentation of text rely on detecting changes in vocabulary usage patterns [11, 14, 15], identifying topical shifts [6, 7, 23], or employing graph based techniques to identify boundaries [9, 28]. The TextTiling [11] document segmentation algorithm uses shifts in vocabulary patterns to mark segment boundaries. Works such as R
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	2153579005	To chunk these documents, we use a divide-and-conquer approach based on topical or content shifts. We represent the content using mean bag-of-word embeddings, which are pretrained word2vec embeddings [20, 21].7 We tokenise words using whitespace, and discard common symbols such as com7Word embeddings are trained on English Wikipedia. mas and periods. When computing the mean embedding, stopwords are exclud
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	2142522063	or chunking/segmentation of text rely on detecting changes in vocabulary usage patterns [11, 14, 15], identifying topical shifts [6, 7, 23], or employing graph based techniques to identify boundaries [9, 28]. The TextTiling [11] document segmentation algorithm uses shifts in vocabulary patterns to mark segment boundaries. Works such as Riedl and Biemann [25] adapt the TextTiling algorithm to work on topi
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	2060772621,2097385711	NLU 66 Modified TextRank [4] 51 Table 3: Percentage proportion of keyphrases identied by instructional designers as being\useful&quot;for possible inclusion in learning objectives extract keyphrases [13, 27, 29], while unsupervised methods often rely on graph-based ranking [19] or topic-based clustering [10, 17]. For our work, we rely on an accessible and eective keyphrase extraction method: IBM Watson Natu
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	2153579005	with ReLU activation functions[24] in each node. The input of the network is the mean bag-ofwords embedding of the document text and the keyphrase. Word embeddings are pre-trained word2vec embeddings [20, 21] trained on the English Wikipedia. Word embeddings are kept static and not updated during back-propagation.13 This approach of predicting bloom verbs was found to be very eective as shown in Section
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	2145049651	signers as being\useful&quot;for possible inclusion in learning objectives extract keyphrases [13, 27, 29], while unsupervised methods often rely on graph-based ranking [19] or topic-based clustering [10, 17]. For our work, we rely on an accessible and eective keyphrase extraction method: IBM Watson Natural Language Understanding (NLU)10 to extract keyphrases. NLU is one of many commercially available ge
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	1777978449	subcomponents of document chunking and learning objective generation. Document chunking: Broadly, most methods for chunking/segmentation of text rely on detecting changes in vocabulary usage patterns [11, 14, 15], identifying topical shifts [6, 7, 23], or employing graph based techniques to identify boundaries [9, 28]. The TextTiling [11] document segmentation algorithm uses shifts in vocabulary patterns to m
2805840025	Document Chunking and Learning Objective Generation for Instruction Design.	152377028	uage Understanding (NLU)10 to extract keyphrases. NLU is one of many commercially available general purpose keyphrase extraction methods that performs eectively in general keyphrase extraction tasks [8, 12]. We also evaluated other methods such as a variant of TextRank [19], which has been used in extracting keyphrases from education material [4]. We chose NLU for the rest of this paper after a blind us
2805903440	Quantifying the dynamics of topical fluctuations in language	2307020448	0) and word2vec (Mikolov et al., 2013). This research broadly falls into two categories. On the one hand, methods proposals and critiques accompanied by exploratory results (Dubossarsky et al., 2017; Frermann and Lapata, 2016; Gulordava and Baroni, 2011; Hamilton et al., 2016b; Jatowt and Duh, 2014; Kulkarni et al., 2015; Sagi et al., 2011; Schlechtweg et al., 2017; Wijaya and Yeniterzi, 2011). On the other, applications
2805903440	Quantifying the dynamics of topical fluctuations in language	2510135622	advection values could possibly be due to some particular semantic cluster of words that might all belong to a similar (trending) topic and thus in ate the results. We computed the APSyn similarity (Santus et al., 2016) on all pairs of the topic vectors of the 133 nouns and found them to be suciently dissimilar, with a mean of 0.004 and a maximum similarity of 0.3, on a standardized scale of 0 to 1 (where 1 stands
2805903440	Quantifying the dynamics of topical fluctuations in language	250892164	al., 2013). This research broadly falls into two categories. On the one hand, methods proposals and critiques accompanied by exploratory results (Dubossarsky et al., 2017; Frermann and Lapata, 2016; Gulordava and Baroni, 2011; Hamilton et al., 2016b; Jatowt and Duh, 2014; Kulkarni et al., 2015; Sagi et al., 2011; Schlechtweg et al., 2017; Wijaya and Yeniterzi, 2011). On the other, applications of these methods, usually wi
2805903440	Quantifying the dynamics of topical fluctuations in language	1570098300	and, methods proposals and critiques accompanied by exploratory results (Dubossarsky et al., 2017; Frermann and Lapata, 2016; Gulordava and Baroni, 2011; Hamilton et al., 2016b; Jatowt and Duh, 2014; Kulkarni et al., 2015; Sagi et al., 2011; Schlechtweg et al., 2017; Wijaya and Yeniterzi, 2011). On the other, applications of these methods, usually with more specic linguistic questions in mind (Dautriche et al., 2016;
2805903440	Quantifying the dynamics of topical fluctuations in language	2510135622	are considered to have similar meaning. A common measure of similarity is the cosine of the angle between the two vectors. Recently, an alternative has been proposed in the form of the APSyn measure (Santus et al., 2016), which involves comparing the rankings of the topmost associated context words instead of the whole vocabulary. The intuition is, only the mmost associated context words hold relevant information abo
2805903440	Quantifying the dynamics of topical fluctuations in language	2036611122	ley et al., 2014; Michel et al., 2011). It has also been noted that times of change and con ict, such as wars and revolutions, are observable in language dynamics, such as the emergence of new words (Bochkarev et al., 2014; Bochkarev et al., 2015) and word growth rates (Petersen et al., 2012). Petersen et al. (2012) conclude that \[t]opical words in media can display long-term persistence patterns /.../ and can result
2805903440	Quantifying the dynamics of topical fluctuations in language	2307020448	ll. 5 A similar proposal does also exist as an extension of the LDA model, the topics-over-time approach (Wang and McCallum, 2006), which we will be not examining in further detail here. Furthermore, Frermann and Lapata (2016) use a Bayesian approach in some aspects similar to classical topic modeling to measure semantic change in a word as change in its distribution of \contexts&quot; (or topics | as probability distribut
2805903440	Quantifying the dynamics of topical fluctuations in language	2073593558	n described as an interdisciplinary approach to language change, evolution, and interlanguage competition, relying on large databases and quantitative modeling, including simulation-based approaches (Wichmann, 2008). Since our contribution applies to corpus research rst and foremost, our focus in the following brief review will be on this strand of language dynamics. 2.1 Previous research Of greatest utility fro
2805903440	Quantifying the dynamics of topical fluctuations in language	1570098300	particular language codies. The latter is labeled as ‘cultural shift’, which is claimed to be more common in nouns than verbs. Detecting ‘signicant’ changes in word meaning has also been attempted (Kulkarni et al., 2015), with the two aforementioned approaches using a similar distributional semantics method for determining semantic similarity across time, and the latter employing a similar signicance detection metho
2805903440	Quantifying the dynamics of topical fluctuations in language	2113283043	tegories. On the one hand, methods proposals and critiques accompanied by exploratory results (Dubossarsky et al., 2017; Frermann and Lapata, 2016; Gulordava and Baroni, 2011; Hamilton et al., 2016b; Jatowt and Duh, 2014; Kulkarni et al., 2015; Sagi et al., 2011; Schlechtweg et al., 2017; Wijaya and Yeniterzi, 2011). On the other, applications of these methods, usually with more specic linguistic questions in mind (
2805903440	Quantifying the dynamics of topical fluctuations in language	1593045043	ts, to avoid log(0) appearing in the expression. The crucial ingredient in the model is the set of weights W for the words in N. Here, we adopt the positive pointwise mutual information (PPMI) score (Church and Hanks, 1990). We provide details of how PPMI is calculated in the Technical Appendix. The idea is that PPMI assigns a higher score to words that are strongly associated, based on their co-occurrence with other wo
2805903440	Quantifying the dynamics of topical fluctuations in language	2591906248	y of research aims to quantify statistical laws of language change over time, those of word growth and decline, and relationships between word frequencies and lexical evolution (Cuskley et al., 2014; Feltgen et al., 2017; Keller and Schultz, 2013; Keller and Schultz, 2014; Lieberman et al., 2007; Newberry et al., 2017; Pagel et al., 2007). This has also involved claims of the eects of real-world events (like wars) o
2805903440	Quantifying the dynamics of topical fluctuations in language	2036611122	z, 2013; Keller and Schultz, 2014; Lieberman et al., 2007; Newberry et al., 2017; Pagel et al., 2007). This has also involved claims of the eects of real-world events (like wars) on these processes (Bochkarev et al., 2014; Petersen et al., 2012; Wijaya and Yeniterzi, 2011). There is also an emerging strand of research investigating semantic change and language dynamics from the point of view of meaning, using diachron
2805938424	The Limitations of Cross-language Word Embeddings Evaluation.	2142625445	) (Baker et al., 2014) 0.24 0.39 0.27 0.27 0.27 V.YP-111 (88.5%) (Yang and Powers, 2006) 0.22 0.37 0.25 0.25 0.25 R.MEN-1146 (94.7%) (Bruni et al., 2014) 0.68 0.66 0.69 0.66 0.68 R.MTurk-551 (91.7%) (Halawi et al., 2012) 0.56 0.51 0.57 0.54 0.57 R.WordSim-193 (96.4%) (Agirre et al., 2009) 0.55 0.53 0.57 0.53 0.55 P@1, dictionary induction 0.31 0.16 0.32 0.29 0.21 P@5, dictionary induction 0.53 0.34 0.52 0.49 0.38 P@1
2805938424	The Limitations of Cross-language Word Embeddings Evaluation.	2502814102	mainstream methods of intrinsic evaluation: some researchers addressed subjectivity of human assessments, obscurity of instructions for certain tasks and terminology confusions (Faruqui et al., 2016; Batchkarov et al., 2016). Despite all these limitations, some of the criticized methods (like the word similarity task) has been started to be actively applied yet for cross-language word embeddings evaluation (Camacho-Colla
2805938424	The Limitations of Cross-language Word Embeddings Evaluation.	2145815109	model for k=1,5,10. 4.3 Extrinsic Task and Our Dataset Cross-language Paraphrase Detection. In an analogy with a monolingual paraphrase detection task (also called sentence similarity identiﬁcation) (Androutsopoulos and Malakasiotis, 2010), the task is to identify whether sentence ain language Aand sentence bin language B are paraphrases or not. This task is highly scalable, and usually ﬁgures as a sub-task of bigger tasks like cross-l
2805938424	The Limitations of Cross-language Word Embeddings Evaluation.	2252211741	tudy did not address the correlation of intrinsic evaluation scores with extrinsic ones (despite that the lack of correlation of intrinsic and extrinsic tasks for mono-language evaluation was proved (Schnabel et al., 2015), it is not obvious if this would also extend to cross-language evaluation). In 2017 a more extensive overview of crosslanguage word embeddings evaluation methods wasproposed (Ruder,2017), but this st
2806081754	DRCD: a Chinese Machine Reading Comprehension Dataset.	2558203065	idespread adoption of deep learning and natural language processing, several large-scale MRC datasets have been compiled (Lai et al. 2017; Hermann et al. 2015; Cui et al. 2016; Rajpurkar et al. 2016; Nguyen et al. 2016; He et al. 2017), providing sufficient training data for deep learning MRC. These datasets have different research purposes and different task definitions, but they can be classified into four types
2806081754	DRCD: a Chinese Machine Reading Comprehension Dataset.	2558203065	while questions in TriviaQA refer to multiple documents. The Chinese Wikipedia dump is obtained in the data 2017/03/20 User log dataset: User log datasets are constructed from real-world search logs. Nguyen et al. (2016) released the MSMARCO dataset with 100,000 queries and answers. In MSMARCO, all questions are real anonymized user queries from the Bing search engine, and the evidence documents used as context passa
2806081754	DRCD: a Chinese Machine Reading Comprehension Dataset.	1544827683	rch Center Delta Electronics, Inc.Thanks to advances in and widespread adoption of deep learning and natural language processing, several large-scale MRC datasets have been compiled (Lai et al. 2017; Hermann et al. 2015; Cui et al. 2016; Rajpurkar et al. 2016; Nguyen et al. 2016; He et al. 2017), providing sufficient training data for deep learning MRC. These datasets have different research purposes and different t
2806081754	DRCD: a Chinese Machine Reading Comprehension Dataset.	1544827683	s in a sentence. Since cloze-style datasets can be constructed without human labeling, it is more practicable to compile one large enough for a data-demanding approach like deep learning. In English, Hermann et al. (2015) created a corpus from CNN and Daily Mail news summaries, and Hill et al. (2015) built the Children’s Book Test. In Chinese, Cui et al. (2016) constructed a cloze dataset from the People’s Daily news
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2095285839	, or position (which means all the goal-consistent objects are not in left, right, top, bottom, or middle). We collected a dataset of InfoJigsaw games on Amazon Mechanical Turk using the framework in Hawkins (2015) as follows: 200 pairs of players 2 3 4 5 6 7 8 0 200 400 600 800 # messages ames (a) Number of exchanged messages per game. 0 1 2 3 4 5 6 7 8 9 10 0 100 200 300 score ames (b) Distribution of ﬁnal ga
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2118508845	90% conﬁdence intervals. which deﬁnes recurrences capturing how one agent reasons about another. Similar ideas were explored in the precursor work of Golland et al. (2010), and much work has ensued (Smith et al., 2013; Qing and Franke, 2014; Monroe and Potts, 2015; Ullman et al., 2016; Andreas and Klein, 2016). Most of this work is restricted to production and comprehension of a single utterance. Hawkins et al. (2
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2337353209	another. Similar ideas were explored in the precursor work of Golland et al. (2010), and much work has ensued (Smith et al., 2013; Qing and Franke, 2014; Monroe and Potts, 2015; Ullman et al., 2016; Andreas and Klein, 2016). Most of this work is restricted to production and comprehension of a single utterance. Hawkins et al. (2015) extend these ideas to two utterances (a question and an answer). Vogel et al. (2013b) int
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2564324149	others assume a predeﬁned convention for communication (Zhang and Lesser, 2013; Tan, 1993). There is also some work that learns the convention itself (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Mordatch and Abbeel, 2018). Lazaridou et al. (2017) puts humans in the loop to make the communication more human-interpretable. In comparison to these works, we seek to predict human behavior instea
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2340944142	d (mutual knowledge, mutual beliefs, and mutual assumptions) in order to coordinate. Recent work in task-oriented dialogue uses POMDPs and end-to-end neural networks (Young, 2000; Young et al., 2013; Wen et al., 2017; He et al., 2017). In this work, instead of learning from a large corpus, we predict human behavior without learning, albeit in a much more strategic, stylized setting (two words per utterance). 7 Co
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2186283982	erence, model-theoretic semantics (Montague, 1973) provides a mechanism for utterances to constrain possible worlds, and this has been implemented recently in semantic parsing (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Finally, for pragmatics, the cooperative principle of Grice (1975) can be realized by models in which a speaker simulates a listener—e.g., Franke (2009) and Frank and Goodman (2012). Find B2 Find B2
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2099618002	ey also only study artiﬁ- cial agents playing together and were not concerned about modeling human behavior. Learning to communicate. There is a rich literature on multi-agent reinforcement learning (Busoniu et al., 2008). Some works assume full visibility and cooperate without communication, assuming the world is completely visible to all agents (Lauer and Riedmiller, 2000; Littman, 2001); others assume a predeﬁned c
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2116492379	hich we will describe in Section 4) to provide context-dependence. One could also learn the literal semantics by backpropagating through these recurrences, which has been done for simpler RSA models (Monroe and Potts, 2015); or learn the literal semantics from data and then put RSA on top (Andreas et al., 2016); we leave this to future work. Suppose a player utters a single word circle. There are multiple possible conte
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2578901186	modeling human behavior. Restricting words to a very restricted natural language has been studied in the context of language games (Wittgenstein, 1953; Lewis, 2008; Nowak et al., 1999; Franke, 2009; Huttegger et al., 2010). Rational speech acts. The pragmatic component of PIP is based on Rational Speech Act framework (Frank and Goodman, 2012; Golland et al., 2010), PIP PIP -prag PIP -plan PIP -infer 3:5 3:4 3:3 3:2 eli
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2769917417	between a questioner and an answerer based on only one round of question answering. Vogel et al. (2013b) proposed a model of two agents playing a restricted version of the game from the Cards Corpus (Potts, 2012), where the agents only communicate once.1 In this work, we seek to capture all three aspects in a single, uniﬁed framework which allows 1Speciﬁcally, two agents must both co-locate with a speciﬁc car
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	2116492379	rrences capturing how one agent reasons about another. Similar ideas were explored in the precursor work of Golland et al. (2010), and much work has ensued (Smith et al., 2013; Qing and Franke, 2014; Monroe and Potts, 2015; Ullman et al., 2016; Andreas and Klein, 2016). Most of this work is restricted to production and comprehension of a single utterance. Hawkins et al. (2015) extend these ideas to two utterances (a qu
2806117557	Planning, Inference, and Pragmatics in Sequential Language Games	1923162067	t al. (2013a)). For inference, model-theoretic semantics (Montague, 1973) provides a mechanism for utterances to constrain possible worlds, and this has been implemented recently in semantic parsing (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Finally, for pragmatics, the cooperative principle of Grice (1975) can be realized by models in which a speaker simulates a listener—e.g., Franke (2009) and Frank an
2806120502	Neural Network Acceptability Judgments.	1846531235	(1999) 233 59.2 Comparatives Dayal (1998) 179 75.4 Modality Gazdar (1981) 110 65.5 Coordination Goldberg and Jackendoff (2004) 106 77.4 Resultative Kadmon and Landman (1993) 93 81.7 Negative Polarity Kim and Sells (2008) 1965 71.2 Syntax Textbook Levin (1993) 1459 69.0 Verb alternations Miller (2002) 426 84.5 Syntax textbook Rappaport Hovav and Levin (2008) 151 69.5 Dative alternation Ross (1967) 1029 61.8 Islands Sa
2806120502	Neural Network Acceptability Judgments.	2130942839	are able to uncover structure in unstructured linguistic data (LeCun et al., 2015). These models are widely used to encode features of sentences in ﬁxed-length sentence embeddings (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). Evaluating general-purpose sequence models and sentence embeddings is an important and challenging problem. Some approaches probe the contents of sentence embeddings using commo
2806120502	Neural Network Acceptability Judgments.	1486649854	earner like a recurrent neural network to perform the task without additional prior knowledge. In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017). For this reason, we use transfer learning in all our models and train large sequence models on auxiliary tasks. In most experiments a large sentence encoder is trained on a re
2806120502	Neural Network Acceptability Judgments.	2250539671	ence embedding goes through a sigmoid output layer. Word Embeddings We experiment with several kinds of word embeddings. While we train models using pre-trained 300-dimensional (6B) GloVe embeddings (Pennington et al., 2014), this is problematic since GloVe is trained on orders of magnitude more words than human learners ever see. Therefore, we also train word embeddings from scratch on the real/fake task or using a lang
2806120502	Neural Network Acceptability Judgments.	2167723982	et al. (2013) 651 70.4 Syntax textbook In-Domain 9515 71.3 Chung et al. (1995) 148 66.9 Sluicing Collins (2005) 66 68.2 Passive Jackendoff (1971) 94 67.0 Gapping Sag (1997) 112 57.1 Relative clauses Sag et al. (2003) 460 70.9 Syntax textbook Williams (1980) 169 76.3 Predication Out-of-Domain 1049 69.2 Total 10657 70.5 Table 2: The contents of CoLA by source. ‘N’ is the total number of examples. ‘%’ is the percent
2806120502	Neural Network Acceptability Judgments.	1846531235	that my father, he was tight as a hoot-owl. Ross (1967) 1 The jeweller inscribed the ring with the name. Levin (1993) 0 We rummaged papers through the desk. Levin (1993) 0 many evidence was provided. Kim and Sells (2008) 1 They can sing. Kim and Sells (2008) 1 This theorem will take only ﬁve minutes to establish that he proved in 1930. Kim and Sells (2008) 1 The men would have been all working. Baltin (1982) 1 Would
2806120502	Neural Network Acceptability Judgments.	2515741950	gs using common natural language processing tasks (Conneau et al., 2017; Wang et al., 2018). Others test whether embeddings encode top level features like sentence length, parsetree depth, and tense (Adi et al., 2016; Shi et al., 2016; Conneau et al., 2018). Acceptability classiﬁcation can be used to probe sentence embeddings for rich linguistic features at a much ﬁner level of granularity. After training an acce
2806120502	Neural Network Acceptability Judgments.	2120339295	lman et al. (2014) and Lau et al. (2016) represent acceptability judgments on a continuous scale from 1 to 4, and average judgments across multiple speakers. Our own labeling approach follows that of Lawrence et al. (2000): they take advantage of the fact that example sentences in linguistics publications are almost always labeled by the author for acceptability. Like us, they adopt these judgments directly. Grammatica
2806120502	Neural Network Acceptability Judgments.	1846531235	ment set. Conveniently, these represent very different genres of linguistics publications. Adger (2003) is a graduatelevel syntax textbook, Baltin and Collins (2001) is a research handbook in syntax, Kim and Sells (2008) is an undergraduate syntax textbook, Levin (1993) is a research monograph on lexical semantics and verb alternations, and Ross (1967) is a dissertation on extraction and movement. As expected, perfor
2806120502	Neural Network Acceptability Judgments.	2064675550	roduce judgments from authors and on topics that are not available to the model at training time. 3.2 Opening the Black Box Recurrent neural network models like Long ShortTerm Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are able to uncover structure in unstructured linguistic data (LeCun et al., 2015). These models are widely used to encode features of sentences in ﬁxed-length sentence embeddings (Cho et al., 2014;
2806120502	Neural Network Acceptability Judgments.	2120339295	rom English into various languages and back. A second approach is to take sentences from essays written by non-native speakers (Heilman et al., 2014). A third takes advantage of linguistics examples. Lawrence et al. (2000) and Lau et al. (2016) build datasets of 133 and 552 examples from a syntax textbook (Adger, 2003). We adopt a similar strategy in building CoLA, but on a larger scale and drawing from multiple source
2806120502	Neural Network Acceptability Judgments.	1846531235	rthogonal to the target contrast. 8.2.1 Test Sets Subject-Verb-Object This test set consists of 100 triples of subject, verb, and object each appearing 11 Model Adger (2003) Baltin and Collins (2001) Kim and Sells (2008) Levin (1993) Ross (1967) Real/Fake 0.023 0.302 0.425 0.160 0.272 Real/Fake + GloVe 0.201 0.471 0.469 0.076 0.268 Real/Fake + LM 0.108 0.388 0.319 0.224 0.487 LM Encoder 0.090 0.299 0.471 0.353 0.356
2806120502	Neural Network Acceptability Judgments.	2251930319	t al. (2009) simply label a sentence unacceptable if it has gone through one of their automatic distortion procedures. We take a similar approach in our auxiliary (real/fake) dataset (see section 6). Heilman et al. (2014) and Lau et al. (2016) represent acceptability judgments on a continuous scale from 1 to 4, and average judgments across multiple speakers. Our own labeling approach follows that of Lawrence et al. (2
2806120502	Neural Network Acceptability Judgments.	1486649854	ucture in unstructured linguistic data (LeCun et al., 2015). These models are widely used to encode features of sentences in ﬁxed-length sentence embeddings (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). Evaluating general-purpose sequence models and sentence embeddings is an important and challenging problem. Some approaches probe the contents of sentence embeddings using common natural language pr
2806120502	Neural Network Acceptability Judgments.	2251930319	verbal inﬂection. Lau et al. (2016) use round-trip machine-translation from English into various languages and back. A second approach is to take sentences from essays written by non-native speakers (Heilman et al., 2014). A third takes advantage of linguistics examples. Lawrence et al. (2000) and Lau et al. (2016) build datasets of 133 and 552 examples from a syntax textbook (Adger, 2003). We adopt a similar strategy
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2251939518	017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input repres
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2153274216	dency paths. arXiv:1805.11461v1 [cs.CL] 28 May 2018 2 Dependency representations Figure 1 illustrates the three different dependency representations we compare: the socalled CoNLL-style dependencies (Johansson and Nugues, 2007) which were used for the 2007, 2008, and 2009 shared tasks of the Conference on Natural Language Learning (CoNLL), the Stanford ‘basic’ dependencies (SB) (Marneffe et al., 2006) and the Universal Depe
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2123442489	e and re-train with the different dependency representations. We also compare to another widely used parser, namely the pre-trained parsing model for English included in the Stanford CoreNLP toolkit (Manning et al., 2014), which outputs Universal Dependencies only. However, it was clearly outperformed by our version of the Bohnet and Nivre (2012) parser in the initial development experiments. 3 Relation extraction sys
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2575480513	ent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Soch
2806348914	Syntactic Dependency Representations in Neural Relation Classification	1750263989	two entities. The path records the direction of arc traversal using left and right arrows (i.e. and !) as well as the dependency relation of the traversed arcs and the predicates involved, following Xu et al. (2015a). The entity codes in the ﬁnal sdp are replaced with the corresponding word tokens at the end of the pre-processing step. For the sentence above, we thus extract the path: knowledge sources SBJ are
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2579166072	ew methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency rep
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2123442489	model. This yields a positive impact (+16:00% F1) on the classiﬁcation task in our initial experiments. 3.2 Pre-processing Sentence and token boundaries are detected using the Stanford CoreNLP tool (Manning et al., 2014). Since most of the entities are multi-word units, we replace the entities with their codes in order to obtain a precise dependency path. Our example sentence All knowledge sources are treated as feat
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2138627627	rget entities in the same sentence. The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (Bunescu and Mooney, 2005). 4.2 Comparison of different dependency representations To investigate the model performance with various parser representations, we create a sdp for each training example using the different parse m
2806348914	Syntactic Dependency Representations in Neural Relation Classification	1750263989	s (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in All knowledge sources are treated as feature functions NMOD
2806348914	Syntactic Dependency Representations in Neural Relation Classification	2153274216	t Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The constituency-based treebank is converted to dependencies using two different conversion tools: (i) the pennconverter software1 (Johansson and Nugues, 2007), which produces the CoNLL dependencies2, and (ii) the Stanford parser using either the option to produce basic dependencies 3 or its default option which is Universal Dependencies v1.34. The parser a
2806385271	Entity Linking in 40 Languages Using MAG	2738183794	[6]. A portion of these approaches claim to be multilingual and most of them rely on models which are trained on English corpora with cross-lingual dictionaries. However, MAG (Multilingual AGDISTIS) [4] showed that the underlying models being trained on English corpora make them prone to failure when migrated to a different language. Additionally, these approaches hardly make their models or data av
2806385271	Entity Linking in 40 Languages Using MAG	2738183794	biguate entities, i.e., PageRank and HITS. Independently of the chosen graph algorithm, the highest candidate score among the set of candidates is chosen as correct disambiguation for a given mention [4]. 3 Demonstration Our demonstration will show the capabilities of MAG for different languages. We provide a graphical, web-based user interface (GUI). In addition, users can choose to use the REST int
2806385271	Entity Linking in 40 Languages Using MAG	2738183794	current implementation offers HITS and PageRank as algorithms, algorithm=hitsor algorithm =pagerank. –Search by Context - This boolean parameter provides a search of candidates using a context index [4]. –Acronyms - This parameter enables a search by acronyms. In this case, MAG uses an additional index to ﬁlter the acronyms by expanding their labels and assigns them a high probability. For example,
2806385271	Entity Linking in 40 Languages Using MAG	2738183794	ta. The goal of an EL approach is as follows: Given a piece of text, a reference knowledge base K and a set of entity mentions in that text, map each entity mention to the corresponding resource in K [4]. A large number of challenges has to be addressed while performing a disambiguation. For instance, a given resource can be referred to using different labels due to phenomena such as synonymy, acrony
2806385271	Entity Linking in 40 Languages Using MAG	98609384	understands two mandatory parameters: (1) text and (2) type. 1. text accepts an UTF-8 and URL encoded string with entities annotated with XML-tag &lt;entity&gt;. It is also capable of recognizing NIF [3] or txt ﬁles. 2. type accepts two different values. First, ’agdistis’ to disambiguate the mentions using the graph-based algorithms, but also ’candidates’ which list all possible entities for a given
2806386169	Visually Grounded, Situated Learning in Neural Models.	2038475577	, 1993). Knowledge of language cannot be separated from its physical context, which allows words and sentences to be learned by grounding them in reference to objects or natural concepts on hand (see Roy and Reiter, 2005, for a review). Nor can knowledge of language be separated from its social context, where language is learned interactively through communicating with others to facilitate problem-solving. Simply put
2806386169	Visually Grounded, Situated Learning in Neural Models.	1514535095	(e(2 v ) 1) (e(2 v ) +1) . To integrate visual context information into the -RNN, we fuse the model with a neural vision system, motivated by promising recent work done in automated image captioning (Xu et al., 2015). We adopt a transfer learning approach and incorporate a state-of-the-art convolutional neural network into the -RNN model, namely the Inception-v3 network (Szegedy et al., 2016). 1 The parameters of
2806386169	Visually Grounded, Situated Learning in Neural Models.	2064675550	g a model using only Equations 1-7. To verify that the experiment generalizes beyond the specic architecture chosen, a Gated Recurrent Unit (GRU, Cho et al., 2014) and a Long Short Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) were also trained. We compare these symbol-only baselines to the two variationsofourproposedmultimodal -RNN,as described in the previous section. The multimodal variantoftheGRU,wherethecontextinforma
2806386169	Visually Grounded, Situated Learning in Neural Models.	2575887804	l trained on linguistic data alone. Prior work in cognitive modeling has focused on models of distributional semantics that capture the similarity relations between words (e.g. Johns and Jones, 2012; Kievit-Kylar and Jones, 2011), whereas the model we propose here is a predictive language model. Due to the ability of language models to probabilistically constrain input on the basis of preceding context and to classify linguis
2806386169	Visually Grounded, Situated Learning in Neural Models.	2064675550	ld images they describe. We ask how adding such real-world contextattrainingcanimprovetheperformanceof language models. We extend the -RNN (Ororbia II et al., 2017), the Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU; Cho et al., 2014) to incorporate visual context information, creating a unied multi-modal connectionist architecture. We nd that the models acquire more knowledge o
2806386169	Visually Grounded, Situated Learning in Neural Models.	2575887804	of models of distributional semantics integrate word co-occurrence data extracted from a corpus with perceptual data, either to achieve a better model of language as it exists in the minds of humans (Kievit-Kylar and Jones, 2011; Johns and Jones, 2012) or to improve performance on machine learning tasks such as object recognition (Frome et al., 2013), image captioning (Kiros etal.,2014), orimagesearch(Socheretal.,2014). Inte
2806386169	Visually Grounded, Situated Learning in Neural Models.	2123024445	ow a word is used in language (Johns and Jones, 2012). As a result, language data can be used to improve object recognition by providing information about unobserved or infrequently observed objects (Frome et al., 2013). By representing the referents of concrete nouns as arrangements of elementary visual features (Biederman,1987),Kievit-KylarandJones(2011) nd that the visual features of nouns capture semantic typica
2806386169	Visually Grounded, Situated Learning in Neural Models.	2183341477	tomated image captioning (Xu et al., 2015). We adopt a transfer learning approach and incorporate a state-of-the-art convolutional neural network into the -RNN model, namely the Inception-v3 network (Szegedy et al., 2016). 1 The parameters of the vision network are xed. As our focus is on language modeling and how the addition of visual context can improve neural network performance on the task, xing the vision system
2806386169	Visually Grounded, Situated Learning in Neural Models.	1895577753	tures gains the ability to discriminate between actions afforded by a verb and actions that are not affordedbytheverb(e.g., hanging acoatonavacuum versus a cup). Image Captioning (Kiros et al., 2014; Vinyals et al., 2015; Xu et al., 2015) systems have shown promising results in generating captions by mapping between vision and language. However such models are restricted to a single language and can introduce irrever
2806386169	Visually Grounded, Situated Learning in Neural Models.	2123024445	ve a better model of language as it exists in the minds of humans (Kievit-Kylar and Jones, 2011; Johns and Jones, 2012) or to improve performance on machine learning tasks such as object recognition (Frome et al., 2013), image captioning (Kiros etal.,2014), orimagesearch(Socheretal.,2014). Integrating language and perception can facilitate language acquisition by allowing models to infer how a new word is used from
2806386169	Visually Grounded, Situated Learning in Neural Models.	1514535095	y to discriminate between actions afforded by a verb and actions that are not affordedbytheverb(e.g., hanging acoatonavacuum versus a cup). Image Captioning (Kiros et al., 2014; Vinyals et al., 2015; Xu et al., 2015) systems have shown promising results in generating captions by mapping between vision and language. However such models are restricted to a single language and can introduce irreversible corruption t
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	2408574227	ample, player types were automatically identiﬁed in Minecraft based on log data of the user’s interaction with the game [24], or clustering has been used to analyze user behavior in click-stream data [25].We argue that sophisticated user models that consider the knowledge, traits and affect have the potential to transform and improve existing applications even beyond educational games, and we hope tha
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	1574055659	ect them. We ﬁrst identify patterns and similarities in spelling errors across the entire word database and represent them in as few error production rules as possible, which we refer to as mal-rules [4] (capitalization and typing errors, letter confusion, phoneme-grapheme matching, phoneme omission, insertion and transposition). From the student input we can extract how much each mal-rule is activat
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	1574055659	All modules included in our architecture are commonly used in ITS and are subject to ongoing research. We have studied the different components for our two training environments Orthograph [2], [3], [4] and Calcularis [5], [6], [7], which we have developed for children with difﬁculties in learning spelling and mathematics, respectively. In the remainder of this article, we discuss each module of the
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	2264767234	n this ﬁeld aims at leveraging machine learning to improve the game play. For example, player types were automatically identiﬁed in Minecraft based on log data of the user’s interaction with the game [24], or clustering has been used to analyze user behavior in click-stream data [25].We argue that sophisticated user models that consider the knowledge, traits and affect have the potential to transform
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	25579054	ponent and will be discussed in Section V-B. Another important trait is learning behavior. We have used clustering approaches to identify multiple subgroups of children with similar learning patterns [17]. This information can be leveraged for optimizing the student model (training individualization) and to provide visual feedback for domain experts. Our method consists of two steps: In a ﬁrst step, w
2806515050	Ten Years of Research on Intelligent Educational Games for Learning Spelling and Mathematics.	2466877100	that are tailored to the particular task and model. Recently, there has been increasing attention to develop more general policies that only have weak requirements on the student model they work with [21]. We extended these general policies towards a universal when-to-stop policy that only requires that the student model is able to output the probability of the next task to be correct. The method work
2806552719	AP18-OLR Challenge: Three Tasks and Their Baselines.	2025198378	anguages themselves also inﬂuence each other via the multilingual interaction, leading to complicated linguistic evolution. This complicated multilingual phenomena attracted lots of research recently [5], [6], [7]. To consistently boost the research on multilingual phenomena, the center for speech and language technologies (CSLT) at Tsinghua University and SpeechOcean organize the oriental language r
2806552719	AP18-OLR Challenge: Three Tasks and Their Baselines.	2278264165	hemselves also inﬂuence each other via the multilingual interaction, leading to complicated linguistic evolution. This complicated multilingual phenomena attracted lots of research recently [5], [6], [7]. To consistently boost the research on multilingual phenomena, the center for speech and language technologies (CSLT) at Tsinghua University and SpeechOcean organize the oriental language recognition
2806552719	AP18-OLR Challenge: Three Tasks and Their Baselines.	1524333225	stem development. IV. BASELINE SYSTEMS We constructed two kinds of baseline LID systems, based on the i-vector model and various DNN models respectively. All the experiments were conducted with Kaldi [14]. The purpose of these experiments is to present a reference for the participants, rather than a competitive submission. The recipes can be downloaded from the web page of the challenge. A. i-vector s
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2134691737	construction of s out of arbitrary entities e 1:::n corresponding to the projections fe 1:::n of f. An extensive explanation of the FrameNet-OWL resource designed according to D&amp;S is presented in [26]. Later, this approach to abridge semiotic and model-theoretical representation of frame semantics has been broadened in order to encompass any linguistic or factual resource, and opened the way to Fr
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2181830759	deﬁned in MetaNet is manually encoded, and is connected to a combination of linguistic frames, often aligned to those available in FrameNet [1]. A detailed survey about metaphor processing systems is [31]. As far as our ontology is concerned, we intend to enrich knowledge extraction pipelines such as FRED [14] with metaphoric sensitivity, thus contributing to automated metaphor interpretation. Metapho
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2134691737	e deploy it as an extension of Framester3 [12] a knowledge graph represented as Linked Open Data (LOD), which integrates heterogeneous linguistic resources (OntoWordNet [9], VerbNet [18], FrameNetOWL [1,26], BabelNet [25], etc.), factual datasets (DBpedia [22], YAGO [33], etc.), and foundational ontologies, by providing them a uniﬁed formal semantics. We give a brief 1https://metaphor.icsi.berkeley.edu/
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2515845560	ich may come from compositions of word pairs such as “sweet” and “person”, where sweet is only metaphorical when composed in phrases with words that do not denote tastable entities, such as “person”. [16] proposes a Compositional Distributional Semantic Model (CDSM), which generates a vector representation of the phrases. [2] also introduces a framework based on CDSM, targeting adjectivenoun construct
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2126530744	ment of statistical methods for detecting metaphors. Many of these techniques take advantage of vector-space models, and perform a binary classiﬁcation of metaphorical vs. literal occurrences in text [35,34], which may come from compositions of word pairs such as “sweet” and “person”, where sweet is only metaphorical when composed in phrases with words that do not denote tastable entities, such as “perso
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	1608050331	nestic Forgery2. We deploy it as an extension of Framester3 [12] a knowledge graph represented as Linked Open Data (LOD), which integrates heterogeneous linguistic resources (OntoWordNet [9], VerbNet [18], FrameNetOWL [1,26], BabelNet [25], etc.), factual datasets (DBpedia [22], YAGO [33], etc.), and foundational ontologies, by providing them a uniﬁed formal semantics. We give a brief 1https://metapho
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	1552620298	olving metaphorical blending, as well as by reusing the whole Framester as background knowledge. Interesting formal ontology work has also been conducted with reference to cognitive conceptual spaces [28], or description logics [30], but none of these works attempts to build a cognitively valid metaphor ontology that can be also exploited empirically in the current huge knowledge graphs that started p
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	1608322251	phrases with words that do not denote tastable entities, such as “person”. [16] proposes a Compositional Distributional Semantic Model (CDSM), which generates a vector representation of the phrases. [2] also introduces a framework based on CDSM, targeting adjectivenoun constructions. In these cases, the meaning of the phrase is derived by composing the representations of adjectives and nouns. As a c
2806631530	Amnestic Forgery: An Ontology of Conceptual Metaphors.	2114544510	presented as Linked Open Data (LOD), which integrates heterogeneous linguistic resources (OntoWordNet [9], VerbNet [18], FrameNetOWL [1,26], BabelNet [25], etc.), factual datasets (DBpedia [22], YAGO [33], etc.), and foundational ontologies, by providing them a uniﬁed formal semantics. We give a brief 1https://metaphor.icsi.berkeley.edu/pub/en/ 2This is a recursive name, since FORGERY IS AMNESIA is a
2806633975	Term Definitions Help Hypernymy Detection.	2407753702	e PMI-formulated score from co-occurrence counts. Dimension reduction is conducted by Singular Value Decomposition (SVD) before feeding representation vectors to a classiﬁer. Dependency-based Context Roller and Erk (2016) compute a syntactic distributional space for terms by counting their dependency neighbors across the corpus. Shwartz et al.(2017) further compare (i) contexts being parent and daughter nodes in the d
2806633975	Term Definitions Help Hypernymy Detection.	2407753702	mits the recall of this method; (ii) Term representation learning depends on a vector embedding of each term, where each entry in the vector expresses an explicit context feature (Baroni et al.,2012a;Roller and Erk, 2016;Shwartz et al.,2017) or a latent semantic (Fu et al.,2014;Vulic and Mrksic,2017;Glavas and Ponzetto,2017). Both approaches hinge on acquiring contextaware term meaning in a large corpus. The generali
2806633975	Term Definitions Help Hypernymy Detection.	1840435438	ralize to any lexical relation between terms. Technically, we implement HYPERDEF by modifying the AttentiveConvNet (Yin and Schutze¨ ,2017), a top-performing system on a textual entailment benchmark (Bowman et al., 2015), to model the input (x, d x; y, d y), where d i (i = x;y) is the deﬁnition of term i. In contrast to earlier work which mostly built separate representations for terms x and y, HYPERDEF instead direc
2806633975	Term Definitions Help Hypernymy Detection.	2302996098	simply a concatenation of two independent subsystems. HYPERDEF enables modeling across (distributional context, deﬁnition). This is expected to generate more indicative features than a similar work (Shwartz et al., 2016), which simply concatenated distributional models with path-based models; HYPERDEF employs deﬁnitions to provide richer information for the terms. But it does not generate an auxiliary term represent
2806716360	NLP-assisted software testing: a systematic review.	1916559533	NLP from different angles. Those sub-fields include: (1) Discourse Analysis[11], a rubric assigned to analyze the discourse structure of text or other forms of communication; (2) Machine Translation [12], intended to translate a text from one human language into another, with popular tools such as”Google Translate”; and (3) information extraction (IE), which is concerned with extracting inform ation
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	2403702038	016;Liu and Lane,2017a;Li et al.,2017b;Liu et al.,2018) for task-oriented dialogs. Wen et al. (2017) designed a supervised training end-to-end neural dialog model with modularly connected components. Bordes and Weston (2017) proposed a neural dialog model using end-to-end memory networks. These models are trained ofﬂine using ﬁxed dialog corpora, and thus it is unknown how well the model performance generalizes to online
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	1996957559	based on Gaussian process for dialog reward learning. These methods still require various levels of annotations of dialog ratings by users, either ofﬂine or online. On the other side of the spectrum, Paek and Pieraccini (2008) proposed inferring a reward directly from dialog corpora with inverse reinforcement learning (IRL) (Ng et al.,2000). However, most of the IRL algorithms are very expensive to run (Ho and Ermon,2016),
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	1522301498	cy network MLP is set as 100. For the discriminator model, a state size of 200 is used for the bidirectional LSTM. We perform mini-batch training with batch size of 32 using Adam optimization method (Kingma and Ba, 2014) with initial learning rate of 1e-3. Dropout (p= 0:5) is applied during model training to prevent the model from over-ﬁtting. Gradient clipping threshold is set to 5. During interactive learning with
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	2513380446	del performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) proposed an RL dialog agent for information access. Such models are trained against rule-based user simulators. A dialog reward from the user simulator is expected at the end of each turn or each dia
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	2581637843	duced by the generator from the real ones. The generator and the discriminator are jointly trained until convergence. GANs were ﬁrstly applied in image generation and recently used in language tasks. Li et al. (2017a) proposed conducting adversarial learning for response generation in open-domain dialogs. Yang et al. (2017) proposed using adversarial learning in neural machine translation. The use of adversarial
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	1932421248	is estimated using a number of dialog features such as number of turns and elapsed time. Yang et al. (2012) proposed a collaborative ﬁltering based method in estimating user satisfaction in dialogs. Su et al. (2015) studied using convolutional neural networks in rating dialog success. Su et al. (2016) further proposed an online active learning method based on Gaussian process for dialog reward learning. These me
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	1975244201	feedback. 2 Related Work Task-Oriented Dialog Learning Popular approaches in learning task-oriented dialog systems include modeling the task as a partially observable Markov Decision Process (POMDP) (Young et al., 2013). Reinforcement learning can be applied in the POMDP framework to learn dialog policy online by interacting with users (Gaˇsi c et al.´ ,2013). Recent efforts have been made in designing endto-end sol
2806936550	Adversarial Learning of Task-Oriented Neural Dialog Models	2513380446	rning dialog models from a datadriven approach using human-human or humanmachine conversations. Williams et al. (2017) designed a hybrid supervised and reinforcement learning end-to-end dialog agent. Dhingra et al. (2017) proposed an RL based model for information access that can learn online via user interactions. Such systems assume the model has access to a reward signal at the end of a dialog, either in the form o
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2162461580	(McCallum and Li, 2003; Finkel et al., 2005). NER systems have also been widely studied in medical NLP, using dictionary lookup methods (Savova et al., 2010), support vector machine (SVM) classiﬁers (Kazama et al., 2002), and sequential models (Tsai et al., 2006; Settles, 2004). In recent years, deep learning models have been used in NER with successful results in many domains (Collobert et al., 2011). Proposed neura
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2158899491	(SVM) classiﬁers (Kazama et al., 2002), and sequential models (Tsai et al., 2006; Settles, 2004). In recent years, deep learning models have been used in NER with successful results in many domains (Collobert et al., 2011). Proposed neural network architectures included hybrid convolutional neural network (CNN) and bi-directional long-short term Evaluation: [Scoring: 1=totally dependent, 2=requires assistance, 3=requir
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2093157872	al., 2016). Though smaller than PubMed, the MIMIC corpus is a large sample of clinical text, which is often difﬁcult to obtain and shows signiﬁcant linguistic differences with biomedical literature (Friedman et al., 2002). As MIMIC is clinical text, it is the closest comparison corpus to the BTRIS data; however, as MIMIC focuses on ICU care, the information in it differs signiﬁcantly from in-domain BTRIS documents. 4
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2614459432	e closest comparison corpus to the BTRIS data; however, as MIMIC focuses on ICU care, the information in it differs signiﬁcantly from in-domain BTRIS documents. 4 Methods We adopt the architecture of Dernoncourt et al. (2017a), due to its successful NER results on CoNLL and i2b2 datasets. The architecture, as depicted in Figure 2, is a stacked LSTM composed of: i) character Bi-LSTM layer that generates character embeddin
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2405884322	hat embeddings trained on a very small corpus of highly relevant documents nearly match the performance of embeddings trained on extremely large out-of-domain corpora, adding to the recent ﬁndings of Diaz et al. (2016). To our knowledge, this is the ﬁrst investigation into automatically recognizing descriptions of patient functioning. Viewing this problem through an NER lens provides a robust framework for model de
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2096765155	igh resource languages such as English. Many of the successful existing NER systems use a combination of engineered features trained using conditional random ﬁelds (CRF) model (McCallum and Li, 2003; Finkel et al., 2005). NER systems have also been widely studied in medical NLP, using dictionary lookup methods (Savova et al., 2010), support vector machine (SVM) classiﬁers (Kazama et al., 2002), and sequential models
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2405884322	includes only a few hundred documents from therapy disciplines among its two million notes. While recent work suggests that using a training corpus from the target domain can mitigate a lack of data (Diaz et al., 2016), even a careful corpus selection may not produce sufﬁ- arXiv:1806.02814v1 [cs.CL] 7 Jun 2018 cient data to train robust word representations. In this paper, we explore the use of an RNN model to reco
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2614459432	lens of named entity recognition (NER), as recent work has illustrated the potential of using recurrent neural network (RNN) NER models to address similar issues in biomedical NLP (Xia et al., 2017; Dernoncourt et al., 2017b; Habibi et al., 2017). An additional strength of RNN models is their ability to leverage pretrained word embeddings, which capture co-occurrence information about words from large text corpora. Prio
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2493916176	moment estimation (Adam) optimization technique (Kingma and Ba, 2014). 4.1 Embedding training We use two popular toolkits for learning word embeddings: word2vec3 (Mikolov et al., 2013) and FastText4 (Bojanowski et al., 2017). We run both toolkits using skip-gram with negative sampling to train 300-dimensional embeddings, and use default settings for all other hyperparameters.5 4.2 Domain adaptation methods We evaluate se
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2159640576	NER systems have also been widely studied in medical NLP, using dictionary lookup methods (Savova et al., 2010), support vector machine (SVM) classiﬁers (Kazama et al., 2002), and sequential models (Tsai et al., 2006; Settles, 2004). In recent years, deep learning models have been used in NER with successful results in many domains (Collobert et al., 2011). Proposed neural network architectures included hybrid co
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2614459432	oth character and word, with a CRF layer on the top of the network. In the biomedical domain, Habibi et al. (2017) used this architecture for chemical and gene name recognition. Liu et al. (2017) and Dernoncourt et al. (2017a) adapted it for state-of-the-art note deidentiﬁcation. In terms of functioning, Kukafka et al. (2006) and Skube et al. (2018) investigate the presence of functioning terminology in clinical data, bu
2807141747	Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility.	2168041406	ta. The entities, deﬁned for this task as contiguous text spans completely describing an aspect of mobility, tend to be quite long: while prior NER datasets such as the i2b2/VA 2010 shared task data (Uzuner et al., 2012) include fairly short entities (2.1 tokens on average for i2b2), Mobility entities Entity Train Valid Test Mobility 1,533 467 947 ScoreDeﬁnition 82 24 48 Table 1: Named entity statistics for training,
2807228710	Measuring Conversational Productivity in Child Forensic Interviews.	1828401780	ows success in tracking and modeling topics discussed in dyadic conversations. One source of inspiration for our work is the topic modeling performed in DiaSumm [14], which uses tf-idf and TextTiling [15] to ﬁnd topics and their boundaries in the CALLHOME dataset. However, since we tasked with scoring the relevance of a response, our approach only uses the interviewers side of the dialogue to extract
2807228710	Measuring Conversational Productivity in Child Forensic Interviews.	397704218	s in the CALLHOME dataset. However, since we tasked with scoring the relevance of a response, our approach only uses the interviewers side of the dialogue to extract the topics. The work presented in [16] takes advantage of knowledgegraphs (e.g. Wikipedia) to improve dialogue summary and topic identiﬁcation robustness. However within the speciﬁc application of Child FI, the vocabulary used by the clie
2807232219	Leolani: A reference machine with a theory of mind for social communication	2149234824	’ concept states that children at some stage of their development become aware that other people’s knowledge, beliefs, and perceptions may be untrue and/or dierent from theirs. Scassellati ([19] and [18]) was the rst to argue that humanoid robots should also have such an awareness. We take his work as a starting point for implementing these principles in a Pepper robot, in order to drive social inter
2807232219	Leolani: A reference machine with a theory of mind for social communication	2293816024	ather than the robot’s. These studies re ect on the phenomenon of anthropomorphism [15] [7]: the human tendency to project human attributes to nonhuman agents such as robots. Closer to our work comes [10] who use the notion of a theory of mind to deal with human variation in response. The robot runs a simulation analysis to estimate the cause of variable behaviour of humans and likewise adapts the res
2807232219	Leolani: A reference machine with a theory of mind for social communication	2097117768	hese modules run continuously as the robot attempts to learn and recognize its surroundings. Speech detection is performed using WebRTC[3] and object recognition has been built on top of the Inception[22] neural network through TensorFlow[1]. During conversation the robot utilizes face recognition and speech recognition to understand who says what. Face recognition has been implemented using OpenFace[
2807232219	Leolani: A reference machine with a theory of mind for social communication	2107315187	on and observations processed by the robot in an articial brain (a triple store) using the GRaSP model [8]. For modeling the interpretation of the world, GRaSP relies on the Simple Event Model (SEM) [23] an RDF model for representing instances of events. RDF triples are used to relate event 4 Piek Vossen, Selene Baez, Lenka Bajcetic, Bram Kraaijeveld instances with sem:hasActor, sem:hasPlace and se
2807232219	Leolani: A reference machine with a theory of mind for social communication	2095436958	robot’s brain. To the best of our knowledge, we are the rst that complement the pioneering work of Scassellati with further components for an explicit model of the theory of mind for robots (see also [13] for a recent overview of the stateof-the-art for human-robot interactive communication). There is a long-tradition of research on multimodal communication [16], human-computer-interfacing [6], and ot
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	2251230461	e is not very friendly to newcomers. QAWI The QALB Annotation Web Interface (QAWI) ﬁrst introduced the concept of token-based text edits for annotating parallel corpora used in text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014). It allowed for the exact recording of all modiﬁcations performed by the annotator which previous tools did not. As we show later on, we utilize this token-based editing syst
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	2250358209	er annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have inﬂuenced the design of our system. DIWAN DIWAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to precompute analyses, and the ability to apply analyses to multiple occurrence
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	2158762489	n tools BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013). For task speciﬁc annotation tools, we can cite the post-editing and error correction tools such as the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle a speciﬁc NLP task and it is not easy
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	201141796	spelling conventionalization. In this paper we present a tool that allows one to do all of these tasks together, eliminating the possibility of error 1All transliteration is in the Buckwalter scheme (Habash et al., 2007). propagation from one annotation level to another. Our tool is named MADARi2 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (MADAR). Next, we present rel
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	1560760997	tenetorp et al., 2012), WebAnno (Yimam et al., 2013). For task speciﬁc annotation tools, we can cite the post-editing and error correction tools such as the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle a speciﬁc NLP task and it is not easy to adapt them to our project.
2807331087	MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction.	1987952203	veral existing annotation tools, however, they are designed to handle a speciﬁc NLP task and it is not easy to adapt them to our project. We can cite tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and Elghobashy et al. (2014) and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool and more recently MADAD (
2807475619	Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition.	2158899491	RNN. In our experiments, we show the efﬁciency of our model in handling OOV words and bilingual word context. 2 Related Work Convolutional Neural Network (CNN) was used in NER task as word decoder byCollobert et al. (2011) and a few years later,Huang et al. (2015) introduced Bidirectional Long-Short Term Memory (BiLSTM) (Sundermeyer et al.,2012). Character-level features were explored by using neural architecture and r
2807475619	Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition.	2780932362	witter data are very noisy, there are many spelling mistakes, irregular ways to use a word and repeating characters. We apply several strategies to overcome the issue. We use 300-dimensional English (Mikolov et al., 2018) and Spanish (Grave et al.,2018) FastText pre-trained word vectors which comprise two million words vocabulary each and they are trained using Common Crawl and Wikipedia. To create the shared vocabula
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	1911240979	bility of better deals, etc. (Table7). Figure2shows the distribution of suggestions and sentiments (expressed towards the hotel) in a hotel review dataset annotated with sentiments by Wachsmuth et al [22]. In this section we answered RQ1, i.e., How do we dene suggestions in the context of open domain suggestion mining? We provided a formal denition for suggestions by identifying four categories of s
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2097606805	certain products. The Feedly mobile application forum and the Windows developer forum are openly accessible. A sample of posts were scraped and split into sentences using the Stanford CoreNLP toolkit [11]. The class ratio in the dataset obtained from suggestion forums is more balanced than the other domains. Many suggestions are in the form of requests, which is less frequent in other domains. The tex
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2160660844	d with positive, negative, con ict, and neutral sentiments. We take a smaller subset of these reviews, where each statement is considered as a sentence in our dataset. Electronics reviews. Hu and Liu [9] provide a dataset comprising of reviews of dierent kinds of electronic products obtained from the website of Amazon.7 The Amazon website collects and displays online reviews of listed products.Hu an
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	1911240979	have given an option to stop this auto-focusing Camera Neutral Yes Fig. 2: Distribution of sentiment polarities towards the hotel in suggestion and non-suggestion sentences for a hotel review dataset [22]. 4 Creating Benchmark Datasets for Suggestion Mining Based on our preliminary annotation study, problem denition and scope of suggestions, we propose a two phase method for manually annotating sente
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2125770490	h as A sentence made by a person, usually as a suggestion or a guide to action and/or conduct relayed in a particular context. The other was to provide an application specic denition of suggestions [3,14,19] such as Sentences where the commenter wishes for a change in an existing product or service. Although the rst category is generic and applies to all domains, the publications listed evaluated suggest
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2093709025	n Source Dataset available Ramanand et al [19] Product reviews No Viswanathan et al [21] Product reviews No Brun and Hagege [3] Product reviews No Moghaddam [14] Hotel reviews No Wicaksono and Myaeng [23,24,25] Travel forum Yes Dong et al [6] Twitter Yes None of the studies that proposed and evaluated rule-based systems for this task [3,19,21], performed an annotation study and provided inter-annotator agre
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2465978385	nd subjective type of text. Therefore, a suggestion bearing sentence may be associated to multiple sentiments. In the case of reviews, some sentiment analysis benchmark datasets like SemEval datasets [17] exclude text that is not about the opinion target, even though the text is found within the same review. The guidelines from the SemEval 2015 Sentiment Analysis task [17] read: Quite often reviews co
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	1990435674	needs Microsoft really did travel tips apps really needs need United States want hope Microsoft Mood. Suggestion expressions often contain what may be referred to as subjunctive and imperative moods [15,16]. Subjunctive mood is a commonly occurring language phenomenon in Indo-European languages, which is typically used in subordinate clauses to express an action that has not yet occurred, 12 Sapna Negi
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2093709025	ner that will suit both open domain and domain specic suggestion mining. Previous work that attempted to dene suggestions, did so in two ways. One was to provide a dictionary-like generic denition [21,23] such as A sentence made by a person, usually as a suggestion or a guide to action and/or conduct relayed in a particular context. The other was to provide an application specic denition of suggesti
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2125770490	ng 5 Table 2: An overview of related work and available datasets. Publication Source Dataset available Ramanand et al [19] Product reviews No Viswanathan et al [21] Product reviews No Brun and Hagege [3] Product reviews No Moghaddam [14] Hotel reviews No Wicaksono and Myaeng [23,24,25] Travel forum Yes Dong et al [6] Twitter Yes None of the studies that proposed and evaluated rule-based systems for t
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2093709025	nts in online reviews. Also, product reviews were popularly used in the related work for suggestion mining. The choice of travel forum dataset was also inspired from the related work on advice mining [23]. Software suggestion forum was chosen because review datasets were highly imbalanced for explicit suggestions, while suggestion forums had a better presence of explicitly expressed suggestions. Also,
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2093709025	as provided in related work, the formal annotation guidelines were still missing. Importantly, such denitions cannot be used to dene the scope of open domain suggestion mining. Wicaksono and Myaeng [23,24,25] performed the extraction of what they refer to as the advice revealing sentences from travel related weblogs and online forums, using supervised learning methods. Their dataset is available to the re
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	2093709025	Tibet [8]. The imperative mood is used to express the requirement that someone perform or refrain from an action. A typical example would be Take an umbrella, Pray everyday [18]. Wicaksono and Myaeng [23,24,25] and Dong et al [6] used the imperative mood as a feature with their classiers. However, subjunctive mood has not been associated with suggestions in previous suggestion mining studies. Sentiment. In
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	1911240979	ts for suggestion mining. On top of that we re-use existing datasets, viewing the labels originally provided as Phase 1 annotations of our two-stage annotation process. Hotel reviews. Wachsmuth et al [22] provide a large dataset of hotel reviews from the TripAdvisor6 website. They segmented the reviews into statements so that each statement has only one sentiment label and have manually labeled the se
2807547831	Open Domain Suggestion Mining: Problem Definition and Datasets.	1514728320	ually read over a short span of time. Apart from suggestions that relate to general topics, industrial and other organizational decision makers seek suggestions to improve their brand or organization [10]. In this case, consumers or other stakeholders are explicitly asked to provide suggestions. Opinions towards persons, brands, social debates etc. are generally expressed through online reviews, blogs
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2105847779	-art parsers work incrementally internally, both for semantic and for syntactic parsing: they use a transition system with a scorer and optionally combine that with beam search to ﬁnd the best parse (Nivre, 2008; Huang and Sagae, 2010; Dyer et al., 2015; Swayamdipta et al., 2016; Kiperwasser and Goldberg, 2016; Zhou et al., 2016; Damonte et al., 2017, inter alia). While they build a structure incrementally g
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2023612782	– the difference between when an output is made (i.e. which amount of input data has been consumed) and the timing of the corresponding input can be measured to obtain an anchored timeliness measure (Baumann et al., 2011). Both the relation to the ﬁrst occurrence of an output (FO) and the relation to the last change of an output (ﬁnal decision, FD) can be measured. E.g. if a word ends at 2.5 seconds of the input audio
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2119631826	. to implement a gatekeeper. Its policy can be guided by observing the time that a hypothesis survived without being discarded (Baumann et al., 2009) or based on the internal state of the recognizer (Selfridge et al., 2011). McGraw and Gruenstein (2012) show that even sophisticated stability estimation only slightly improves upon the simple age-based estimation by (Baumann et al., 2009). Given that the SR is the same fo
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2003791490	al output. The characteristics of this (human) process varies by language, e.g. the delay is relatively long when translating from German to English because the verb in the input tends to be delayed (Goldman-Eisler, 1972). 3.6 Trade-Off between properties Incremental components have to make a trade-off between timeliness (i.e. the amount of delay introduced between input and output), output quality, and the amount of
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2301095666	arsing: they use a transition system with a scorer and optionally combine that with beam search to ﬁnd the best parse (Nivre, 2008; Huang and Sagae, 2010; Dyer et al., 2015; Swayamdipta et al., 2016; Kiperwasser and Goldberg, 2016; Zhou et al., 2016; Damonte et al., 2017, inter alia). While they build a structure incrementally going from left to right, they don’t produce intermediate structures meant for incremental consumptio
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1539309091	better ﬁt than the one created by the rules. Kohn and Baumann (2016) showed that using the delexicalized predictions to¨ augment 5-gram language models improves perplexity on the Billion Word Corpus (Chelba et al., 2013). 4.4 Natural language generation Skantze and Hjalmarsson (2013) compare a non-incremental dialogue system and an incremental one, with which language learners interact to buy items at a ﬂea market. T
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2419292002	correct assignment to “a” stays in output. Dashed: exempliﬁes output used to compute inc acc (incremental accuracy). EO: edit overhead. Accuracy: measured on complete output. 5.1 Measuring timeliness Cho and Esipova (2016) propose to measure timeliness by counting for each output element tof an output sequence Yhow many input elements from the input sequence Xhave been consumed before its production (s(t)). ˝(X;Y) then
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2133564696	ecoded again. This can be performed using recurrent networks which represent the input as a single ﬁxed-length vector and possibly modeling attention to the input when generating the output sequence (Bahdanau et al., 2014) or even without a recurrent network, using only attention to model the inﬂuences from the input sequence to the output sequence to generate (Vaswani et al., 2017). In all these cases, all input is co
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2123893795	eedy parsing (i.e. it does not use a beam). This accuracy drops signiﬁcantly to 56% without lookahead because the parser often commits to a structure incompatible with the continuation of a sentence. Roark (2001) presents a top-down phrase structure parser that performs beam-search to generate connected intermediate structures for every sentence preﬁx. As the parser is based on a probabilistic generative mode
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1991646969	emental components have to make a trade-off between timeliness (i.e. the amount of delay introduced between input and output), output quality, and the amount of non-monotonicity (Beuck et al., 2011a; Baumann et al., 2009). High-quality, monotonic output can be obtained by delaying all output. This strategy taken to the extreme results in a non-incremental system that only produces one complete output once all input is
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2394671344	er. Its policy can be guided by observing the time that a hypothesis survived without being discarded (Baumann et al., 2009) or based on the internal state of the recognizer (Selfridge et al., 2011). McGraw and Gruenstein (2012) show that even sophisticated stability estimation only slightly improves upon the simple age-based estimation by (Baumann et al., 2009). Given that the SR is the same for the incremental case as for
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2146734135	erances (DeVault et al., 2009), training actor policies for rapid task-based dialogue (Paetzel et al., 2015), continuous understanding and acting (Stoness et al., 2005), incremental repair detection (Hough and Purver, 2014), or incremental reference resolution (Schlangen et al., 2009). 2 Psycholinguistic evidence Humans still pose the gold standard for language processing, especially if the processing does not happen on
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2461231802	imal for simultaneous translations. Human simultaneous translators produce sentences that systematically deviate from the “normal” target language but such data is not readily available for training (He et al., 2016). The training data is adapted by generating phrase-structure trees for the target sentences and applying (manually written) syntax-based reordering rules. The system then checks whether the reorderin
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1986532700	LP has been primarily driven by speech-based research questions, such as understanding based on partial speech recognition (Sagae et al., 2009), determining when to respond during ongoing utterances (DeVault et al., 2009), training actor policies for rapid task-based dialogue (Paetzel et al., 2015), continuous understanding and acting (Stoness et al., 2005), incremental repair detection (Hough and Purver, 2014), or in
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2515003191	and optionally combine that with beam search to ﬁnd the best parse (Nivre, 2008; Huang and Sagae, 2010; Dyer et al., 2015; Swayamdipta et al., 2016; Kiperwasser and Goldberg, 2016; Zhou et al., 2016; Damonte et al., 2017, inter alia). While they build a structure incrementally going from left to right, they don’t produce intermediate structures meant for incremental consumption; the intermediate states consist of sev
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2153190884	to perform a speciﬁc task. compared to text, research on incrementality in NLP has been primarily driven by speech-based research questions, such as understanding based on partial speech recognition (Sagae et al., 2009), determining when to respond during ongoing utterances (DeVault et al., 2009), training actor policies for rapid task-based dialogue (Paetzel et al., 2015), continuous understanding and acting (Stone
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1991646969	r) standpoints: ﬁrst, how much intermediate output will be retracted again? And second: how sure can we be that a certain output is reliable, i.e. will also be part of the ﬁnal output of a processor? Baumann et al. (2009) and Baumann (2013) tackle the ﬁrst question by deﬁning the edit overhead generated by a non-monotonic processor producing sequential output by deﬁning three edit operations on an output sequence: add
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2134036914	rnally, both for semantic and for syntactic parsing: they use a transition system with a scorer and optionally combine that with beam search to ﬁnd the best parse (Nivre, 2008; Huang and Sagae, 2010; Dyer et al., 2015; Swayamdipta et al., 2016; Kiperwasser and Goldberg, 2016; Zhou et al., 2016; Damonte et al., 2017, inter alia). While they build a structure incrementally going from left to right, they don’t produc
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2105847779	sentence during each step, making the computation non-incremental as it depends on the complete input being available. Noji and Miyao (2014) propose a transition system based on the ones discussed by Nivre (2008) that introduces dummy nodes which denote an expectation of upcoming words. This transition system still produces disconnected trees for sentence preﬁxes but is able to predict processing difﬁculties
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2156985047	sor. 4Data that is discrete along the time axis can of course include continuous data such as word embeddings. 5At least not on the word level, but alignments can be generated automatically, see e.g. Och and Ney (2003) Non-monotonic output can only be generated sensibly if the consumers of the output can deal with non-monotonic input. Otherwise these consumers might ignore the revisions made to previous output and
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2250764876	STMs (such as Kiperwasser and Goldberg (2016)) effectively make use of the whole sentence during each step, making the computation non-incremental as it depends on the complete input being available. Noji and Miyao (2014) propose a transition system based on the ones discussed by Nivre (2008) that introduces dummy nodes which denote an expectation of upcoming words. This transition system still produces disconnected t
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1991646969	tally even for non-incremental use-cases. It is therefore possible to look into a speech recognizer (SR) to obtain the most probable hypothesis at each point in time without modifying the recognizer (Baumann et al., 2009). Because the SR is not changed from the non-incremental one, the only optimization point is when to let new output through, i.e. to implement a gatekeeper. Its policy can be guided by observing the t
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	1991646969	ternal state of the recognizer (Selfridge et al., 2011). McGraw and Gruenstein (2012) show that even sophisticated stability estimation only slightly improves upon the simple age-based estimation by (Baumann et al., 2009). Given that the SR is the same for the incremental case as for the non-incremental one, no trade-off is performed against the accuracy. In addition, Selfridge et al. (2011) noted that if during decod
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2119631826	ut of multiple processors. To bridge the gap between certainty (monotonic output) and uncertainty (non-monotonic output), the likelihood of an output being stable could be attached to the output (see Selfridge et al. (2011) § 5). Modern NLP processors heavily rely on sub-symbolic representation and use attention mechanisms to obtain the relevant information. With this approach, an explicit grounding for partial recomput
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2124861326	work incrementally internally, both for semantic and for syntactic parsing: they use a transition system with a scorer and optionally combine that with beam search to ﬁnd the best parse (Nivre, 2008; Huang and Sagae, 2010; Dyer et al., 2015; Swayamdipta et al., 2016; Kiperwasser and Goldberg, 2016; Zhou et al., 2016; Damonte et al., 2017, inter alia). While they build a structure incrementally going from left to right
2807563632	Incremental Natural Language Processing: Challenges, Strategies, and Evaluation	2131698806	yielding – in contrast to the approaches described in § 4.3.1 – the same accuracy for complete sentences as its non-incremental counterpart. The underlying parser performs a graph-based optimization (Martins et al., 2013) and has no notion of incrementality; instead, the gold standard the parser is trained on is adapted to consist of syntactic structures for sentence preﬁxes which contain prediction nodes as stand-ins
2807710978	Challenges of language technologies for the indigenous languages of the Americas	1587787082	0hourswithTranscription Cavar et al. (2016) Speech Blackfoot, Nata, Gitksan, Okanagan, Tlingit, Plains Cree, Ktunaxa, Coeur d’Alene, Kwak’wala 19.8 GB Dunham et al. (2014) Speech Mapudungun 170 hours Huenchullan (2000) and Monson et al. (2004) Morphological Inﬂection Quechua, Navajo, Haida 31K words Cotterell et al. (2017) Morphological Inﬂection 20 Oto-Manguean languages 13K verbs Palancar and Feist (2015) Morphol
2807710978	Challenges of language technologies for the indigenous languages of the Americas	1587787082	ol they gather 19,187word forms, 324texts and 18.8GB of audio. A collection of datasets have been developed for the Mapudungun or Mapuche language spoken mainly in Chile (Araucanian language family) (Huenchullan, 2000; Monson et al., 2004). An audio dataset with 170hours of spoken Mapudungun, that covers three dialects (120hours of Nguluche, 30 hours of Lafkenche and 20hours of Pewenche) has been released. This re
2807710978	Challenges of language technologies for the indigenous languages of the Americas	1905100302	ords, but also in assigning tags to each part of the word. There are several approaches to do these tasks, i.e., rule-based, semi-supervised and unsupervised (Goldsmith, 2001; Creutz and Lagus, 2002; Kohonen et al., 2010). Some examples of rule-based methods applied to the Americas languages are the Finite State approaches to model the morphology of a language: plains Cree (Arppe et al., 2017; Harrigan et al., 2017; W
2807710978	Challenges of language technologies for the indigenous languages of the Americas	2611767671	pus (Mayer and Cysouw, 2014) and trained a multilingual NMTmodel toimprove overall translation performance. Experiments included Oto-manguean, Quechua and Mayan families. Moreover, empirical results (Vania and Lopez, 2017) show that problem of data sparsity of rich morphological languages can be handled with subword models: the usage of character level NMT improve performance over token level translation and unsupervis
2807710978	Challenges of language technologies for the indigenous languages of the Americas	2250339149	s or tokens (Mager Hois et al., 2016; Mager Hois, 2017)13. A similar case can be found for the Nahuatl-Spanish pair. Uto-Aztecan languages can be highly agglutinative with the polysynthetic tendency, Gutierrez-Vasques (2015) extracts bilingual correspondences from a parallel corpus, by aligning the Nahuatl non-grammatical morphs to Spanish words. Another example was collected for the pair Mixteco-Spanish (Santiago, 2017)
2807710978	Challenges of language technologies for the indigenous languages of the Americas	2117621558	in the segmentation of words, but also in assigning tags to each part of the word. There are several approaches to do these tasks, i.e., rule-based, semi-supervised and unsupervised (Goldsmith, 2001; Creutz and Lagus, 2002; Kohonen et al., 2010). Some examples of rule-based methods applied to the Americas languages are the Finite State approaches to model the morphology of a language: plains Cree (Arppe et al., 2017; H
2807710978	Challenges of language technologies for the indigenous languages of the Americas	2117621558	sity of rich morphological languages can be handled with subword models: the usage of character level NMT improve performance over token level translation and unsupervised morphological segmentation (Creutz and Lagus, 2002). But their experiment also conclude that a canonical segmentation enhances character level translation. In order to alleviate the lack of resources automatic data recollection has been proposed, this
2807753879	Generative Neural Machine Translation	1959608418	(yjz(s);x) using beam search. end while 2.2 Training To learn the parameters of the model, we use stochastic gradient variational Bayes (SGVB) to perform approximate maximum likelihood estimation [Kingma and Welling, 2014, Rezende et al., 2014]. To do this, we parameterize a Gaussian inference distribution q ˚ (zjx;y) = N( (x;y); ˚(x;y)). This allows us to maximize the following lower bound on the log likelihood: log
2807753879	Generative Neural Machine Translation	2133564696	1;:::;T x, are: p(x t= v xjx 1;:::;x t 1;z) /exp((W xhx t )e(v x)) (2) where hx t is computed as: hx t = LSTM(h x t 1 ;z;e(x 1)) (3) 2.1.2 Target sentence To compute p (yjx;z), we modify RNNSearch [Bahdanau et al., 2015] to accommodate the latent variable z. Firstly, the source sentence is encoded using a bidirectional LSTM. The encoder hidden states for t= 1;:::;T xare computed as: henc t= ! LSTM(henc t 1 ;z;e(x))
2807753879	Generative Neural Machine Translation	2549476280	nce across to the decoder, because the cross-language parameter sharing prevents this. 3 Related work Whilst there have been many attempts at designing generative models of text [Bowman et al., 2016, Dieng et al., 2017, Yang et al., 2017], their usage for translation has been limited. Most closely related to our work is Variational Neural Machine Translation (VNMT) [Zhang et al., 2016], which introduces a latent va
2807753879	Generative Neural Machine Translation	2210838531	ply copying the sentence across to the decoder, because the cross-language parameter sharing prevents this. 3 Related work Whilst there have been many attempts at designing generative models of text [Bowman et al., 2016, Dieng et al., 2017, Yang et al., 2017], their usage for translation has been limited. Most closely related to our work is Variational Neural Machine Translation (VNMT) [Zhang et al., 2016], which in
2807753879	Generative Neural Machine Translation	2210838531	we use the following two techniques: KL divergence annealing We multiply the KL divergence term by a constant weight, which we linearly anneal from 0 to 1 over the ﬁrst 50,000 iterations of training [Bowman et al., 2016, Sønderby et al., 2016]. Word dropout In equation (3), the dependence of the hidden state on the previous word means that the RNN can often afford to ignore the latent variable whilst still maintaini
2807821078	Multimodal Grounding for Language Processing	2120526973	4) and Collell et al. (2017) proceed in the reverse text-to-image direction to ground words in the visual world. Similar propagation approaches had already been examined by Johns and Jones (2012) and Hill and Korhonen (2014), but they used human-elicited perceptual features from the McRae dataset (McRae et al., 2005) instead of automatically derived image representations. Joint learning The mapping approaches assume a di
2807821078	Multimodal Grounding for Language Processing	155596317	airs of words that have been annotated with similarity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations outperform unimodal ones (Feng and Lapata, 2010; Silberer and Lapata, 2012; Bruni et al., 2014; Kiela et al., 2014). Kiela et al. (2016) perform a comparison of different image sources and architectures and their ability to model semantic similari
2807821078	Multimodal Grounding for Language Processing	2251970440	been annotated with similarity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations outperform unimodal ones (Feng and Lapata, 2010; Silberer and Lapata, 2012; Bruni et al., 2014; Kiela et al., 2014). Kiela et al. (2016) perform a comparison of different image sources and architectures and their ability to model semantic similarity. Despite the advantages
2807821078	Multimodal Grounding for Language Processing	2251970440	An approach inspired by topic modeling interprets aligned data as a multimodal document and uses Latent Dirichlet Allocation to derive multimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the skip-gram model by Mikolov et al. (2013) with visual
2807821078	Multimodal Grounding for Language Processing	2112184938	an arithmetic operation (e.g., average, max) over the concept representations for each word in the sequence. Multimodal fusion is then performed on this averaged representation (Glavaˇs et al., 2017; Bruni et al., 2014). Shutova et al. (2016) work with short phrases consisting of two words and directly learn phrase representations. Missing concept representations in one modality can be obtained by mapping functions
2807821078	Multimodal Grounding for Language Processing	1486723856	he average pairwise cosine distance in a set of images to model the assumption that an image collection for an abstract concept like happiness is more diverse than for a concrete concept like ladder. Lazaridou et al. (2015) and Hessel et al. (2018) propose alternative concreteness measures based on the same idea. Unfortunately, these measures are highly dependent on the image retrieval algorithm which might be optimized
2807821078	Multimodal Grounding for Language Processing	2250815964	detail from both, engineering and theoretical perspectives. Multimodal grounding of verbs Verbs play a fundamental role for expressing relations between concepts and their situational functionality (Hartshorne et al., 2014). The dynamic nature of verbs poses a challenge for multimodal grounding. To our best knowledge, only Hill et al. (2014) and Collell et al. (2017) consider verbs in their evaluation. They report that
2807821078	Multimodal Grounding for Language Processing	2184188583	cognition (Juang and Rabiner, 2005) and subtitle generation (Daelemans et al., 2004). For lipreading tasks, mute video input of people speaking is transformed into text representing their utterances (Ngiam et al., 2011). In these cross-modal transfer tasks, synchronous processing of the input in one modality is not directly inﬂuenced by information from the output modality. The main challenge lies in ﬁnding appropri
2807821078	Multimodal Grounding for Language Processing	2286410738	conceptual grounding indicate that providing a multimodal representation for abstract concepts is signiﬁcantly more challenging due to the lack of perceptual patterns associated with abstract words (Hill et al., 2014). For grounding phrases, the meaning for concrete and abstract concepts need to be combined (see Section 3.3). Bruni et al. (2012) examine the compositional meaning of color adjectives and ﬁnd that mu
2807821078	Multimodal Grounding for Language Processing	2250742840	conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations
2807821078	Multimodal Grounding for Language Processing	2150206717	d present the most relevant information in an intuitive way (Kucher and Kerren, 2015). The most popular approach are so-called word clouds which are frequency-based visualizations for topic modeling (Bateman et al., 2008). More recent approaches include semantic relations between words for a more conceptual-driven interpretation (Xu et al., 2016). Concept maps highlight structural relations between concepts in a graph
2807821078	Multimodal Grounding for Language Processing	2250742840	d their visual counterparts (supervised max-margin objective for (c m 1 ;c m 2 )). In their approach, the visual representations remain ﬁxed, but the textual representations are learned from scratch. Silberer and Lapata (2014) go one step further and use stacked multimodal autoencoders to simultaneously learn good representations for each modality (unsupervised reconstruction objective for m 1 and m 2) and their optimal mu
2807821078	Multimodal Grounding for Language Processing	2286410738	dal representations that has been established by several researchers would mainly be due to a more robust representation of highly redundant information. The results by Silberer and Lapata (2014) and Hill et al. (2014) support the intuitive assumption that textual representations better model textual similarity and visual representations better model visual similarity. As the multimodal models improve on both simil
2807821078	Multimodal Grounding for Language Processing	2112184938	detail and to develop more elaborate models of selective multimodal grounding. 5.1 Combining complementary information Different modalities contribute qualitatively different conceptual information. Bruni et al. (2014) argue that highly relevant visual properties are often not represented by linguistic models because they are too obvious to be explicitly mentioned in text (e.g., birds have wings, violins are brown)
2807821078	Multimodal Grounding for Language Processing	1956340063	by deﬁnition subjective and divergent solutions can be equally valid. Accumulations over various human ratings are currently considered to be better quality approximations than any automatic metrics (Vedantam et al., 2015). 2.3 Joint multimodal processing Due to a wave of experimental ﬁndings that support the cognitive theory of embodied processing, the separating aspects between different modalities have become blurre
2807821078	Multimodal Grounding for Language Processing	2604673901	e imSitu dataset which consists of images depicting verbs and annotations which link the verb arguments to visual referents. This dataset can be used for the multimodal task of situation recognition (Mallya and Lazebnik, 2017; Zellers and Choi, 2017), and it serves as a multimodal resource for verb processing. Grounding verbs is particularly challenging because of the variety of their possible visual instantiations. For e
2807821078	Multimodal Grounding for Language Processing	2252122141	e textual and the visual modality has received most attention for conceptual grounding, but perceptual information from the auditory and the olfactory channel have also been used for dedicated tasks (Kiela et al., 2015; Kiela and Clark, 2017). In order to provide a more concrete discussion, we focus on the combination of textual and visual cues for the remainder of the survey. The quality of concept representations
2807821078	Multimodal Grounding for Language Processing	2112184938	ermine cross-modal similarity. In order to smooth the concatenated representations while maintaining multimodal correlations, dimensionality reduction techniques such as singular value decomposition (Bruni et al., 2014) or canonical correlation analysis (Silberer and Lapata, 2012) have been applied. 3.2 Projection In practice, concepts that have a representation in one modality are not necessarily covered by represe
2807821078	Multimodal Grounding for Language Processing	2620761940	ernal knowledge bases. Novel interactive approaches make it possible to directly modulate the information ﬂow in one modality by input from another modality (Vries et al., 2017) or by human feedback (Ling and Fidler, 2017). The main challenge for joint processing lies in efﬁciently combining information from the modalities, so that redundant information is integrated without losing complementary cues. In human language
2807821078	Multimodal Grounding for Language Processing	2286410738	he highest quartile (135 pairs), low embodiment contains pairs with embodiment ratings in the lowest quartile (81 pairs) like know-decide.5 Coherent with previous work on concrete and abstract nouns (Hill et al., 2014), it can be 1The pre-trained embeddings and the script to reproduce our results are available for research purposes: https://github.com/UKPLab/coling18-multimodalSurvey. 2Two pairs had to be excluded
2807821078	Multimodal Grounding for Language Processing	2739585628	ists of images depicting verbs and annotations which link the verb arguments to visual referents. This dataset can be used for the multimodal task of situation recognition (Mallya and Lazebnik, 2017; Zellers and Choi, 2017), and it serves as a multimodal resource for verb processing. Grounding verbs is particularly challenging because of the variety of their possible visual instantiations. For example, an image of an ad
2807821078	Multimodal Grounding for Language Processing	1854884267	ity to model semantic properties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for the two concepts. Several evaluat
2807821078	Multimodal Grounding for Language Processing	2124033848	labeling (Frome et al., 2013). In this task, the mapping approach is applied in the image-to-text direction to classify unknown objects in images based on their semantic similarity to known objects (Socher et al., 2013) . Lazaridou et al. (2014) and Collell et al. (2017) proceed in the reverse text-to-image direction to ground words in the visual world. Similar propagation approaches had already been examined by Joh
2807821078	Multimodal Grounding for Language Processing	2137735870	due to the lack of perceptual patterns associated with abstract words (Hill et al., 2014). For grounding phrases, the meaning for concrete and abstract concepts need to be combined (see Section 3.3). Bruni et al. (2012) examine the compositional meaning of color adjectives and ﬁnd that multimodal representations are superior in modeling color. However, they fail to distinguish between literal and non-literal usage o
2807821078	Multimodal Grounding for Language Processing	1486723856	ltimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the skip-gram model by Mikolov et al. (2013) with visual features. Their model optimizes two constraints: the representation of concepts cwith respect to their textual contexts (unsupervised s
2807821078	Multimodal Grounding for Language Processing	2112912048	mapping between the two modalities and can be adjusted to induce a directional projection for zero shot learning. Joint learning of multimodal representations is very popular in the vision community (Karpathy et al., 2014; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). 3.3 Compositional representations For tasks that require representing longer sequences, a na¨ıve approach is sequence-level fusion. In this s
2807821078	Multimodal Grounding for Language Processing	2557865186	modalities and can be adjusted to induce a directional projection for zero shot learning. Joint learning of multimodal representations is very popular in the vision community (Karpathy et al., 2014; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). 3.3 Compositional representations For tasks that require representing longer sequences, a na¨ıve approach is sequence-level fusion. In this setting, the unimodal sequence repres
2807821078	Multimodal Grounding for Language Processing	1486723856	r modality provides an elegant method for zero shot learning, but it is questionable whether multimodal relations between concrete concepts are sufﬁcient to infer relations between abstract concepts. Lazaridou et al. (2015) analyze projected abstract words by extracting the nearest visual neighbor from their multimodal representation. The neighbors were paired with random images and human raters judged how well each ima
2807821078	Multimodal Grounding for Language Processing	2250539671	n the cosine similarity of two verbs and their corresponding similarity rating in the SimVerb dataset (Gerz et al., 2016). We compare the quality of 3498 verb pairs2 in textual Glove representations (Pennington et al., 2014) and two visual datasets: the Google dataset that performed best in Kiela et al. (2016) and has the highest coverage for the verb pairs (493 pairs, 14%)3 and the imSitu dataset which has been intentio
2807821078	Multimodal Grounding for Language Processing	2250539671	n recent years, distributional approaches have become the most widely accepted solution to model the associative character of word meaning (Harris, 1954; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). These approaches learn word representations in a high-dimensional vector space based on context patterns in large text collections. Machine learning researchers aim at reducing external knowledge to
2807821078	Multimodal Grounding for Language Processing	2036931463	n shared representations instead (Figure 2b). An approach inspired by topic modeling interprets aligned data as a multimodal document and uses Latent Dirichlet Allocation to derive multimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the sk
2807821078	Multimodal Grounding for Language Processing	2184188583	nduce a directional projection for zero shot learning. Joint learning of multimodal representations is very popular in the vision community (Karpathy et al., 2014; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011). 3.3 Compositional representations For tasks that require representing longer sequences, a na¨ıve approach is sequence-level fusion. In this setting, the unimodal sequence representation is obtained
2807821078	Multimodal Grounding for Language Processing	155596317	ns instead (Figure 2b). An approach inspired by topic modeling interprets aligned data as a multimodal document and uses Latent Dirichlet Allocation to derive multimodal topics (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Unfortunately, this approach cannot be easily used for zero shot learning. Lazaridou et al. (2015) enrich the skip-gram model by Mikolo
2807821078	Multimodal Grounding for Language Processing	2251970440	oncatenated representations while maintaining multimodal correlations, dimensionality reduction techniques such as singular value decomposition (Bruni et al., 2014) or canonical correlation analysis (Silberer and Lapata, 2012) have been applied. 3.2 Projection In practice, concepts that have a representation in one modality are not necessarily covered by representations in another modality. The projection of unseen concept
2807821078	Multimodal Grounding for Language Processing	2067438047	ons is commonly evaluated by their ability to model semantic properties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for t
2807821078	Multimodal Grounding for Language Processing	187290754	orms text into artiﬁcially generated phonemes for users who cannot read (Zen et al., 2009). The reverse task of transcribing audio and video content is addressed by approaches for speech recognition (Juang and Rabiner, 2005) and subtitle generation (Daelemans et al., 2004). For lipreading tasks, mute video input of people speaking is transformed into text representing their utterances (Ngiam et al., 2011). In these cross
2807821078	Multimodal Grounding for Language Processing	2137735870	perties. Different approaches to learning conceptual models are compared by their performance on similarity datasets, e.g., WordSim353 (Finkelstein et al., 2002), SimLex-999 (Hill et al., 2015), MEN (Bruni et al., 2012), SemSim, and VisSim (Silberer and Lapata, 2014)). These datasets contain pairs of words that have been annotated with similarity scores for the two concepts. Several evaluations of semantic models ha
2807821078	Multimodal Grounding for Language Processing	2112184938	rand of research for future work. 5.2 Imageability of abstract words Conceptual grounding of language can be intuitively performed for concrete words that have direct referents in sensory experience. Bruni et al. (2014) and Hill and Korhonen (2014) show that multimodal representations are beneﬁcial for evaluating concrete words, but have little to no impact on the evaluation of abstract words. Projecting unseen conc
2807821078	Multimodal Grounding for Language Processing	2122563357	representations and crossmodal alignment. Figure 2: Methods for learning multimodal representations. Blue and yellow shapes indicate the representation space of modality A and B. emotion recognition (Morency et al., 2011) or persuasiveness prediction (Santos et al., 2016), the actual content of an utterance and paraverbal cues (e.g., pitch, facial expression) need to be jointly evaluated. An ironic tone of voice might
2807821078	Multimodal Grounding for Language Processing	2112184938	rity scores for the two concepts. Several evaluations of semantic models have shown that multimodal concept representations outperform unimodal ones (Feng and Lapata, 2010; Silberer and Lapata, 2012; Bruni et al., 2014; Kiela et al., 2014). Kiela et al. (2016) perform a comparison of different image sources and architectures and their ability to model semantic similarity. Despite the advantages of multimodal models
2807821078	Multimodal Grounding for Language Processing	2111078031	s to´ induce multilingual representations. Grounding sequences in actions Situational grounding of action descriptions requires the representation of sequences and their compositional interpretation. Regneri et al. (2013) build a corpus that grounds descriptions of actions in videos showing these actions. For the interpretation of sequences, evaluating verbs and their arguments plays a fundamental role. Yatskar et al.
2807821078	Multimodal Grounding for Language Processing	2142192571	step further and tackles tasks that imperatively require an interactive ﬂow of information. In visual question answering, a human user can ask questions about an image that the system should answer (Malinowski et al., 2015). This requires several steps: understanding the question, determining the salient elements in the image, interpreting the image with respect to the question, and generating a coherent natural languag
2807821078	Multimodal Grounding for Language Processing	2250742840	superior performance of multimodal representations that has been established by several researchers would mainly be due to a more robust representation of highly redundant information. The results by Silberer and Lapata (2014) and Hill et al. (2014) support the intuitive assumption that textual representations better model textual similarity and visual representations better model visual similarity. As the multimodal model
2807821078	Multimodal Grounding for Language Processing	1514535095	tion mechanisms (Bahdanau et al., 2014) are used for the identiﬁcation of relevant information, see Figure 1b. A textual interpretation of a visually presented scene is generated in image captioning (Xu et al., 2015) and sketch recognition (Li et al., 2015). The goal is to identify relevant elements, group individual elements to semantic concepts, identify relations between concepts, and express these relations i
2807821078	Multimodal Grounding for Language Processing	1544268587	tion recognition to approximate this problem. Complementary approaches attempt to generate visual representations to summarize documents and present the most relevant information in an intuitive way (Kucher and Kerren, 2015). The most popular approach are so-called word clouds which are frequency-based visualizations for topic modeling (Bateman et al., 2008). More recent approaches include semantic relations between word
2807821078	Multimodal Grounding for Language Processing	2177113878	by a tunable parameter : v mm(c) = v m 1 (c) a (1 )v m 2 (c). The concatenation occurs directly on the concept level and is thus called feature-level fusion or early fusion (Leong and Mihalcea, 2011; Bruni et al., 2011). In the case of pure concatenation, the unimodal representations reside in separate conceptual spaces. The concatenated representation for cat could give us the information that cat is visually simil
2807821078	Multimodal Grounding for Language Processing	2123024445	ue pairs of concept representations (c m 1 ;c m 2 ) and minimizes the similarity for pairs with random target representations (c m 1 ;random m2) has been shown to be a good choice for image labeling (Frome et al., 2013). In this task, the mapping approach is applied in the image-to-text direction to classify unknown objects in images based on their semantic similarity to known objects (Socher et al., 2013) . Lazarid
2807821078	Multimodal Grounding for Language Processing	2251970440	in unimodal tasks, researchers experiment with many different variations of representing concepts and their relations. Earlier work on multimodal representations used human-elicited visual features (Silberer and Lapata, 2012; Roller and Schulte Im Walde, 2013). Conveniently, integrating knowledge from different modalities has been facilitated due to the now common low-level representations of the input (also known as emb
2807821078	Multimodal Grounding for Language Processing	2120526973	ure work. 5.2 Imageability of abstract words Conceptual grounding of language can be intuitively performed for concrete words that have direct referents in sensory experience. Bruni et al. (2014) and Hill and Korhonen (2014) show that multimodal representations are beneﬁcial for evaluating concrete words, but have little to no impact on the evaluation of abstract words. Projecting unseen concepts into the representation
2807821078	Multimodal Grounding for Language Processing	2286410738	xpressing relations between concepts and their situational functionality (Hartshorne et al., 2014). The dynamic nature of verbs poses a challenge for multimodal grounding. To our best knowledge, only Hill et al. (2014) and Collell et al. (2017) consider verbs in their evaluation. They report that results for verbs are signiﬁcantly worse, but Figure 3: Illustration for the quality of verb representations indicated a
2807931239	Morphological and Language-Agnostic Word Segmentation for NMT	2525778437	luating the segmentation methods. 1 http://github.com/rsennrich/subword-nmt/ arXiv:1806.05482v1 [cs.CL] 14 Jun 2018 2 Dominik Machacek, Jonas Vidra, and Ondrej Bojar commercial eld is wordpieces [10]. Yet another variant of the technique is implemented in Google’s open-sourced toolkit Tensor2Tensor,2 namely the SubwordTextEncoder class (abbreviated as STE below). The common property of these appr
2807931239	Morphological and Language-Agnostic Word Segmentation for NMT	1816313093	sed by model size and GPU RAM, the vocabulary size of custom subwords can be kept small. The current most common technique of subword construction is called bytepair encoding (BPE) by Sennrich et al. [6].1 Its counterpart originating in the ? This work has been supported by the grants 18-24210S of the Czech Science Foundation, SVV 260 453 and \Progress&quot; Q18+Q48 of Charles University, H2020-ICT20
2807997917	Deep Learning to Detect Redundant Method Comments.	2064675550	al n-gram language models. The current state of the art in natural language [38] are language models based on a type of recurrent neural network (RNN) called the long-short term memory network (LSTM) [24]. More details on the LSTM can be found in Goodfellow et al. [18], but at a high level, an LSTM computes a hidden state vector hi ∈ RK, that corresponds to every word wi and that summarizes the inform
2807997917	Deep Learning to Detect Redundant Method Comments.	2170196926	hould govern the code and then veriﬁes whether the code accompanying the comment obeys the rules. aComment extracts assertion macros from code, and assertional phrases from comments, and combines them[51].Tanetal.[52]analyses Javadoccommentstoinferpropertiesofthemethodaccompanyingthecomment.Itthengenerates random test cases forthecodetoidentify inconsistencies between the comment and the code. Our wor
2807997917	Deep Learning to Detect Redundant Method Comments.	2060384944	M trained on over a billion tokens and demonstrating that predicting identiﬁer names causes the most diﬃculty to current LMs for code. LMs for code have been applied widely, to discover syntax errors [11], to learn coding conventions [2], andwithincross-languageportingtools[40].Manylanguagemodels that are speciﬁcally adapted to code have also been proposed [7, 9, 21, 34, 41, 46]. Recent work has also
2807997917	Deep Learning to Detect Redundant Method Comments.	2152874840	odebase, then ﬁnds uncommented clones using detection as a black box [55]. Tan and coauthors have presented methods for automatically producing code annotations from comments [50–52]. First, iComment [50] looks at comments to extract rules that should govern the code and then veriﬁes whether the code accompanying the comment obeys the rules. aComment extracts assertion macros from code, and assertiona
2807997917	Deep Learning to Detect Redundant Method Comments.	2402619042	omments and code; however, this work focused only on autosuggestion rather thancommententailment.Also,sequence-to-sequencemodelshave been applied within software engineering to the API mining problem [19]. But this work also did not consider the comment entailment problem. Instead it predicts code based on natural language, rather than comments based on code. More recently, Lin et al. [33] apply seque
2807997917	Deep Learning to Detect Redundant Method Comments.	2142403498	on. Unfortunately, the pseudo-code that can currently be generated by these methods seems to be relatively literal transcriptions of the code, line-by-line. LanguageModelsforcode(unimodal):Hindleetal.[23]were theﬁrstto applyn-gramlanguage models(LM) tosourcecode.Allamanis and Sutton [4] continued in this line by presenting the ﬁrst source code LM trained on over a billion tokens and demonstrating that
2807997917	Deep Learning to Detect Redundant Method Comments.	2165747537	ous tokens ofcontext aresuﬃcient forpredictingeach word.Standard n-gram models do not perform as well as the deep language modelsthatwedescribenext,althoughforcode,theextensionofcache language models [22, 53] are considerably better, and competitive with deep models. Nevertheless, even these cache models are not easily applied to the sequence-to-sequence learning setting that we require here, so we do not
2807997917	Deep Learning to Detect Redundant Method Comments.	1551431154,2444132761,2619311298	pplied widely, to discover syntax errors [11], to learn coding conventions [2], andwithincross-languageportingtools[40].Manylanguagemodels that are speciﬁcally adapted to code have also been proposed [7, 9, 21, 34, 41, 46]. Recent work has also applied deep language models for code in a unimodal fashion. Feedforward neural network language models, simpler than the recurrent models applied here, have been applied to cod
2807997917	Deep Learning to Detect Redundant Method Comments.	2018389835,2497764072	recurrent models applied here, have been applied to code by Allamanis et al. [3]. Deep language models, such as RNNs and LSTMs, have been presented for the unimodal setting of code by several authors [15, 54]. 3 PROBLEM DEFINITION As a way of attacking the problem of identifying redundant comments, in this paper we deﬁne and address a task which we call commententailment.Intuitively, the comment entailmen
2807997917	Deep Learning to Detect Redundant Method Comments.	2148190602	seems to be relatively literal transcriptions of the code, line-by-line. LanguageModelsforcode(unimodal):Hindleetal.[23]were theﬁrstto applyn-gramlanguage models(LM) tosourcecode.Allamanis and Sutton [4] continued in this line by presenting the ﬁrst source code LM trained on over a billion tokens and demonstrating that predicting identiﬁer names causes the most diﬃculty to current LMs for code. LMs f
2807997917	Deep Learning to Detect Redundant Method Comments.	2162045655	tablecomments oroptionallyremoves them entirely if desired. Future versions could integrate into workﬂows for code review, for example, colouring comments in a heatmap, in a manneranalogoustoTarantula[26],toshowareviewer whichregions of the code contain highest concentrations of uninformative comments. Craicpromises to graduallyimprove the qualityof a codebase’scommentsbynudgingdevelopersawayfromuninf
2807997917	Deep Learning to Detect Redundant Method Comments.	2123442489	ve comments. This code-full comment corpuscontains over 3M pairs. We preprocess this corpus in a few ways. For code, we use a lexer to tokenizethemethod.Forthecomments,weusetheStanfordCoreNLP toolkit [35] to tokenize the text. In both methods and comments, we subtokenize any camelCase names into separate words. Theresultingmethodsandcommentscanvarywidelyinlength. Table 1 shows how the token counts of
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2753465613	2008; Mowery et al., 2017a). Social media have become an increasingly important source of data related to mental health conditions (Cohan et al., 2017; Mowery et al., 2017b; Coppersmith et al., 2017; Yates et al., 2017), as it is now a prominent platform for individuals to engage in daily discussions, share information, seek advice and simply communicate with peers that have shared interests. In addition to its ubiq
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	1537829113	2015a) and ours (SMHD). I was ofﬁcially diagnosed with ADHD last year. I have a diagnosed history of PTSD. my dr just diagnosed me as schizo. Figure 1: Examples of self-reported diagnoses statements. Coppersmith et al. (2014a) speciﬁcally focused on self-reports of mental health diagnoses. In particular, Coppersmith et al. (2015a) constructed a dataset of various mental health conditions using Twitter statements. Finally
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2753465613	age usage between the users: by removing speciﬁc mental health signals and discussions, we focus on patterns of language in normal (general) discussions. While our dataset creation method is close to Yates et al. (2017), we extend theirs by investigating multiple high-precision matching patterns to identify self-reported diagnoses for a range of conditions. Part of our patterns are obtained through synonym discovery
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2590318218	al linguistic insights. Some have investigated the language of users of an online crisis forum to identify posts of users who are at highest risk to allow for faster intervention (Milne et al., 2016; Cohan et al., 2017). Losada and Crestani (2016) applied the self-reported diagnosis strategy to identify approximately 150 Reddit users who suffer from depression, and paired them with 750 control users. Yates et al. (2
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2542188028	ase the likelihood of matching diagnostic posts, we expanded each set of medical expressions to include synonyms, common misspellings, vernacular terms and abbreviations. Our steps mirror the ones of Soldaini and Yom-Tov (2017), who were also interested in identifying self-diagnosed users, albeit on query logs. In particular, we leveraged two synonym mappings to generate alternative formulations of the disorders of interest
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	263633337	entiﬁed diagnoses. Leading submissions to CLPsych 2015 relied on the LIWC lexicon (Pennebaker et al., 2015), topic modeling, manual lexicons, and other domain-dependent features (Resnik et al., 2015; Preotiuc-Pietro et al., 2015). Coppersmith et al. (2015a) expands the research on Twitter to eleven self-identiﬁed mental health conditions. (Benton et al., 2017) uses this dataset (and others) with a neural multi-task learning a
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2250375872	er users with self-identiﬁed diagnoses. Leading submissions to CLPsych 2015 relied on the LIWC lexicon (Pennebaker et al., 2015), topic modeling, manual lexicons, and other domain-dependent features (Resnik et al., 2015; Preotiuc-Pietro et al., 2015). Coppersmith et al. (2015a) expands the research on Twitter to eleven self-identiﬁed mental health conditions. (Benton et al., 2017) uses this dataset (and others) with
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2252128283	et al., 2014a; Yates et al., 2017), suicide ideation (Cohan et al., 2017; De Choudhury and Kıcıman, 2017; Kshirsagar et al., 2017; Desmet and Hoste, 2018), and other conditions such as schizophrenia (Mitchell et al., 2015). While social media data is abundantly available, the amount of labeled data for studying mental health conditions is limited. This is due to the high cost of annotation and the difﬁculty of access t
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2295598076	ic characters; we normalized tokens by folding them to lowercase. Words occurring less than 20 times in the training set were removed. Features were weighted using tf-idf and ‘ 2-normalized. XGBoost (Chen and Guestrin, 2016): we evaluated the performance of an ensemble of decision trees. For this model, the same features described above were used. We set the number of estimators to 100. Support vector machine (SVM): we i
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	107610923	also interested in identifying self-diagnosed users, albeit on query logs. In particular, we leveraged two synonym mappings to generate alternative formulations of the disorders of interest: MedSyn (Yates and Goharian, 2013) is a laypeople-oriented synonym mapping ontology. It was generated from a subset of UMLS6 ﬁltered to remove irrelevant semantic types. Behavioral (Yom-Tov and Gabrilovich, 2013) maps expressions com
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	1537829113	l health makes social media an invaluable resource of mental health-related data. Lack of data has been one of the key limitations to understanding and addressing the challenges the domain is facing (Coppersmith et al., 2014a). Data from social media can not only be used to potentially provide clinical help to users in need, but also to broaden our understanding of the various mental health conditions. Social media analy
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2753465613	le sources increases the variety of the diagnosed users and linguistic nuances. We also explore nine common mental health 1This limitation was doubled to 280 characters in late 2017. conditions while Yates et al. (2017) focus only on depression. We explore classiﬁcation methods for identifying mental health conditions through social media language and provide detailed analysis that helps us understand the difference
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2590318218	lity of life and wellness of individuals in society (Strine et al., 2008; Mowery et al., 2017a). Social media have become an increasingly important source of data related to mental health conditions (Cohan et al., 2017; Mowery et al., 2017b; Coppersmith et al., 2017; Yates et al., 2017), as it is now a prominent platform for individuals to engage in daily discussions, share information, seek advice and simply commu
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	1537829113	milar cross-cultural tendencies. Due to the cost and bias introduced by relying on surveys, work shifted to identifying mental health conditions by examining the content shared by social media users. Coppersmith et al. (2014a) identiﬁed approximately 1,200 Twitter users with 4 mental health conditions (bipolar, depression, PTSD, SAD) using diagnosis statements found in tweets (e.g., “I was diagnosed with depression”). Fo
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2484061479	odels to predict depression and neuroticism based on student-written essays, ﬁnding clear clusters of words when students are asked to write about their feelings in a stream-of-consciousness setting. Althoff et al. (2016) uses text message conversations from a mental health crisis center to improve counseling techniques. This work addresses important limitations of previous efforts. Similar to RSDD (Yates et al., 2017
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2590318218	rstanding of the various mental health conditions. Social media analysis has already been proven valuable for identifying depression (Coppersmith et al., 2014a; Yates et al., 2017), suicide ideation (Cohan et al., 2017; De Choudhury and Kıcıman, 2017; Kshirsagar et al., 2017; Desmet and Hoste, 2018), and other conditions such as schizophrenia (Mitchell et al., 2015). While social media data is abundantly available,
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2753465613	s in need, but also to broaden our understanding of the various mental health conditions. Social media analysis has already been proven valuable for identifying depression (Coppersmith et al., 2014a; Yates et al., 2017), suicide ideation (Cohan et al., 2017; De Choudhury and Kıcıman, 2017; Kshirsagar et al., 2017; Desmet and Hoste, 2018), and other conditions such as schizophrenia (Mitchell et al., 2015). While soci
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2252128283	s statements found in tweets (e.g., “I was diagnosed with depression”). Following this work, detailed studies were conducted on users experiencing PTSD (Coppersmith et al., 2014b), and schizophrenia (Mitchell et al., 2015; Ernala et al., 2017). The shared task at the 2nd Computational Linguistics and Clinical Psychology Workshop (CLPsych 2015) focused on identifying depression and PTSD users on Twitter (Coppersmith et
2808024810	SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions	2753465613	speciﬁcally focused on self-reports of mental health diagnoses. In particular, Coppersmith et al. (2015a) constructed a dataset of various mental health conditions using Twitter statements. Finally, Yates et al. (2017) introduced a large dataset of depressed users obtained from Reddit. We extend the previous efforts on addressing the lack of large-scale mental health-related language data. Particularly, we propose
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2567101557	bias. All this is part of an ongoing effort to understand how trust can be built into AI systems. I. INTRODUCTION With the latest wave of learning models taking advantage of advances in deep learning [21], [22], [23], Artiﬁcial Intelligence (AI) systems are gaining widespread publicity, coupled with a drive from industry to incorporate intelligence into all manner of processes that handle our private
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2081580037	heir difference, as described in Sec. II-C, giving us a set of 5 potential gender projections. Each gender projection was tested against an independent set of paired gender words sourced from WordNet [13]. After applyingthe genderprojectionto the test word-pairs,following the procedure of [2], we measured the average difference between the word-pairs. The gender projection that led to the word-pairs t
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2752201871	part of our lives. In the domain of text, many modern approaches often begin by embedding the input text data into an embedding space that is used as the ﬁrst layer in a subsequent deep network [4], [14]. These word embeddings have been shown to contain the same biases [3], due to the source data from which they are trained. In effect, biases from the source data, such as in the differences in repres
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2483215953	nt set of occupations found in [3]. D. Experiment 3: Minimising Associations via Orthogonal Projection In this experiment, we deploy a method for removing bias from word embeddings, ﬁrst published in [2], and repeat all previous association tests related to gender reported in this paper, empirically showing the effect of bias removal on the word associations. 1) Finding an Orthogonal Projection for G
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2483215953	ons. Each gender projection was tested against an independent set of paired gender words sourced from WordNet [13]. After applyingthe genderprojectionto the test word-pairs,following the procedure of [2], we measured the average difference between the word-pairs. The gender projection that led to the word-pairs that are closest together (smallest difference) was then selected as our gender projection
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2483215953	real world and the media, where we found a correlation between the number of men and women in an occupationandits associationwith eachset ofmale andfemale names. Finally, using a projection algorithm [2], we were able to reduce the gender bias shown in the embeddings, resulting in a decrease in the difference between associations for all tests based upon gender. Further work in this direction will in
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2483215953	rmore, we investigate gender bias in words that represent different occupations, comparing these associations with UK national employment statistics. In thelast experiment,we use orthogonalprojections[2] to debias our word embeddings, and measure the reduction in the biases demonstrated in the previous two experiments. A. Data Description and Embedding In all of our experiments, the ﬁrst step is to o
2808158873	Biased Embeddings from Wild Data: Measuring, Understanding and Removing	2483215953	se two vectors (w1 and w2) must be considered “opposite” of each other semantically, in terms of the bias that is required to be removed. The following method of debiasing is the same as presented in [2]: w b =wˆ1 −wˆ2, (4) where the vector w b will have the direction of bias in the embedding (for example, he and she are different genders and could potentially be used to capture a gender direction).
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2176263492	17; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), reduce the occurrence of out-of-vocabulary tokens in summaries (See et al., 2017), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models generally requires large amounts of data; they are ofte
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2609482285	6). Variants of the neural encoder-decoder architecture have been exploited to reduce word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), reduce the occurrence of out-of-vocabulary tokens in summaries (See et al., 2017), improve the learning objective and search (Ra
2808224048	Abstract Meaning Representation for Multi-Document Summarization	655477013	amp loss we wish to minimize. This formulation is similar to the objective of the structured support vector machines (SSVMs, Tsochantaridis et al., 2005). Becase decent results have been reported by (Liu et al., 2015), we adopt structured ramp loss in all experiments. Lperc(;˚) = max G score(G) score(G) = score(G^) score(G) (2) Lramp(;˚) = max G (score(G)+cost(G;G)) max G (score(G) cost(G;G)) (3) 3.2 Surface Rea
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2524520086	ang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our m
2808224048	Abstract Meaning Representation for Multi-Document Summarization	655477013	ata. Our research contributions are summarized as follows: we investigate AMR, a linguistically-grounded semantic formalism, as a new form of content representation for multi-document summarization. Liu et al. (2015) conducted a pilot study using AMR for single-document summarization. This paper exploits the structured prediction framework but presents a full pipeline for generating abstractive summaries from mul
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2738955616	chine comprehension (Sachan and Xing, 2016). Moreover, signiﬁcant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konsta
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2148404145	content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-speciﬁc templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difﬁcult. There is an increasing need to exploit a semant
2808224048	Abstract Meaning Representation for Multi-Document Summarization	655477013	e edge score. score(G;;˚) = XN i=1 v i [&gt;f(i)] v | {z }i node score + X (i;j)2E e i;j [˚&gt;g(i;j)] e | {z i;j} edge score (1) Features characterizing the graph nodes and edges are adopted from (Liu et al., 2015). They include concept/relation labels and their frequencies in the documents, average depth of the concept in sentence AMR graphs, position of sentences containing the concept/relation, whether the c
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2252123671	e system framework. In the following sections we describe details of the components. 3.1 Content Planning The meaning of a source sentence is represented by a rooted, directed, and acyclic AMR graph (Banarescu et al., 2013), where nodes are concepts and edges are semantic relations. A sentence AMR graph can be obtained by applying an AMR parser to a natural language sentence. In this work we investigate two AMR parsers
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2105842272	e scoring difference between the two system graphs becomes the structured ramp loss we wish to minimize. This formulation is similar to the objective of the structured support vector machines (SSVMs, Tsochantaridis et al., 2005). Becase decent results have been reported by (Liu et al., 2015), we adopt structured ramp loss in all experiments. Lperc(;˚) = max G score(G) score(G) = score(G^) score(G) (2) Lramp(;˚) = max G (sc
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2341401723	ed Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zh
2808224048	Abstract Meaning Representation for Multi-Document Summarization	151579683	entify salient text units from source documents, arrange them in a compact form, such as domain-speciﬁc templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999; Genest and Lapalme, 2011; Oya et al., 2014; Gerani et al., 2014; Fabbrizio et al., 2014). A challenge faced by these approaches is that there Content Planning Surface Realization a cluster of source documents discussing the
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2341349540	f research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechani
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2164755781	igated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-speciﬁc templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difﬁcult. There is an increasing ne
2808224048	Abstract Meaning Representation for Multi-Document Summarization	655477013	in. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, signiﬁcant research efforts are dedicated to map English sentences to
2808224048	Abstract Meaning Representation for Multi-Document Summarization	1939882552	of being included in the summary; ext-KL-Sum (Haghighi and Vanderwende, 2009) describes a method that greedily adds sentences to the summary so long as it decreases the KL divergence; abs-Opinosis (Ganesan et al., 2010) generates abstractive summaries by searching for salient paths on a word co-occurrence graph created from source documents; 1We use N=M=5 in our experiments. This setting fuses 5 source sentences to
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2154652894	ivity by introducing edges between concepts so that salient summary edges can be effectively preserved. 5.2 Results on Summarization In Table 2 we report the summarization results evaluated by ROUGE (Lin, 2004). In particular, R-1, R2, and R-SU4 respectively measure the overlap of unigrams, bigrams, and skip bigrams (up to 4 words) ROUGE-1 ROUGE-2 ROUGE-SU4 System P R F P R F P R F DUC 2004 ext-SumBasic 37.
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2252123671	malism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and the neo-Davidsonian event representation (Parsons, 1990; Schein, 1993). It represents the meaning of
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2250309395	mary graphs are both data-driven and not speciﬁcally designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, si
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2252123671	MR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (x3.2) converts a summary graph to its PENMAN representation (Banarescu et al., 2013) and generates a natural language sentence from it. Source sentence extraction (x3.3) selects sets of similar sentences from source documents discussing different aspects of the topic. The three compo
2808224048	Abstract Meaning Representation for Multi-Document Summarization	1645937837	or multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient text units from source documents, arrange them in a compact form, such as domain-speciﬁc templates, and sub
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2608053514	ng, 2016). Moreover, signiﬁcant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These s
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2515003191	niﬁcant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way fo
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2148404145	nts, arrange them in a compact form, such as domain-speciﬁc templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999; Genest and Lapalme, 2011; Oya et al., 2014; Gerani et al., 2014; Fabbrizio et al., 2014). A challenge faced by these approaches is that there Content Planning Surface Realization a cluster of source documents discussing the same topic multiple sets of similar sen
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2251246917	number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, signiﬁcant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al.,
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2251302966	phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-speciﬁc templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difﬁcult. There is an increasing need to exploit a semantic formalism so that condensing source documents to
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2148172987	rce sentences to ﬁnd salient topic aspects and their corresponding sets of similar sentences. Spectral clustering has been shown to perform strongly on different clustering problems (Ng et al., 2002; Yogatama and Tanaka-Ishii, 2009). The approach constructs an afﬁnity matrix by applying a pairwise similarity function to all source sentences. It then calculates the eigenvalues of the matrix and performs clustering in the low-dime
2808224048	Abstract Meaning Representation for Multi-Document Summarization	1975579663	s; ext-SumBasic (Vanderwende et al., 2007) is an extractive approach that assumes words occurring frequently in a document cluster have a higher chance of being included in the summary; ext-KL-Sum (Haghighi and Vanderwende, 2009) describes a method that greedily adds sentences to the summary so long as it decreases the KL divergence; abs-Opinosis (Ganesan et al., 2010) generates abstractive summaries by searching for salient
2808224048	Abstract Meaning Representation for Multi-Document Summarization	1939882552	these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-speciﬁc templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representat
2808224048	Abstract Meaning Representation for Multi-Document Summarization	151579683	set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-speciﬁc templates (Pighin et al., 2014). Nonetheless
2808224048	Abstract Meaning Representation for Multi-Document Summarization	655477013	a set of values assigned to the binary variables v i and e i;j. We implement a set of linear constraints to ensure the decoded graph is connected, forms a tree structure, and limits to L graph nodes (Liu et al., 2015). The parameter update process adjusts  and ˚ to minimize a loss function capturing the difference between the system-decoded summary graph (G^) and the goldstandard summary graph (G). Eq. (2) presen
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2250595592	from source documents, arrange them in a compact form, such as domain-speciﬁc templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999; Genest and Lapalme, 2011; Oya et al., 2014; Gerani et al., 2014; Fabbrizio et al., 2014). A challenge faced by these approaches is that there Content Planning Surface Realization a cluster of source documents discussing the same topic multipl
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2293174806	speciﬁcally designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, signiﬁcant research efforts are dedic
2808224048	Abstract Meaning Representation for Multi-Document Summarization	2165874743	tering on all source sentences to ﬁnd salient topic aspects and their corresponding sets of similar sentences. Spectral clustering has been shown to perform strongly on different clustering problems (Ng et al., 2002; Yogatama and Tanaka-Ishii, 2009). The approach constructs an afﬁnity matrix by applying a pairwise similarity function to all source sentences. It then calculates the eigenvalues of the matrix and p
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2251302966	, 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspeciﬁc templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1We made our system public
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2251384446	on (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstrac
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2250861254	011; Rush et al., 2015). The task is to reduce the ﬁrst sentence of an article to a title-like summary. We obtain dependency parse trees for source sentences using the Stanford neural network parser (Chen and Manning, 2014). We also use the standard train/valid/test data splits. Following (Rush et al., 2015), the train and valid splits are pruned2 to improve the data quality. Spurious pairs that are repetitive, overly l
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2251654079	arcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2064675550	to a continuous vector; it also learns a vector representation for each unit of the source text (e.g., words as units). In this work we use a two-layer stacked bi-directional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) networks as the encoder, where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the ﬁrst layer. We obtain the hidden states of the second la
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341349540	describe the dataset, experimental settings, baselines, and ﬁnally, evaluation results and analysis. 4.1 Data Sets We evaluate our proposed models on the Gigaword summarization dataset (Parker, 2011; Rush et al., 2015). The task is to reduce the ﬁrst sentence of an article to a title-like summary. We obtain dependency parse trees for source sentences using the Stanford neural network parser (Chen and Manning, 2014)
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	1902237438	et al., 2015) are the ﬁrst work introducing an encoder-decoder architecture for summarization. Luong-NMT (Chopra et al., 2016) is a re-implementation of the attentive stacked LSTM encoder-decoder of Luong et al. (2015a). RAS-LSTM and RAS-Elman(Chopra et al., 2016) describe a convolutional attentive encoder that ensures the decoder focuses on appropriate words at each step of generation. ASC+FSC1 (Miao and Blunsom,
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2148404145	f the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspeciﬁc t
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2115937944	line approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntact
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2154652894	mmaries more informative than using the word “meet.” 4.3 Results ROUGE results on valid set. We ﬁrst report results on the Gigaword valid-2000 dataset in Table 4. We present R-1, R-2, and R-L scores (Lin, 2004) that respectively measures the overlapped unigrams, bigrams, and longest common subsequences between the system and reference summaries3. Our baseline system (“Baseline”) implements the seq2seq archi
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2610661645	modeling keywords, capturing sentence-toword structure, and handling rare words. Multi-Task w/ Entailment (Pasunuru et al., 2017) combines entailment with summarization in a multi-task setting. DRGD(Li et al., 2017b) describes a deep recurrent generative decoder learning latent structure of summary sequences via variational inference. Table 8: Existing summarization methods. by the parser may affect the “Struct
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2065354331	murder” ﬂips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	1902237438	nclude both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (p gen
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2250539671	oach We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A source word is replaced by its Glove embedding (Pennington et al., 2014) before it is fed to the system; the vector is denoted by x i (i2[S]; ‘S’ for source). Similarly, a summary word is denoted by y t (t2[T]; ‘T’ for target). If a word does not appear in the input vocab
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341401723	ontent of the source (e.g., main verbs) despite their syntactic importance. The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source texts. Table 1 presents two
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341401723	pra et al., 2016) 33.10 14.45 30.71 RAS-LSTM (Chopra et al., 2016) 32.55 14.70 30.03 RAS-Elman (Chopra et al., 2016) 33.78 15.97 31.15 ASC+FSC1 (Miao and Blunsom, 2016) 34.17 15.94 31.92 lvt2k-1sent (Nallapati et al., 2016) 32.67 15.59 30.64 lvt5k-1sent (Nallapati et al., 2016) 35.30 16.64 32.62 Multi-Task (Pasunuru et al., 2017) 32.75 15.35 30.82 DRGD (Li et al., 2017b) 36.27 17.57 33.62 Baseline (this paper) 35.43 17.
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	1522301498	r/dataset/filter.py Input vocabulary size 70K Output vocabulary size 5K (default) Dim. of word embeddings 100 Dim. of structural embeddings 16 Num. of encoder/decoder hidden units 256 Adam optimizer (Kingma and Ba, 2015) lr= 1e-4 Coeff. for coverage-based regularizer = 1 Coeff. for beam search with reference ˇ13.5 Beam size K= 5 Minibatch size M= 64 Early stopping criterion (max 20 epochs) valid. loss Gradient clip
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2335367650	sed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2151258001	sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with t
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2610661645	serve them in the summaries at test time to aid reproduction of factual details. Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in part because NMT does not handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise an
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341349540	source dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated 3w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000 Gigaword Test-1951 System R-1 R-2 R-L ABS (Rush et al., 2015) 29.55 11.32 26.42 ABS+ (Rush et al., 2015) 29.76 11.88 26.96 Luong-NMT (Chopra et al., 2016) 33.10 14.45 30.71 RAS-LSTM (Chopra et al., 2016) 32.55 14.70 30.03 RAS-Elman (Chopra et al., 2016) 33.78 1
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341349540	st-1951 set (fulllength F1). Models with structure-infused copy mechanisms (“Struct+*”) perform well. Their R-2 F-scores are on-par with or outperform state-of-the-art published systems. ABS and ABS+(Rush et al., 2015) are the ﬁrst work introducing an encoder-decoder architecture for summarization. Luong-NMT (Chopra et al., 2016) is a re-implementation of the attentive stacked LSTM encoder-decoder of Luong et al. (
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341401723	at each step of generation. ASC+FSC1 (Miao and Blunsom, 2016) presents a generative auto-encoding sentence compression model jointly trained on labelled/unlabelled data. lvt2k-1sent and lvt5k-1sent (Nallapati et al., 2016) address issues in the attentive encoder-decoder framework, including modeling keywords, capturing sentence-toword structure, and handling rare words. Multi-Task w/ Entailment (Pasunuru et al., 2017)
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2122311631	structural labels. Example labels are generated for word ‘had’ in Figure 1. Relative word positions are discretized into ten buckets. Inspired by compressive summarization via structured prediction (Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013), we hypothesize that structural labels, such as the incoming dependency arc and the depth in a dependency parse tree, can be helpful to predict word importance. We conside
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2609482285	tence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341349540	ter tuning and early stopping. “valid-2000” is used for evaluation; it allows the models to be trained and evaluated on pruned instances. Finally, we report results on the standard Gigaword test set (Rush et al., 2015) containing 1,951 instances (“test-1951”). 4.2 Experimental Setup We use the Xavier scheme (Glorot and Bengio, 2010) for parameter initialization, where weights are initialized using a Gaussian distri
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2610661645	unsom, 2016) 34.17 15.94 31.92 lvt2k-1sent (Nallapati et al., 2016) 32.67 15.59 30.64 lvt5k-1sent (Nallapati et al., 2016) 35.30 16.64 32.62 Multi-Task (Pasunuru et al., 2017) 32.75 15.35 30.82 DRGD (Li et al., 2017b) 36.27 17.57 33.62 Baseline (this paper) 35.43 17.49 33.39 Struct+Input (this paper) 35.32 17.50 33.25 Struct+2Way+Relation (this paper) 35.46 17.51 33.28 Struct+Hidden (this paper) 35.49 17.61 33.3
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2341349540	ve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance. The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source tex
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	655477013	verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspeciﬁc templates built using a text planner or an
2808293684	Structure-Infused Copy Mechanisms for Abstractive Summarization	2307381258	y mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (p gen) vs. copying it from the source text (1 p gen). The basic model i
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2251409655	, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion tokens [10], word vectors [15], and emographics [29] are some of the information that are found useful for improving sentiment analysis accuracies. Although these features are bene•cial, extracting them requires language-dependent data (e.g., a sentime
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2097726431	ces [6, 19, 27] for sentiment analysis such as SentiWordNet [2, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion tokens [10], word vectors [15], and emographics [29] are some of the information that are found useful for improving sentiment analysis accuracies. Although these features are bene•cial, ext
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2250539671	Dataset name # of observations Amazon reviews 9;478;095 Yelp restaurant reviews 8;539 Competition restaurant reviews 68;170 Table 1: Datasets used for training. We utilized pre-trained global vectors [22]. „e training phase is depicted in Figure 2. 4 EXPERIMENTS To evaluate the proposed approach for multilingual sentiment analysis task, we conducted experiments. „is section •rst presents the corpora u
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2274912527	ing support vector machines on translated text. Boyd-Graber and Resnik [8] built Latent Dirichlet Allocation models to investigate how multilingual concepts are clustered into topics. Mohammed et al. [17] translate Twi−er posts to English as well as the English sentiment lexicons. Tellez et al. [28] propose a framework where language-dependent and independent features are used with an SVM classi•er. „
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2294501066	ive). 4.1.1 Training Sets. With the goal of building a generalizable sentiment analysis model, we used three di‡erent training sets as provided in Table 1. One of these three datasets (Amazon reviews [12, 16]) is larger and has product reviews from several di‡erent categories including book reviews, electronics products reviews, and application reviews. „e other two datasets are to make the model more spe
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	1970592556	and require researchers to build such resources for every language to process. Feature engineering is a large part of the model building phase for most sentiment analysis and emotion detection models [19]. Determining the correct set of features is a task that requires thorough investigation. Furthermore, these features are mostly language and dataset dependent making it even further challenging to bu
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	1549434858,2084046180	in most of the sentiment analysis work is that features that are speci•c to sentiment analysis are extracted (e.g., sentiment lexicons) and used in di‡erent machine learning models. Lexical resources [6, 19, 27] for sentiment analysis such as SentiWordNet [2, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion t
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	1508001288	only sentiment units with a pa−ern-based approach. Balahur and Turchi [5] used uni-grams, bi-grams and tf-idf features for building support vector machines on translated text. Boyd-Graber and Resnik [8] built Latent Dirichlet Allocation models to investigate how multilingual concepts are clustered into topics. Mohammed et al. [17] translate Twi−er posts to English as well as the English sentiment le
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2071332064	si•er. „ese machine learning approaches also require a feature extraction phase where we eliminate by incorporating a deep learning approach that does the feature learning intrinsically. Further, Wan [30] uses an ensemble approach where the resources (e.g., lexicons) in both the original language and the translated language are used – requiring resources to be present in both languages. Brooke et al.
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2108765529	that are speci•c to sentiment analysis are extracted (e.g., sentiment lexicons) and used in di‡erent machine learning models. Lexical resources [6, 19, 27] for sentiment analysis such as SentiWordNet [2, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion tokens [10], word vectors [15], and emographics [29]
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	1549434858,2084046180	timent analysis has long been studied by the research community, leading to several sentiment-related resources such as sentiment dictionaries that can be used as features for machine learning models [6, 14, 26, 27]. „ese resources help increase sentiment analysis accuracies; however, they are highly dependent on language and require researchers to build such resources for every language to process. Feature engi
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2034090215	tracted (e.g., sentiment lexicons) and used in di‡erent machine learning models. Lexical resources [6, 19, 27] for sentiment analysis such as SentiWordNet [2, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion tokens [10], word vectors [15], and emographics [29] are some of the information that are foun
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2020854665	here more training data is available and use the translated corpora to do inference on. Machine translation for multilingual sentiment analysis has also seen a−ention from researchers. Hiroshi et al. [13] translated only sentiment units with a pa−ern-based approach. Balahur and Turchi [5] used uni-grams, bi-grams and tf-idf features for building support vector machines on translated text. Boyd-Graber
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	2113459411	uch as SentiWordNet [2, 11], linguistic features and expressions [7], polarity dictionaries [14, 26], other features such as topic-oriented features and syntax [21], emotion tokens [10], word vectors [15], and emographics [29] are some of the information that are found useful for improving sentiment analysis accuracies. Although these features are bene•cial, extracting them requires language-dependent
2808423603	Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data.	1970592556	y a deep learning model to the multilingual sentiment analysis task. 2 RELATED WORK „ere is a rich body of work in sentiment analysis including social media platforms such as Twi−er [20] and Facebook [19]. One common factor in most of the sentiment analysis work is that features that are speci•c to sentiment analysis are extracted (e.g., sentiment lexicons) and used in di‡erent machine learning models
2808626814	An Empirical Analysis of the Correlation of Syntax and Prosody.	28194048	cted: tempo, avg. pitch, avg. loudness and the duration of a following pause. These features can be measured easily and objectively and do not take into account any linguistic structure (unlike, e.g. [15]). We ﬁnd correlations by performing a likelihood ratio test between two linear models: (a) the basic model is ﬁtted to predict the outcome, e.g. the mean pitch of the word using nonlexicalized textua
2808626814	An Empirical Analysis of the Correlation of Syntax and Prosody.	2123409957	G parsers [7, 8] and very recently in neural networks-based dependency parsing [9]. While the addition of discrete prosodic symbols has been helpful in tasks like speech recognition and understanding [10], this was not helpful for parsing [7]. In contrast, a neural networks-based syntax parser can use a continuous representation of prosody and shows a merit on overall parsing accuracy for pausing, wor
2808626814	An Empirical Analysis of the Correlation of Syntax and Prosody.	2118661157	h as whether a noun is used as a subject or object). Similar in spirit to our work, prosodic features and their relation to broad syntactic features such as content vs. function words are analyzed in [14]. Although that work uses a small corpus of conversations, they similarly use automatic alignments and automatic annotations (in that case parts of speech). The remainder of this paper is structured a
2808626814	An Empirical Analysis of the Correlation of Syntax and Prosody.	2131698806	sentences4 with a total of 348,062 word tokens in 57,265 word forms. We enrich the annotation with a dependency tree as well as Part-of-Speech tags for each sentence in our dataset using TurboParser [18] trained for German on the Hamburg Dependency Treebank [19]. The reported performance of the parser is &gt;93% labeled accuracy [19]. We parsed the complete German Spoken Wikipedia and these parses as
2808626814	An Empirical Analysis of the Correlation of Syntax and Prosody.	2067030274	tially through features more closely related to prominence like duration, loudness and pitch). The merit of prosody has indeed been used to improve syntax parsing, by including cues into PCFG parsers [7, 8] and very recently in neural networks-based dependency parsing [9]. While the addition of discrete prosodic symbols has been helpful in tasks like speech recognition and understanding [10], this was n
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1840435438	.08 2.0 33.44 69.50 83.48 3.0 SkipThought 37.66 71.02 84.06 2.6 30.67 65.74 80.98 3.0 As we can see in Table 9, InferSent [10] achieved excellent results on the three raking evaluations (R@k for k in [1, 5, 10]) and for both tasks (caption retrieval and image retrieval), a similar performance to the results reported by [10]. 11 0 25 50 75 Accuracy InferSent (AllNLI) Word2Vec (BoW, google news) Random Embedd
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2153579005	. Ranging from count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3], Word2Vec [28], GloVe [32], and more recently ELMo [33], to name a few. Although most of the recent word embedding techniques rely on the distributional linguistic hypothesis, they differ on the assumptions of how
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2612953412	5] also demonstrated improvements over SIF and traditional averaging by concatenating power means of the embeddings, closing the gap with other complex sentence embedding techniques such as InferSent [10]. Other sentence embedding techniques were also developed based on encoder/decoder architectures, such as the Skip-Thought [23], where the skip-gram model from Word2Vec [28] was abstracted to form a s
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2153579005	chniques such as InferSent [10]. Other sentence embedding techniques were also developed based on encoder/decoder architectures, such as the Skip-Thought [23], where the skip-gram model from Word2Vec [28] was abstracted to form a sentence level encoder that is trained on a self-supervised fashion. Recently, bi-directional LSTM models were also employed by InferSent [10] on a supervised training scheme
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2153579005	contains a 2.2M vocabulary and was trained on the Common Crawl (840B tokens) dataset. A traditional bag-of-words averaging was employed to produce the sentence embedding. Word2Vec (BoW, Google News) [28]: this model was obtained from the authors website at https: //code.google.com/archive/p/word2vec/. According to the authors, it was trained on part of the Google News dataset (about 100 billion words
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2626778328	E) was obtained from the TF Hub website at https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder-large/1. According to the TF Hub website, the model was trained with a Transformer [37] encoder. 4.2 Downstream classiﬁcation tasks As in [9], a classiﬁer was employed on top of the sentence embeddings for the classiﬁcation tasks. In this work, a Multi-Layer Perceptron (MLP) was used wi
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2028175314	e works 0 Subjectivity / Objectivity (SUBJ) [30] Classify the sentence as Subjective or Objective A movie that doesn’t aim too high , but doesn’t need to . Subjective Text REtrieval Conference (TREC) [38] Question and answering What are the twin cities ? LOC:city main verb in a given sentence is in the present or past tense. For more information about these tasks, please refer to the original article
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2250539671	ec was later found [25] to be implicitly factorizing a word-context matrix, where the cells are the pointwise mutual information (PMI) of the respective word and context pairs. Global Vectors (GloVe) [32] aims to overcome some limitations of Word2Vec, focusing on the global context for learning the representations. The global context is captured by the statistics of word co-occurrences in a corpus (co
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1840435438	ed on a self-supervised fashion. Recently, bi-directional LSTM models were also employed by InferSent [10] on a supervised training scheme using the Stanford Natural Language Inference (SNLI) dataset [5] to predict entailment/contradiction. InferSent [10] proved to yield much better results on a variety of downstream tasks when compared to many strong baselines or self-supervised methods such as Skip
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2612953412	en though word embeddings produce high-quality representations for words (or sub-words), representing large chunks of text such as sentences, paragraphs or documents is still an open research problem [10]. The tantalizing idea of learning sentence representations that could achieve good performance on a wide variety of downstream tasks, also called universal sentence encoder is, of course, the major g
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2250790822	es from0(not similar) to 5(very similar) Liquid ammonia leak kills 15 in Shanghai Liquid ammonia leak kills at least 15 in Shanghai 4:6 Sentences Involving Compositional Knowledge Entailment (SICK-E) [27] To measure semantics in terms of Entailment, Contradiction, or Neutral A man is sitting on a chair and rubbing his eyes There is no man sitting on a chair and rubbing his eyes Contradiction Sentences
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2131744502	ether and then used as input to a deep neural network that computes the sentence embeddings. Some other efforts in creating sentences embeddings include, but are not limited to, Doc2Vec/Paragraph2Vec [24], fastSent [17] and Sent2Vec [29]. In our work, we did not include these other approaches since we believe that the chosen ones are already representative of the existing ones and can enable indirect
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2194775991	etrieval tasks for the Microsoft COCO [26] dataset. Table 9: Results for the image retrieval and caption retrieval tasks using the Microsoft COCO [26] dataset and features extracted with a ResNet-101 [16]. In this table we present Recall at 1 (R@1), Recall at 5 (R@5) and so on, as well as the median. Caption Retrieval Image Retrieval Approach R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r ELMo (BoW, all layers
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2493916176	I) [5] To measure semantics in terms of Entailment, Contradiction, or Neutral A small girl wearing a pink jacket is riding on a carousel The carousel is moving Entailment FastText (BoW, Common Crawl) [4]: this model was obtained from the authors website at https: //fasttext.cc/docs/en/english-vectors.html. According to the authors, this model contains 2 million word vectors trained on Common Crawl (6
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2194775991	k in Table 5. 4.3 Semantic relatedness and textual similarity tasks We used the same scheme as in [9] to evaluate the semantic relatedness (SICK-R, STS Benchmark) and semantic textual similarity (STS-[12, 13, 14, 15, 16]). For semantic relatedness, which predicts a semantic value between 0 and 5 between two input sentences, we learn to predict the probability distribution of relatedness scores. For the semantic textu
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2612953412	A major advantage of Skip Thought Vectors for representing sentences when compared with a simple average of word embeddings is that order is considered during the encoding/decoding process. InferSent [10] proposes a supervised training for the sentence embeddings, contrasting with previous works such as Skip-Thought. The sentence encoders are trained using Stanford Natural Language Inference (SNLI) da
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2612953412	ment by leveraging the Transformer architecture [37], which is solely based on attention mechanisms, although without providing an evaluation with other baselines and previous works such as InferSent [10]. Neural Language Models can be tracked back to [3], and more recently deep bi-directional language models (biLM) [33] have successfully been applied to word embeddings in order to incorporate context
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2250539671	this model contains 2 million word vectors trained on Common Crawl (600B tokens) dataset. A traditional bag-ofwords averaging was employed to produce the sentence embedding. GloVe (BoW, Common Crawl) [32]: this model was obtained from the authors website at https: //nlp.stanford.edu/projects/glove/. According to the authors, it contains a 2.2M vocabulary and was trained on the Common Crawl (840B token
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1522301498	mployed on top of the sentence embeddings for the classiﬁcation tasks. In this work, a Multi-Layer Perceptron (MLP) was used with a single hidden layer of 50 neurons with no dropout added, using Adam [22] optimizer and a batch size of 64. We provide more information about the number of classes and validation scheme employed for each task in Table 5. 4.3 Semantic relatedness and textual similarity task
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2160660844	n a sentence are inverted or not, while Past Present (Tense) aims to detect if the 4 Table 2: Downstream classiﬁcation tasks description and samples. Dataset Task Example Output Customer Reviews (CR) [19] Sentiment analysis of customer products’ reviews We tried it out Christmas night and it worked great . Positive Multi-Perspective Question and Answering (MPQA) [39] Evaluation of opinion polarity Don
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2250539671	om count-based to predictive or task-based methods, in the past years, many approaches were developed to produce word embeddings, such as Neural Probabilistic Language Model [3], Word2Vec [28], GloVe [32], and more recently ELMo [33], to name a few. Although most of the recent word embedding techniques rely on the distributional linguistic hypothesis, they differ on the assumptions of how meaning or c
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2194775991	ontaining 5 captions. The metric used to rank caption and image retrieval in this task is recall at K (Recall@K), with K = 1, 5, 10, and also median over 5 splits of 1k images. COCO uses a ResNet-101 [16] for image embedding extraction, yielding 2048-d representation. 4.5 Linguistic probing tasks For the linguistic probing tasks, a MLP was also used with a single hidden layer of 50 neurons, with no dr
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1522301498	raction, yielding 2048-d representation. 4.5 Linguistic probing tasks For the linguistic probing tasks, a MLP was also used with a single hidden layer of 50 neurons, with no dropout added, using Adam [22] optimizer with a batch size of 64, except for the Word Content (WC) probing task, as in [11], in which a Logistic Regression was used since it provided consistently better results. 5 Experimental res
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1840435438	rence (SNLI) dataset, which consists of 570k human-generated English sentence-pairs and it is considered one of the largest high-quality labeled datasets for building sentence semantics understanding [5]. The authors tested 7 different architectures for the sentence encoder and the best results are achieved with a bi-directional LSTM (BiLSTM) encoder. p-mean [35] emerged as a response to InferSent [1
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2153579005	s. There are several implementations of word embeddings in the literature. Following the pioneering work by [3] on the neural language model for distributed word representations, the seminal Word2Vec [28] is one of the ﬁrst popular approaches of word embeddings based on neural networks. This type of representation is able to preserve semantic relationships between words and their context, where contex
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	1840435438	between sentences from 0 (not related) to 5 (related) A man is singing a song and playing the guitar A man is opening a package that contains headphones 1:6 Stanford Natural Language Inference (SNLI) [5] To measure semantics in terms of Entailment, Contradiction, or Neutral A small girl wearing a pink jacket is riding on a carousel The carousel is moving Entailment FastText (BoW, Common Crawl) [4]: t
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2493916176	he statistics of word co-occurrences in a corpus (count-based, as opposed to the prediction-based method as in Word2Vec), while still capturing semantic and syntactic meaning as in Word2Vec. FastText [4] is a recent method for learning word embeddings for large datasets. It can be seen as an extension of Word2Vec that treats each word as a composition of character n-grams. The sub-word representation
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2493916176	text dataset. ELMo [33] representations are a function of the internal layers of the bi-directional Language Model (biLM), which provides a very rich representation about the tokens. Like in fastText [4], ELMo [33] breaks the tradition of word embeddings by incorporating sub-word units, but ELMo [33] has also some fundamental differences with previous shallow representations such as fastText or Word2
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2612953412	tional LSTM models were also employed by InferSent [10] on a supervised training scheme using the Stanford Natural Language Inference (SNLI) dataset [5] to predict entailment/contradiction. InferSent [10] proved to yield much better results on a variety of downstream tasks when compared to many strong baselines or self-supervised methods such as Skip-Thought [23], by leveraging strong supervision. Lat
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2626778328	utation of mean is based on power means [14]. Google recently introduced Universal Sentence Encoder [8], where two different encoders were implemented. The ﬁrst is the Transformer based encoder model [37], which aims for high-accuracy but has larger complexity and uses more computational resources. The second model uses a deep averaging network (DAN) [20], where embeddings for words and bi-grams are a
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2114524997	ve Stanford Sentiment Analysis 5 (SST-5) [36] Sentiment analysis with 5 classes, that range from0(most negative) to 5(most positive) Nothing about this movie works 0 Subjectivity / Objectivity (SUBJ) [30] Classify the sentence as Subjective or Objective A movie that doesn’t aim too high , but doesn’t need to . Subjective Text REtrieval Conference (TREC) [38] Question and answering What are the twin ci
2808681756	Evaluation of sentence embeddings in downstream and linguistic probing tasks.	2626778328	versal Sentence Encoder (USE) [8] mixed an unsupervised task using a large corpus together with the supervised SNLI task and showed a signiﬁcant improvement by leveraging the Transformer architecture [37], which is solely based on attention mechanisms, although without providing an evaluation with other baselines and previous works such as InferSent [10]. Neural Language Models can be tracked back to
2808682925	Unsupervised Word Segmentation from Speech with Attention	191292882,2063655091,2756778986	[10] and from speech [23, 25, 26, 27]. Word discovery experiments from text input on Mboshi were reported in [28]. Bilingual setups (cross-lingual supervision) for word segmentation were discussed by [29, 30, 31, 9], but applied to speech transcripts (true phones). Looking at NMT from speech, the research by [11, 12] are recent examples of approaches to end-to-end spoken language translation, but usFigure 2: NMT
2808682925	Unsupervised Word Segmentation from Speech with Attention	1959608418	1, W 2, and b 2 are learned jointly with the other model parameters. At each time step (t) a score e t;i is computed for each encoder state h i, using the current decoder state s t Auto-Encoder (VAE) [19] where the posterior distribution of1. These scores are then normalized using a softmax function, thus giving a probability distribution over the input sequenceP A i=1 t;i = 1 and 8t;i;0  t;i 1. The
2808682925	Unsupervised Word Segmentation from Speech with Attention	2582956876	28]. Bilingual setups (cross-lingual supervision) for word segmentation were discussed by [29, 30, 31, 9], but applied to speech transcripts (true phones). Looking at NMT from speech, the research by [11, 12] are recent examples of approaches to end-to-end spoken language translation, but usFigure 2: NMT output alignment for true phones (top) and AUD using MBN SVAE (bottom). For illustration purposes, we
2808682925	Unsupervised Word Segmentation from Speech with Attention	2756778986	he absence of (manually obtained) phonetic labels. As a ﬁrst contribution in this direction, we recently proposed to leverage attentional encoder-decoder approaches for unsupervised word segmentation [9]. However, this was done from an unsegmented sequence of (manually obtained) true phone symbols, not from speech. It was shown that the approach proposed can compete with a Bayesian Non Parametric (BN
2808682925	Unsupervised Word Segmentation from Speech with Attention	2133564696	anslating from the WRL into the UL. Then, these soft-alignment matrices are post-processed to derive word boundaries. 2.1. Neural Architecture The NMT architecture, equations (1)-(4), are inspired by [14]. A bidirectional encoder reads the input sequence x 1;:::;x A and produces a sequence of encoder states h = h 1;:::;h A 2R2 n, where nis the chosen encoder cell size. At each time step t, the decoder
2808682925	Unsupervised Word Segmentation from Speech with Attention	2756778986	is not applied to a language documentation scenario, while the latter does not provide a full coverage of the speech corpus analyzed. 6. Conclusions Different from these related works and inspired by [9], this paper presented word segmentation from speech, in a bilingual setup and for a real language documentation scenario (Mboshi). The proposed approach ﬁrst performs AUD to generate pseudophones fro
2808682925	Unsupervised Word Segmentation from Speech with Attention	2756778986	d by [5]. Rather than enforcing additional constraints on the alignments, as in the latter reference, we propose to reverse the architecture and to translate from WRL words into UL symbols, following [9]. This “reverse” architecture notably prevents the attention model from ignoring some UL symbols. As experiments with actual phone sequences have shown that the best results were obtained with this WR
2808682925	Unsupervised Word Segmentation from Speech with Attention	1833498382	the data with a relatively small unit set. We implemented two variants of this original model. The HMM, approximates the Dirichlet Process prior by a simpler symmetric Dirichlet prior, as proposed by [17]. This approximation, while retaining the sparsity constraint, avoids the complication of dealing with the variational treatment of the stick breaking process in Bayesian non-parametric models. The se
2808682925	Unsupervised Word Segmentation from Speech with Attention	2251025892	ds are generated by a bigram model over a non-ﬁnite inventory, through the use of a Dirichlet process (refered to as dpseg). We evaluate with the Boundary metric from the Zero Resource Challenge 2017 [22, 3]. It measures the quality of a word segmentation and the discovered boundaries with respect to a gold segmentation (P, R and F-score are computed). 4.2. Details of the NMT system We use the LIG-CRIStA
2808682925	Unsupervised Word Segmentation from Speech with Attention	2133564696	easier to segment. For instance, even if the attention helps the system to better deal with long sentences, it is still prone to performance degradation when faced with very long sequences of symbols [14]. Table 3 (left side) shows how the different AUD approaches are encoding the UL sentences. We observe that the HMM model uses more symbols to represent an utterance, while the SVAE model offers a mor
2808682925	Unsupervised Word Segmentation from Speech with Attention	2055408826	ion to build acoustic and language models. However, for many languages of the world, text transcripts are limited or nonexistent; therefore, recent efforts have been devoted to Zero Resource Settings [1, 2, 3] where the aim is to build speech systems without textual or linguistic resources for e.g.: (1) unwritten languages [4, 5]; (2) models that mimic child language development [6]; (3) documentation of e
2808682925	Unsupervised Word Segmentation from Speech with Attention	1778492285,2107038463	ity) soft alignment (attention) matrix produced in our best setup (MBN SVAE). 5. Related Work Word segmentation in a monolingual setup was previously investigated from text input [10] and from speech [23, 25, 26, 27]. Word discovery experiments from text input on Mboshi were reported in [28]. Bilingual setups (cross-lingual supervision) for word segmentation were discussed by [29, 30, 31, 9], but applied to speec
2808682925	Unsupervised Word Segmentation from Speech with Attention	2133564696	a non-linear transformation), z 0 = &lt;BOS&gt; (special token), and E 2R jV n is the target embedding matrix. The output function uses a maxout layer, followed by a linear projection to R jV , as in [14]. The attention mechanism is deﬁned as: e t;i = v T tanh(W 1h i + W 2s t ﬁrst one, referred to as1 + b 2) (5) t;i = softmax(e t;i) (6) c t = attn(h;s t 1) = XA i=1 t;ih i (7) where v, W 1, W 2, and b
2808682925	Unsupervised Word Segmentation from Speech with Attention	2582956876	t each time step t, the decoder uses its current state s t 1 and an attention mechanism 2End-to-end speech processing can be performed with an encoderdecoder architecture for speech translation (e.g. [11, 12]); early attempts to train end-to-end from speech to translated text, in our language documentation scenario, were not viable due to limited data. arXiv:1806.06734v1 [cs.CL] 18 Jun 2018 to compute a p
2808682925	Unsupervised Word Segmentation from Speech with Attention	2756778986	th the scores of the two neighboring words. Even if boosting many-to-one alignments should not hold in the case of the reverse architecture, we keep it for our experiments given the gains reported by [9], even in the reverse case. Hard Segmentation Generation: once the soft-alignment matrices are obtained for all utterances in the corpus, a word segmentation is inferred as follows. We ﬁrst transform
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	2189217339	me (this frequency is not rare, but much lower than one would expect given the circumstances). In their work on evaluating the performance of personal assistants such as Siri, Google Now and Cortana, Jiang et al. (2015) show that 67% of user action types are commands, but they don’t specify the proportion of imperatives within those. A substantial percentage of to-do tasks are expressed as fragments, such as \Hotel
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	2145626381	agments are in fact ellipsis. Even tweets, the shortest of today’s social media language, are often grammatically ill-formed, contain a main verb. If we now turn to the interpretation of to-do tasks, Gil et al. (2012) explored dierent kinds of intelligent assistance for to-do lists. Gil et al. (2012) provides a manual data entry to categorize the to-do task and Jiang et al. (2015) yields a performance evaluation,
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	2189217339	the interpretation of to-do tasks, Gil et al. (2012) explored dierent kinds of intelligent assistance for to-do lists. Gil et al. (2012) provides a manual data entry to categorize the to-do task and Jiang et al. (2015) yields a performance evaluation, however, neither automatically categorize the IA as we do. Likewise (Tur et al.,2014) distinguishes between requests that are covered by the current interpreter and t
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	1969221592	ng precision of existing extraction systems. However, our work covers a broader domain of general English to-do tasks and does not t in any existing method of tagging arguments. The approach taken by Ghani et al. (2006) is the most similar to ours with regards to their argument extraction for textual product descriptions. However, their argument set is restricted to a predened set where our model learns the argumen
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	2145626381	oss two corpora that span sub-domains, one of which we released. 1 Introduction To-do lists are pervasive and oer a concise representation of tasks that need to be accomplished (Bellotti et al.,2004;Gil et al., 2012), with studies showing that up to 60% of the population use them (Jones and Thomas, 1997). To-do tasks are often expressed as short utterances and written in list form. Examples of to-do task are \Buy
2808810287	A Supervised Approach To The Interpretation Of Imperative To-Do Lists.	2122223050	ry of knowledge, whose language is not telegraphic, and mostly well formed. Earlier work in argument extraction includes processing sequences of executable actions for the Windows operating system by Branavan et al. (2009). Other argument extraction work includes eorts in deep learning by Meerkamp et al. (2017) for boosting precision of existing extraction systems. However, our work covers a broader domain of general
2809324505	The Natural Language Decathlon: Multitask Learning as Question Answering	1840435438	[Hovy et al.,2006]. Zero-shot domain adaptation for text classiﬁcation. Because MNLI is included in decaNLP, it is possible to adapt to the related Stanford Natural Language Inference Corpus (SNLI) [Bowman et al., 2015]. Fine-tuning a MQAN pretrained on decaNLP achieves an 87% exact match score, which is a 2 point increase over training from a random initialization and 2 points from the state of the art [Kim et al.
2809324505	The Natural Language Decathlon: Multitask Learning as Question Answering	2251199578	and predicate (typically a verb) and must determine ‘who did what to whom,’ ‘when,’ and ‘where’ [Johansson and Nugues,2008]. We use an SRL dataset that treats the task as question answering, QA-SRL [He et al., 2015]. This dataset covers both news and Wikipedia domains, but we only use the latter in order to ensure that all data for decaNLP can be freely downloaded. We evaluate QA-SRL with the nF1 metric used fo
2809676241	Dictionary-Guided Editing Networks for Paraphrase Generation	2143017621	sing L size PPBD. PPDB contains ﬁve types of entailment relations and we exact paraphrased pairs with equivalent entailment relations to ensure the quality of our paraphrased dictionary. We use NLTK (Bird and Loper 2004) to tokenize the sentences and keep words that appear more than 10 times in our vocabulary. Following the data preprocessing method in previous work (Prakash et al. 2016; Gupta et al. 2017), we reduce
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2250397939	1 word cluster labeling engineering Naseem, Barzilay, and Globerson(2012) Generative 17 / 10 syntactic parsing Täckström, McDonald, and Nivre(2013) Discriminative graph-based 16 / 7 syntactic parsing Zhang and Barzilay (2015) Discriminative tensor-based 10 / 4 syntactic parsing Daiber, Stanojevic,´ and Sima’an(2016) One-to-many MLP 22 / 5 reordering for machine translation Ammar et al.(2016) Multi-lingual transition-based
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2115057736	arkers, and their order. This information can be cast in the form of priors for unsupervised syntax-based Bayesian models (Titov and Klementiev2012), guidance for alignments in annotation projection (Padó and Lapata 2009;Van der Plas, Merlo, and Henderson2011), or regularizers for model transfer in order to tailor the source model to the grammar of the target language (Kozhevnikov and Titov2013). Cross-lingual inform
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2131134557	r equivalence. The enhancement from a monolingual baseline grows with the number of languages involved (Snyder et al.2009) and holds true for most of their combinations (although it varies markedly) (Naseem et al. 2009). In a similar spirit, monolingual objectives can be regularized with a constraint that penalizes disagreement in predictions across languages at inference time (Titov and Klementiev2012). Nevertheles
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2672781320	and Knight (2016) Typology-based selection 227 grapheme to phoneme Agic´(2017) PoS divergence metric 26 / 5 syntactic parsing Søgaard and Wulff (2012) Typology-based weighing 12 / 1 syntactic parsing Wang and Eisner (2017) Word-order-based tree synthesis 17 / 7 syntactic parsing Ponti et al.(2018) Construction-based tree preprocessing 6 / 3 machine translation, sentence similarity Table 3: An overview of the approaches
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	59306127	m a source directly to several target languages 21 Computational Linguistics Volume xx, Number xx Author Details Requirements Langs Features annotation Liu(2010) Treebank count Treebank 20 word order Lewis and Xia (2008) IGT projection IGT, source chunker 97 word and morpheme order, determiners Bender et al.(2013) IGT projection IGT, source chunker 31 word order and case alignment Östling(2015) Treebank projection Pa
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2351252181	odels can be conditioned on typological features. Character-level language models can be trained jointly for several languages, provided that these are encoded by universal symbols like IPA phonemes (Tsvetkov et al. 2016). An input x and a language vector ‘at time t are initially mapped to a local context representation and then passed to a global LSTM. This hidden representation is factored by a non-linear transforma
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2672781320	phonology, morphology, syntax ence Takamura, Nagata, and Kawasaki (2016) Logistic regression WALS whole whole Murawaki(2017) Bayesian + feature and language interactions Genealogy and WALS 2607 whole Wang and Eisner (2017) Feed-forward Neural Network WALS, tagger, synthetic treebanks 37 word order Cotterell and Eisner(2017) Determinant Point Process with neural features WALS 200 vowel inventory Daumé III and Campbell(2
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2250397939	s. For instance, ‘Order of subject, verb, and object’ (81A) is taken into account only when the head is a verb and the dependent is a noun. This approach was further extended to tensor-based models byZhang and Barzilay (2015), in order to avoid the shortcomings of manual feature selection. They induce a compact hidden representation of atomic features and languages by factorizing a tensor constructed from their combinatio
2810095012	Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing	2344508595	y, and the number of segments in a class. A small number of papers broadens the range of utilized typological features to the entire feature inventory of a given database. In particularAgic´(2017) andAmmar et al. (2016) harvest all the features in WALS, while (Deri and Knight2016) use all the features in URIEL. Similarly,Søgaard and Wulff(2012) utilize all the WALS features with the exception of phonological feature
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2415755012	) that uses attention to directly pick the answer from the context, which is motivated by the Pointer Network (Vinyals et al., 2015). Instead of summing the attention of query-to-document, GA Reader (Dhingra et al., 2017) deﬁned an element-wise product to endowing attention on each word of the document using the entire query representation to build query-speciﬁc representations of words in the document for accurate an
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1938755728	; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word representations (Kim et al., 2016; Yang et al., 2017; Luong and Manning, 2016; Huang et al., 2018). Yang et al. (2017) explored a ﬁne-grained gating mechanism (FG Reader) to dynamically combine word-level and character-level represen
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2538616903	5 Related Work 5.1 Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2417763662	6). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (20
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2415755012	73.2 74.3 71.9 FG Reader z 79.1 75.0 75.3 72.0 GA Reader z 76.8 72.5 73.1 69.6 SAW Reader 78.5 74.9 75.0 71.6 Table 5: Accuracy on CBT dataset. Results marked with zare of previously published works (Dhingra et al., 2017; Cui et al., 2016; Yang et al., 2017). 0 1k 2k 5k 10k 20k Merging Times 76.0 76.5 77.0 77.5 78.0 78.5 79.0 Accuracy dev test Figure 2: Case study of the subword vocabulary size of BPE. Figure 3: Quan
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2606342375	ation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who used morphological tags instead of n-grams. They jointly trained their morphological and semantic embeddings, implicitly assuming that morphologic
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2608256743	d embedding to feed a two-layer Highway Network. Not only for machine reading comprehension tasks, character embedding has also beneﬁt other natural language process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only sh
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1544827683	d by a placeholder to indicate the answer to ﬁll in. Table 1 gives data statistics. Different from the current cloze-style datasets for English reading comprehension, such as CBT, Daily Mail and CNN (Hermann et al., 2015), the three Chinese datasets do not provide candidate answers. Thus, the model has to ﬁnd the correct answer from the entire document. Besides, we also use the Children’s Book Test (CBT) dataset (Hill
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1544827683	duction A recent hot challenge is to train machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of wh
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2417736714	e process tasks, such as word segmentation (Cai et al., 2017), machine translation (Luong and Manning, 2016), tagging (Yang et al., 2016; Li et al., 2018) and language modeling (Verwimp et al., 2017; Miyamoto and Cho, 2016). However, character embedding only shows marginal improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Ca
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2436788615	e query-to-document and document-to-query are mutually attentive and interactive to each other. 5.2 Augmented Word Embedding Distributed word representation plays a fundamental role in neural models (Cai and Zhao, 2016; Qin et al., 2016; Zhao et al., 2017; Peters et al., 2018; He et al., 2018; Wang et al., 2018a; Bai and Zhao, 2018; Zhang et al., 2018a). Recently, character embeddings are widely used to enrich word
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2427527485	his end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al.
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2512457506	er z 79.1 75.0 75.3 72.0 GA Reader z 76.8 72.5 73.1 69.6 SAW Reader 78.5 74.9 75.0 71.6 Table 5: Accuracy on CBT dataset. Results marked with zare of previously published works (Dhingra et al., 2017; Cui et al., 2016; Yang et al., 2017). 0 1k 2k 5k 10k 20k Merging Times 76.0 76.5 77.0 77.5 78.0 78.5 79.0 Accuracy dev test Figure 2: Case study of the subword vocabulary size of BPE. Figure 3: Quantitative study on
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2512457506	est-human AS Reader 64.1 67.2 33.1 GA Reader 67.2 69.0 36.9 CAS Reader 65.2 68.1 35.0 SAW Reader 72.8 75.1 43.8 Table 4: Accuracy on PD and CFT datasets. Results of AS Reader and CAS Reader are from (Cui et al., 2016). PD &amp; CFT Since there is no training set for CFT dataset, our model is trained on PD training set. Note that the CFT dataset is harder for the machine to answer because the test set is further pr
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2415755012	ile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang et al., 2017b; Seo et al., 2017; Cui et al., 2017a; Kadlec et al., 2016; Dhingra et al., 2017; Zhang and Zhao, 2018). However, how to represent word in an effective way remains an open problem for diverse natural language processing tasks, including machine reading comprehension for different
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1816313093	ion should live in the same space. However, the linguistic knowledge resulting subwords, typically, morphological sufﬁx, preﬁx or stem, may not be suitable for different kinds of languages and tasks. Sennrich et al. (2016) introduced the byte pair encoding (BPE) compression algorithm into neural machine translation for being capable of open-vocabulary translation by encoding rare and unknown words as subword units. Ins
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2493916176	l improvement due to a lack internal semantics. Lexical, syntactic and morphological information are also considered to improve word representation (Cao and Rei, 2016; Bergmanis and Goldwater, 2017). Bojanowski et al. (2017) proposed to learn representations for character n-gram vectors and represent words as the sum of the n-gram vectors. Avraham and Goldberg (2017) built a model inspired by (Joulin et al., 2017), who u
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2512457506	n machines to read and comprehend human languages. Towards this end, various machine reading comprehension datasets have been released, including cloze-style (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016) and user-query types (Joshi et al., 2017; Rajpurkar et al., 2016). Meanwhile, a number of deep learning models are designed to take up the challenges, most of which focus on attention mechanism (Wang
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1505680913	ods of word formation across human languages, which inspires us to represent word by meaningful sub-word units. Recently, researchers have started to work on morphologically informed word embeddings (Botha and Blunsom, 2014; Cao and Rei, 2016), aiming at better capturing syntactic, lexical and morphological information. With ready subwords, we do not have to work with characters, and segmentation could be stopped at the
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2411480514	omprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-centric classiﬁer with a careful analysis based on handful features. Kadl
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2417763662	oss human languages, which inspires us to represent word by meaningful sub-word units. Recently, researchers have started to work on morphologically informed word embeddings (Botha and Blunsom, 2014; Cao and Rei, 2016), aiming at better capturing syntactic, lexical and morphological information. With ready subwords, we do not have to work with characters, and segmentation could be stopped at the subword-level to re
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2512457506	our proposed model, we conduct multiple experiments on three Chinese Machine Reading Comprehension datasets, namely CMRC-2017 (Cui et al., 2017b), People’s Daily (PD) and Children Fairy Tales (CFT) (Cui et al., 2016)2. In these datasets, a story containing consecutive sentences is formed as the Document and one of the sentences is either automatically or manually selected as the Query where one token is replaced
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2516930406	more relevant pieces. 5 Related Work 5.1 Machine Reading Comprehension Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the compr
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2415755012	sed to a fully connected layer to obtain the ﬁnal subword embedding SE(w). SE(w) = W ! h t +b 2.3 Attention Module Our attention module is based on the Gated attention Reader (GA Reader) proposed by (Dhingra et al., 2017). We choose this model due to its simplicity with comparable performance so that we can focus on the effectiveness of SAW strategies. This module can be described in the following two steps. After aug
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	2415755012	sion Recently, many deep learning models have been proposed for reading comprehension (Sordoni et al., 2016; Trischler et al., 2016; Wang and Jiang, 2016; Munkhdalai and Yu, 2017; Wang et al., 2017a; Dhingra et al., 2017; Zhang et al., 2018b; Wang et al., 2018b). Notably, Chen et al. (2016) conducted an in-depth and thoughtful examination on the comprehension task based on an attentive neural network and an entity-ce
2810134635	Subword-augmented Embedding for Cloze Reading Comprehension	1816313093	t frequency and merge them in all the sequences. Note the segmentation status is updating now. 3.If the merging times do not reach the speciﬁed number, go back to 1, otherwise the algorithm ends. In (Sennrich et al., 2016), BPE is adopted to segment infrequent words into sub-word units for machine translation. However, there is a key difference between the motivations for subword segmentation. We aim to reﬁne the word
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	1957740064	The audio-visual encoding is obtained by multi-modal attention described in the next section. 3.2 Multimodal-attention based Video Features To predict a word sequence in video description, prior work [14] extracted content vectors from image features of VGG-16 and spatiotemporal motion features of C3D, and combined them into one vector in the fusion layer as: g(av) n = tanh XK k=1 d k;n ! ; (10) where
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2117488952,2130607791	dialog technologies have been applied in real-world human-machine interfaces including smart phone digital assistants, car navigation systems, voice-controlled smart speakers, and humanfacing robots [1, 2, 3]. Generally, a dialog system consists of a pipeline of data processing modules, including automatic speech recognition, spoken language understanding, dialog management, sentence generation, and speec
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2425121537	e the performance of video description using multimodal attention-based video features in this paper. 4.1 Datasets We evaluated our proposed feature fusion using the MSVD (YouTube2Text) [15], MSR-VTT [16], and Charades [13] video data sets. MSVD (YouTube2Text) covers a wide range of topics including sports, animals, and music. We applied the same condition deﬁned by [15]: a training set of 1,200 vide
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2603266952	ep towards conversational visual AI, the new task of visual dialog was introduced [9], in which an AI agent holds a meaningful dialog with humans about an image using natural, conversational language [10]. While VQA and visual dialog take signiﬁcant steps towards human-machine interaction, they only consider a single static image. To capture the semantics of dynamic scenes, recent research has focused
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2584992898	ion, in which multimodal feature vectors are combined using projection matrices W ckfor Kdifferent modalities (input sequences x k1;:::;x kLfor k= 1;:::;K). To fuse multimodal information, prior work [11] proposed method extends the attention mechanism. We call this fusion approach multimodal attention. The approach can pay attention to speciﬁc modalities of input based on the current state of the dec
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2273038706	oder-decoder-based system whose outputs are natural-language responses. Using this end-to-end framework, visual question answering (VQA) has been intensively researched in the ﬁeld of computer vision [6, 7, 8]. The goal of VQA is to generate answers to questions about an imaged scene, using the information present in a single static image. As a further step towards conversational visual AI, the new task of
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2584992898	owever, approvimatebly 12 of the MSR-VTT videos on YouTube have been removed. We used the available data consists of 5,763, 419, and 2,616 clips for train, validation, and test respectively deﬁned by [11]. Charades [13] is split into 7985 clips for training and 1863 clips for validation. provides 27,847 textual descriptions for the videos, As these textual descriptions are only available in the train
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2584992898	uses a multimodal attention mechanism that selectively attends to different input modalities (feature types), such spatiotemporal motion features and audio features, in addition to temporal attention [11]. In this paper, we propose a new research target, a dialog system that can discuss dynamic scenes with humans, which lies at the intersection of multiple avenues of research in natural language proce
2810643877	End-to-End Audio Visual Scene-Aware Dialog using Multimodal Attention-Based Video Features.	2142900973	ﬁrstly evaluate the performance of video description using multimodal attention-based video features in this paper. 4.1 Datasets We evaluated our proposed feature fusion using the MSVD (YouTube2Text) [15], MSR-VTT [16], and Charades [13] video data sets. MSVD (YouTube2Text) covers a wide range of topics including sports, animals, and music. We applied the same condition deﬁned by [15]: a training set
2810840719	Neural Approaches to Conversational AI	1591706642	. (2015b), which proposed an RNN-based approach to conversational response generation (similar to Fig. 2.2) that exploited longer context. Together with the contemporaneous works (Shang et al., 2015; Vinyals and Le, 2015), these papers presented the ﬁrst neural approaches to fully E2E conversation modeling. While these three papers have some distinct properties, they are all based on RNN architectures, which 53 nowada
2810840719	Neural Approaches to Conversational AI	2127426251	. 3.3 Embedding-based Methods To address the paraphrasing problem, embedding-based methods map entities and relations in a KB to continuous vectors in a neural space; see, e.g., Bordes et al. (2013); Socher et al. (2013); Yang et al. (2015); Yih et al. (2015b). This space can be viewed as a hidden semantic space where various expressions with the same semantic meaning map to the same continuous vector. Most KB embedd
2810840719	Neural Approaches to Conversational AI	2399880602	, by adding a high-dimensional stochastic latent variable to the target. This additional latent variable is meant to address the challenge associated with the shallow generation process. As noted in (Serban et al., 2017), this process is problematic from an inference standpoint because the generation model is forced to produce a high-level structure— i.e., an entire response—on a word-by-word basis. This generation p
2810840719	Neural Approaches to Conversational AI	1673923490	.” In the case of deep learning, this gaming would be reminiscent of making non-random perturbations to an input to drastically change the network’s predictions, as it was done, e.g., with images in (Szegedy et al., 2013) to show how easily deep learning models can be fooled. However, preventing such a gaming is difﬁcult if the machine-learned metric is to become a standard evaluation, and this would presumably requir
2810840719	Neural Approaches to Conversational AI	2099542783	-craft heuristics to select a proper candidate to generate sentences. Even though machine learning can be used to train certain parts of these systems (Langkilde and Knight, 1998; Stent et al., 2004; Walker et al., 2007), the cost to write and maintain templates and rules leads to challenges in adapting to new domains or different user populations. Furthermore, the quality of these NLG systems is limited by the quali
2810840719	Neural Approaches to Conversational AI	1934909785	. It is expected that off-policy techniques can ﬁnd important use in evaluating and optimizing dialogue systems. Another related line of research is deep reinforcement learning applied to text games (Narasimhan et al., 2015), which is in many ways similar to a conversation, except that the scenarios are predeﬁned by the game designers. Recent advances for solving text games, such as handling natural-language actions (Nar
2810840719	Neural Approaches to Conversational AI	2099471712	“gaming” may occur unbeknownst to the system developer. If a generation system is optimized directly on a trainable metric, then the system and the metric become akin to an adversarial pair in GANs (Goodfellow et al., 2014), where the only goal of the generation system (Generator) is to fool the metric (Discriminator). Arguably, such attempts become easier with trainable metrics as they typically incorporate thousands o
2810840719	Neural Approaches to Conversational AI	836999996	, which offers captions of many commercial movies in different languages. This dataset contains about 8 billion words as of 2011, in multiple languages (Tiedemann, 2012). Ubuntu: The Ubuntu dataset (Lowe et al., 2015) has also been used extensively for E2E conversation modeling. It differs from other datasets such as Twitter in that it is less focused on chitchat but more goal-oriented, as it contains many dialogu
2810840719	Neural Approaches to Conversational AI	2399880602	), so that the other parts of the model can focus on lower-level aspects of generation, e.g., ensuring ﬂuency. The VHRED model incidentally helps reducing blandness as suggested by sample outputs of (Serban et al., 2017). Indeed, as the content of the response is conditioned on the latent variable, the generated response is only bland and devoid of semantic content if the latent variable determines that the response
2810840719	Neural Approaches to Conversational AI	2162059449	). The Passage Chunking module segments the top documents into a set of candidate passages, which are further ranked by the Passage Ranking module based on another passage-level boosted trees ranker (Wu et al., 2010). Finally, the MRC module identiﬁes the answer span “2012” from the top-ranked passages. Although turning Bing QA into a conversational QA agent of Sec. 3.8 requires the integration of additional comp
2810840719	Neural Approaches to Conversational AI	2251079237	on of (x;y). For example, is a query-document pair for Web search ranking (Huang et al., 2013; Shen et al., 2014), a document pair in recommendation (Gao et al., 2014b), a question-answer pair in QA (Yih et al., 2015a), a sentence pair of different languages in machine translation (Gao et al., 2014a), and an image-text pair in image captioning (Fang et al., 2015) and so on. 15 Figure 2.4: The architecture of seq2
2810840719	Neural Approaches to Conversational AI	2403702038	. The ﬁrst is based on supervised learning, where desired system responses are ﬁrst collected and then used to train multiple components of a dialogue system in order to maximize prediction accuracy (Bordes et al., 2017; Wen et al., 2017; Yang et al., 2017b; Eric et al., 2017; Madotto et al., 2018; Wu et al., 2018). Wen et al. (2017) introduced a modular neural dialogue system, where most modules are represented by
2810840719	Neural Approaches to Conversational AI	2022166150	1 Knowledge Base Organizing the world’s facts and storing them in a structured database, large scale Knowledge Bases (KB) like DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago (Suchanek et al., 2007) have become important resources for supporting open-domain QA. A typical KB consists of a collection of subject-predicate-object triples (s;r;t) where s;t2Eare entities and r2Ris a predicate or relat
2810840719	Neural Approaches to Conversational AI	2189066119	5). Another direction is to enrich the dialogue context. Rather than text-only or speech-only ones, our daily dialogues are often multimodal, and involve both verbal and nonverbal inputs like vision (Bohus et al., 2014; DeVault et al., 2014; de Vries et al., 2017; Zhang et al., 2018a). Challenges such as how to combine information from multiple modalities to make decisions arise naturally. So far, we have looked at
2810840719	Neural Approaches to Conversational AI	1518951372,1993378086	6c) constitutes the ﬁrst attempt to use RL in a fully E2E approach to conversational response generation. Instead of training the system on human-to-human conversations as in the supervised setup of (Sordoni et al., 2015b; Vinyals and Le, 2015), the system of Li et al. (2016c) is trained by conversing with a user simulator which mimics human users’ behaviors. As depicted in Fig. 5.4, human users have to be replaced w
2810840719	Neural Approaches to Conversational AI	2616122292	aken by Madotto et al. (2018) in their Mem2Seq model. This model uses mechanisms from pointer networks (Vinyals et al., 2015a) so as to incorporate external information from knowledge bases. Finally, Eric et al. (2017) proposed an end-to-end trainable Key-Value Retrieval Network, which is equipped with an attention-based key-value retrieval mechanism over entries of a KB, and can learn to extract relevant informati
2810840719	Neural Approaches to Conversational AI	2158548602	al. (2010) proposed to use hierarchical reinforcement learning to optimize a composite task’s dialogue policy, with tabular versions of the MAXQ (Dietterich, 2000) and Hierarchical Abstract Machine (Parr and Russell, 1998) approaches. While promising, their solutions assume ﬁnite states, so do not scale well to large conversational problems. 43 C5 =5áC5 =6áC5 =7áC5 C6 =8áC6 =9áC6 =:áC6 Cl Top -level Dial
2810840719	Neural Approaches to Conversational AI	2120045257	alogue systems. This is a new area with exciting research opportunities. Here, we brieﬂy describe a few of them. Evaluation remains a major research challenge. Although user simulation can be useful (Schatzmann and Young, 2009; Li et al., 2016d; Wei et al., 2018), a more appealing solution is to use real human-human conversation corpora for evaluation. Unfortunately, this problem, known as offpolicy evaluation in the RL li
2810840719	Neural Approaches to Conversational AI	2109910161	do not apply directly to larger-scale conversational problems. More recently, Peng et al. (2017) tackled the composite-task dialogue policy learning problem under the more general options framework (Sutton et al., 1999b), where the task hierarchy has two levels. As illustrated in Fig. 4.6, a top-level policy ˇ g selects which subtask gto solve, and a low-level policy ˇ a;gsolves the subtask speciﬁed by ˇ g. Assumin
2810840719	Neural Approaches to Conversational AI	2175256910	apter, their authors have created their own (subsets of) conversational data for training and testing, and then evaluated their systems against baselines and competing systems on these ﬁxed datasets. Dodge et al. (2016) used an existing dataset to deﬁne standard training and test sets, but it is relatively small. Some of the most notable E2E and chitchat datasets include: Twitter: Used since the ﬁrst data-driven re
2810840719	Neural Approaches to Conversational AI	2467764055	ased Simulation. Another approach to building user simulators is entirely based on data (Eckert et al., 1997; Levin et al., 2000; Chandramohan et al., 2011). Here, we describe a recent example due to Asri et al. (2016). Similar to the agenda-based approach, the simulator also starts an episode with a randomly generated user goal and constraints. These are ﬁxed during a conversation. In each turn, the user model tak
2810840719	Neural Approaches to Conversational AI	2592647456	asoning is simple yet efﬁcient and the model parameters can be trained using the classical back-propagation algorithm, thus it is adopted by most of the systems (Chen et al., 2016b; Seo et al., 2016; Wang et al., 2017b; Liu et al., 2017; Chen et al., 2017a; Weissenborn et al., 2017; Hu et al., 2017). However, since humans often solve question answering tasks by re-reading and re-digesting the document multiple tim
2810840719	Neural Approaches to Conversational AI	2122514299	ate the study of more data-driven approaches, known as corpus-based methods, that aim to optimize a generation module from corpora (Oh and Rudnicky, 2002; Angeli et al., 2010; Kondadadi et al., 2013; Mairesse and Young, 2014). Most such methods are based on supervised learning, while Rieser and Lemon (2010) takes a decision-theoretic view and uses reinforcement learning to make a trade-off between sentence length and info
2810840719	Neural Approaches to Conversational AI	2400801499	hows an ATIS (Airline Travel Information System) utterance example in the Inside-Outside-Beginning (IOB) format (Ramshaw and Marcus, 1995), where for each word the model is to predict a semantic tag. Yao et al. (2013) and Mesnil et al. (2015) applied recurrent neural networks to slot tagging, where inputs are one-hot encoding of the words in the utterance, and obtained higher accuracy than statistical baselines su
2810840719	Neural Approaches to Conversational AI	630532510	made available on the opensubtitles.org website, which offers captions of many commercial movies in different languages. This dataset contains about 8 billion words as of 2011, in multiple languages (Tiedemann, 2012). Ubuntu: The Ubuntu dataset (Lowe et al., 2015) has also been used extensively for E2E conversation modeling. It differs from other datasets such as Twitter in that it is less focused on chitchat bu
2810840719	Neural Approaches to Conversational AI	1518951372,1993378086	c for generation tasks such as machine translation and summarization. E2E dialogue is no different. While it is common to evaluate response generation systems using human raters (Ritter et al., 2011; Sordoni et al., 2015b; Shang et al., 2015, etc.), this type of evaluation is often expensive and researchers often have to resort to automatic metrics for quantifying day-to-day progress and for performing automatic syst
2810840719	Neural Approaches to Conversational AI	1518951372,1993378086	by capturing longer context (Yao et al., 2015; Serban et al., 2016, 2017; Xing et al., 2018). One popular approach is the Hierarchical Recurrent Encoder-Decoder (HRED) model, originally proposed in (Sordoni et al., 2015a) for query suggestion and applied to response generation in (Serban et al., 2016). The HRED architecture is depicted in Fig. 5.1, where it is compared to the standard RNN architecture. HRED models d
2810840719	Neural Approaches to Conversational AI	2121092017	in the case of Deep Q-Network (DQN), Q(s;a;) takes the form of deep neural networks, such as multi-layer perceptrons and convolutional networks (Tesauro, 1995; Mnih et al., 2015), recurrent network (Hausknecht and Stone, 2015; Li et al., 2015), etc. More examples will be seen in later chapters. Furthermore, it is possible to represent the Q-function in a non-parametric way, using decision trees (Ernst et al., 2005) or Gau
2810840719	Neural Approaches to Conversational AI	2755402024	cedure, which can generate conversations between a simulated user and the system agent. Promising results have been observed in negotiation dialogues (Lewis et al., 2017) and task-oriented dialogues (Liu and Lane, 2017; Shah et al., 2018; Wei et al., 2018). It provides an interesting solution to avoid the evaluation cost of involving human users as well as overﬁtting to untruthful simulated users. In practice, it i
2810840719	Neural Approaches to Conversational AI	2130942839	cently begun to explore fully data-driven and end-to-end approaches to conversational response generation, e.g., within the sequence-to-sequence (seq2seq) framework (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014). These models are trained entirely from data without resorting to any expert knowledge, which means they do not rely on the four traditional components of dialogue systems noted in Chapter 4. Such en
2810840719	Neural Approaches to Conversational AI	2140054881	ch higher than that at sentence-level (Przybocki et al., 2009), e.g., with Spearman’s ˆ above 0.95 for the best metrics on WMT translation tasks (Graham and Baldwin, 2014).13 In the case of dialogue, Galley et al. (2015) showed that the correlation of string-based metrics (BLEU and deltaBLEU) signiﬁcantly increases with the units of measurement bigger than a sentence. Speciﬁ- cally, their Spearman’s ˆcoefﬁcient goes
2810840719	Neural Approaches to Conversational AI	2119717200	cided by the controller on the ﬂy. More complex queries need more steps. Thus, IRN learns a stochastic policy to get a distribution over termination and prediction actions by the REINFORCE algorithm (Williams, 1992), which is described in Sec. 2.3.2 and Eqn. 2.7. Since all the modules of IRN are differentiable, IRN is an end-to-end differentiable neural model whose parameters, including the embedded KB matrix M,
2810840719	Neural Approaches to Conversational AI	10957333	ction rather than completing speciﬁc tasks, they are often developed to mimic human conversations by training DNN-based response generation models on large amounts of human-human conversational data (Ritter et al., 2011; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015). Only recently have researchers begun to explore how to ground the chitchat in world knowledge (Ghazvininejad et al., 2018) and image
2810840719	Neural Approaches to Conversational AI	2007221309	d dialogue state; the 37 distribution is sometimes referred to as the “belief state.” Since exact optimization in POMDPs is computationally intractable, authors have studied approximation techniques (Roy et al., 2000; Williams and Young, 2007; Young et al., 2010; Li et al., 2009; Gasiˇ ´c and Young, 2014) and alternative representations such as the information states framework (Larsson and Traum, 2000; Daubigney
2810840719	Neural Approaches to Conversational AI	2060833990	d to move beyond sentence-level correlation is probably even more critical in dialogue. When measured at a corpus- or system-level, correlations are typically much higher than that at sentence-level (Przybocki et al., 2009), e.g., with Spearman’s ˆ above 0.95 for the best metrics on WMT translation tasks (Graham and Baldwin, 2014).13 In the case of dialogue, Galley et al. (2015) showed that the correlation of string-bas
2810840719	Neural Approaches to Conversational AI	1521413921	d templates and rules. These challenges motivate the study of more data-driven approaches, known as corpus-based methods, that aim to optimize a generation module from corpora (Oh and Rudnicky, 2002; Angeli et al., 2010; Kondadadi et al., 2013; Mairesse and Young, 2014). Most such methods are based on supervised learning, while Rieser and Lemon (2010) takes a decision-theoretic view and uses reinforcement learning t
2810840719	Neural Approaches to Conversational AI	1591706642	they are often developed to mimic human conversations by training DNN-based response generation models on large amounts of human-human conversational data (Ritter et al., 2011; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015). Only recently have researchers begun to explore how to ground the chitchat in world knowledge (Ghazvininejad et al., 2018) and images (Mostafazadeh et al., 2017) so as to make t
2810840719	Neural Approaches to Conversational AI	2131494463	to devise an iterative way to ﬁnd answers as multi-step reasoning. Multi-Step Reasoning. Multi-step reasoning models are pioneered by Hill et al. (2015); Dhingra et al. (2016); Sordoni et al. (2016); Kumar et al. (2016), who used a pre-determined ﬁxed number of reasoning steps. Shen et al. (2017b,c) showed that multi-step reasoning outperforms single-step ones and dynamic multi-step reasoning further outperforms the
2810840719	Neural Approaches to Conversational AI	2432004435	for dialogue and related tasks (Kannan and Vinyals, 2016; Li et al., 2017c; Holtzman et al., 2018) offer solutions to this problem, but it is also well-known that such setups suffer from instability (Salimans et al., 2016) due to the nature of GANs’ minimax formulation. This fragility is potentially troublesome as the outcome of an automatic evaluation should ideally be stable (Cer et al., 2010) and reproducible over t
2810840719	Neural Approaches to Conversational AI	2119168550	will be discussed in Chapter 7. 5.1 End-to-End Conversation Models Most of the earliest end-to-end (E2E) conversation models are inspired by statistical machine translation (SMT) (Koehn et al., 2003; Och and Ney, 2004), including neural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Bahdanau et al., 2015). The casting of the conversational response generation task (i.e., predict a response
2810840719	Neural Approaches to Conversational AI	2114875951	e action is chosen conditioned on this high-level decision. 4.4.4 Multi-domain Dialogues A multi-domain dialogue can converse with a user to have a conversation that may involve more than one domain (Komatani et al., 2006; Hakkani-Tur et al., 2012; Wang et al., 2014). Table 4.2 shows¨ an example, where the dialogue covers both the hotel and restaurant domains, in addition to a special metadomain for sub-dialogues that
2810840719	Neural Approaches to Conversational AI	1958706068,2311783643,2410983263	e to book 3 tickets for Zoolander 2 tomorrow at Regal Meridian 16 theater in Seattle at 9:25 pm. User Thank you. System Thank you. Good bye! Table 4.1: An example movie-booking dialogue, adapted from Li et al. (2016d). as ticket price. Note that a slot can be both informable and requestable, an example being movie name. Dialogue Acts The interaction between a dialogue agent and a user, as shown in the previous e
2810840719	Neural Approaches to Conversational AI	2007221309	e DST is perfect). Both assumptions are often violated in real-world applications, given ambiguity in user utterance and unavoidable errors in NLU. To handle uncertainty inherent in dialogue systems, Roy et al. (2000) and Williams and Young (2007) proposed to use Partially Observable Markov Decision Process (POMDP) as a principled mathematical framework for modeling and optimizing dialogue systems. The idea is to
2810840719	Neural Approaches to Conversational AI	2007807439	e other hand, allow users to query a KB interactively without composing complicated queries, motivated by the observations: Users are more used to issuing simple queries of length less than 5 words (Spink et al., 2001). In many cases, it is unreasonable to assume that users can construct compositional queries without prior knowledge of the structure of the KB to be queried. A conversational KB-QA agent is useful f
2810840719	Neural Approaches to Conversational AI	889023230	e histories can often be long and there is sometimes a need to exploit longer-term context. Hierarchical models were designed to address this limitation by capturing longer context (Yao et al., 2015; Serban et al., 2016, 2017; Xing et al., 2018). One popular approach is the Hierarchical Recurrent Encoder-Decoder (HRED) model, originally proposed in (Sordoni et al., 2015a) for query suggestion and applied to response
2810840719	Neural Approaches to Conversational AI	2592647456	e need to locate an answer span A= (a start;a end) in P. In spite of the variety of model structures and attention types (Chen et al., 2016a; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017c; Wang et al., 2017b), a typical neural MRC model performs reading comprehension in three steps, as outlined in Fig. 1.4: (1) encoding the symbolic representation of the questions and passages into a set of vectors in a
2810840719	Neural Approaches to Conversational AI	2145755360	e. Some new datasets are developed to foster the research on common sense reasoning, such as Winograd Schema Challenge (WSC) (Morgenstern and Ortiz, 2015) and Choice Of Plausible Alternatives (COPA) (Roemmele et al., 2011). Model interpretability: In some cases, a dialogue agent is required not only to give a recommendation or an answer, but also provide explanations. This is very important in e.g., business scenarios
2810840719	Neural Approaches to Conversational AI	1518951372,1993378086	e often short (e.g., a few word utterance such as “really?”), in which case conversational models critically need longer contexts to produce plausible responses. This limitation motivated the work of Sordoni et al. (2015b), which proposed an RNN-based approach to conversational response generation (similar to Fig. 2.2) that exploited longer context. Together with the contemporaneous works (Shang et al., 2015; Vinyals
2810840719	Neural Approaches to Conversational AI	1996957559	some of the earlier examples (Cole, 1999; Larsson and Traum, 2000; Rich et al., 2001; Allen et al., 2001; Bos et al., 2003; Bohus and Rudnicky, 2009), as well as excellent surveys like McTear (2002), Paek and Pieraccini (2008), and Young et al. (2013) for more information. Here, we review a small subset of traditional approaches from the decision-theoretic view we take in this paper. Levin et al. (2000) viewed conversation
2810840719	Neural Approaches to Conversational AI	1958706068,2311783643,2410983263	earned from a real conversational corpus. Agenda-Based Simulation. As an example, we describe a popular hidden agenda-based user simulator developed by Schatzmann and Young (2009), as instantiated in Li et al. (2016d) and Ultes et al. (2017c). Each dialogue simulation starts with a randomly generated user goal that is unknown to the dialogue manager. In general the user goal consists of two parts: the inform-slo
2810840719	Neural Approaches to Conversational AI	2521709538	easoning. Multi-step reasoning models are pioneered by Hill et al. (2015); Dhingra et al. (2016); Sordoni et al. (2016); Kumar et al. (2016), who used a pre-determined ﬁxed number of reasoning steps. Shen et al. (2017b,c) showed that multi-step reasoning outperforms single-step ones and dynamic multi-step reasoning further outperforms the ﬁxed multi-step ones on two distinct MRC datasets (SQuAD and MS MARCO). But
2810840719	Neural Approaches to Conversational AI	2177156214	ecision. 4.5.4 Multi-Domain Dialogues A multi-domain dialogue can converse with a user to have a conversation that may involve more than one domains (Komatani et al., 2006; Hakkani-T¨ur et al., 2012; Wang et al., 2014). Table 4.3 shows an example,where the dialogue covers both the hotel and restaurant domains, in addition to a special meta domain. Different from composite tasks, subdialogues corresponding to differ
2810840719	Neural Approaches to Conversational AI	2109910161	ected agent, chooses primitive actions to complete the subtask. Such hierarchical decision making processes can be cast in the mathematical framework of options over Markov Decision Processes (MDPs) (Sutton et al., 1999b), where options generalize primitive actions to higher-level actions. This is an extension to the traditional MDP setting where an agent can only choose a primitive action at each time step, with op
2810840719	Neural Approaches to Conversational AI	2159640018	ed to mimic human conversations by training DNN-based response generation models on large amounts of human-human conversational data (Ritter et al., 2011; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015). Only recently have researchers begun to explore how to ground the chitchat in world knowledge (Ghazvininejad et al., 2018) and images (Mostafazadeh et al., 2017) so as to make the conversation more
2810840719	Neural Approaches to Conversational AI	1598178035	eds and evaluation measures for dialogue state tracking. Starting from Williams et al. (2013), it has successfully attracted many research teams to focus on a wide range of technical problems in DST (Williams et al., 2014; Henderson et al., 2014b,a; Kim et al., 2016a,b; Hori et al., 2017). Corpora used by DSTC over the years have covered human-computer and human-human conversations, different domains such as restauran
2810840719	Neural Approaches to Conversational AI	2525778437	ely as a single ﬁxed-size vector. While attention models and variants (Bahdanau et al., 2015; Luong et al., 2015, etc.) have contributed to signiﬁcant progress in the state-of-the-art in translation (Wu et al., 2016) and are very commonly used in neural machine translation nowadays, attention models have been somewhat less effective in E2E dialogue modeling. This can probably be explained by the fact that attenti
2810840719	Neural Approaches to Conversational AI	2559038528,2579486552,2581637843,2592808142,2604688337	engagement. To address these limitations, some researchers have explored reinforcement learning (RL) for E2E systems (Li et al., 2016c) which could be augmented with human-in-the-loop architectures (Li et al., 2017a,b). Unlike RL 60 Figure 5.4: Deep reinforcement learning for response generation, pitching the system to optimize against a user simulator (both systems are E2E generation systems.) Figure credit: L
2810840719	Neural Approaches to Conversational AI	2121863487	ent. In other words, while SL learns from previous experiences provided by a knowledgeable external supervisor, RL learns by experiencing on its own. RL differs from SL in several important respects (Sutton and Barto, 2018; Mitchell, 1997) Exploration-exploitation tradeoff. In RL, the agent needs to collect reward signals from the environment. This raises the question of which experimentation strategy results in more
2810840719	Neural Approaches to Conversational AI	10957333	ention mechanism. An instance of this model is CopyNet (Gu et al., 2016), which was shown to signiﬁcantly improve over RNNs thanks to its ability to repeat proper nouns and other words of the input. 4Ritter et al. (2011) also found that alignment produced by an off-the-shelf word aligner (Och and Ney, 2003) produced alignments of poor quality, and an extension of their work with attention models (Ritter 2018, pc) yie
2810840719	Neural Approaches to Conversational AI	2073384958	er chapters. For a comprehensive survey, interested readers are referred to excellent textbooks and reviews, such as Sutton and Barto (2018); Kaelbling et al. (1996); Bertsekas and Tsitsiklis (1996); Szepesvari (2010); Wiering and van Otterlo (2012); Li (2019).´ 2.3.1 Foundations Reinforcement learning is a learning paradigm where an intelligent agent learns to make optimal decisions by interacting with an initial
2810840719	Neural Approaches to Conversational AI	2153579005	er. It extracts information from Qand Pat the word level and normalizes for lexical variants. It typically maps each word to a vector space using a pre-trained word embedding model, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), such that semantically similar words are mapped to the vectors that are close to each other in the neural space (also see Sec. 2.2.1). Word embedding can be enhanc
2810840719	Neural Approaches to Conversational AI	1535960497	er utterance is like an observation, and the system utterance is an action selected by the dialogue agent. The dialogue acts theory gives a formal foundation for this intuition (Core and Allen, 1997; Traum, 1999). In this framework, the utterances of a user or agent are considered actions that can change the (mental) state of both the user and the system, thus the state of the conversation. These actions can
2810840719	Neural Approaches to Conversational AI	1956174786	eration results. As shown in Fig. 4.8, a basic SC-LSTM cell has two parts: a typical LSTM cell (upper part in the ﬁgure) and a sentence planning cell (lower part) for semantic control. 3Some authors (Stone et al., 2003; Koller and Stone, 2007) have taken a similar, decision-theoretic point of view for NLG. Their formulate NLG as a planning problem, as opposed to data-driven or corpus-based methods being discussed h
2810840719	Neural Approaches to Conversational AI	2100677568	erence” r Q(s;a;): (2.4) The above update is known as Q-learning (Watkins, 1989), which applies a small change to , controlled by the step-size parameter and computed from the temporal difference (Sutton, 1988). While popular, in practice, Q-learning can be quite unstable and requires many samples before reaching a good approximation of Q. Two modiﬁcations are often helpful. The ﬁrst is experience replay (L
2810840719	Neural Approaches to Conversational AI	2133564696	ersation models are inspired by statistical machine translation (SMT) (Koehn et al., 2003; Och and Ney, 2004), including neural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Bahdanau et al., 2015). The casting of the conversational response generation task (i.e., predict a response T ibased on the previous dialogue turn T i 1) as an SMT problem is a relatively natural one, as one can treat tur
2810840719	Neural Approaches to Conversational AI	2559038528,2579486552,2581637843,2592808142,2604688337	erstanding and Dialogue State Tracking NLU and DST are two closely related components essential to a dialogue system. They can have a signiﬁcant impact on the overall system’s performance (see, e.g., Li et al. (2017e)). This section reviews some of the classic and state-of-the-art approaches. 4.3.1 Natural Language Understanding The NLU module takes user utterance as input, and performs three tasks: domain detec
2810840719	Neural Approaches to Conversational AI	2559038528,2579486552,2581637843,2592808142,2604688337	ervised training. This inspire another line of work that uses reinforcement learning to optimize end-to-end dialogue systems (Zhao and Esk´enazi, 2016; Williams and Zweig, 2016; Dhingra et al., 2017; Li et al., 2017d; Braunschweiler and Papangelis, 2018; Strub et al., 2017; Liu and Lane, 2017; Liu et al., 2018a). Zhao and Eskenazi (2016) proposed a model that takes user utterance as input and outputs a semantic´
2810840719	Neural Approaches to Conversational AI	2141848945	erﬁtting and “gaming of the metric” (Albrecht and Hwa, 2007),14 which might explain why previously proposed machine-learned evaluation metrics (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Lita et al., 2005; Albrecht and Hwa, 2007; Gim´enez and Marquez, 2008; Pado et al., 2009; Stanojevi` ´c and Sima’an, 2014, etc.) are not commonly used in ofﬁcial machine translation benchmarks. The problem of “gameabl
2810840719	Neural Approaches to Conversational AI	2167665328	estion can then be obtained by ﬁnding a set of paths in the KB that match the query and retrieving the end nodes of these paths (Richardson et al., 1998; Berant et al., 2013; Yao and Van Durme, 2014; Bao et al., 2014; Yih et al., 2015b). We take the example used in Yih et al. (2015a) to illustrate the QA process. Fig. 3.1 (right) shows the logical form in -calculus and its equivalent graph representation, known
2810840719	Neural Approaches to Conversational AI	1902237438	u et al., 2015) helps prevent repetition errors in machine translation as that task is relatively one-to-one,8 the attention models originally designed for machine translation (Bahdanau et al., 2015; Luong et al., 2015) often do not help reduce word repetitions in dialogue. In light of the above limitations, Shao et al. (2017) proposed a new model that adds self-attention to the decoder, aiming at improving the gene
2810840719	Neural Approaches to Conversational AI	2053463056	et al., 2017d). Among them, the ﬁrst two tasks are often framed as a classiﬁcation problem, which infers the domain or intent (from a predeﬁned set of candidates) based on the current user utterance (Schapire and Singer, 2000; Yaman et al., 2008; Sarikaya et al., 2014). Neural approaches to multi-class classiﬁcation have been used in the recent literature and outperformed traditional statistical methods. Ravuri and Stolck
2810840719	Neural Approaches to Conversational AI	2473965551	everal large cities) using search words extracted from the conversation context. While the dialogue encoder of this model is a standard LSTM, the facts encoder is an instance of the Memory Network of Chen et al. (2016b), which uses an associative memory for modeling the facts relevant to a particular problem, which in this case is a restaurant mentioned in a conversation. There are two main beneﬁts to this approac
2810840719	Neural Approaches to Conversational AI	2132997613	or expert-deﬁned rules can be used to set parameters in the stack-update process. Model-based Simulation. Another approach to building user simulators is entirely based on data (Eckert et al., 1997; Levin et al., 2000; Chandramohan et al., 2011). Here, we describe a recent example due to El Asri et al. (2016). Similar to the agenda-based approach, the simulator also starts an episode with a randomly generated user
2810840719	Neural Approaches to Conversational AI	2121863487	off between exploitation (choosing good actions to maximize reward) and exploration (choosing novel actions to discover potentially better alternatives), leading to the need for efﬁcient exploration (Sutton and Barto, 2018). In the context of dialogue policy learning, the implication is that the policy learner actively tries new ways to converse with a user, in the hope to discover a better policy in the long run; see,
2810840719	Neural Approaches to Conversational AI	2251079237	f semantic parsing for KB-QA. (Left) A subgraph of Freebase related to the TV show Family Guy. (Right) A question, its logical form in -calculus and query graph, and the answer. Figures adapted from Yih et al. (2015a). (KG) due to its graphical representation, i.e., the entities are nodes and the relations the directed edges that link the nodes. Fig. 3.1 (Left) shows a small subgraph of Freebase related to the T
2810840719	Neural Approaches to Conversational AI	1811254738	g issues. 5.2.1 Response blandness Utterances generated by neural response generation systems are often bland and deﬂective. While this problem has been noted in other tasks such as image captioning (Mao et al., 2015), the problem is particularly acute in E2E response generation, as commonly used models such as seq2seq tend to generate uninformative responses such as “I don’t know” or “I’m OK”. Li et al. (2016a) s
2810840719	Neural Approaches to Conversational AI	2108738385	i has a Gaussian posterior distribution, N( i;˙2 i ) and ˙ i = log(1 + exp(ˆ i)). The posterior 45 information leads to a natural exploration strategy, inspired by Thompson Sampling (Thompson, 1933; Chapelle and Li, 2012; Russo et al., 2018). When selecting actions, the agent simply draws a random weight w~ ˘q, and then selects the action with the highest value output by the network. Experiments show that BBQ explore
2810840719	Neural Approaches to Conversational AI	2109038907	gue state, and converses with an RL dialogue system. Substantial research has gone into building realistic user simulators (Schatzmann et al., 2005a; Georgila et al., 2006; Pietquin and Dutoit, 2006; Pietquin and Hastie, 2013). There are many differ38 { request_slots: { ticket: UNK theater: UNK start_time: UNK }, inform_slots: { number_of_people: 3 date: tomorrow movie_name: batman vs. superman } } Figure 4.2: An example u
2810840719	Neural Approaches to Conversational AI	2354968889	gue: This is useful for both chatbots and QA bots. For example, XiaoIce incorporates an EQ module so as to deliver a more understandable response or recommendation (as in 3.1 of (Shum et al., 2018)). Fung et al. (2016) embedded an empathy module into a dialogue agent to recognize users’ emotion using multimodality, and generate emotion-aware responses. Scalable training for task-oriented dialogues: It is important
2810840719	Neural Approaches to Conversational AI	2044924490	gues (Horvitz, 1999) and negotiations (Barlier et al., 2015; Lewis et al., 2017). More generally, there may be multiple parties involved in a conversation, where turn taking becomes more challenging (Bohus and Horvitz, 2009, 2011). In such scenarios, it is helpful to take a game-theoretic view, more general than the MDP view as in single-agent decision making. Weaker Learning Signals. In the literature, a dialogue syste
2810840719	Neural Approaches to Conversational AI	2123442489	h the rise of data-driven and statistical approaches, these components have remained and have been adapted as a rich source of engineered features to be fed into a variety of machine learning models (Manning et al., 2014). Neural approaches do not reply on any human-deﬁned symbolic representations but learn a taskspeciﬁc neural space where task-speciﬁc knowledge is implicitly represented as semantic concepts using low
2810840719	Neural Approaches to Conversational AI	2521709538	hes are developed to represent queries and KB using continuous semantic vectors so that the inference can be performed at the semantic level in a compact neural space. We use Implicit ReasoNet (IRN) (Shen et al., 2017a) and M-Walk (Shen et al., 2018) as examples to illustrate the implementation details. We also describe the typical architecture of multi-turn, conversational KB-QA agents, using a movie-on-demand ag
2810840719	Neural Approaches to Conversational AI	1771410628	hods to compute the gradient r Jmore effectively than Eqn. 2.8. Interested readers can refer to a few related works and the references therein for further details (Kakade, 2001; Peters et al., 2005; Schulman et al., 2015a,b; Mnih et al., 2016; Gu et al., 2017; Dai et al., 2018a; Liu et al., 2018b). 2.3.3 Exploration So far we have described basic algorithms for updating either the value function or the policy, when t
2810840719	Neural Approaches to Conversational AI	2473965551	Hori et al. (2015) treated conversation history as a long sequence of words, with alternating roles (words from user, vs. words from system), and proposed a variant to LSTM with roledependent layers. Chen et al. (2016b) built on memory networks that learn which part of contextual information should be attended to, when making slot-tagging predictions. Both models achieved higher accuracy than context-free models.
2810840719	Neural Approaches to Conversational AI	2094728533	iewing TREC QA open benchmarks. 3.1 Knowledge Base Organizing the world’s facts and storing them in a structured database, large scale Knowledge Bases (KB) like DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago (Suchanek et al., 2007) have become important resources for supporting open-domain QA. A typical KB consists of a collection of subject-predicate-object triples (s;r;t) where s;t2Eare entiti
2810840719	Neural Approaches to Conversational AI	2560678344	ing, where desired system responses are ﬁrst collected and then used to train multiple components of a dialogue system in order to maximize prediction accuracy (Bordes et al., 2017; Wen et al., 2017; Yang et al., 2017b; Eric et al., 2017). Wen et al. (2017) introduced a modular neural dialogue system, where most modules are represented by a neural network. However, their approach relies on non-differentiable knowl
2810840719	Neural Approaches to Conversational AI	1902237438	ing the next target word, thus moving away from a framework that represents the entire source sequence merely as a single ﬁxed-size vector. While attention models and variants (Bahdanau et al., 2015; Luong et al., 2015, etc.) have contributed to signiﬁcant progress in the state-of-the-art in translation (Wu et al., 2016) and are very commonly used in neural machine translation nowadays, attention models have been s
2810840719	Neural Approaches to Conversational AI	2110310640	from instability (Salimans et al., 2016) due to the nature of GANs’ minimax formulation. This fragility is potentially troublesome as the outcome of an automatic evaluation should ideally be stable (Cer et al., 2010) and reproducible over time, e.g., to track progress of E2E dialogue research over the years. All of this suggests that automatic evaluation for E2E dialogue is far from a solved problem. 5.7 Open Ben
2810840719	Neural Approaches to Conversational AI	2626778328	ion, but its encoding of the entire source sequence into a ﬁxed-size vector has certain limitations, especially when dealing with long source sequences. Attention-based models (Bahdanau et al., 2015; Vaswani et al., 2017) alleviate this limitation by allowing the model to search and condition on parts of a source sentence that are relevant to predicting the next target word, thus moving away from a framework that repr
2810840719	Neural Approaches to Conversational AI	2121863487	ion reviews reinforcement learning to facilitate discussions in later chapters. For a comprehensive treatment of this topic, interested readers are referred to existing textbooks and reviews, such as Sutton and Barto (2018); Kaelbling et al. (1996); Bertsekas and Tsitsiklis (1996); Szepesvari´ (2010); Wiering and van Otterlo (2012); Li (2018). 4Similar to DSSM, seq2seq can be applied to a variety of generation tasks dep
2810840719	Neural Approaches to Conversational AI	2251235149	ipeline approaches (c.f., Sec. 4.6). Dialogue State Tracking Challenge (DSTC) is a series of challenges that provide common testbeds and evaluation measures for dialogue state tracking. Starting from Williams et al. (2013), it has successfully attracted many research teams to focus on a wide range of technical problems in DST (Williams et al., 2014; Henderson et al., 2014b,a; Kim et al., 2016a,b; Hori et al., 2017). Co
2810840719	Neural Approaches to Conversational AI	2583679610	ively train the Generator and Discriminator as in GANs, but most trainable metrics in the literature do not exploit this iterative process. Adversarial setups proposed for dialogue and related tasks (Kannan and Vinyals, 2016; Li et al., 2017c; Holtzman et al., 2018) offer solutions to this problem, but it is also well-known that such setups suffer from instability (Salimans et al., 2016) due to the nature of GANs’ minima
2810840719	Neural Approaches to Conversational AI	2156974606	k (Hausknecht and Stone, 2015; Li et al., 2015), etc. Furthermore, it is possible to represent the Q-function in a non-parametric way, using decision trees (Ernst et al., 2005) or Gaussian processes (Engel et al., 2005), which is outside of the scope of this introductory section. To learn the Q-function, we modify the parameter using the following update rule, after observing a state transition (s;a;r;s0):  +  r
2810840719	Neural Approaches to Conversational AI	2251079237	used in the KB. As in the example of Fig. 3.1, we need to measure how likely the predicate used in the question matches that in Freebase, such as “Who ﬁrst voiced Meg on Family Guy?” vs. cast-actor. Yih et al. (2015a) proposed to use a learned DSSM 1CVT is not a real-world entity, but is used to collect multiple ﬁelds of an event or a special relationship. 21 described in Sec. 2.2.2, which is conceptually an emb
2810840719	Neural Approaches to Conversational AI	1756422141	a KB query (Obama, citizenship, ?). Figure adapted from Shen et al. (2018). Table 3.1: A sample of relational paths learned by PRA. For each relation, its top-2 PRA paths are presented, adapted from Lao et al. (2011). ID PRA Path # Comment athlete-plays-for-team 1 (athlete-plays-in-league, league-players, athlete-plays-for-team) # teams with many players in the athlete’s league 2 (athlete-plays-in-league, league-
2810840719	Neural Approaches to Conversational AI	2134466368	it keeps track of the dialogue state, and converses with an RL dialogue system. Substantial research has gone into building realistic user simulators (Schatzmann et al., 2005a; Georgila et al., 2006; Pietquin and Dutoit, 2006; Pietquin and Hastie, 2013). There are many differ38 { request_slots: { ticket: UNK theater: UNK start_time: UNK }, inform_slots: { number_of_people: 3 date: tomorrow movie_name: batman vs. superman
2810840719	Neural Approaches to Conversational AI	1533230146	on L 1 distance as o= argmin e2Ejo x ej 1, where x eis the embedding vector of entity e. 3The use of vectors rather than matrices for relation representations is inspired by the bilinear-diag model (Yang et al., 2015), which restricts the relation representations to the class of diagonal matrices. 24 Shared Memory The shared memory M is differentiable, and consists of a list of vectors fm ig 1jMjthat are randoml
2810840719	Neural Approaches to Conversational AI	2059157630	l. (2018)); (3) a beliefs summary module to summarize the state into a vector; and (4) a dialogue policy which selects the next action based on the current state. The policy can be either programmed (Wu et al., 2015) or trained on dialogues (Wen et al., 2017; Dhingra et al., 2017). Wu et al. (2015) presented an Entropy Minimization Dialogue Management (EMDM) strategy. The agent always asks for the value of the at
2810840719	Neural Approaches to Conversational AI	2625113742	l. (2018) described such a dialogue self-play procedure, which can generate conversations between a simulated user and the system agent. Promising results have been observed in negotiation dialogues (Lewis et al., 2017) and task-oriented dialogues (Liu and Lane, 2017; Shah et al., 2018; Wei et al., 2018). It provides an interesting solution to avoid the evaluation cost of involving human users as well as overﬁtting
2810840719	Neural Approaches to Conversational AI	1847211030	l., 2018), dialogue histories can often be long and there is sometimes a need to exploit longer-term context. Hierarchical models were designed to address this limitation by capturing longer context (Yao et al., 2015; Serban et al., 2016, 2017; Xing et al., 2018). One popular approach is the Hierarchical Recurrent Encoder-Decoder (HRED) model, originally proposed in (Sordoni et al., 2015a) for query suggestion an
2810840719	Neural Approaches to Conversational AI	2560678344	l state s as a query for lookup. It remains challenging to recover the symbolic representations of queries and paths (or ﬁrst-order logical rules) from the neural controller. See (Shen et al., 2017a; Yang et al., 2017a) for some interesting preliminary results of interpretation of neural methods. 3.4.3 Reinforcement Learning based Methods DeepPath (Xiong et al., 2017), MINERVA (Das et al., 2017b) and M-Walk (Shen
2810840719	Neural Approaches to Conversational AI	1958706068,2311783643,2410983263	lation (Sutskever et al., 2014; Cho et al., 2014b), an image-text pairs in image captioning (Vinyals et al., 2015b) (where f 1 is a CNN), and message-response pairs in dialogue (Vinyals and Le, 2015; Li et al., 2016a). 16 Figure 2.5: Interaction between an RL agent and the external environment. 2.3.1 Foundations Reinforcement learning (RL) is a learning paradigm where an intelligent agent learns to make optimal
2810840719	Neural Approaches to Conversational AI	2141559645	le popular, in practice, Q-learning can be unstable and requires many samples before reaching a good approximation of Q. Two modiﬁcations are often helpful in practice. The ﬁrst is experience replay (Lin, 1992), popularized by Mnih et al. (2015). Instead of using an observed transition to update just once using Eqn. 2.3, one may store it in a replay buffer, and periodically sample transitions from it to pe
2810840719	Neural Approaches to Conversational AI	1211946649	learner to interact with users to improve its policy; the batch approach assumes a ﬁxed set of transitions, and optimizes the policy based on the data only, without interacting with users (see, e.g., Li et al. (2009); Pietquin et al. (2011)). In this chapter, we discuss the online setting which often has batch learning as an internal step. Many covered topics can be useful in the batch setting. Here, we use the D
2810840719	Neural Approaches to Conversational AI	2755402024	learning to optimize end-to-end dialogue systems (Zhao and Esk´enazi, 2016; Williams and Zweig, 2016; Dhingra et al., 2017; Li et al., 2017d; Braunschweiler and Papangelis, 2018; Strub et al., 2017; Liu and Lane, 2017; Liu et al., 2018a). Zhao and Eskenazi (2016) proposed a model that takes user utterance as input and outputs a semantic´ system action. Their model is a recurrent variant of DQN based on LSTM, which
2810840719	Neural Approaches to Conversational AI	2109190044	logical form) and then translated to a KB query. The answers to the question can then be obtained by ﬁnding a set of paths in the KB that match the query and retrieving the end nodes of these paths (Richardson et al., 1998; Berant et al., 2013; Yao and Van Durme, 2014; Bao et al., 2014; Yih et al., 2015b). We take the example used in Yih et al. (2015a) to illustrate the QA process. Fig. 3.1 (right) shows the logical fo
2810840719	Neural Approaches to Conversational AI	2059216172	have looked at dialogues that involve two parties—the user and the dialogue agent, and the latter is to assist the former. In general, the task can be more complex such as mixed-initiative dialogues (Horvitz, 1999) and negotiations (Barlier et al., 2015; Lewis et al., 2017). More generally, there may be multiple parties involved in a conversation, where turn taking becomes more challenging (Bohus and Horvitz, 2
2810840719	Neural Approaches to Conversational AI	2146574666	luation, and this would presumably require model parameters to be publicly available. 63 for example in the frequent cases where automatic evaluation metrics are used directly as training objectives (Och, 2003; Ranzato et al., 2015) as unintended “gaming” may occur unbeknownst to the system developer. If a generation system is optimized directly on a trainable metric, then the system and the metric become
2810840719	Neural Approaches to Conversational AI	2123301721	ly borrowed those metrics from machine translation and summarization, using string and n-gram matching metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Proposed more recently, METEOR (Banerjee and Lavie, 2005) aims to improve BLEU by identifying synonyms and paraphrases between the system output and the human reference, and has also been used to evaluate dialogue. deltaBLEU (Galley et al., 2015) is an exte
2810840719	Neural Approaches to Conversational AI	2108806737	m data by statistical learning algorithms like conditional random ﬁelds (Henderson, 2015). More recently, neural approaches have started to gain popularity, with applications of deep neural networks (Henderson et al., 2013) and recurrent networks (Mrkˇsi ´c et al., 2015) as some of the early examples. A more recent DST model is the Neural Belief Tracker proposed by Mrkˇsi c et al. (2017), shown in´ Fig. 4.5. The model t
2810840719	Neural Approaches to Conversational AI	2159640018	such as machine translation and summarization. E2E dialogue is no different. While it is common to evaluate response generation systems using human raters (Ritter et al., 2011; Sordoni et al., 2015b; Shang et al., 2015, etc.), this type of evaluation is often expensive and researchers often have to resort to automatic metrics for quantifying day-to-day progress and for performing automatic system optimization. E2E
2810840719	Neural Approaches to Conversational AI	1958706068,2311783643,2410983263	the major limitation of user simulation-based dialogue policy optimization. Some user simulators are publicly available for research purposes. Other than the aforementioned agenda-based simulators by Li et al. (2016d); Ultes et al. (2017c), a large corpus with an evaluation environment, called AirDialogue (in the ﬂight booking domain), was recently made available (Wei et al., 2018). At the IEEE workshop on Spoke
2810840719	Neural Approaches to Conversational AI	2119717200	mate the gradient r Jfrom sampled trajectories, one can do stochastic gradient ascent6 to maximize J:  + r J(); (2.6) where is again a stepsize parameter. One such algorithm, known as REINFORCE (Williams, 1992), estimates the gradient as follows. Let ˝be a length-Htrajectory generated by ˇ(;); that is, a t ˘ˇ(s t;) for every t. Then, a stochastic gradient based on this single trajectory is given by r J(
2810840719	Neural Approaches to Conversational AI	1533230146	Methods To address the paraphrasing problem, embedding-based methods map entities and relations in a KB to continuous vectors in a neural space; see, e.g., Bordes et al. (2013); Socher et al. (2013); Yang et al. (2015); Yih et al. (2015b). This space can be viewed as a hidden semantic space where various expressions with the same semantic meaning map to the same continuous vector. Most KB embedding models are devel
2810840719	Neural Approaches to Conversational AI	2559038528,2579486552,2581637843,2592808142,2604688337	on metrics used to evaluate a model’s prediction quality. NLU is a pre-processing step for later modules in the dialogue system, whose quality has a signiﬁcant impact on the system’s overall quality (Li et al., 2017d). Among them, the ﬁrst two tasks are often framed as a classiﬁcation problem, which infers the domain or intent (from a predeﬁned set of candidates) based on the current user utterance (Schapire and
2810840719	Neural Approaches to Conversational AI	2110310640	or millions of parameters, compared to a relatively parameterless metric like BLEU that is known to be fairly robust to such exploitation and was shown to be the best metric for direct optimization (Cer et al., 2010) among other established string-based metrics. To prevent machine-learned metrics from being gamed, one would need to iteratively train the Generator and Discriminator as in GANs, but most trainable m
2810840719	Neural Approaches to Conversational AI	2755402024	mimic an expertprovided policy. A popular option is to use supervised learning to directly learn the expert’s action in a state; see Su et al. (2016b); Dhingra et al. (2017); Williams et al. (2017); Liu and Lane (2017) for a few recent examples. Li et al. (2014) turned imitation learning into an induced reinforcement learning problem, and then applied an off-the-shelf RL algorithm to learn the expert’s policy. Fina
2810840719	Neural Approaches to Conversational AI	1895577753	morning” in response to “good morning”) or uses rare words such as proper nouns, which the model would have difﬁculty generating with a standard RNN. Originally inspired by the Pointer Network model (Vinyals et al., 2015a)—which produces an output sequence consisting of elements from the input sequence—these models hypothesize target words that are either drawn from a ﬁxed-size vocabulary (akin to a seq2seq model) or
2810840719	Neural Approaches to Conversational AI	2109636054	n is to enrich the dialogue context. Rather than text-only or speech-only ones, our daily dialogues are often multimodal, and involve both verbal and nonverbal inputs like vision (Bohus et al., 2014; DeVault et al., 2014; de Vries et al., 2017; Zhang et al., 2018a). Challenges such as how to combine information from multiple modalities to make decisions arise naturally. So far, we have looked at dialogues that involv
2810840719	Neural Approaches to Conversational AI	2412715517	n expert policies that produce responses for supervised training. This inspire another line of work that uses reinforcement learning to optimize end-to-end dialogue systems (Zhao and Esk´enazi, 2016; Williams and Zweig, 2016; Dhingra et al., 2017; Li et al., 2017d; Braunschweiler and Papangelis, 2018; Strub et al., 2017; Liu and Lane, 2017; Liu et al., 2018a). Zhao and Eskenazi (2016) proposed a model that takes user utt
2810840719	Neural Approaches to Conversational AI	1753482797	n Models Most of the earliest end-to-end (E2E) conversation models are inspired by statistical machine translation (SMT) (Koehn et al., 2003; Och and Ney, 2004), including neural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Bahdanau et al., 2015). The casting of the conversational response generation task (i.e., predict a response T ibased on the previous dialogue turn T i 1) as an SMT problem is a r
2810840719	Neural Approaches to Conversational AI	2560678344	n Sec. 3.3, to collapse and cluster PRA paths according to their relation embeddings. 3.4.2 Neural Methods Implicit ReasoNet (IRN) (Shen et al., 2016, 2017a) and Neural Logic Programming (Neural LP) (Yang et al., 2017a) are two of the recently proposed methods that perform multi-step KBR in a neural space and achieve state-of-the-art results on popular benchmarks. The overall architecture of these methods is shown
2810840719	Neural Approaches to Conversational AI	2121863487	nal environment. 2.3.1 Foundations Reinforcement learning (RL) is a learning paradigm where an intelligent agent learns to make optimal decisions by interacting with an initially unknown environment (Sutton and Barto, 2018). Compared to supervised learning, a distinctive challenge in RL is to learn without a teacher (that is, without supervisory labels). As we will see, this will lead to algorithmic considerations that
2810840719	Neural Approaches to Conversational AI	2153653739	narios going beyond chitchat, e.g., recommendation. 5.1 End-to-End Conversation Models Most of the earliest end-to-end (E2E) conversation models are inspired by statistical machine translation (SMT) (Koehn et al., 2003; Och and Ney, 2004), including neural machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Bahdanau et al., 2015). The casting of the conversational response generation task (i.e.,
2810840719	Neural Approaches to Conversational AI	2738015883	nding the intended meaning(s) of the given, and it is suggested that this amounts to the process of inferring the appropriate context(s) in which to interpret the given” (Bell, 1999). The analysis by Jia and Liang (2017); Chen et al. (2016a) revealed that state of the art neural MRC models, e.g., developed on SQuAD, mostly excel at matching questions to local context via lexical matching and paragraphing, but struggl
2810840719	Neural Approaches to Conversational AI	2583186419	this is that neural approaches provide 9 Figure 1.3: Symbolic and Neural Computation. a consistent representation for many modalities, capturing linguistic and non-linguistic (e.g., image and video (Mostafazadeh et al., 2017)) features in the same modeling framework. 10 Chapter 2 Machine Learning Background This chapter presents a brief review of deep learning and reinforcement learning that are most relevant to conversat
2810840719	Neural Approaches to Conversational AI	2162059449	ney buy lucasﬁlms?”, a set of candidate documents are retrieved from Web Index via a fast, primary ranker. Then in the Document Ranking module, a sophisticated document ranker based on boosted trees (Wu et al., 2010) is used to assign relevance scores for these documents. The top-ranked relevant documents are presented in a SERP, with their captions generated from a Query-Focused Captioning module, as shown in Fi
2810840719	Neural Approaches to Conversational AI	1895577753	ng on the deﬁnition of (x;y). For example, (x;y) is a sentence pair of different languages in machine translation (Sutskever et al., 2014; Cho et al., 2014b), an image-text pairs in image captioning (Vinyals et al., 2015b) (where f 1 is a CNN), and message-response pairs in dialogue (Vinyals and Le, 2015; Li et al., 2016a). 16 Figure 2.5: Interaction between an RL agent and the external environment. 2.3.1 Foundations
2810840719	Neural Approaches to Conversational AI	2128892113	ng new sentences word-by-word), a more conservative solution to mitigating blandness is to replace generation-based models with retrieval-based models for response generation (Jafarpour et al., 2010; Lu and Li, 2014; Inaba and Takahashi, 2016; Al-Rfou et al., 2016; Yan et al., 2016), in which the pool of possible responses is constructed in advance (e.g., pre-existing human responses). These approaches come at t
2810840719	Neural Approaches to Conversational AI	2473965551	ning(s) of the given, and it is suggested that this amounts to the process of inferring the appropriate context(s) in which to interpret the given” (Bell, 1999). The analysis by Jia and Liang (2017); Chen et al. (2016a) revealed that state of the art neural MRC models, e.g., developed on SQuAD, mostly excel at matching questions to local context via lexical matching and paragraphing, but struggle with questions th
2810840719	Neural Approaches to Conversational AI	10957333	o deﬁne standard training and test sets, but it is relatively small. Some of the most notable E2E and chitchat datasets include: Twitter: Used since the ﬁrst data-driven response generation systems (Ritter et al., 2011), Twitter data offers a wealth of conversational data that is practically unbounded, as Twitter produces new data each day that is more than most system developers can handle.11 While the data itself
2810840719	Neural Approaches to Conversational AI	59659882	o test-use a baseline dialogue system, so that the two can be compared against various metrics. Many published studies involving human subjects are of the ﬁrst type (Walker, 2000; Singh et al., 2002; Ai et al., 2007; Rieser and Lemon, 2011; Gasiˇ c et al., 2013; Wen et al., 2015; Young et al.,´ 2016; Peng et al., 2017; Lipton et al., 2018). While this approach has beneﬁts over simulation-based evaluation, it is
2810840719	Neural Approaches to Conversational AI	2403702038	o training of the components is done separately in a supervised manner. This challenge is addressed by Dhingra et al. (2017) who proposed “soft” knowledge-base lookups; see Sec. 3.5 for more details. Bordes et al. (2017) treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses. They show memory networks and supervised embedding models outperform standard baseli
2810840719	Neural Approaches to Conversational AI	2592647456	oals. Tang et al. (2018) considered the problem of discovering subgoals from dialogue demonstrations. Inspired by a sequence segmentation approach that is successfully applied to machine translation (Wang et al., 2017a), the authors developed the Subgoal Discovery Network (SDN), which learns to identify “bottleneck” states in successful dialogues. It is shown that the hierarchical DQN optimized with subgoals disco
2810840719	Neural Approaches to Conversational AI	2251079237	obtained by ﬁnding a set of paths in the KB that match the query and retrieving the end nodes of these paths (Richardson et al., 1998; Berant et al., 2013; Yao and Van Durme, 2014; Bao et al., 2014; Yih et al., 2015b). We take the example used in Yih et al. (2015a) to illustrate the QA process. Fig. 3.1 (Right) shows the logical form in -calculus and its equivalent graph representation, known as query graph, of
2810840719	Neural Approaches to Conversational AI	2417786368	od (Strehl et al., 2009; Jaksch et al., 2010; Osband and Roy, 2017; Dann et al., 2017), exploration when deep models are used is an active research topic (Bellemare et al., 2016; Osband et al., 2016; Houthooft et al., 2016; Jiang et al., 2017). Here, we describe a general-purpose exploration strategy that is particularly suited for dialogue systems that may change over time. After a task-oriented dialogue system is dep
2810840719	Neural Approaches to Conversational AI	2098217544	ogue managers. A comprehensive survey is out of the scope of the this chapter. Interested readers are referred to some of the earlier examples (Cole, 1999; Larsson and Traum, 2000; Rich et al., 2001; Allen et al., 2001; Bos et al., 2003; Bohus and Rudnicky, 2009), as well as excellent surveys like McTear (2002), Paek and Pieraccini (2008), and Young et al. (2013) for more information. Here, we review a small subset
2810840719	Neural Approaches to Conversational AI	2123395566	on-based (i.e., generating new sentences word-by-word), a more conservative solution to mitigating blandness is to replace generation-based models with retrieval-based models for response generation (Jafarpour et al., 2010; Lu and Li, 2014; Inaba and Takahashi, 2016; Al-Rfou et al., 2016; Yan et al., 2016), in which the pool of possible responses is constructed in advance (e.g., pre-existing human responses). These app
2810840719	Neural Approaches to Conversational AI	1958706068,2311783643,2410983263	on, although this is a relatively under-investigated direction. In the context of chatbots (Chapter 5), coherence, diversity and personal styles have been used to result in more human-like dialogues (Li et al., 2016a,b). They can be useful for task-oriented dialogues as well. In Sec. 4.4.6, we will review a few recent works that aim to learn reward functions automatically from data. 4.2.2 Simulation-Based Evalua
2810840719	Neural Approaches to Conversational AI	2153653739	onse generation system. This was the idea originally proposed in one of the ﬁrst works on fully data-driven conversational AI (Ritter et al., 2011), which applied a phrase-based translation approach (Koehn et al., 2003) to dialogue datasets extracted from Twitter (Serban et al., 2015). A different E2E approach was proposed in (Jafarpour et al., 2010), but it relied on IR-based methods rather than machine translation
2810840719	Neural Approaches to Conversational AI	1980340273	to optimize a generation module from corpora (Oh and Rudnicky, 2002; Angeli et al., 2010; Kondadadi et al., 2013; Mairesse and Young, 2014). Most such methods are based on supervised learning, while Rieser and Lemon (2010) takes a decision-theoretic view and uses reinforcement learning to make a trade-off between sentence length and information revealed.3 In recent years, there is a growing interest in neural approache
2810840719	Neural Approaches to Conversational AI	2583186419	ordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015). Only recently have researchers begun to explore how to ground the chitchat in world knowledge (Ghazvininejad et al., 2018) and images (Mostafazadeh et al., 2017) so as to make the conversation more contentful and interesting. 7 Table 1.2: Reinforcement Learning for Dialogue. dialogue state action reward QA understanding of user query intent clariﬁcation quest
2810840719	Neural Approaches to Conversational AI	2159640018	ork of Sordoni et al. (2015b), which proposed an RNN-based approach to conversational response generation (similar to Fig. 2.2) that exploited longer context. Together with the contemporaneous works (Shang et al., 2015; Vinyals and Le, 2015), these papers represented the ﬁrst neural approaches to fully E2E conversation modeling. While these three papers have some distinct properties, they are all based on recurrent
2810840719	Neural Approaches to Conversational AI	2294699749	orrelation analysis was performed at the sentence level, but decent sentence-level correlation has long been known to be difﬁcult to achieve even for machine translation (Callison-Burch et al., 2009; Graham et al., 2015), the task for which the underlying metrics (e.g., BLEU and METEOR) were speciﬁcally intended.12 In particular, BLEU (Papineni et al., 2002) was designed from the outset to be used as a corpus-level r
2810840719	Neural Approaches to Conversational AI	1895577753	outperform standard baselines on a number of simulated dialogue tasks. A similar approach was taken by Madotto et al. (2018) in their Mem2Seq model. This model uses mechanisms from pointer networks (Vinyals et al., 2015a) so as to incorporate external information from knowledge bases. Finally, Eric et al. (2017) proposed an end-to-end trainable Key-Value Retrieval Network, which is equipped with an attention-based k
2810840719	Neural Approaches to Conversational AI	2559038528,2579486552,2581637843,2592808142,2604688337	to be overall relatively bland due to the p(TjS) inference criterion (beam search), MMI rescoring often mitigates rather than completely eliminates the blandness problem. More recently, researchers (Li et al., 2017c; Xu et al., 2017; Zhang et al., 2018e) have used adversarial training and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), which often have the effect of reducing blandness. Intuitiv
2810840719	Neural Approaches to Conversational AI	2521709538	P= (p 1;:::;p J), we need to locate an answer span A= (a start;a end) in P. In spite of the variety of model structures and attention types (Chen et al., 2016a; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017c; Wang et al., 2017b), a typical neural MRC model performs reading comprehension in three steps, as outlined in Fig. 1.3: (1) encoding the symbolic representation of the questions and passages into a
2810840719	Neural Approaches to Conversational AI	2099471712	pletely eliminates the blandness problem. More recently, researchers (Li et al., 2017c; Xu et al., 2017; Zhang et al., 2018e) have used adversarial training and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), which often have the effect of reducing blandness. Intuitively, the effect of GAN on blandness can be understood as follows: adversarial training puts a Generator and Discriminator against each othe
2810840719	Neural Approaches to Conversational AI	1518951372,1993378086	pleting speciﬁc tasks, they are often developed to mimic human conversations by training DNN-based response generation models on large amounts of human-human conversational data (Ritter et al., 2011; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015). Only recently have researchers begun to explore how to ground the chitchat in world knowledge (Ghazvininejad et al., 2018) and images (Mostafazadeh et al.
2810840719	Neural Approaches to Conversational AI	2120045257	proach, or a model-based approach with the model learned from real conversational corpus. Agenda-Based Simulation. As an example, we describe a popular hidden agenda-based user simulator developed by Schatzmann and Young (2009), as instantiated in Li et al. (2016d) and Ultes et al. (2017c). Each dialogue simulation starts with a randomly generated user goal that is unknown to the dialogue manager. In general the user goal c
2810840719	Neural Approaches to Conversational AI	2250539671	Qand Pat the word level and normalizes for lexical variants. It typically maps each word to a vector space using a pre-trained word embedding model, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), such that semantically similar words are mapped to the vectors that are close to each other in the neural space (also see Sec. 2.2.1). Word embedding can be enhanced by concatenating each word embed
2810840719	Neural Approaches to Conversational AI	1996203119	quality of hand-crafted templates and rules. These challenges motivate the study of more data-driven approaches, known as corpus-based methods, that aim to optimize a generation module from corpora (Oh and Rudnicky, 2002; Angeli et al., 2010; Kondadadi et al., 2013; Mairesse and Young, 2014). Most such methods are based on supervised learning, while Rieser and Lemon (2010) takes a decision-theoretic view and uses rei
2810840719	Neural Approaches to Conversational AI	2251079237	Question Answering (QA), deep semantics and dialogue with intelligent agents. Recent years have seen the rise of a small industry of tutorials and survey papers on deep learning and dialogue systems. Yih et al. (2015b, 2016); Gao (2017) reviewed deep learning approaches for a wide range of IR and NLP tasks, including dialogues. Chen et al. (2017e) presented a tutorial on dialogues, with a focus on task-oriented a
2810840719	Neural Approaches to Conversational AI	1961647136	rature on managing (spoken) dialogue systems. A comprehensive survey is out of the scope of the this chapter. Interested readers are referred to earlier examples (Cole, 1999; Larsson and Traum, 2000; Rich et al., 2001; Bos et al., 2003; Bohus and Rudnicky, 2009), as well as excellent surveys like McTear (2002) and Young et al. (2013) for more information. Here, we review a small subset of traditional approaches th
2810840719	Neural Approaches to Conversational AI	2521709538	raverses the KB according to the query and B r, and score each candidate answer tusing a linear model score(q;t) = X ˇ2B r  ˇP(tjs;ˇ); (3.3) 22 Figure 3.3: An overview of the neural methods for KBR (Shen et al., 2017a; Yang et al., 2017a). The KB is embedded in neural space as matrix M that is learned to store compactly the connections between related triples (e.g., the relations that are semantically similar are
2810840719	Neural Approaches to Conversational AI	36942980	rd corresponding to this metric is 0 for every turn, except for the last turn where it is +1 for a successful dialogue and 1 otherwise. Many examples are found in the literature (Walker et al., 1997; Williams, 2006; Peng et al., 2017). Other variants have also been used, such as those used to measure partial success (Singh et al., 2002; Young et al., 2016). The second class measure cost incurred in a dialogue,
2810840719	Neural Approaches to Conversational AI	2560678344	rding to the query and B r, and score each candidate answer tusing a linear model score(q;t) = X ˇ2B r  ˇP(tjs;ˇ); (3.3) 22 Figure 3.3: An overview of the neural methods for KBR (Shen et al., 2017a; Yang et al., 2017a). The KB is embedded in neural space as matrix M that is learned to store compactly the connections between related triples (e.g., the relations that are semantically similar are stored as a cluster
2810840719	Neural Approaches to Conversational AI	1514587017	real human-human conversation corpora directly for evaluation. Unfortunately, this problem, known as off-policy evaluation in the RL literature, is challenging with numerous current research efforts (Precup et al., 2000; Jiang and Li, 2016; Thomas and Brunskill, 2016; Liu et al., 2018c). Such off-policy techniques can ﬁnd important use in evaluating and optimizing dialogue systems. Another related line of research i
2810840719	Neural Approaches to Conversational AI	2127795553	reasoning in Sec. 3.4. 3.3 Embedding-based Methods To address the paraphrasing problem, embedding-based methods map entities and relations in a KB to continuous vectors in a neural space; see, e.g., Bordes et al. (2013); Socher et al. (2013); Yang et al. (2015); Yih et al. (2015b). This space can be viewed as a hidden semantic space where various expressions with the same semantic meaning map to the same continuous
2810840719	Neural Approaches to Conversational AI	2510759893	recent big progress on MRC is largely due to the availability of a multitude of large-scale datasets that the research community has created over various text sources such as Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016), WikiHop (Welbl et al., 2017), DRCD (Shao et al., 2018)), news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et
2810840719	Neural Approaches to Conversational AI	2117488952	re referred to some of the earlier examples (Cole, 1999; Larsson and Traum, 2000; Rich et al., 2001; Allen et al., 2001; Bos et al., 2003; Bohus and Rudnicky, 2009), as well as excellent surveys like McTear (2002), Paek and Pieraccini (2008), and Young et al. (2013) for more information. Here, we review a small subset of traditional approaches from the decision-theoretic view we take in this paper. Levin et al
2810840719	Neural Approaches to Conversational AI	2280163991	rehl et al., 2009; Jaksch et al., 2010; Osband and Roy, 2017; Dann et al., 2017), exploration when parametric models like neural networks are used is an active research topic (Bellemare et al., 2016; Osband et al., 2016; Houthooft et al., 2016; Jiang et al., 2017). Here, a general-purpose exploration strategy is described, which is particularly suited for dialogue systems that may evolve over time. After a task-orie
2810840719	Neural Approaches to Conversational AI	1756422141	relations. As a result, PRA can easily produce millions of categorically distinct paths even for a small path length, which not only hurts generalization but makes reasoning prohibitively expensive. Lao et al. (2011) used heuristics and L 1 regularization to reduce the number of relational paths that need to be considered in KBR. To address these limitations, Gardner et al. (2014) proposed a modiﬁcation to PRA th
2810840719	Neural Approaches to Conversational AI	2521709538	the revised internal state s as a query for lookup. It remains challenging to recover the symbolic representations of queries and paths (or ﬁrst-order logical rules) from the neural controller. See (Shen et al., 2017a; Yang et al., 2017a) for some interesting preliminary results of interpretation of neural methods. 3.4.3 Reinforcement Learning based Methods DeepPath (Xiong et al., 2017), MINERVA (Das et al., 2017
2810840719	Neural Approaches to Conversational AI	2399880602	roach, as conversational data for a given user or persona might not always be available. In (Bhatia et al., 2017), the idea of (Li et al., 2016b) is extended to a social-graph embedding model. While (Serban et al., 2017) is not a persona-based response generation model per se, their work shares some similarities with speaker embedding models such as (Li et al., 2016b). Indeed, both Li et al. (2016b) and Serban et al.
2810840719	Neural Approaches to Conversational AI	2464171116	rs can be much larger, thus resulting in greater ﬂexibility in evaluation. In this process, many online and ofﬂine evaluation techniques such as A/B-testing and counterfactual estimation can be used (Hofmann et al., 2016). The major downside of experimenting with actual users is the risk of negative user experience and disruption of normal services. 1https://github.com/xiul-msr/e2e_dialog_challenge 40 4.2.4 Other Eval
2810840719	Neural Approaches to Conversational AI	1211946649	s the “belief state.” Since exact optimization in POMDPs is computationally intractable, authors have studied approximation techniques (Roy et al., 2000; Williams and Young, 2007; Young et al., 2010; Li et al., 2009; Gasiˇ ´c and Young, 2014) and alternative representations such as the information states framework (Larsson and Traum, 2000; Daubigney et al., 2012). Still, compared to the neural approaches covered
2810840719	Neural Approaches to Conversational AI	2399456070	s. The more challenging task of slot tagging is often treated as sequence classiﬁcation, where the classiﬁer predicts semantic class labels for subsequences of the input utterance (Wang et al., 2005; Mesnil et al., 2013). Table 4.2 shows an ATIS (Airline Travel Information System) utterance example in the Inside-Outside-Beginning (IOB) format (Ramshaw and Marcus, 1995), where for each word the model is to predict a s
2810840719	Neural Approaches to Conversational AI	1527442667	s deﬁned as “the process of ﬁnding the intended meaning(s) of the given, and it is suggested that this amounts to the process of inferring the appropriate context(s) in which to interpret the given” (Bell, 1999). The analysis by Jia and Liang (2017); Chen et al. (2016a) revealed that state of the art neural MRC models, e.g., developed on SQuAD, mostly excel at matching questions to local context via lexical
2810840719	Neural Approaches to Conversational AI	2130942839	s have some distinct properties, they are all based on recurrent (RNN) ar48 chitectures, which nowadays are often modeled with a Long Short-Term Memory (LSTM) model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014). 5.1.1 The LSTM Model We give an overview here of LSTM-based response generation, as LSTM is arguably the most popular seq2seq model, though alternative models such as GRU (Cho et al., 2014b) are oft
2810840719	Neural Approaches to Conversational AI	2473965551	s the j-th vector of M. Single-step reasoning is simple yet efﬁcient and the model parameters can be trained using the classical back-propagation algorithm, thus it is adopted by most of the systems (Chen et al., 2016b; Seo et al., 2016; Wang et al., 2017b; Liu et al., 2017; Chen et al., 2017a; Weissenborn et al., 2017; Hu et al., 2017). However, since humans often solve question answering tasks by re-reading and
2810840719	Neural Approaches to Conversational AI	2101105183	s and for performing automatic system optimization. E2E dialogue research mostly borrowed those metrics from machine translation and summarization, using string and n-gram matching metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Proposed more recently, METEOR (Banerjee and Lavie, 2005) aims to improve BLEU by identifying synonyms and paraphrases between the system output and the human reference, and ha
2810840719	Neural Approaches to Conversational AI	175897666	s reasoning prohibitively expensive. Lao et al. (2011) used heuristics and L 1 regularization to reduce the number of relational paths that need to be considered in KBR. To address these limitations, Gardner et al. (2014) proposed a modiﬁcation to PRA that leverages the KB embedding methods, as described in Sec. 3.3, to collapse and cluster PRA paths according to their relation embeddings. 3.4.2 Neural Methods Implici
2810840719	Neural Approaches to Conversational AI	2133564696	s the response generation task much more challenging, as generating a given word or phrase doesn’t completely preclude the need of generating the same word or phrase again. While the attention model (Bahdanau et al., 2015) helps prevent repetition errors in machine translation as that task is relatively one-to-one,8 the attention models originally designed for machine translation (Bahdanau et al., 2015; Luong et al., 2
2810840719	Neural Approaches to Conversational AI	2399880602	s RNN, GRU, LSTM, etc. (2) Two-level hierarchy representative of HRED. Note: To simplify the notation, the ﬁgure represents utterances of length 3. HRED model was later extended with the VHRED model (Serban et al., 2017) (further discussed in Sec. 5.2), which adds a latent variable to the target to address a different issue. 5.1.3 Attention models The seq2seq framework has been tremendously successful in text generat
2810840719	Neural Approaches to Conversational AI	1544827683	s text sources such as Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016), WikiHop (Welbl et al., 2017), DRCD (Shao et al., 2018)), news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017), ReCoRD (Zhang et al., 2018d)), ﬁctional stories (MCTest (Richardson et al., 2013), CBT (Hill et al., 2015), NarrativeQA (Kocisky et al.,ˇ 20
2810840719	Neural Approaches to Conversational AI	2162211144	s trainable leads to other potential problems such as overﬁtting and “gaming of the metric” (Albrecht and Hwa, 2007),14 which might explain why previously proposed machine-learned evaluation metrics (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Lita et al., 2005; Albrecht and Hwa, 2007; Gim´enez and Marquez, 2008; Pado et al., 2009; Stanojevi` ´c and Sima’an, 2014, etc.) are not commonly used in ofﬁcial machine t
2810840719	Neural Approaches to Conversational AI	1847211030	s true in dialogue data. Indeed, in dialogue entire spans of the source may not map to anything in the target and vice-versa.4 Some speciﬁc attention models for dialogue have been shown to be useful (Yao et al., 2015; Mei et al., 2017; Shao et al., 2017), e.g., to avoid word repetitions (which are discussed further in Sec. 5.2). 5.1.4 Pointer-Network Models Multiple model extensions (Gu et al., 2016; He et al., 2
2810840719	Neural Approaches to Conversational AI	2136189984	he semantic similarity of a pair of inputs (x;y). They can be applied to a wide range of tasks depending on the deﬁnition of (x;y). For example, (x;y) is a query-document pair for Web search ranking (Huang et al., 2013; Shen et al., 2014), a document pair in recommendation (Gao et al., 2014b), a question-answer pair in QA (Yih et al., 2015a), a sentence pair of different languages in machine translation (Gao et al.
2810840719	Neural Approaches to Conversational AI	2625113742	ser and the dialogue agent, and the latter is to assist the former. In general, the task can be more complex such as mixed-initiative dialogues (Horvitz, 1999) and negotiations (Barlier et al., 2015; Lewis et al., 2017). More generally, there may be multiple parties involved in a conversation, where turn taking becomes more challenging (Bohus and Horvitz, 2009, 2011). In such scenarios, it is helpful to take a game-
2810840719	Neural Approaches to Conversational AI	2147138267	shown in Fig. 4.8, a basic SC-LSTM cell has two parts: a typical LSTM cell (upper part in the ﬁgure) and a sentence planning cell (lower part) for semantic control. 3Some authors (Stone et al., 2003; Koller and Stone, 2007) have taken a similar, decision-theoretic point of view for NLG. Their formulate NLG as a planning problem, as opposed to data-driven or corpus-based methods being discussed here. 49 Figure 4.8: A Sem
2810840719	Neural Approaches to Conversational AI	2156985047	to signiﬁcantly improve over RNNs thanks to its ability to repeat proper nouns and other words of the input. 4Ritter et al. (2011) also found that alignment produced by an off-the-shelf word aligner (Och and Ney, 2003) produced alignments of poor quality, and an extension of their work with attention models (Ritter 2018, pc) yield attention scores that did not correspond to meaningful alignments. 55 5.2 Challenges
2810840719	Neural Approaches to Conversational AI	102708294	sks and models are extended to the conversational QA setting. 3.1 Knowledge Base Organizing the world’s facts and storing them in a structured database, large scale Knowledge Bases (KB) like DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago (Suchanek et al., 2007) have become important resources for supporting open-domain QA. A typical KB consists of a collection of subject-predicate-object tr
2810840719	Neural Approaches to Conversational AI	2583186419	speaker or addressee (Li et al., 2016b; Al-Rfou et al., 2016), textual knowledge sources such as Foursquare (Ghazvininejad et al., 2018), the user’s or agent’s visual environment (Das et al., 2017a; Mostafazadeh et al., 2017), and affect or emotion of the user (Huber et al., 2018). At a high level, most of these works have in common the idea of augmenting their context encoder to not only represent the conversation histor
2810840719	Neural Approaches to Conversational AI	2109910161	he sum of rewards along the entire trajectory. Its variance may be reduced by the use of an estimated value function of the current policy, often referred to as the critic in actor-critic algorithms (Sutton et al., 1999a; Konda and Tsitsiklis, 1999): r J() = HX 1 t=1 t 1  r logˇ(a tjs t;)Q^(s t;a t;h)  ; (2.8) where Q^(s;a;h) is an estimated value function for the current policy ˇ(s;) that is used to approxim
2810840719	Neural Approaches to Conversational AI	2127838323	systems. A comprehensive survey is out of the scope of the this chapter. Interested readers are referred to earlier examples (Cole, 1999; Larsson and Traum, 2000; Rich et al., 2001; Bos et al., 2003; Bohus and Rudnicky, 2009), as well as excellent surveys like McTear (2002) and Young et al. (2013) for more information. Here, we review a small subset of traditional approaches that are relevant to the decision-theoretic vie
2810840719	Neural Approaches to Conversational AI	1591706642	t attempt to use RL in a fully E2E approach to conversational response generation. Instead of training the system on human-to-human conversations as in the supervised setup of (Sordoni et al., 2015b; Vinyals and Le, 2015), the system of Li et al. (2016c) is trained by conversing with a user simulator which mimics human users’ behaviors. As depicted in Fig. 5.4, human users have to be replaced with a user simulator bec
2810840719	Neural Approaches to Conversational AI	10957333	tanding research topic for generation tasks such as machine translation and summarization. E2E dialogue is no different. While it is common to evaluate response generation systems using human raters (Ritter et al., 2011; Sordoni et al., 2015b; Shang et al., 2015, etc.), this type of evaluation is often expensive and researchers often have to resort to automatic metrics for quantifying day-to-day progress and for per
2810840719	Neural Approaches to Conversational AI	2057244568	tantial domain knowledge is needed (Sec. 4.1). This inspires the use of machine learning to ﬁnd a good reward function from data (Asri et al., 2012) which can better correlate with user satisfaction (Rieser and Lemon, 2011), or is more consistent with expert demonstrations (Li et al., 2014). Su et al. (2015) proposed to rate dialogue success with two neural network models, a recurrent and a convolutional network. Their
2810840719	Neural Approaches to Conversational AI	1533230146	task than KB-QA since it only needs to predict whether a fact is true or not, and thus does not suffer from the search complexity problem. The bilinear model is one of the basic KB embedding models (Yang et al., 2015; Nguyen, 2017). It learns a vector x e 2Rdfor each entity e2Eand a matrix W r 2R dfor each relation r2R. The model scores how likely a triple (s;r;t) holds using score(s;r;t;) = x&gt; s W rx t: (3.1
2810840719	Neural Approaches to Conversational AI	2132997613	Tear (2002), Paek and Pieraccini (2008), and Young et al. (2013) for more information. Here, we review a small subset of traditional approaches from the decision-theoretic view we take in this paper. Levin et al. (2000) viewed conversation as a decision making problem. Walker (2000) and Singh et al. (2002) are two early applications of reinforcement learning to manage dialogue systems. While promising, these approac
2810840719	Neural Approaches to Conversational AI	2154652894	tem optimization. E2E dialogue research mostly borrowed those metrics from machine translation and summarization, using string and n-gram matching metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Proposed more recently, METEOR (Banerjee and Lavie, 2005) aims to improve BLEU by identifying synonyms and paraphrases between the system output and the human reference, and has also been used to ev
2810840719	Neural Approaches to Conversational AI	1544826511	tential problems such as overﬁtting and “gaming of the metric” (Albrecht and Hwa, 2007),14 which might explain why previously proposed machine-learned evaluation metrics (Corston-Oliver et al., 2001; Kulesza and Shieber, 2004; Lita et al., 2005; Albrecht and Hwa, 2007; Gim´enez and Marquez, 2008; Pado et al., 2009; Stanojevi` ´c and Sima’an, 2014, etc.) are not commonly used in ofﬁcial machine translation benchmarks. The
2810840719	Neural Approaches to Conversational AI	2140054881	TEOR (Banerjee and Lavie, 2005) aims to improve BLEU by identifying synonyms and paraphrases between the system output and the human reference, and has also been used to evaluate dialogue. deltaBLEU (Galley et al., 2015) is an extension of BLEU that exploits numerical ratings associated with conversational responses. There has been signiﬁcant debate as to whether such automatic metrics are actually appropriate for ev
2810840719	Neural Approaches to Conversational AI	1756422141	ti-step KBR methods. They differ in whether reasoning is performed in a discrete symbolic space or a continuous neural space. 3.4.1 Symbolic Methods Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) is one of the primary symbolic approaches to learning relational paths in large KBs. PRA uses random walks with restarts to perform multiple bounded depth-ﬁrst search to ﬁnd relational paths. Table 3
2810840719	Neural Approaches to Conversational AI	2089327077	tion. While there has been much work on user simulation, building a human-like simulator remains challenging. In fact, even user simulator evaluation itself continues to be an ongoing research topic (Williams, 2008; Ai and Litman, 2008; Pietquin and Hastie, 2013). In practice, it is often observed that dialogue policies that are overﬁtted to a particular user simulator may not work well when serving another use
2810840719	Neural Approaches to Conversational AI	2123395566	tter et al., 2011), which applied a phrase-based translation approach (Koehn et al., 2003) to dialogue datasets extracted from Twitter (Serban et al., 2015). A different E2E approach was proposed in (Jafarpour et al., 2010), but it relied on IR-based methods rather than machine translation. While these two papers constituted a paradigm shift compared to earlier work in dialogue, they had several limitations. Their most
2810840719	Neural Approaches to Conversational AI	2131876387	ty of a pair of inputs (x;y). They can be applied to a wide range of tasks depending on the deﬁnition of (x;y). For example, (x;y) is a query-document pair for Web search ranking (Huang et al., 2013; Shen et al., 2014), a document pair in recommendation (Gao et al., 2014b), a question-answer pair in QA (Yih et al., 2015a), a sentence pair of different languages in machine translation (Gao et al., 2014a), and an ima
2810840719	Neural Approaches to Conversational AI	2473965551	uAD, where given a question Q= (q 1;:::;q I) and a passage P= (p 1;:::;p J), we need to locate an answer span A= (a start;a end) in P. In spite of the variety of model structures and attention types (Chen et al., 2016a; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017c; Wang et al., 2017b), a typical neural MRC model performs reading comprehension in three steps, as outlined in Fig. 1.4: (1) encoding the sy
2810840719	Neural Approaches to Conversational AI	1591706642	uages in machine translation (Sutskever et al., 2014; Cho et al., 2014b), an image-text pairs in image captioning (Vinyals et al., 2015b) (where f 1 is a CNN), and message-response pairs in dialogue (Vinyals and Le, 2015; Li et al., 2016a). 16 Figure 2.5: Interaction between an RL agent and the external environment. 2.3.1 Foundations Reinforcement learning (RL) is a learning paradigm where an intelligent agent learns
2810840719	Neural Approaches to Conversational AI	2593172760	ublic MRC datasets we described above, TREC6 also provides a series of text-QA open benchmarks. The automated QA track. This is one of the most popular tracks in TREC for many years, up to year 2007 (Dang et al., 2007; Agichtein et al., 2015). It has focused on the task of providing automatic answers for human questions. The track primarily dealt with factual questions, and the answers provided by participants wer
2810840719	Neural Approaches to Conversational AI	1623072288	ubsequences of the input utterance (Wang et al., 2005; Mesnil et al., 2013). Fig. 4.3 shows an ATIS (Airline Travel Information System) utterance example in the Inside-Outside-Beginning (IOB) format (Ramshaw and Marcus, 1995), where for each word the model is to predict a semantic tag. Yao et al. (2013) and Mesnil et al. (2015) applied recurrent neural networks to slot tagging, where inputs are one-hot encoding of the wor
2810840719	Neural Approaches to Conversational AI	2133564696	uch as machine translation, but its encoding of the entire source sequence into a ﬁxed-size vector has certain limitations, especially when dealing with long source sequences. Attention-based models (Bahdanau et al., 2015; Vaswani et al., 2017) alleviate this limitation by allowing the model to search and condition on parts of a source sentence that are relevant to predicting the next target word, thus moving away fro
2810840719	Neural Approaches to Conversational AI	2251058040	ures for dialogue state tracking. Starting from Williams et al. (2013), it has successfully attracted many research teams to focus on a wide range of technical problems in DST (Williams et al., 2014; Henderson et al., 2014b,a; Kim et al., 2016a,b; Hori et al., 2017). Corpora used by DSTC over the years have covered human-computer and human-human conversations, different domains such as restaurant and tourist, cross-lan
2810840719	Neural Approaches to Conversational AI	2583186419	when the user herself is engaged.6 The Replika system (Fedorenko et al., 2017) for chitchat combines neural generation and retrievalbased methods, and is able to condition responses on images as in (Mostafazadeh et al., 2017). The neural generation component of Replika is persona-based (Li et al., 2016b), as it is trained to mimic 4https://www.zo.ai 5https://www.facebook.com/Ruuh 6https://www.leiphone.com/news/201807/rgyK
2810840719	Neural Approaches to Conversational AI	2109038907	on user simulation, building a human-like simulator remains challenging. In fact, even user simulator evaluation itself continues to be an ongoing research topic (Williams, 2008; Ai and Litman, 2008; Pietquin and Hastie, 2013). In practice, it is often observed that dialogue policies that are overﬁtted to a particular user simulator may not work well when serving another user simulator or real humans (Schatzmann et al., 20
2810840719	Neural Approaches to Conversational AI	1591706642	ve. LSTM is an extension of the RNN model represented in Fig. 2.2, and is often more effective at exploiting long-term context. An LSTM-based response generation system is usually modeled as follows (Vinyals and Le, 2015; Li et al., 2016a): Given a dialogue history represented as a sequence of words S= fs 1;s 2;:::;s N s g(Shere stands for source), the LSTM associates each time step kwith input, memory, and output ga
2810840719	Neural Approaches to Conversational AI	2101105183	ve even for machine translation (Callison-Burch et al., 2009; Graham et al., 2015), the task for which the underlying metrics (e.g., BLEU and METEOR) were speciﬁcally intended.12 In particular, BLEU (Papineni et al., 2002) was designed from the outset to be used as a corpus-level rather than sentence-level metric, since assessments based on n-gram matches are brittle when computed on a single sentence. Indeed, the empi
2810840719	Neural Approaches to Conversational AI	2149029524	ways obvious, and substantial domain knowledge is needed (Sec. 4.1). This inspires the use of 48 machine learning to ﬁnd a good reward function from data (Walker et al., 2000; Rieser and Lemon, 2008; Rieser et al., 2010; El Asri et al., 2012) which can better correlate with user satisfaction (Rieser and Lemon, 2011), or is more consistent with expert demonstrations (Li et al., 2014). Su et al. (2015) proposed to rat
2810840719	Neural Approaches to Conversational AI	2117488952	well, it can be challenging to solve the “credit assignment” problem, namely, to identify which component in the system causes undesired system response and needs to be improved. Indeed, as argued by McTear (2002), “[t]he key to a successful dialogue system is the integration of these components into a working system.” The recent marriage of differentiable neural models and reinforcement learning allows a dial
2810840719	Neural Approaches to Conversational AI	2250445771,2251008987	wide range of tasks depending on the deﬁnition of (x;y). For example, (x;y) is a query-document pair for Web search ranking (Huang et al., 2013; Shen et al., 2014), a document pair in recommendation (Gao et al., 2014b), a question-answer pair in QA (Yih et al., 2015a), a sentence pair of different languages in machine translation (Gao et al., 2014a), and an image-text pair in image captioning (Fang et al., 2015)
2810840719	Neural Approaches to Conversational AI	10957333	y off-the-shelf SMT algorithm to a conversational dataset to build a response generation system. This was the idea originally proposed in one of the ﬁrst works on fully data-driven conversational AI (Ritter et al., 2011), which applied a phrase-based translation approach (Koehn et al., 2003) to dialogue datasets extracted from Twitter (Serban et al., 2015). A different E2E approach was proposed in (Jafarpour et al.,
2810840719	Neural Approaches to Conversational AI	2438667436	y take ﬁnitely many possible values, and is fully observable. Both assumptions are often violated in real-world applications. To handle uncertainty inherent in dialogue systems, Roy et al. (2000) and Williams and Young (2007) proposed to use Partially Observable Markov Decision Process (POMDP) as a principled mathematical framework for modeling and optimizing dialogue systems. The idea is to use observed user utterances t
2810840719	Neural Approaches to Conversational AI	2133564696	ys, attention models have been somewhat less effective in E2E dialogue modeling. This can probably be explained by the fact that attention models effectively attempt to “jointly translate and align” (Bahdanau et al., 2015), which is a desirable goal in machine translation as each information piece in the source sequence (foreign sentence) typically needs to be conveyed in the target (translation) exactly once, but this
2810840719	Neural Approaches to Conversational AI	2465628802	ys similar to a conversation, except that the scenarios are predeﬁned by the game designer. Recent advances for solving text games, such as handling natural-language actions (Narasimhan et al., 2015; He et al., 2016; Cotˆ ´e et al., 2018) and interpretable policies (Chen et al., 2017c) may inspire similar algorithms that are useful for taskoriented dialogues. 52 Chapter 5 Fully Data-Driven Conversation Models an
2810840719	Neural Approaches to Conversational AI	2616122292	ystem responses are ﬁrst collected and then used to train multiple components of a dialogue system in order to maximize prediction accuracy (Bordes et al., 2017; Wen et al., 2017; Yang et al., 2017b; Eric et al., 2017; Madotto et al., 2018; Wu et al., 2018). Wen et al. (2017) introduced a modular neural dialogue system, where most modules are represented by a neural network. However, their approach relies on non-d
2810840719	Neural Approaches to Conversational AI	2399880602	Zhang et al. (2018c) explicitly optimizes a variational lower bound on pairwise mutual information between query and response to encourage generating more informative responses during training time. Serban et al. (2017) presented a latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED) model that also aims to generate less bland and more speciﬁc responses. It extends the hierarchical HRED model described pre
2810914326	The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems.	2519091744,2608207374	Deep learning. 1 Introduction One of the major components of human-agent interaction systems is the speech synthesis module. The state-of-the-art speech synthesis systems such as wavenet and tacotron[19,23,21] are giving impressive results. They can produce, intelligible, expressive, even human-like speech. But, they cannot yet be used to control the emotional dimensionality in speech which is a crucial pa
2810914326	The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems.	2018613638	netic level are available. A subset of these were used to build our database. The phonetic annotations are not time-aligned with our data yet, but methods can be used such as forced alignment systems [2]. We chose ﬁve diﬀerent emotions: amusement, anger, sleepiness, disgust and neutral. These emotions were chosen because of the ease to produce them by actors and in order to cover a diverse space in t
2810914326	The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems.	2214475686,2225419387	speaker in noiseless environment. However the sentences are neutral and do not express any emotions. TheAmuS databasecontainaudiodatadedicated toamusedspeechsynthesis[12]. We showed in previous work [9,8,11] that this database was well suited for amused speech synthesis. But AmuS contains data only for amused speech and not other emotions. 3 Motivations This database’s primary purpose it to build models
2810946745	Discourse-Wizard: Discovering Deep Discourse Structure in your Conversation with RNNs.	2123442489	atural language processing for different tasks, named entity detection, text tokenization, part of speech tagging, sentiment analysis, and word embedding demos (Loper and Bird,2002;Socher et al.,2013;Manning et al., 2014;Kutuzov and Kuzmenko,2017). In our work, we add discourse analysis to this list of useful demonstrations. 3 Approach In the following sections, we describe the two models used for the demonstration,
2810946745	Discourse-Wizard: Discovering Deep Discourse Structure in your Conversation with RNNs.	2402144811	ble for the web-demo using a client-server architecture. The overall architecture is shown in Figure2. 5.1 Technical details The neural models are developed using Keras (Chollet,2015) and TensorFlow (Abadi et al., 2016). Both models share similar properties and parameters, such as the vocabulary, embeddings, and most importantly, the context-based model uses the representation from the no-context model to encode the
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2556468274	application to a different set of problems. Inspired by transfer learning, the appearance of holistic architectures to respond to complex NLP tasks [5, 6] startedtoemerge,eitherwithmany-tasksolutions[3,6,9]orsimply end-to-end [4, 12] architectures. The quest to a simple unified model, which tries to leverage on multi problems insights in a hierarchical way drives our path. The word representation proble
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2250539671	ation content, with potential losses in terms of syntactical context. The creation of a dense vector representation can be achieved through several different techniques. In our case we consider GloVe [17] and word2vec [15]. Both are word-based, taking into consideration the frequency of proximal words to optimize their numeric representation. Words that are used in the same context get mapped closer i
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2179519966	d problem, Named Entity Recognition, deals with the identification of entities in textual content. In a similar fashion to the Part-of-Speech tagging task, end-to-end sequence labelling architectures [4, 12] dominate recent approach paths. In Section 3 we will delve into the details of our proposal, a hierarchical Natural Language Parser. 3 SEMANTIC SEARCH Common approaches for information retrieval enco
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2134036914	e able to mount a solution in a transitional manner, we felt the necessity of implementing a different model, due to the inherent speed problems, as described in Section 4.2. Thus, inspired by Dyer’s [7] Stack-LSTM work, we have modified the architecture into an end-to-end sequence labelling problem. Contrary to our previous approach, where the model is run N times until the final state is reached, t
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2135843243	ed, such as fastText [11], we decided to delay their exploration to a later stage. To provide syntactic insights, Part-of-Speech tagging solutions took a leap forward from hidden markov models trends [1], with a use of deep endto-end sequence labelling techniques [12]. The task of associating dependencies between words, Dependency Parsing, evolved into a set of two different approaches. Transition-ba
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	1614298861	following modules. We only use short and long product descriptions, where each word can be represented by its context. We have made an extensive study with two well-known embeddingtechniques:word2vec[15]andGloVe[17].Thehyper-parameters fine tuning was set according to the original papers. We keep an embedding dimension of 300 across all models. PoS: In the Part-of-Speech tagging task, we aim to learn
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	1614298861	h potential losses in terms of syntactical context. The creation of a dense vector representation can be achieved through several different techniques. In our case we consider GloVe [17] and word2vec [15]. Both are word-based, taking into consideration the frequency of proximal words to optimize their numeric representation. Words that are used in the same context get mapped closer in the vectorial sp
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2134036914	ing techniques [12]. The task of associating dependencies between words, Dependency Parsing, evolved into a set of two different approaches. Transition-based architectures, based in Chen [3] and Dyer [7] work, and graph-based architectures [16] dominate the state-of-the-art solutions to enrich with syntactic association information. A vastly explored problem, Named Entity Recognition, deals with the
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2250861254	n. In this context, a Dependency Parser is responsible for creating a graph connection between each word. We started by implementing a transition-based dependency parser, inspired by Chen and Manning [2]. Traditionally, this is a sequential task, in which the input is unfolded into a stack, a buffer and a list of operations. In the initial state, the buffer contains the list of embeddings of the sent
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	1888026739	with numerous dedicated works, continues to present itself as a challenge.Therecentproliferationofnoveldeepneuralnetworksalgorithms and methods paved the way to new capabilities in solving NLP tasks [8, 12]. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial a
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2250539671	odules. We only use short and long product descriptions, where each word can be represented by its context. We have made an extensive study with two well-known embeddingtechniques:word2vec[15]andGloVe[17].Thehyper-parameters fine tuning was set according to the original papers. We keep an embedding dimension of 300 across all models. PoS: In the Part-of-Speech tagging task, we aim to learn the correct
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2158899491	ores the reuse of pre-trained models and subsequent application to a different set of problems. Inspired by transfer learning, the appearance of holistic architectures to respond to complex NLP tasks [5, 6] startedtoemerge,eitherwithmany-tasksolutions[3,6,9]orsimply end-to-end [4, 12] architectures. The quest to a simple unified model, which tries to leverage on multi problems insights in a hierarchical
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	1614298861	problem explores embeddings techniques in order to map words in a vectorial space while being able to capture semantic similarities. Word embedding solutions presented in literature such as word2vec [15] or GloVe [17] establish a starting point towards our objective. Although new solutions that explore character embedding techniques emerged, such as fastText [11], we decided to delay their exploratio
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2250539671	res embeddings techniques in order to map words in a vectorial space while being able to capture semantic similarities. Word embedding solutions presented in literature such as word2vec [15] or GloVe [17] establish a starting point towards our objective. Although new solutions that explore character embedding techniques emerged, such as fastText [11], we decided to delay their exploration to a later s
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2179519966	set of problems. Inspired by transfer learning, the appearance of holistic architectures to respond to complex NLP tasks [5, 6] startedtoemerge,eitherwithmany-tasksolutions[3,6,9]orsimply end-to-end [4, 12] architectures. The quest to a simple unified model, which tries to leverage on multi problems insights in a hierarchical way drives our path. The word representation problem explores embeddings techn
2811090138	A Hierarchical Deep Learning Natural Language Parser for Fashion.	2251949442	ting dependencies between words, Dependency Parsing, evolved into a set of two different approaches. Transition-based architectures, based in Chen [3] and Dyer [7] work, and graph-based architectures [16] dominate the state-of-the-art solutions to enrich with syntactic association information. A vastly explored problem, Named Entity Recognition, deals with the identification of entities in textual con
2811335494	Encoding Spatial Relations from Natural Language.	2160279799	and cultures (Haun et al. 2011), with added complexity in how humans represent geometric properties when describing spatial experiences (Landau and Jackendoff 1993) and in the layering of locatives (Kracht 2002). While there has been considerable research into the Joint First authors. Preprint. Work in progress. arXiv:1807.01670v2 [cs.CL] 5 Jul 2018 Figure 1: Example descriptions with corresponding ground tr
2811335494	Encoding Spatial Relations from Natural Language.	2250673545	ile research on visualising spatial descriptions has predominantly employed heavily hand engineered representations that do not offer the generic cross task advantages of distributed representations (Chang et al. 2014; Hassani and Lee 2016). 2 Dataset of visually grounded scene descriptions We construct virtual scenes with multiple views each presented in multiple modalities: image, and synthetic or natural langua
2811335494	Encoding Spatial Relations from Natural Language.	1548656082	scriptions from text and their mapping into formal symbolic languages (Kordjamshidi et al. 2012a,b), with numerous such annotation schemes and methods proposed (Shen et al. 2009; Bateman et al. 2010; Rouhizadeh et al. 2011). Meanwhile research on visualising spatial descriptions has predominantly employed heavily hand engineered representations that do not offer the generic cross task advantages of distributed represent
2811335494	Encoding Spatial Relations from Natural Language.	2467945436	ualising spatial descriptions has predominantly employed heavily hand engineered representations that do not offer the generic cross task advantages of distributed representations (Chang et al. 2014; Hassani and Lee 2016). 2 Dataset of visually grounded scene descriptions We construct virtual scenes with multiple views each presented in multiple modalities: image, and synthetic or natural language descriptions (see Fi
2811470724	Seq2RDF: An End-to-end Application for Deriving Triples from Natural Language Text.	2250618585	ions 2 of relying only on the last hidden state of the encoder. Furthermore, in order to capture the semantics of the entities and relations within our training data, we apply domain specic resources[2] to obtain the word embeddings and the TransE model[1] to obtain KG embeddings for entities and relations in the KG. We use these pre-trained Word embeddings and KG embeddings for entities and relatio
2811470724	Seq2RDF: An End-to-end Application for Deriving Triples from Natural Language Text.	2539469848	ize the encoder and decoder embedding matrix, respectively, and results show that this approach improves the overall performance. 3 Experiments Data Sets We ran experiments on two public datasets NYT4[4], ADE5 with selected vocabularies and a Wiki-DBpedia dataset that is produced by distant supervision6. For data obtained by distant supervision, the test set is manually labeled to ensure its quality.
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2436788615	, recommendation, negotiation and chitchat) based on over 20 commodities. As word segmentation treatment is the primary step in Chinese language processing tasks (Zhao et al., 2017; Cai et al., 2017; Cai and Zhao, 2016), we adopt BaseSeg (Zhao et al., 2006) to tokenize the texts. For a discriminative learning, we add negative responses by ranking the response corpus based on the last utterance along with the top-5 k
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2891416139	-turn conversation modeling plays a key role in dialogue systems, either for generation-based (Serban et al., 2017b; Serban et al., 2017a; Zhou et al., 2017; Wu et al., 2018) or retrieval-based ones (Wu et al., 2017; Zhou et al., 2016) in which the latter is the focus of this paper. A natural approach for multi-turn modeling is simply concatenating the context utterances (Lowe et al., 2015; Yan et al., 2016). Ho
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2608175740	ang et al., 2018b; Bai and Zhao, 2018), developing an intelligent dialogue system becomes realizable, which means training machines to converse with human in natural languages (Williams et al., 2017; He et al., 2017; Dhingra et al., 2017; Zhang et al., 2018a). Towards this end, a number of data-driven dialogue systems are designed (Lowe et al., 2015; Wu et al., 2017; Wen et al., 2017; Mei et al., 2017; Young et
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	836999996	after attention learning against each utterance itself and the response. 4 Experiment 4.1 Dataset We evaluate our model on three multi-turn conversation datasets, the Ubuntu Dialogue Corpus (Ubuntu) (Lowe et al., 2015), the Douban Conversation Corpus (Douban) (Wu et al., 2017) and our released Ecommerce Dialogue Corpus (ECD) 3. Data statistics are in Table 1. UbuntuDialogueCorpus Ubuntu Dialogue Corpus consists of
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2221711388	c models in (Kadlec et al., 2015; Lowe et al., 2015), including TF-IDF, CNN, RNN, LSTM and biLSTM ; We also explore other advanced single-turn matching models, MV-LSTM (Wan et al., 2016), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015), Multi-Channels (Wu et al., 2017); These models concatenate the context utterances together to match a response. Advanced multi-turn matching models: Multi-view mo
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	836999996	converse with human in natural languages (Williams et al., 2017; He et al., 2017; Dhingra et al., 2017; Zhang et al., 2018a). Towards this end, a number of data-driven dialogue systems are designed (Lowe et al., 2015; Wu et al., 2017; Wen et al., 2017; Mei et al., 2017; Young et al., 2018; Lipton et al., 2018), in which modeling multi-turn conversation has drawn more and more attention. To acquire a contextual re
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	836999996	etrics: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Precision at 1 (P@1) and Recall at position kin ncandidates (Rn@k) , which are widely used for relevance evaluation (Wu et al., 2017; Lowe et al., 2015). For the sake of computational efﬁciency, the maximum number of utterances is specialized as 10 and each utterance contains at most 50 words. We apply truncating and zero-padding when necessary. Word
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2891416139	important parts from the utterance according to the current word and the whole utterance representation through fusing each previous utterance and the last utterance. 3.4 Response Matching Following (Wu et al., 2017), we use word-level and utterance-level representations to build two matching matrices and employ CNN to obtain salient matching information from the matrices. Suppose we have matching matrices M 1 an
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2608256743	logistics express, recommendation, negotiation and chitchat) based on over 20 commodities. As word segmentation treatment is the primary step in Chinese language processing tasks (Zhao et al., 2017; Cai et al., 2017; Cai and Zhao, 2016), we adopt BaseSeg (Zhao et al., 2006) to tokenize the texts. For a discriminative learning, we add negative responses by ranking the response corpus based on the last utterance a
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2891416139	n in natural languages (Williams et al., 2017; He et al., 2017; Dhingra et al., 2017; Zhang et al., 2018a). Towards this end, a number of data-driven dialogue systems are designed (Lowe et al., 2015; Wu et al., 2017; Wen et al., 2017; Mei et al., 2017; Young et al., 2018; Lipton et al., 2018), in which modeling multi-turn conversation has drawn more and more attention. To acquire a contextual response, previous
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2338325072	ngle-turn matching models: Basic models in (Kadlec et al., 2015; Lowe et al., 2015), including TF-IDF, CNN, RNN, LSTM and biLSTM ; We also explore other advanced single-turn matching models, MV-LSTM (Wan et al., 2016), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015), Multi-Channels (Wu et al., 2017); These models concatenate the context utterances together to match a response. Advanced multi-
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	836999996	or retrieval-based ones (Wu et al., 2017; Zhou et al., 2016) in which the latter is the focus of this paper. A natural approach for multi-turn modeling is simply concatenating the context utterances (Lowe et al., 2015; Yan et al., 2016). However, this will introduce much noise since previous utterances as the context is lengthy and redundant. The gist is to identify pertinent information in previous utterances and
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2891416139	strategies and combined them with last utterance to form a reformulated context. Zhou et al. (2016) performed context-response matching with a multi-view model on both word level and utterance level. Wu et al. (2017) improved the leveraging of utterances relationship and contextual information by matching a response with each utterance in the context based on a convolutional neural network. Different from previou
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	836999996	Ti). We run all the models up to 5 epochs and select the model that achieves the best result in validation. Our baselines include: Single-turn matching models: Basic models in (Kadlec et al., 2015; Lowe et al., 2015), including TF-IDF, CNN, RNN, LSTM and biLSTM ; We also explore other advanced single-turn matching models, MV-LSTM (Wan et al., 2016), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2
2822830299	Modeling Multi-turn Conversation with Deep Utterance Aggregation	2891416139	undant. The gist is to identify pertinent information in previous utterances and properly model the utterance relationships to ensure conversation consistency. To avoid unnecessary information loss, (Wu et al., 2017) matches a response with each utterance in the context, paying little attention on distrinct importance of each utterance and also failing to touch internal semantics inside utterances. In fact, the r
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2570781790	(Zeman et al., 2017). Dozat and Manning (2016) and its extension (Dozat et al., 2017) have shown very competitive performance in both the shared task (Dozat et al., 2017) and previous parsing works (Ma and Hovy, 2017; Shi et al., 2017a; Liu et al., 2018b; Ma et al., 2018). A nature question that raises is how can we further improve their part of speech (POS) tagger and parser via simple yet effective technique. I
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2250741688	4 languages do not even have any training data. It’s difﬁcult to train reasonable parser on these low-resource languages. We deal with these treebanks by adopting the word embedding transfer idea of Guo et al. (2015). We transfer the word embeddings of the rich-resource language to the space of lowresource language using the bilingual word vectors transformation technique (Smith et al., 2017) and trained a parser
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2344508595	d as well. Letting these treebanks help each other has been shown an effective way to improve parsing performance in both the crosslingual-cross-domain parsing community and last year’s shared tasks (Ammar et al., 2016; Guo et al., 2015; Che et al., 2017; Shi et al., 2017b; Bjorkelund et al., 2017). In our system, we ap-¨ ply the simple concatenation to the treebanks that are potentially helpful to each other and e
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2493916176	embedding transfer idea in the cross-lingual dependency parsing (Guo et al., 2015) and use the bilingual word vectors transformation technique (Smith et al., 2017)1 to map fasttext2 word embeddings (Bojanowski et al., 2016) of the source richresource language and target low-resource language into the same space. The transferred parser trained on the source language is used for the target low-resource language. We conduc
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2250741688	hese treebanks help each other has been shown an effective way to improve parsing performance in both the crosslingual-cross-domain parsing community and last year’s shared tasks (Ammar et al., 2016; Guo et al., 2015; Che et al., 2017; Shi et al., 2017b; Bjorkelund et al., 2017). In our system, we ap-¨ ply the simple concatenation to the treebanks that are potentially helpful to each other and explore different w
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2344508595	ided. There are also treebanks that comes from the same language families. Taking the advantages of the relation between treebanks has been shown a promising direction in both the research community (Ammar et al., 2016; Guo et al., 2015, 2016a) and in the CoNLL 2017 shared task (Che et al., 2017; Bjorkelund et al., 2017; Shi et al.,¨ 2017b). In our system, we adopt the treebank concatenation technique as Ammar et a
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2250741688	nation to improve the parser’s performance (x5). In dealing with the small languages and lowresource languages (x6), we adopt the word embedding transfer idea in the cross-lingual dependency parsing (Guo et al., 2015) and use the bilingual word vectors transformation technique (Smith et al., 2017)1 to map fasttext2 word embeddings (Bojanowski et al., 2016) of the source richresource language and target low-resourc
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2740215900	need to note that training the tagger and parser includes W(ELMo). To avoid overﬁtting, we impose a dropout function on projected vector W(ELMo) ELMo i during training. 4 Parser Ensemble According to Reimers and Gurevych (2017), neural network training can be sensitive to initialization and Liu et al. (2018a) shows that ensemble neural network trained with different initialization leads to performance improvements. We follo
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2754843079	ose tokenizations are non-trival. 7.1 Sentence Segmentation For some treebanks, sentence segmentation can be problematic since there is no explicitly sentence delimiters. de Lhoneux et al. (2017) and Shao (2017) presented a joint tokenization and sentence segmentation model8 that outperformed the baseline model in last year’s shared task (Zeman et al., 2017). We select a set of treebanks whose udpipe sentenc
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2100664567	a randomly sampled from the raw text released by the shared task for each language. Similar to Peters et al. (2018), we use the sample softmax technique to make training on large vocabulary feasible (Jean et al., 2015). However, we use a window of words14 surrounding the target word as negative samples and it shows better performance in our preliminary experiments. The training of ELMo on one language takes roughly
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2250741688	tenating treebanks from different languages only achieves improved performance on uk iu. This results also indicate that in cross lingual parsing, sophisticated methods like word embeddings transfer (Guo et al., 2015, 2016b) and treebank transfer (Guo et al., 2016a) are still necessary. 9.4 Effects of Better Preprocessing We also study how preprocessing contributes to the ﬁnal parsing performance. The experimenta
2833383446	Towards Better UD Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation.	2250741688	treebanks that comes from the same language families. Taking the advantages of the relation between treebanks has been shown a promising direction in both the research community (Ammar et al., 2016; Guo et al., 2015, 2016a) and in the CoNLL 2017 shared task (Che et al., 2017; Bjorkelund et al., 2017; Shi et al.,¨ 2017b). In our system, we adopt the treebank concatenation technique as Ammar et al. (2016) but limi
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2081835714	[14], as a winning submission to the 2013 Emotion Recognition in the Wild Challenge, used CNNs to recognize facial expressions.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2195242508	[23], for instance, constructed a sentiment image-caption dataset via crowdsourcing, where annotators were asked to include either positive sentiment (e.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1931639407	[8] detected words from visual regions and used a maximum entropy language model to generate candidate captions.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2041616772	It is around 7% better than the human performance (65 ± 5%) on the test set [11].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1514535095	In addition, we would like to explore alternative architectures for injecting facial emotions, like the soft injection approach of [37].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2041616772	As is apparent, these models usually employ CNNs with a fairly standard deep architecture to produce good results on the FER-2013 dataset [11], which is a large dataset collected ‘in the wild’.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2506483933	We apply METEOR for the learning rate decay and the model selection because it shows reasonable correlation with human judgments but calculates more quickly than SPICE (as it does not require dependency parsing) [2].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2097726431	ation into image captions [10,23,38]; they typically require the collection of a supplementary dataset, with a sentiment vocabulary derived from that, drawing from work in Natural Language Processing [25] where sentiment is usually characterized as one of positive, neutral or negative. Mathews et al. [23], for instance, constructed a sentiment image-caption dataset via crowdsourcing, where annotators
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1905882502	aximum entropy language model to generate candidate captions. Instead of using LSTMs, they utilized a re-ranking method called deep multi-modal similarity to select the captions. Karpathy and Fei-Fei [16] applied a region-based image captioning model consisting of two separate models to detect an image region and generate its corresponding caption. Johnson et al. [12], based on the work of Ren et al.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1996430422	Caption Analysis To analyze what it is about the captions themselves that differs under the various models, with respect to our aim of injecting information about emotional states of the faces in images, we first extracted all generated adjectives, which are tagged using the Stanford part-of-speech tagger software [33].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2254252455	he captions. Karpathy and Fei-Fei [16] applied a region-based image captioning model consisting of two separate models to detect an image region and generate its corresponding caption. Johnson et al. [12], based on the work of Ren et al. [28] on detecting image regions, incorporated the detection and generation tasks in an end-to-end training task. Attention mechanisms (either hard or soft) were appli
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2041616772	CNNs and linear support vector machines were trained to detect basic facial expressions by Tang [32], who won the 2013 FER challenge [11].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1480583224	ction approach, to recognize facial expressions using transfer learning. The face detection approach was applied to detect faces areas and remove irrelevant noises in the target samples. Kahou et al. [13] also used CNNs for extracting visual features together with audio features in a multi-modal framework. As is apparent, these models usually employ CNNs with a fairly standard deep architecture to pro
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1895577753	eparately. They made a joint multi-modal space to encode the information and a multi-modal log-bilinear model (in the form of a language model) to generate new captions. In comparison, Vinyals et al. [35] encoded image contents using a CNN and applied a LSTM to generate a caption for the image in an end-to-end neural network model. In general, the global encoding approaches generate captions according
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2154652894,2101105183,2133459682,2506483933	To evaluate Face-CapF and Face-CapL, we use standard evaluation metrics including BLEU [26], ROUGE-L [21], METEOR [5], CIDEr [34], and SPICE [2].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2130942839	This follows the paradigm employed in neural machine translation, using deep neural networks [31] to translate an image into a caption.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2246249023	hines were trained to detect basic facial expressions by Tang [32], who won the 2013 FER challenge [11]. In FER tasks, CNNs can be also used for transfer learning and feature extraction. Yu and Zhang [40] used CNNs, in addition to a face detection approach, to recognize facial expressions using transfer learning. The face detection approach was applied to detect faces areas and remove irrelevant noise
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2185175083	An image caption dataset that includes human faces which we have extracted from Flickr 30K dataset [39], which we term FlickrFace11K.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1956340063	ing every caption word. 4 Experiments 4.1 Evaluation Metrics and Testing To evaluate Face-Cap F and Face-Cap L, we use standard evaluation metrics including BLEU [26], ROUGE-L [21], METEOR [5], CIDEr [34], and SPICE [2]. All ve metrics with larger values mean better results. Face-Cap: Image Captioning using Facial Expression Analysis 9 Fig.2. The frameworks of Face-Cap F (top), and Face-Cap L (bottom)
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2246249023	ixels. We split the training set of FER-2013 into two sections after removing 11 completely black examples: 25,109 for training and 3589 for validating the model. Similar to other work in this domain [17,27,40], we use the private test set of FER-2013 for the performance evaluation of the model after the training phase. To compare with the related work, we do not apply the public test set either for trainin
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2195242508	A few models have incorporated sentiment or other non-factual information into image captions [10,23,38]; they typically require the collection of a supplementary dataset, with a sentiment vocabulary derived from that, drawing from work in Natural Language Processing [25] where sentiment is usually characterized as one of positive, neutral or negative.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1965947362	Now, there is a large body of work in recognizing basic facial expressions [9,29] most often using the framework of six purportedly universal emotions [6] of happiness, sadness, fear, surprise, anger, and disgust plus neutral expressions.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1895577753,1905882502,2254252455	splayed in the image in order to generate a meaningful description. Most of the state-of-the-art methods, including deep neural networks, generate captions that re ect the factual aspects of an image [3,8,12,16,20,35,37]; the emotional aspects which can provide richer and attractive image captions are usually ignored in this process. Emotional properties, including recognizing and expressing emotions, are required in
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1514535095,1931639407	Most of the state-of-the-art methods, including deep neural networks, generate captions that reflect the factual aspects of an image [3,8,12,16,20,35,37]; the emotional aspects which can provide richer and attractive image captions are usually ignored in this process.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2185175083	In the next step, we then use the model as a feature extractor on the images of FlickrFace11K, our extracted dataset from Flickr 30K [39].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2114524997	ster cat for a negative one); they do not aim to capture the emotional content of the image, as in Fig. 1. This distinction has been recognized in the sentiment analysis literature: the early work of [24], for instance, proposed a graph-theoretical method for predicting sentiment expressed by a text’s author by rst removing text snippets that are positive or negative in terms of the actual content of
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	1514535095	It also takes the image features which are extracted by Oxford VGGnet [30], learned on the ImageNet dataset, and weighted using the attention mechanism [37].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2041616772	To train our facial expression recognition model, we use the facial expression recognition 2013 (FER-2013) dataset [11].
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2185175083	To train our image captioning models, we have extracted a subset of the Flickr 30K dataset with image captions [39], which we term FlickrFace11K.
2854688924	Face-Cap: Image Captioning Using Facial Expression Analysis	2115252128	ubset of the Flickr 30K dataset with image captions [39], which we term FlickrFace11K. It contains 11,696 examples including human faces, which are detected using a CNN-based face detection algorithm [18].4 We observe that the Flickr 30K dataset is a good source for our dataset, because it has a larger portion of samples that include human faces, in comparison with other image caption datasets such as
2858442588	NMT-Keras: a Very Flexible Toolkit with a Focus on Interactive NMT and Online Learning	1816313093	rd replacement: Replace unknown words according to the attention model(Jeanetal.,2015).Thereplacementmayrelyonastatisticaldictionary. Tokenizing options: Including full support to byte-pair-encoding (Sennrich et al., 2016). Integration with other tools: SupportforSpearmint(Gelbartetal.,2914),forBayesian optimization of the hyperparameters and Tensorboard, the visualization toolofTensorFlow. Apartfromthesemodeloptions,N
2858681465	Formal Semantics of Architectural Decision Models.	2162923122	Architectural Decision Models between concepts and relations were usually shown with UML class diagrams. Even if they are presented in a form of mathematical relations with more detailed constraints [8], only the metamodel (i.e., the abstract syntax of the language) for documenting architectural decisions is dened, while the denition of the semantics of its elements is still missing. This way, pro
2858681465	Formal Semantics of Architectural Decision Models.	2159835652	eFor(a) 2M= :triggeredBy(a): (12) Denition 4 (Models). With Model we denote a set of all the architectural decision models as in denition 3. Our past experience with the formalization of UML models [12,13] has shown that it is highly dicult to avoid any aws in such denitions. The kinds of problems we were struggling with can be summarized with following informal questions: are all formulas correct? A
2858681465	Formal Semantics of Architectural Decision Models.	2001007953	equired? In the current work, to cope with these problems, the above mathematical denition of an architectural decision model has been worked out together with its specication in the Alloy language [14,15]. The specication is presented in Appendix A. The model expressed in Alloy was being veried with the Alloy Analyzer tool through model simulations, i.e., nding model instances. A number of instances
2858681465	Formal Semantics of Architectural Decision Models.	2159835652	erstanding the meaning of architectural decision models and relations that appear in those models. It opens possibility to reason on such models in a way similar to the one proposed for UML models in [12,13]. However, an issue related to reasoning about consistency of an architectural decision model has only Page 9 Marcin Szlenk Formal Semantics of Architectural Decision Models
2858681465	Formal Semantics of Architectural Decision Models.	2162923122	ne decision makes possible the other one), subsumes (one decision is wider than the other one) and con icts with (two decisions are mutually exclusive), but their semantics is not formally dened. In [8], decisions made are distinguished from decisions required from the architect. The proposed UML metamodel for capturing architectural decisions contains, among others, three core entities: ADIssue, AD
2858681465	Formal Semantics of Architectural Decision Models.	2162923122	r, the formal denitions of their semantics are missing. An attempt to formally dene the semantics of the elements of architectural decision models is presented in [11]. It borrows the notation from [8], introducing sets of architectural decision alternatives, issues and outcomes. The proposed semantics denes the meaning of an architectural decision as a set of software systems in which this decisi
2858681465	Formal Semantics of Architectural Decision Models.	1600334244,2162923122	s accompanied with illustrating diagrams [1{6]. In the latter case, architectural decision models focus on showing possible decisions and relationships between them within the decision making process [7,8]. In [7], architectural knowledge is dened as architecture design with design decisions, assumptions, context and other factors that determine a particular solution. A design decision comprises both
2858681465	Formal Semantics of Architectural Decision Models.	2100503493	s are presented in Section 6. The conclusions with plans for further work are outlined in Section 7. 2 Related Work Various methods and tools for architectural decision modeling have been surveyed in [9,10]. In general, the role of architectural decision models can be twofold. They can be used to document decisions that have already been made and/or to ontologize the decision making process in a given d
2858681465	Formal Semantics of Architectural Decision Models.	2162923122	he semantics of the whole architectural decision model. Each element on the model is considered separately. In Section 3, selected concepts and relationships from the architectural decision models in [8] and [11] are presented informally. They will be the starting point for further formalization of an architectural decision model. 3 Overview of Basic Concepts A simple example of an architectural deci
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2295030615	6 72.52 63.71 Baseline 18. 17. 20. 17. 45. 46. 50. 50. Table 5: Direct transfer results for CRC and MTX. Scores are macro-F1. Embeddings are BISKIP-100. to OOV words, they are still affected by them (Ma and Hovy, 2016; Mu¨ller et al., 2013). This “blurring effect” at test time then makes it more difﬁcult to detect exact argument component spans. While this is true in general, it is not true for punctuation symbols
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2609776793	ad been replaced by synonyms that did not occur in the train data. While systems using embeddings as input are more robust 4Our in-language results are slightly below our previous results reported in Eger et al. (2017) (table 6), where we obtained scores of 72-75% for token-level component extraction, even though the architecture is in principle the same. Reasons may be the different word embeddings used as well as
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2148708890	al. (2016). BISKIP is a variant of the standard skip-gram model which predicts mono- and cross-lingual contexts. It requires word alignments between parallel sentences and we use fast-align for this (Dyer et al., 2013). For EN↔ZH we train the same models on the UN corpus (Ziemski et al., 2016), which comprises &gt;11 million parallel sentences. We train embeddings of sizes 100 and 200. Projection To implement proje
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2609776793	ation, debate modeling, and law, among others (Peldszus and Stede, 2013a). Recent studies have successfully applied computational methods to analyze monological argumentation (Wachsmuth et al., 2017; Eger et al., 2017). Most of these studies view arguments as consisting of (at least) claims and premises—and so do we in this work. Thereby, our focus is on token-level argument component extraction, that is, the segme
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2609776793	ed representation to the word embedding. Our model is essentially the same as the ones proposed by Ma and Hovy (2016) and Lample et al. (2016); it is also a state-of-the-art model for monolingual AM (Eger et al., 2017). We name it BLCRF+char, when character information is included, and BLCRF when disabled. For all experiments, we use the same architectural setup: we use two LSTM hidden layers with 100 hidden units
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2251663606	ent extraction, that is, the segmentation and typing of argument components. AM has thus far almost exclusively been performed monolingually, e.g. in English (Mochales-Palau and Moens, 2009), German (Eckle-Kohler et al., 2015), or Chinese (Li et al., 2017). Working only monolingually is problematic, however, because AM is a difﬁcult task even for humans due to its dependence on background knowledge and parsing of complex p
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2251663606	f these approaches are speciﬁcally designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise scheme are available (Eckle-Kohler et al., 2015; Liebeck et al., 2016). Furthermore, Peldszus and Stede (2015) annotated a small German dataset with claims and premises and translated it to English subsequently. There are very few works studying A
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2295030615	low the model to learn a character-based representation (via another LSTM) and concatenate this learned representation to the word embedding. Our model is essentially the same as the ones proposed by Ma and Hovy (2016) and Lample et al. (2016); it is also a state-of-the-art model for monolingual AM (Eger et al., 2017). We name it BLCRF+char, when character information is included, and BLCRF when disabled. For all e
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2612953412	n, that 5This is a similar ﬁnding as in Daxenberger et al. (2017). 6We conducted a formal test if MTcan reliably be distinguished from HTin our setup. We trained a system (an adaptation of InferSent (Conneau et al., 2017)) to predict whether for an English original eand a second input sentence it could determine if the second input is a human or machine translation of e. The system’s performance of 54% accuracy (which
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2609776793	nents (Mochales-Palau and Moens, 2009; Habernal and Gurevych, 2017), recognizing argumentative discourse relations (Nguyen and Litman, 2016) or extracting argument components and relations endto-end (Eger et al., 2017). However, most of these approaches are speciﬁcally designed for English and there are only few resources for other languages. For German, a few datasets annotated according to the claim-premise schem
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2338266296	POS tagging and named-entity recognition (NER) are standard tasks in NLP. In recent years, there is increased interest not only in evaluating POS and NER models within multiple individual languages (Plank et al., 2016), but also cross-lingually (Zhang et al., 2016; Tsai et al., 2016; Mayhew et al., 2017; Yang et al., 2017). Two standard approaches are projection (Yarowsky et al., 2001; Das and Petrov, 2011; Ta¨ckst
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2251765408	rs of our word embeddings, wechoose to train them ourselves instead of using pre-trained ones. ForEN↔DE we induce bilingual word embeddings by training BIVCD(Vulic and Moens, 2015) and BISKIP models (Luong et al., 2015) on &gt;2 million aligned sentences from the Europarl corpus (Koehn, 2005). BIVCD concatenates bilingually aligned sentences (or documents), randomly shufﬂes each concatenation and trains a standard m
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2609414830	S tagging, and only few are devoted to NER (Mayhew et al., 2017; Tsai et al., 2016), aspect-based sentiment classiﬁcation (Lambert, 2015), or even more challenging problems such as discourse parsing (Braud et al., 2017). While POS tagging and NERare insome sense very similar to AM,namely, insofar asboth can be modeled assequential tagging of tokens, there are also important differences. For example, in POS tagging a
2875291789	Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!	2152691628	sides contributing new datasets, (3) we perform the ﬁrst evaluations of cross-lingual (token-level) AM, based on suitable adaptations of two popular cross-lingual techniques, namely, direct transfer (McDonald et al., 2011) and projection (Yarowsky et al., 2001). We ﬁnd that projection works considerably better than direct transfer and almost closes the cross-lingual gap, i.e., cross-lingual performance is almost on par
2876388465	Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings.	2126530744	alize, some call up images, e.g., torture calls up an emotional and even visual image. There are concrete things that are hard to visualize too, for example, abbey is harder to visualize than banana (Tsvetkov et al., 2014). Both notions have proven to be useful in computational linguistics as well.Turney et al.(2011) present a supervised model that exploits concreteness to correctly classify 79% of adjectivenoun pairs
2876388465	Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings.	2126530744	eady successful transfers within a language based on word embeddings (Tsvetkov et al.,2014;Rothe and Schutze¨ ,2016), the only cross-lingual transfer was based on transfer via bilingual dictionaries (Tsvetkov et al., 2014). In this paper we compare the effectiveness of cross-lingual transfer via word embeddings and via bilingual dictionaries. A byproduct of this research is a lexical resource in 77 languages containing
2876388465	Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings.	2017433125	reteness and imageability. 3 Data 3.1 Lexicons In our experiments we use two existing English and one Croatian lexicon with concreteness and imageability ratings. For English we use the MRC database (Wilson, 1988) (MRC onwards), consisting of 4,293 words with ratings for concreteness and imageability. The ratings range from 100 to 700 and were obtained by merging three different resources (Wilson,1988). We als
2883066278	Clustering Prominent People and Organizations in Topic-Specific Text Corpora.	2096765155	amed entities of other types such as “location” and “time” have been discarded. However, other named entity types should be considered in future extensions of the method. In this paper, Stanford NER (Finkel et al., 2005) is used in the first step in the proposed method. Stanford NER, which is based on an older mechanism of detecting named entities, is a highly accurate tagger that performs well on text extracted from
2883066278	Clustering Prominent People and Organizations in Topic-Specific Text Corpora.	2020278455	med Entity Recognition (NER) is the task of processing a body of text and classifying segments in the text that are named entities. Researchers have been attempting to solve this problem for decades (Nadeau and Sekine 2006). Several highly accurate and reliable algorithms for capturing named entities currently exist (Finkel, Grenager, and Manning 2005; Mccallum and Li 2003). Recent work in the area has focused on increa
2883066278	Clustering Prominent People and Organizations in Topic-Specific Text Corpora.	1570098300	is similarity feature may be used, for example, to capture and detect the semantic changes in the meaning of particular words over a pre-defined period of time (Hamilton, Leskovec, and Jurafsky 2016; Kulkarni et al. 2015). This application demonstrates the potential of using the new feature to solve new and challenging questions in the text-mining domain. The objective of this paper is to resolve one of these question
2883066278	Clustering Prominent People and Organizations in Topic-Specific Text Corpora.	2141099517	ttempting to solve this problem for decades (Nadeau and Sekine 2006). Several highly accurate and reliable algorithms for capturing named entities currently exist (Finkel, Grenager, and Manning 2005; Mccallum and Li 2003). Recent work in the area has focused on increasing the accuracy of named entity taggers by leveraging advances in neural networks (Chiu and Nichols 2016; Lample et al. 2016). However, these new model
2883115072	A novel ILP framework for summarizing content with high lexical variety	2251748188	2014b), improve student engagement and reducing student retention (Wen et al., 2014a; Rose and Siemens, 2014), and using language generation techniques to automatically generate feedback to students (Gkatzia et al., 2013). Our focus of this paper is to automatically summarizing student responses so that instructors can collect feedback in a timely manner. We expect the developed summarization techniques and result ana
2883115072	A novel ILP framework for summarizing content with high lexical variety	2122311631	2017), a state-of-the-art neural encoder-decoder approach for abstractive summarization. The system was trained on the CNN/Daily Mail data sets (Hermann et al., 2015; Nallapati et al., 2016). 5) ILP (Berg-Kirkpatrick et al., 2011), a baseline ILP framework without matrix completion. The Pointer-Generator Networks (See et al., 2017) describes a neural encoderdecoder architecture. It encourages the system to copy words from the
2883115072	A novel ILP framework for summarizing content with high lexical variety	2122311631	A2RN M, where A ij= 1 indicates the i-th concept appears in the j-th sentence, and A ij= 0 otherwise. In the literature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use ‘concept’ and ‘bigram’ interchangeably in this paper. Two sets of linear constraints are specied to ensure the ILP validity: (1) a concept is selected if and only i
2883115072	A novel ILP framework for summarizing content with high lexical variety	2148404145	ao Liu, and Diane Litman exploited for a number of text domains, including news articles (Barzilay et al., 1999; Dang and Owczarzak, 2008; Durrett et al., 2016; Grusky et al., 2018), product reviews (Gerani et al., 2014), online forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013), scientic articles (Teufel and Moens, 2002; Qazvinian et al., 2013), student course responses (Luo and Litman
2883115072	A novel ILP framework for summarizing content with high lexical variety	2335367650	arginal relevance (Carbonell and Goldstein, 1998), submodularity (Lin and Bilmes, 2010), integer linear programming (Gillick et al., 2008; Almeida and Martins, 2013; Li et al., 2013; Li et al., 2014; Durrett et al., 2016), minimizing reconstruction error (He et al., 2012), graph-based models (Erkan and Radev, 2004; Radev et al., 2004; Cohan and Goharian, 2015; Cohan and Goharian, 2017), determinantal point processes (
2883115072	A novel ILP framework for summarizing content with high lexical variety	2733902990	ber of text domains, including news articles (Barzilay et al., 1999; Dang and Owczarzak, 2008; Durrett et al., 2016; Grusky et al., 2018), product reviews (Gerani et al., 2014), online forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013), scientic articles (Teufel and Moens, 2002; Qazvinian et al., 2013), student course responses (Luo and Litman, 2015; Luo et al., 2016b), and many others. Sum
2883115072	A novel ILP framework for summarizing content with high lexical variety	2471640453,2575340443	concepts or not. An example of the annotation is shown in Table 2, where phrases in the student responses that are semantically 2 Publicly available at http://www.coursemirror.com/download/dataset2 (Luo et al., 2016a) A Novel ILP Framework for Summarization 11 \Forrest Gump&quot; is one of the best movies of all time, guaranteed. I just love this movie. It truly is amazing... What an amazing story and moving mea
2883115072	A novel ILP framework for summarizing content with high lexical variety	2471640453,2575340443	or cosine similarity of word embeddings (Goldberg and Levy, 2014) cannot serve our goal well. This manuscript extends our previous work on summarizing student course responses (Luo and Litman, 2015; Luo et al., 2016a; Luo et al., 2016b) submitted after each lecture via a mobile app named CourseMIRROR (Luo et al., 2015; Fan et al., 2015; Fan et al., 2017). The students are asked to respond to re ective prompts su
2883115072	A novel ILP framework for summarizing content with high lexical variety	2574248532	d to analyzing educationally-oriented language data. These studies seek to identify student leaders from MOOC discussion forums (Moon et al., 2014), perform sentiment analysis on student discussions (Wen et al., 2014b), improve student engagement and reducing student retention (Wen et al., 2014a; Rose and Siemens, 2014), and using language generation techniques to automatically generate feedback to students (Gkat
2883115072	A novel ILP framework for summarizing content with high lexical variety	2154652894	Document Understanding Conferences (DUC) and Text Analysis Conferences (TAC). For comparison, we select DUC 20043 to evaluate our approach (henceforth DUC04), which is widely used in the literature (Lin, 2004; Hong et al., 2014; Ren et al., 2016; Takase et al., 2016; Wang et al., 2016b). It consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire,
2883115072	A novel ILP framework for summarizing content with high lexical variety	1512874001	dularity (Lin and Bilmes, 2010), integer linear programming (Gillick et al., 2008; Almeida and Martins, 2013; Li et al., 2013; Li et al., 2014; Durrett et al., 2016), minimizing reconstruction error (He et al., 2012), graph-based models (Erkan and Radev, 2004; Radev et al., 2004; Cohan and Goharian, 2015; Cohan and Goharian, 2017), determinantal point processes (Taskar, 2012), neural networks and reinforcement le
2883115072	A novel ILP framework for summarizing content with high lexical variety	2154652894	E scores with lower computational cost. The results without using this low-frequency ltering are shown in the Appendix for comparison. In Table 8, we present summarization results evaluated by ROUGE (Lin, 2004) and human judges.6 To compare with the ocial participants in DUC 2004 (Paul and James, 2004), we selected the top-5 systems submitted in the competition (ranked by R-1), together with the 8 human an
2883115072	A novel ILP framework for summarizing content with high lexical variety	2251425698	ed sentences, and (iii) ensure the summary length does not exceed a limit. Previous work has largely focused on improving the estimation of concept weights in the ILP framework (Galanis et al., 2012; Qian and Liu, 2013; Boudin et al., 2015; Li et al., 2015; Durrett et al., 2016). However, distinct lexical items such as \bike elements&quot; and \bicycle parts&quot; are treated as dierent concepts and their weights
2883115072	A novel ILP framework for summarizing content with high lexical variety	2606974598	eger linear programming (ILP)-based summarization framework with a low-rank approximation of the co-occurrence matrix, and further evaluate the approach on a broad range of datasets exhibiting high 1 See et al. (2017) suggest that the pointer-generator model can copy 35% of summary sentences from the source documents. Similar ndings are also reported by Liao et al. (2018), where 99.6% of the summary unigrams, 95.2
2883115072	A novel ILP framework for summarizing content with high lexical variety	2122311631	enhancements to it to summarize text content with high lexical diversity. The ILP framework is shown to perform strongly on extractive summarization (Gillick and Favre, 2009; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011). It produces an optimal selection of sentences that (i) maximize the coverage of important concepts discussed in the source, (ii) minimize the redundancy in pairs of selected sentences, and (iii) ens
2883115072	A novel ILP framework for summarizing content with high lexical variety	2335367650	exceed a limit. Previous work has largely focused on improving the estimation of concept weights in the ILP framework (Galanis et al., 2012; Qian and Liu, 2013; Boudin et al., 2015; Li et al., 2015; Durrett et al., 2016). However, distinct lexical items such as \bike elements&quot; and \bicycle parts&quot; are treated as dierent concepts and their weights are not shared. In this paper we overcome this issue by propo
2883115072	A novel ILP framework for summarizing content with high lexical variety	2251384446	been explored, including maximal marginal relevance (Carbonell and Goldstein, 1998), submodularity (Lin and Bilmes, 2010), integer linear programming (Gillick et al., 2008; Almeida and Martins, 2013; Li et al., 2013; Li et al., 2014; Durrett et al., 2016), minimizing reconstruction error (He et al., 2012), graph-based models (Erkan and Radev, 2004; Radev et al., 2004; Cohan and Goharian, 2015; Cohan and Goharian
2883115072	A novel ILP framework for summarizing content with high lexical variety	2471640453,2575340443	forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013), scientic articles (Teufel and Moens, 2002; Qazvinian et al., 2013), student course responses (Luo and Litman, 2015; Luo et al., 2016b), and many others. Summarizing content contributed by multiple authors is particularly challenging. This is partly because people tend to use dierent expressions to convey the same semantic meaning
2883115072	A novel ILP framework for summarizing content with high lexical variety	2122311631	ILP Framework for Summarization 3 lexical diversity. The ILP framework, being extractive in nature, has demonstrated considerable success on a number of summarization tasks (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011). It generates a summary by selecting a set of sentences from the source documents. The sentences shall maximize the coverage of important source content, while minimizing the redundancy among themsel
2883115072	A novel ILP framework for summarizing content with high lexical variety	587839613	ith shorter sentences, there will be less context to leverage the low-rank approximation. Second, our proposed method works better on movie and peer reviews, but not camera reviews. As pointed out by Xiong (2015), both movie reviews and peer reviews are potentially more complicated than the camera reviews, as the review content consists of both the reviewer’s evaluations of the subject (e.g., a movie or paper
2883115072	A novel ILP framework for summarizing content with high lexical variety	2606974598	large amount of parallel data, yet the cost of annotating gold-standard summaries for most domains can be prohibitive. We validate the eectiveness of a state-of-the-art neural summarization system (See et al., 2017) on our collection of datasets and report results in x6.2. In this paper we focus on the integer linear programming-based summarization framework and propose enhancements to it to summarize text conte
2883115072	A novel ILP framework for summarizing content with high lexical variety	1843891098	if lexically-distinct but semantically-similar expressions are included in the summary. Existing neural encoder-decoder models may not work well at summarizing such content with high lexical variety (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017). On one hand, training the neural sequence-to-sequence models requires a large amount of parallel data. The cost of annotating goldstan
2883115072	A novel ILP framework for summarizing content with high lexical variety	1843891098	maries to understand how domain-specic factors may aect the applicability of the proposed approach. Neural summarization has seen promising improvements in recent years with encoder-decoder models (Rush et al., 2015; Nallapati et al., 2016). The encoder condenses the source text to a dense vector, whereas the decoder unrolls the vector to a summary sequence by predicting one word at a time. A number of studies h
2883115072	A novel ILP framework for summarizing content with high lexical variety	2182572585	marization algorithm. arXiv:1807.09671v1 [cs.CL] 25 Jul 2018 2 Wencan Luo, Fei Liu, Zitao Liu, and Diane Litman exploited for a number of text domains, including news articles (Barzilay et al., 1999; Dang and Owczarzak, 2008; Durrett et al., 2016; Grusky et al., 2018), product reviews (Gerani et al., 2014), online forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013), scientic articles (Teufel
2883115072	A novel ILP framework for summarizing content with high lexical variety	1544827683	mary; 4) Pointer-Generator Networks (PGN) (See et al., 2017), a state-of-the-art neural encoder-decoder approach for abstractive summarization. The system was trained on the CNN/Daily Mail data sets (Hermann et al., 2015; Nallapati et al., 2016). 5) ILP (Berg-Kirkpatrick et al., 2011), a baseline ILP framework without matrix completion. The Pointer-Generator Networks (See et al., 2017) describes a neural encoderdecod
2883115072	A novel ILP framework for summarizing content with high lexical variety	2471640453,2575340443	menon has also been observed in the news domain, where reporters use dierent nicknames, e.g., \Bronx Zoo&quot; and \New York Highlanders,&quot; to refer to the baseball team \New York Yankees.&quot; Luo et al., (2016b) report that about 80% of the document bigrams occur only once or twice for the news domain, whereas the ratio is 97% for student responses, suggesting the latter domain has a higher level of lexica
2883115072	A novel ILP framework for summarizing content with high lexical variety	2606974598	ncluded in the summary. Existing neural encoder-decoder models may not work well at summarizing such content with high lexical variety (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017). On one hand, training the neural sequence-to-sequence models requires a large amount of parallel data. The cost of annotating goldstandard summaries for many domains such as student responses can be
2883115072	A novel ILP framework for summarizing content with high lexical variety	2250327935	ncy in pairs of selected sentences, and (iii) ensure the summary length does not exceed a limit. Previous work has largely focused on improving the estimation of concept weights in the ILP framework (Galanis et al., 2012; Qian and Liu, 2013; Boudin et al., 2015; Li et al., 2015; Durrett et al., 2016). However, distinct lexical items such as \bike elements&quot; and \bicycle parts&quot; are treated as dierent concept
2883115072	A novel ILP framework for summarizing content with high lexical variety	2154652894	ng and CS2016 on ROUGE scores, but not other courses. ROUGE is often adopted in research papers to evaluate the quality of summarization because it is fast and is correlated well to human evaluation (Lin, 2004; Graham, 2015). However, it is also criticized that ROUGE cannot thoroughly capture the semantic similarity between system and reference summaries. Dierent alternatives have been proposed to enhance
2883115072	A novel ILP framework for summarizing content with high lexical variety	2150869743	ppearance of concepts in the summary. Each concept iis assigned a weight of w i, often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentence
2883115072	A novel ILP framework for summarizing content with high lexical variety	2251877085	pressions can share co-occurrence frequencies. Note that out-of-vocabulary expressions and domain-specic terminologies are abundant in our datasets, therefore simply calculating the lexical overlap (Rus et al., 2013) or cosine similarity of word embeddings (Goldberg and Levy, 2014) cannot serve our goal well. This manuscript extends our previous work on summarizing student course responses (Luo and Litman, 2015;
2883115072	A novel ILP framework for summarizing content with high lexical variety	2027049304	previous work on summarizing student course responses (Luo and Litman, 2015; Luo et al., 2016a; Luo et al., 2016b) submitted after each lecture via a mobile app named CourseMIRROR (Luo et al., 2015; Fan et al., 2015; Fan et al., 2017). The students are asked to respond to re ective prompts such as \describe what you found most interesting in today’s class&quot; and \describe what was confusing or needed more det
2883115072	A novel ILP framework for summarizing content with high lexical variety	2118733980	about the proposed summarization algorithm. arXiv:1807.09671v1 [cs.CL] 25 Jul 2018 2 Wencan Luo, Fei Liu, Zitao Liu, and Diane Litman exploited for a number of text domains, including news articles (Barzilay et al., 1999; Dang and Owczarzak, 2008; Durrett et al., 2016; Grusky et al., 2018), product reviews (Gerani et al., 2014), online forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013),
2883115072	A novel ILP framework for summarizing content with high lexical variety	2047756776	provided by Xiong and Litman (2014), consisting of 3 categories. The rst one is a subset of product reviews from a widely used data set in review opinion mining and sentiment analysis, contributed by Jindal and Liu (2008). In particular, it randomly sampled 3 set of reviews from a representative product (digital camera), each with 18 reviews from an individual product type (e.g. \summarizing 18 camera reviews for Niko
2883115072	A novel ILP framework for summarizing content with high lexical variety	2027049304	rable to automatically summarize the student feedback produced in online and oine environments, although it is only recently that a data collection eort to support such research has been initiated (Fan et al., 2015; Luo et al., 2015). In our data, one particular type A Novel ILP Framework for Summarization 9 Statistics Tasks Docs/task WC/task WC/sen Length Student response (Eng)* 36 49 375.4 9.1 30 Student resp
2883115072	A novel ILP framework for summarizing content with high lexical variety	2150869743	ramming-based summarization framework and propose enhancements to it to summarize text content with high lexical diversity. The ILP framework is shown to perform strongly on extractive summarization (Gillick and Favre, 2009; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011). It produces an optimal selection of sentences that (i) maximize the coverage of important concepts discussed in the source, (ii) minimize the
2883115072	A novel ILP framework for summarizing content with high lexical variety	2606974598	rce text to a dense vector, whereas the decoder unrolls the vector to a summary sequence by predicting one word at a time. A number of studies have been proposed to deal with out-of-vocabulary words (See et al., 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), avoid generating repetitive words (See et al., 2017; Suzuki and Nagata, 2017), adjust summary length (Kikuch
2883115072	A novel ILP framework for summarizing content with high lexical variety	2606974598	red with its title or a few highlights. On the other hand, the summaries produced by existing neural encoder-decoder models are far from perfect. The summaries are mostly extractive with minor edits (See et al., 2017)1, contain repetitive words and phrases (Suzuki and Nagata, 2017) and may not accurately reproduce factual details (Cao et al., 2018; Song et al., 2018). We examine the performance of a state-of-the-a
2883115072	A novel ILP framework for summarizing content with high lexical variety	2471640453,2575340443	s. For DUC04, we construct a matrix for each document cluster instead of the entire corpus due to its high computational cost. 6 The results on Eng are slightly dierent from the results published by Luo et al. (2016b) as we used leave-one-lecture-out cross-validation instead of 3-fold cross-validation to select the parameter . We also changed the order of student responses by grouping same responses together, a
2883115072	A novel ILP framework for summarizing content with high lexical variety	1962684803	a single document or a cluster of documents related to a particular topic. Various techniques have been explored, including maximal marginal relevance (Carbonell and Goldstein, 1998), submodularity (Lin and Bilmes, 2010), integer linear programming (Gillick et al., 2008; Almeida and Martins, 2013; Li et al., 2013; Li et al., 2014; Durrett et al., 2016), minimizing reconstruction error (He et al., 2012), graph-based m
2883115072	A novel ILP framework for summarizing content with high lexical variety	2150869743	the source texts. A Novel ILP Framework for Summarization 3 lexical diversity. The ILP framework, being extractive in nature, has demonstrated considerable success on a number of summarization tasks (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011). It generates a summary by selecting a set of sentences from the source documents. The sentences shall maximize the coverage of important source content, while minimiz
2883115072	A novel ILP framework for summarizing content with high lexical variety	2609482285	summary sequence by predicting one word at a time. A number of studies have been proposed to deal with out-of-vocabulary words (See et al., 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), avoid generating repetitive words (See et al., 2017; Suzuki and Nagata, 2017), adjust summary length (Kikuchi et al., 2016), encode long text (Celikyilmaz et al., 2018; Cohan et a
2883115072	A novel ILP framework for summarizing content with high lexical variety	2027049304	uo and Litman, 2015), available at the link: http://www.coursemirror. com/download/dataset. The remaining three courses are collected by us using a mobile application, CourseMIRROR (Luo et al., 2015; Fan et al., 2015) and then the reference summaries for each course are created by human annotators with the proper background. The human annotators are allowed to create abstract summaries using their own words in add
2883115072	A novel ILP framework for summarizing content with high lexical variety	2335367650	v:1807.09671v1 [cs.CL] 25 Jul 2018 2 Wencan Luo, Fei Liu, Zitao Liu, and Diane Litman exploited for a number of text domains, including news articles (Barzilay et al., 1999; Dang and Owczarzak, 2008; Durrett et al., 2016; Grusky et al., 2018), product reviews (Gerani et al., 2014), online forum threads (Tarnpradab et al., 2017), meeting transcripts (Liu and Liu, 2013), scientic articles (Teufel and Moens, 2002; Qazv
2883115072	A novel ILP framework for summarizing content with high lexical variety	2176263492	ve words (See et al., 2017; Suzuki and Nagata, 2017), adjust summary length (Kikuchi et al., 2016), encode long text (Celikyilmaz et al., 2018; Cohan et al., 2018) and improve the training objective (Ranzato et al., 2016; Paulus et al., 2017; Guo et al., 2018). To date, these studies focus primarily on singledocument summarization and headline generation. This is partly because training neural encoder-decoder models
2883115072	A novel ILP framework for summarizing content with high lexical variety	2089391273	work is also dierent from the traditional approaches using dimensionality reduction techniques such as non-negative matrix factorization (NNMF) and latent semantic analysis (LSA) for summarization (Wang et al., 2008; Lee et al., 2009; Conroy et al., 2013; Conroy and Davis, 2015; Wang et al., 2016a). In particular, Wang et al. (2008) use NNMF to group sentences into clusters; Conroy et al. (2013) explore NNMF and
2883115072	A novel ILP framework for summarizing content with high lexical variety	2293931116	y length does not exceed a limit. Previous work has largely focused on improving the estimation of concept weights in the ILP framework (Galanis et al., 2012; Qian and Liu, 2013; Boudin et al., 2015; Li et al., 2015; Durrett et al., 2016). However, distinct lexical items such as \bike elements&quot; and \bicycle parts&quot; are treated as dierent concepts and their weights are not shared. In this paper we overc
2883544151	Concurrent Learning of Semantic Relations.	2068737686	er of positive examples. As such, semi-supervised learning proposes a much more interesting framework where unlabeled word pairs can massively3 be obtained through selected lexico-syntactic patterns (Hearst, 1992) or paraphrase alignments (Dias et al.,2010). To test our hypotheses, we propose a self-learning strategy where conﬁdently tagged unlabeled word pairs are iteratively added to the labeled dataset. Pre
2883544151	Concurrent Learning of Semantic Relations.	2106137280	with yis based on the distributional representation of these words following the distributional hypothesis (Harris, 1954), i.e. on the separate contexts of xand y. Earlier works developed symmetric (Dias et al., 2010) and asymmetric (Kotlerman et al.,2010) similarity measures based on discrete representation vectors, followed by numerous supervised learning strategies for a wide range of semantic relations (Baroni
2883603666	Recent Advances in Deep Learning: An Overview.	2428112462	(2017)) • query classiﬁcation (Shi et al., 2016b) • sentence classiﬁcation (Kim, 2014) • sentence modelling (Kalchbrenner et al., 2014) • word processing (Mikolov et al., 2013a) • premise selection (Alemi et al., 2016) • document and sentence processing (Le and Mikolov (2014), Mikolov et al. (2013b)) • generating image captions (Vinyals et al. (2014), Xu et al. (2015)) 15 • photographic style transfer (Luan et al.,
2883603666	Recent Advances in Deep Learning: An Overview.	1947481528	• generating textures and stylized images (Ulyanov et al., 2016) • visual andtextual question answering (Xiong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15)) • visual recognition anddescription (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014)) • object detection (Lee et al. (2017), Ranzato et al. (2011), Redmon et al. (2015), Liu et al. (2015)) • document processing (Hinton and Salakhutdinov, 2
2883603666	Recent Advances in Deep Learning: An Overview.	2214429195	(Graves et al., 2014). 5.9.3 NeuralRandom-AccessMachines Kurach et al. (2015) proposed Neural Random Access Machine, which uses an external variable-size random-access memory. 5.9.4 NeuralProgrammer Neelakantan et al. (2015) proposed Neural Programmer, an augmented neural network with arithmetic and logic functions. 5.9.5 NeuralProgrammer-Interpreters Reed and de Freitas (2015) proposed Neural Programmer-Interpreters (NP
2883603666	Recent Advances in Deep Learning: An Overview.	648143168	). Some more improvements proposed for GAN byMao et al. (2016), Kim et al. (2017) etc. Salimans et al. (2016) presented several methods for training GANs. 6.5.1 LaplacianGenerativeAdversarialNetworks Denton et al. (2015) proposed a Deep Generative Model (DGM) called Laplacian Generative Adversarial Networks (LAPGAN) using Generative Adversarial Networks (GAN) approach. The model also uses convolutional networks withi
2883603666	Recent Advances in Deep Learning: An Overview.	2293453011	016) • image colorization (Zhang et al., 2016b) • image question answering (Yang et al., 2015) • generating textures and stylized images (Ulyanov et al., 2016) • visual andtextual question answering (Xiong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15)) • visual recognition anddescription (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014)) • object detection (Lee et al. (2017), Ranzato et al. (20
2883603666	Recent Advances in Deep Learning: An Overview.	2467604901	16) 5.1.1 Variational Autoencoders Variational Auto-Encoders (VAE) can be counted as decoders (Wang). VAEs are built upon standard neural networks and can be trained with stochastic gradient descent (Doersch, 2016) 5.1.2 StackedDenoising Autoencoders In early Auto-Encoders (AE), encoding layer had smaller dimensions than the input layer. In Stacked Denoising Auto-Encoders (SDAE), encoding layer is wider than th
2883603666	Recent Advances in Deep Learning: An Overview.	2120615054	2015), Amodei et al. (2015)) • text-to-speech generation (Wang et al. (2017b), Arik et al. (2017)) • query classiﬁcation (Shi et al., 2016b) • sentence classiﬁcation (Kim, 2014) • sentence modelling (Kalchbrenner et al., 2014) • word processing (Mikolov et al., 2013a) • premise selection (Alemi et al., 2016) • document and sentence processing (Le and Mikolov (2014), Mikolov et al. (2013b)) • generating image captions (Viny
2883603666	Recent Advances in Deep Learning: An Overview.	1836465849	7.4 Deep Residual Learning He et al. (2015) proposed Deep Residual Learning framework for Deep Neural Networks (DNN), which are called ResNets with lower training error (He). 7.5 Batch Normalization Ioﬀe and Szegedy (2015) proposed Batch Normalization, a method for accelerating deep neural network training by reducing internal covariate shift. Ioﬀe (2017) proposed Batch Renormalization extending the previous approach.
2883603666	Recent Advances in Deep Learning: An Overview.	2613904329	al. (2015) proposed a CNN architecture named YOLO (You Only Look Once) for uniﬁed and real-time object detection. Zeiler and Fergus (2013) proposed a method for visualizing the activities within CNN. Gehring et al. (2017) proposed a CNN architecture for sequence-to-sequence learning. Bansal et al. (2017) proposed PixelNet, using pixels for representations. Goodfellow et al. (2016) explained the basic CNN architecures
2883603666	Recent Advances in Deep Learning: An Overview.	2193145675	ALMBZP15)) • visual recognition anddescription (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014)) • object detection (Lee et al. (2017), Ranzato et al. (2011), Redmon et al. (2015), Liu et al. (2015)) • document processing (Hinton and Salakhutdinov, 2011) • character motion synthesis and editing (Holden et al., 2016) • singing synthesis (Blaauw and Bonada, 2017) • person identiﬁcation (Li et al.,
2883603666	Recent Advances in Deep Learning: An Overview.	1995997122	ann Machines (RBM) for document processing. 6.3 Deep Belief Networks Deep Belief Networks (DBN) are generative models with several layers of latent binary or real variables (Goodfellow et al., 2016). Ranzato et al. (2011) built a deep generative model using Deep Belief Network (DBN) for images recognition. 6.4 Deep LambertianNetworks Tang et al. (2012) proposed Deep Lambertian Networks (DLN) which is a multilayer gene
2883603666	Recent Advances in Deep Learning: An Overview.	2580175322	arning Reinforcement learning uses reward and punishment system for the next move generated by the learning model. This is mostly used for games and robots, solves usually decision making 4 problems (Li, 2017). Schmidhuber (2014) described advances of deep learning in Reinforcement Learning (RL) and uses of Deep Feedforward Neural Netowrk (FNN) and Recurrent Neural Network (RNN) for RL. Li (2017) discussed
2883603666	Recent Advances in Deep Learning: An Overview.	2279098554	ayers. The architecture used Graphics Processing Units (GPU) for convolution operation, Rectiﬁed Linear Units (ReLU) as activation function and Dropout (Srivastava et al., 2014) to reduce overﬁtting. Iandola et al. (2016) proposed a small CNN architecture called SqueezeNet. Szegedy et al. (2014) proposed a Deep CNN architecture named Inception. An improvement of Inception-ResNet is proposed by Dai et al. (2017). Redmo
2883603666	Recent Advances in Deep Learning: An Overview.	2163922914	d on many challenges of Deep Learning e.g. scaling algorithms for larger models and data, reducing optimization diﬃculties, designing eﬃcient scaling methods etc. along with optimistic DL researches. Bengio et al. (2013) discussed on Representation and Feature Learning aka Deep Learning. They explored various methods and models from the perspectives of applications, techniques and challenges. Deng (2011) gave an over
2883603666	Recent Advances in Deep Learning: An Overview.	2004935438	d generate maps, then apply non-linear activation function. Max-pooling layers downsample images and keep the maximum value of a sub-region. And fully-connected layers does the linear multiplication (Masci et al., 2013a). In Deep MPCNN, convolutional and max-pooling layers are used periodically after the input layer, followed by fully-connected layers (Giusti et al., 2013). 5.2.2 Very DeepConvolutionalNeuralNetwork
2883603666	Recent Advances in Deep Learning: An Overview.	2260756217	d Neural Netowrk (FNN) and Recurrent Neural Network (RNN) for RL. Li (2017) discussed Deep Reinforcement Learning(DRL), its architectures e.g. Deep Q-Network (DQN), and applications in various ﬁelds. Mnih et al. (2016) proposed a DRL framework using asynchronous gradient descent for DNN optimization. van Hasselt et al. (2015) proposeda DRLarchitecture usingdeep neuralnetwork (DNN). 5. Deep NeuralNetworks In this se
2883603666	Recent Advances in Deep Learning: An Overview.	2173051530	d Neural Turing Machine (NTM) architecture, consisting of a neural network controller and a memory bank. NTMs usually combine RNNs with external memory bank (Olah and Carter, 2016). 9 5.9.2 NeuralGPU Kaiser and Sutskever (2015) proposed Neural GPU, which solves the parallel problem of NTM (Graves et al., 2014). 5.9.3 NeuralRandom-AccessMachines Kurach et al. (2015) proposed Neural Random Access Machine, which uses an extern
2883603666	Recent Advances in Deep Learning: An Overview.	1983364832	da, 2017) • person identiﬁcation (Li et al., 2014) • face recognition and veriﬁcation (Taigman et al., 2014) • action recognition in videos (Simonyan and Zisserman, 2014a) • human action recognition (Ji et al., 2013) • action recognition (Sharma et al., 2015) • classifying and visualizing motion capture sequences (Cho and Chen, 2013) • handwriting generation and prediction (Carter et al., 2016) • automated andmac
2883603666	Recent Advances in Deep Learning: An Overview.	2742947407	They described DL methods and approaches in great ways as well as their applications and directions for future research. Here, we are going to brief some outstanding overview papers on deep learning. Young et al. (2017) talked about DL models and architectures, mainly used in Natural Language Processing (NLP). They showed DL applications in various NLP ﬁelds, compared DL models, and discussed possible future trends.
2883603666	Recent Advances in Deep Learning: An Overview.	2333796428	or and easily trained with Residual Learning. More deeper ResNets achieve more better performance (He). ResNets are considered an important advance in the ﬁeld of Deep Learning. 5.5.1 Resnetin Resnet Targ et al. (2016) proposed Resnet in Resnet (RiR) which combines ResNets (He et al., 2015) and standard Convolutional Neural Networks (CNN) in a deep dual stream architecture (Targ et al., 2016). 5.5.2 ResNeXt Xie et
2883603666	Recent Advances in Deep Learning: An Overview.	1810943226	elds. Such as - • image classiﬁcation and recognition (Simonyan and Zisserman (2014b), Krizhevsky et al. (2012), He et al. (2015)) • video classiﬁcation (Karpathy et al., 2014) • sequence generation (Graves, 2013) • defect classiﬁcation (Masci et al., 2013b) • text, speech, image and video processing (LeCun et al., 2015) • text classiﬁcation (Conneau et al., 2016) • speech processing (Arel et al., 2009) • spee
2883603666	Recent Advances in Deep Learning: An Overview.	2295130376	er (Luan et al., 2017) • natural image manifold (Zhu et al., 2016) • image colorization (Zhang et al., 2016b) • image question answering (Yang et al., 2015) • generating textures and stylized images (Ulyanov et al., 2016) • visual andtextual question answering (Xiong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15)) • visual recognition anddescription (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014
2883603666	Recent Advances in Deep Learning: An Overview.	113709255	es. Bengio et al. (2013) discussed on Representation and Feature Learning aka Deep Learning. They explored various methods and models from the perspectives of applications, techniques and challenges. Deng (2011) gave an overview of deep structured learning and its architectures from the perspectives of information processing and related ﬁelds. Arel et al. (2010) provided a short overview on recent DL techniq
2883603666	Recent Advances in Deep Learning: An Overview.	2601564443	g. Iandola et al. (2016) proposed a small CNN architecture called SqueezeNet. Szegedy et al. (2014) proposed a Deep CNN architecture named Inception. An improvement of Inception-ResNet is proposed by Dai et al. (2017). Redmon et al. (2015) proposed a CNN architecture named YOLO (You Only Look Once) for uniﬁed and real-time object detection. Zeiler and Fergus (2013) proposed a method for visualizing the activities
2883603666	Recent Advances in Deep Learning: An Overview.	1613249581	gorized with machine learning approaches, and uses of deep learning in the neural networks. Deng and Yu (2014) described deep learning classes and techniques, and applications of DL in several areas. Bengio (2013) did quick overview on DL algorithms i.e. supervised and unsupervised networks, optimization and training models from the perspective of representation learning. He focused on many challenges of Deep
2883603666	Recent Advances in Deep Learning: An Overview.	2525778437	ion (Sharma et al., 2015) • classifying and visualizing motion capture sequences (Cho and Chen, 2013) • handwriting generation and prediction (Carter et al., 2016) • automated andmachine translation (Wu et al. (2016), Cho et al. (2014), Bahdanau et al. (2014), Hermann et al. (2015), Luong et al. (2015)) • named entity recognition (Lample et al., 2016) • mobile vision (Howard et al., 2017) • conversational agents
2883603666	Recent Advances in Deep Learning: An Overview.	1995997122	iong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15)) • visual recognition anddescription (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014)) • object detection (Lee et al. (2017), Ranzato et al. (2011), Redmon et al. (2015), Liu et al. (2015)) • document processing (Hinton and Salakhutdinov, 2011) • character motion synthesis and editing (Holden et al., 2016) • singing synthesis (Blaauw and Bonada,
2883603666	Recent Advances in Deep Learning: An Overview.	2193413348	ken language understanding(Hinton et al. (2012), Zhang et al. (2015b), Zhang et al. (2016c), Zhang et al. (2016c), Zhang et al. (2015a), Shi et al. (2016a), Mesnil et al. (2015), Peng and Yao (2015), Amodei et al. (2015)) • text-to-speech generation (Wang et al. (2017b), Arik et al. (2017)) • query classiﬁcation (Shi et al., 2016b) • sentence classiﬁcation (Kim, 2014) • sentence modelling (Kalchbrenner et al., 2014)
2883603666	Recent Advances in Deep Learning: An Overview.	1836465849	ller model. 7.7 Layer Normalization Ba et al. (2016) proposed Layer Normalization, for speeding-up training of deep neural networksespecially for RNNsand solves the limitations ofbatch normalization (Ioﬀe and Szegedy, 2015). 14 8. Deep Learning frameworks Thereare a good numberof open-source libraries and frameworks available for deep learning. Most of them are built for python programming language. Such as Theano (Berg
2883603666	Recent Advances in Deep Learning: An Overview.	2149933564	long as well. For example, Nguyen et al. (2014) showed that Deep Neural Networks (DNN) can be easily fooled while recognizing images. There are other issues like transferability of features learned (Yosinski et al., 2014). Huang et al. (2017) proposed an architecture for adersarial attacks on neural networks, where they think future works are needed for defenses against those attacks. Zhang et al. (2016a) presented an
2883603666	Recent Advances in Deep Learning: An Overview.	1923186697	n recognition in videos (Simonyan and Zisserman, 2014a) • human action recognition (Ji et al., 2013) • action recognition (Sharma et al., 2015) • classifying and visualizing motion capture sequences (Cho and Chen, 2013) • handwriting generation and prediction (Carter et al., 2016) • automated andmachine translation (Wu et al. (2016), Cho et al. (2014), Bahdanau et al. (2014), Hermann et al. (2015), Luong et al. (201
2883603666	Recent Advances in Deep Learning: An Overview.	2408279554	ncorporates memory mechanism into Convolutional Neural Networks (CNN). It augments convolutional residual networks with a long short term memory mechanism (Moniz and Pal, 2016). 5.19 Fractal Networks Larsson et al. (2016) proposed Fractal Networks i.e. FractalNet, as an alternative to residual nets. They claimed to train ultra deep neural networks without residual learning. Fractals are repeated architecture generated
2883603666	Recent Advances in Deep Learning: An Overview.	2102605133	oling layer instead of fully connected layers. Deep NIN architectures can be made from multi-stacking of this proposed NIN structure (Lin et al., 2013). 5.4 Region-based Convolutional Neural Networks Girshick et al. (2014) proposedRegion-based Convolutional Neural Network (R-CNN) which uses regions for recognition. R-CNN uses regions to localize and segment objects. This architecture consists of three modules i.e. cate
2883603666	Recent Advances in Deep Learning: An Overview.	1821462560	ormalization, a method for accelerating deep neural network training by reducing internal covariate shift. Ioﬀe (2017) proposed Batch Renormalization extending the previous approach. 7.6 Distillation Hinton et al. (2015) proposed Distillation, from transferring knowledge from ensemble of highly regularized models i.e. neural networks into compressed and smaller model. 7.7 Layer Normalization Ba et al. (2016) proposed
2883603666	Recent Advances in Deep Learning: An Overview.	2439078092	osed Deep Neural Support Vector Machines (DNSVM), which uses Support Vector Machine (SVM) as the top layer for classiﬁcation in a Deep Neural Network (DNN) 5.18 Convolutional Residual Memory Networks Moniz and Pal (2016) proposed Convolutional Residual Memory Networks, which incorporates memory mechanism into Convolutional Neural Networks (CNN). It augments convolutional residual networks with a long short term memor
2883603666	Recent Advances in Deep Learning: An Overview.	1947481528	osed Highway Long Short-Term Memory (HLSTM) RNN, which extends deep LSTM networks with gated direction connections i.e. Highways, between memory cells in adjacent layers. 5.16 Long-Term Recurrent CNN Donahue et al. (2014) proposed Long-term Recurrent Convolutional Networks (LRCN), which uses CNN for inputs, then LSTM for recurrent sequence modeling and generating predictions. 11 5.17 Deep Neural SVM Zhang et al. (2015
2883603666	Recent Advances in Deep Learning: An Overview.	2064675550	osed Neural Programmer-Interpreters (NPI) which can learn. NPI consists of recurrent core, program memory and domain-speciﬁc encoders (Reed and de Freitas, 2015). 5.10 Long Short Term Memory Networks Hochreiter and Schmidhuber (1997) proposed Long Short-Term Memory (LSTM) which overcomes the error back-ﬂow problems of Recurrent Neural Networks (RNN). LSTM is based on recurrentnetwork along with gradient-based learningalgorithm (H
2883603666	Recent Advances in Deep Learning: An Overview.	113709255	pattern recognition (Deng and Yu, 2014). Deep Learning i.e. Representation Learning is class or sub-ﬁeld of Machine Learning. Recent deep learning methods are mostly said to be developed since 2006 (Deng, 2011). This paper is an overview of most recent techniques of deep learning, mainly recommended for upcoming researchers in this ﬁeld. This article includes the basic idea of DL, major approaches and metho
2883603666	Recent Advances in Deep Learning: An Overview.	2104839068	randmasters in strategical and other games, from only hours of training. For example, AlphaGo and AlphaGo Zero for game of GO (Silver et al. (2017b), Silver et al. (2016), Dong et al. (2017)), Dota2 (Batsford (2014)), Atari (Mnih et al. (2013),Mnih et al. (2015), van Hasselt et al. (2015)), Chess and Shougi (Silver et al., 2017a). 10. Discussion Though Deep Learning has achieved tremendous success in many areas,
2883603666	Recent Advances in Deep Learning: An Overview.	2138806976	RBM) are special type of Markov random ﬁeld containing one layer of stochastic hidden units i.e. latent variables and one layer of observable variables (Deng and Yu (2014), Goodfellow et al. (2016)). Hinton and Salakhutdinov (2011) proposed a Deep Generative Model using Restricted Boltzmann Machines (RBM) for document processing. 6.3 Deep Belief Networks Deep Belief Networks (DBN) are generative models with several layers of la
2883603666	Recent Advances in Deep Learning: An Overview.	2004935438	and recognition (Simonyan and Zisserman (2014b), Krizhevsky et al. (2012), He et al. (2015)) • video classiﬁcation (Karpathy et al., 2014) • sequence generation (Graves, 2013) • defect classiﬁcation (Masci et al., 2013b) • text, speech, image and video processing (LeCun et al., 2015) • text classiﬁcation (Conneau et al., 2016) • speech processing (Arel et al., 2009) • speech recognition and spoken language understa
2883603666	Recent Advances in Deep Learning: An Overview.	2102605133	roposals which deﬁnes the set of candidate regions, large Convolutional Neural Network (CNN) for extracting features from the regions, and a set of class speciﬁc linear Support Vector Machines (SVM) (Girshick et al., 2014). 5.4.1 Fast R-CNN Girshick (2015) proposed Fast Region-based Convolutional Network (Fast R-CNN). This method exploits R-CNN (Girshick et al., 2014) architecture and produces fast results. Fast R-CNN
2883603666	Recent Advances in Deep Learning: An Overview.	1951216520	roposed later to solve this problem. Goodfellow et al. (2016) provided details of Recurrent and Recursive Neural Networks and architectures, its variants along with related gated and memory networks. Karpathy et al. (2015) used character-level language models for analyzing and visualizing predictions, representations training dynamics, and error types of RNN and its variants e.g. LSTMs. Jo´zefowicz et al. (2016) explor
2883603666	Recent Advances in Deep Learning: An Overview.	2171810632	t al. (2014), Xu et al. (2015)) 15 • photographic style transfer (Luan et al., 2017) • natural image manifold (Zhu et al., 2016) • image colorization (Zhang et al., 2016b) • image question answering (Yang et al., 2015) • generating textures and stylized images (Ulyanov et al., 2016) • visual andtextual question answering (Xiong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15)) • visual recognition anddescription (
2883603666	Recent Advances in Deep Learning: An Overview.	2432004435	ted against an adversary i.e. a discriminative model to learn model or data distribution (Goodfellow et al., 2014). Some more improvements proposed for GAN byMao et al. (2016), Kim et al. (2017) etc. Salimans et al. (2016) presented several methods for training GANs. 6.5.1 LaplacianGenerativeAdversarialNetworks Denton et al. (2015) proposed a Deep Generative Model (DGM) called Laplacian Generative Adversarial Networks
2883603666	Recent Advances in Deep Learning: An Overview.	2138806976	tion (Donahue et al. (2014), Razavian et al. (2014), Oquab et al. (2014)) • object detection (Lee et al. (2017), Ranzato et al. (2011), Redmon et al. (2015), Liu et al. (2015)) • document processing (Hinton and Salakhutdinov, 2011) • character motion synthesis and editing (Holden et al., 2016) • singing synthesis (Blaauw and Bonada, 2017) • person identiﬁcation (Li et al., 2014) • face recognition and veriﬁcation (Taigman et al
2883603666	Recent Advances in Deep Learning: An Overview.	2525778437	TM creates a channel of information exchange between LSTMs using Variational Auto-Encoders (VAE), for learning better representations (Shabanian et al., 2017). 5.11 Googles Neural Machine Translation Wu et al. (2016) proposed Googles Neural Machine Translation (GNMT) System for automated translation, which incorporates an encoder network, a decoder network and an attention network following the common sequence-to
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2064675550	(2) where ht and ht−1 represent the current and previous hidden states, xt denotes the current LSTM input and here we use the embedding wt of the current wordwt, andW andU denote the weight matrices [17]. In order to take advantage of the bi-sense emoji embedding, we modify the input layer into the LSTM units. We first obtain the senti-emoji embedding as an weighted average of the bi-sense emoji embe
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2122522916	27, 48]. For instance, [18, 48] pre-define sentiment labels to emoticons and construct a emoticon-sentiment dictionary. [27] applies emoticons for smoothing noisy sentiment labels. Similar work from [31] first considers emoji as a component in extracting the lexical feature for further sentiment analysis. [32] constructs an emoji sentiment ranking based on the occurrences of emojis and the human-anno
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2604944277	with over 50% of the Instagram posts containing one or more emojis [11] and 92% of the online population using emojis [40]. The extensive use of emojis has drawn a growing attention from researchers [19, 25] because the emojis convey fruitful semantical and sentimental information to visually complement the textual information which is significantly useful in understanding the embedded emotional signals
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	1614298861,2493916176	attention, and sentiment classification via the proposed attention-based LSTM networks. 3.1 Bi-sense Embedding Recent research shows great success in word embedding task such as word2vec and fasttext [7, 29]. We use fasttext to initialize emoji embeddings by considering each emoji as a special word, together with word embeddings. The catch is, different from conventional approaches where each emoji respo
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2046682605,2048783874	bedding approaches including skip-grams, continuous bag-of-words (CBoW) and skip-thoughts [7, 22, 29, 30]. It was until recent years when researchers start focusing on image and multimodal sentiments [8, 46] and analyzing how to take advantage of the cross-modality resources [44, 45]. For multimodal sentiment analysis, an underlying assumption is that both modalities express similar sentiment and such si
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2099813784	ce that contains sufficient emoji-tweets with sentiment labels, we construct our own emoji-tweets dataset by automatically generating weak labels using a rule-based sentiment analysis algorithm Vader [20] for pre-traning the networks, and manually labeling a subset of tweets for fine tuning and testing purposes. The experimental results demonstrate that the bi-sense emoji embedding is capable of extra
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2099813784	ch emoji, of which one is for the particular emoji used in positive sentimental contexts and the other one is for this emoji used in negative sentimental contexts (text sentiment initialized by Vader [20], details will be discussed in Section 4.1), respectively; the same fasttext training process is used to embed each token into a distinct vector, and we thus obtain the positivesense and negative-sens
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2139511937,2395693197	cons) are extensively used. Non-verbal cues of sentiment, such as emoticon which is considered as the previous generation of emoji, has been studied for their sentiment effect before emojis take over [18, 27, 48]. For instance, [18, 48] pre-define sentiment labels to emoticons and construct a emoticon-sentiment dictionary. [27] applies emoticons for smoothing noisy sentiment labels. Similar work from [31] fir
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2267835966	d semantics for emojis where each emoji is embedded into two distinct vectors, namely positive-sense and negative-sense vector, respectively. For our specific task which is Twitter sentiment analysis [23, 38], we initialize the bi-sense embedding vectors together with word embedding vectors using word embedding algorithm fasttext [7] by extracting two distinct embeddings for each emoji according to the se
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	1486649854,1614298861,2153579005,2493916176	d and each sentiment and phase-level features (n-grams and unigrams) [34, 43], to deep neural network based embedding approaches including skip-grams, continuous bag-of-words (CBoW) and skip-thoughts [7, 22, 29, 30]. It was until recent years when researchers start focusing on image and multimodal sentiments [8, 46] and analyzing how to take advantage of the cross-modality resources [44, 45]. For multimodal sent
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2493916176	ectively. For our specific task which is Twitter sentiment analysis [23, 38], we initialize the bi-sense embedding vectors together with word embedding vectors using word embedding algorithm fasttext [7] by extracting two distinct embeddings for each emoji according to the sentiment of its corresponding textual contexts, namely bi-senseembedding. A long short-term memory (LSTM) based recurrent neural
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	1853947067	such as flags and sports, are widely used in daily communications to express people’s feelings 1. Since their first release in 2010, emojis have taken the place of emoticons (such as “:-)” and “:-P”) [37] to create a new form of language for social media users [4]. According to recent science reports, there are 2,823 emojis in unicode standard in Emoji 11.0 2, with over 50% of the Instagram posts cont
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2604944277	i. Therefore, previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts [19], or when the emojis convey multiple 1Real time emoji tracker: http://emojitracker.com/ 2https://emojipedia.org/emoji-11.0/ arXiv:1807.07961v2 [cs.CL] 7 Aug 2018 Table 1: Tweet examples with emojis. T
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2527467788	information which is significantly useful in understanding the embedded emotional signals in texts [6]. For example, emoji embeddings have been proposed to understand the semantics behind the emojis [12, 26], and the embedding vectors can be used to visualize and predict emoji usages given their corresponding contexts. Previous work also shows that, it is useful to pre-train a deep neural network on an e
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2608224443	is. 4.2 Sentiment Analysis Models We set up the baselines and proposed models as follows: LSTM with text embedding: CNNs and LSTMs are widely used to encode textual contents for sentiment analysis in [10, 41] and many online tutorials. Here we select the standard LSTM with pre-trained Table 3: Twitter Sentiment Analysis. Models AA-Sentiment HA-Sentiment Precision Recall ROC Area Accuracy F1 Score Precisio
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2527467788	lexical- and dictionary-based approaches lacks insightful understanding of the complexed semantics of emojis. Therefore, inspired by the success of word semantic embedding algorithms such as [7, 30], [12] obtains semantic embeddings of each emoji by averaging the words from its descriptions 4 and shows it is effective to take advantage of the emoji embedding for the task of Twitter sentiment analysis.
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	782228145	ment Analysis Sentiment analysis is to extract and quantify subjective information including the status of attitudes, emotions and opinions from a variety of contents such as texts, images and audios [47]. Sentiment analysis has been drawing great attentions because of its wide applications in business and government intelligence, political science, sociology and psychology [2, 3, 13, 33]. From a tech
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2153579005,2493916176	mojis by lexical- and dictionary-based approaches lacks insightful understanding of the complexed semantics of emojis. Therefore, inspired by the success of word semantic embedding algorithms such as [7, 30], [12] obtains semantic embeddings of each emoji by averaging the words from its descriptions 4 and shows it is effective to take advantage of the emoji embedding for the task of Twitter sentiment ana
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	38739846	n the occurrences of emojis and the human-annotated sentiments of the corresponding tweets where each emoji is assigned with a sentiment score from negative to positive 3, similar to the SentiWordNet [14]. However, the relatively intuitive use of emojis by lexical- and dictionary-based approaches lacks insightful understanding of the complexed semantics of emojis. Therefore, inspired by the success of
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	1010415138,2250418535,2420099857	a novel scheme that consists of an attention-based recurrent neural network (RNN) with robust bi-sense emoji embeddings. Inspired by the word sense embedding task in natural language processing (NLP) [21, 24, 39] where each sense of an ambiguous word responds to one unique embedding vector, the proposed bi-sense embedding is a more robust and fine-grained representation of the complicated semantics for emojis
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2153579005	ositive-sense embeddings are paired with red circles, negative-sense embeddings are paired with green circles, andtheirsubtractionsarepairedwithyellowcircles,respectively. Best viewed when zoomed in. [30]. The positive-sense of emoji ( and ), and the negative-sense of emoji ( , and ) are embedded far from the two main clusters as observed in Figure 4(a), suggesting that the semantics of these emojis a
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2097726431	is With the overwhelming development of Internet of Things (IOT), the growing accessibility and popularity of subjective contents have provided new opportunities and challenges for sentiment analysis [35]. For example, social medias such as Twitter and Instagram have been explored because the massive user-generated contents with rich user sentiments [1, 16, 34] where emojis (and emoticons) are extensi
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	40549020,1743243001,2250489604	rtunities and challenges for sentiment analysis [35]. For example, social medias such as Twitter and Instagram have been explored because the massive user-generated contents with rich user sentiments [1, 16, 34] where emojis (and emoticons) are extensively used. Non-verbal cues of sentiment, such as emoticon which is considered as the previous generation of emoji, has been studied for their sentiment effect
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	40549020,2022204871	s including keywords [5, 36] where each word corresponds to a sentiment vector with entries representing the possibility of the word and each sentiment and phase-level features (n-grams and unigrams) [34, 43], to deep neural network based embedding approaches including skip-grams, continuous bag-of-words (CBoW) and skip-thoughts [7, 22, 29, 30]. It was until recent years when researchers start focusing on
2883853499	Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM	2099813784	ss than ten words, and contents including the urls, mentions, and emails. Data Annotation For acquiring the sentiment annotations, we first useVader which is a rule-based sentiment analysis algorithm [20] for text tweets only togenerateweaksentimentlabels.Thealgorithmoutputssentiment scores ranging from -1 (negative) to 1 (positive) with neutral in the middle.Weconsiderthesentimentanalysisasabinarycla
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1689711448	. The internal workings of nodes in RvNNs (RNNs included) may vary. Single neurons may be used or complex networks may be used. One highly engineered RNN is the long short-term memory (LSTM) network [Greff et al. 2017;Hochreiter and Schmidhuber1997]. In LSTMs, the recursive nodes are composed of several individual neurons (or in some variants small ANNs) connected in a manner designed to retain specific informatio
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2147880316	-level inputs and word embeddings. The inputs were combined and then fed to a bidirectional LSTM, whose outputs were in turn fed to a layer that performed conditional random field (CRF) computations [Lafferty et al. 2001]. The model, when trained using dropout, obtained state-of-the-art performance in both German and Spanish. The LSTM-CRF model was also very close in both English and Dutch. The main claim of this stu
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2197913429	, making it an extremely robust model. Jozefowicz et al. [2016] tested a number of architectures including some mentioned above, and a few others producing character level outputs [Chelba et al. 2013;Ji et al. 2015;Shazeer et al. 2015;Williams et al. 2015]. Whereas many of these models had only been tested on small scale language modeling, this study tested them on large scale language modeling, testing them wi
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2153579005	, phrases, sentences, or documents at some level. The term meaning is difficult to define, and linguists and philosophers have been debating about it for centuries. Word embeddings, such as Word2Vec [Mikolov et al. 2013a,b] and GloVe [Pennington et al. 2014], claim to capture meanings of words, following the Distributional Hypothesis of Meaning [Harris1954]. As a corollary, when vectors corresponding to phrases, sen
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2162390675	he 1940s [McCulloch and Pitts1943] as computational devices to capture human intelligence. Recent increases in computational power and parallelization, harnessed by Graphical Processing Units (GPUs) [Coates et al. 2013;Raina et al. 2009], now allow for &quot;deep learning&quot;, which utilizes ANNs, sometimes with billions of trainable parameters [Goodfellow et al. 2016]. Additionally, the contemporary availability
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1514535095	to allow information to pass through. These gates control i) the input, ii) the recurrent internal state, and iii) the output. simply altering which parts of it particular examples are attentive to [Xu et al. 2015;Yang et al. 2016]. In another line of work, a slightly simpler variant of the LSTM, called the Gated Recurrent Unit (GRU), has been shown to perform as well as or better than standard LSTMs in many N
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2063524507	c-eager approach [Nivre2003,2004], words are connected to their parents as soon as possible, regardless of whether or not their children are all connected to them. Finally, in the Swap-Lazy approach [Nivre et al. 2009], the arc-standard approach is modified to allow swapping of positions on the stack. This makes the graphing of non-projective edges possible. :14 Otter, Medina, and Kalita 3.3.1 Early Neural Parsing
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1957740064	and decoder using (a) recurrent layer(s) [Donahue et al. 2015;Venugopalan et al. 2014]. Recent approaches use hierarchical recurrent networks to perform this task [Baraldi et al. 2017;Pan et al. 2016;Yu et al. 2016]. These networks implement recurrent layers over several frames, and then implement another such layer over the Deep Learning in Natural Language Processing :25 final outputs of the previous layer, r
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2101390659	e number of extractive summarization algorithms have been proposed over the years. These include frequency-based approaches [Edmundson1969;Luhn1958]; machine-learning (naive Bayes) -based algorithms [Kupiec et al. 1995]; and graph-based algorithms, computing centrality measure [Radev et al. 2004] and relative importance of sentences [Erkan and Radev2004] using the PageRank algorithm [Page et al. 1999]. Abstractive
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2410539690	ed the usefulness of CNNs for the task. Sun et al. [2018] used an attention-based GRU model with a copy mechanism. This network was novel in its use of a data structure known as a coverage mechanism [Tu et al. 2016], Deep Learning in Natural Language Processing :21 which helped to ensure that all important information was extracted and that it was not extracted multiple times. 4.2 Text Classification Another cl
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1917432393	el. Jozefowicz et al. [2016] tested a number of architectures including some mentioned above, and a few others producing character level outputs [Chelba et al. 2013;Ji et al. 2015;Shazeer et al. 2015;Williams et al. 2015]. Whereas many of these models had only been tested on small scale language modeling, this study tested them on large scale language modeling, testing them with the Billion Word Benchmark. The most e
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2164019165	el performed better, noting the relationships between the stems, but also accounting for other features such as the prefix &quot;un&quot;. The model was also tested on several other popular datasets [Huang et al. 2012;Miller and Charles1991;Rubenstein and Goodenough1965], significantly outperforming previous embedding models on all. A good morphological analyzer is often important in the context of larger linguist
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2117130368	end deep neural networks. The first use of such a model [Kalchbrenner and Blunsom2013] stemmed from the success of continuous recurrent representations in capturing syntax, semantics, and morphology [Collobert and Weston 2008] in addition to the ability of recurrent neural networks to build robust language models [Mikolov et al. 2010]. This original NMT model used a combination of generative convolutional and recurrent la
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	655477013	ents [Jurafsky and Martin2000]. Historically, abstractive summarization algorithms have included graph-based algorithms [Ganesan et al. 2010] as well as parse tree and graph-to-graph transformations [Liu et al. 2015]. More recently—and with increasing success—deep learning methods have been used for abstractive summarization. Deep learning approaches generally use recurrent encoder–decoder architectures. Breakin
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1869752048	er, images were compared with those in a database and were each assigned the caption of the most similar image. The epitome of neural captioning models, Google’s Neural Image Caption (NIC) algorithm [Vinyals et al. 2015b], used a deep CNN, trained for image classifications purposes, as the encoder and an LSTM network for decoding. Karpathy et al. [2015] developed an algorithm capturing alignments between image regio
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2120615054	esting English sentence completion, responding to Chinese tweets, and comparing English sentence meaning. The approach outperformed a number of existing models. Building on prior work [Hu et al. 2014;Kalchbrenner et al. 2014;Socher et al. 2011], Yin and Schütze [2015] proposed a Bi-CNN-MI (MI for multigranular interaction features), consisting of a pretrained CNN sentence model, a CNN interaction model, and a logistic re
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1494910745	ever, typically it is desirable to evaluate language models independently of the applications in which they appear. A number of metrics have been proposed, but no perfect solution has yet been found [Chen et al. 1998;Clarkson and Robinson2001;Iyer et al. 1997]. The most commonly used metric is perplexity, which is the inverse probability of a test set normalized by the number of words. Perplexity is a reasonable
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2142222368	further extended this work by testing several similar models on the Wall Street Journal dataset, the Web English Treebank dataset, and the Question Treebank data, as well as datasets from CoNLL 2009 [Hajič et al. 2009] for a number of languages. State-of-the-art performance was achieved in both UAS and LAS on the three English datasets as well as for most of the languages tested from CoNLL 2009. Another model was
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2155870214	kernel sizes for filters varied (in this case they were three and five). All of the networks were tested on three different datasets: the Penn Treebank, Europarl-NC [Bojar et al. [n. d.]], and ukWaC [Baroni et al. 2009]. The results of this study showed that stacking convolutional layers was actually detrimental in language modeling, but that both MLPConv and COM were effective at reducing the perplexity on each of
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2594229957	on labels, part-of-speech tags, and syntactic dependency labels has been shown to improve models, and concatenating or factorizing these with embeddings has been shown to increase robustness further [Sennrich et al. 2017;Sennrich and Haddow2016]. For remembering long-term dependencies, vertically stacked recurrent units have been the standard, with the optimum number of layers having been determined to be roughly bet
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1850668662	language models independently of the applications in which they appear. A number of metrics have been proposed, but no perfect solution has yet been found [Chen et al. 1998;Clarkson and Robinson2001;Iyer et al. 1997]. The most commonly used metric is perplexity, which is the inverse probability of a test set normalized by the number of words. Perplexity is a reasonable measurement for LMs trained on the same dat
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2250539671	at some level. The term meaning is difficult to define, and linguists and philosophers have been debating about it for centuries. Word embeddings, such as Word2Vec [Mikolov et al. 2013a,b] and GloVe [Pennington et al. 2014], claim to capture meanings of words, following the Distributional Hypothesis of Meaning [Harris1954]. As a corollary, when vectors corresponding to phrases, sentences, or other components of text ar
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2148461049	lly, the contemporary availability of large datasets, facilitated by sophisticated data collection processes, enables the training of such deep architectures via their associated learning algorithms [Ciresan et al. 2011;LeCun et al. 2015;Schmidhuber2015]. In recent years, researchers and practitioners in natural language processing have leveraged the power of modern artificial neural networks with many propitious re
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2171810632	mation to pass through. These gates control i) the input, ii) the recurrent internal state, and iii) the output. simply altering which parts of it particular examples are attentive to [Xu et al. 2015;Yang et al. 2016]. In another line of work, a slightly simpler variant of the LSTM, called the Gated Recurrent Unit (GRU), has been shown to perform as well as or better than standard LSTMs in many NLP tasks [Cho et
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2557264465	mbined between the standard encoder and decoder using (a) recurrent layer(s) [Donahue et al. 2015;Venugopalan et al. 2014]. Recent approaches use hierarchical recurrent networks to perform this task [Baraldi et al. 2017;Pan et al. 2016;Yu et al. 2016]. These networks implement recurrent layers over several frames, and then implement another such layer over the Deep Learning in Natural Language Processing :25 final o
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2251957808	ncoder, and a multidomain encoder. The third model was a one-to-many model, using a single encoder, but separate decoders for each domain. Each model was trained on the &quot;OVERNIGHT&quot; dataset [Wang et al. 2015]. Exceptional results were achieved for all models, with a state-of-the-art performance exhibited by the one-to-one model. Similar conclusions were drawn in a recently published study by Brunner et a
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2130942839	nd recurrent layers to encode and optimize a source language model and cast this into a target language. Numerous novel and effective advances to this model have since been made [Bahdanau et al. 2014;Sutskever et al. 2014], as derived models are continually improving, finding answers to the shortcomings of their predecessors and overcoming any need for hand engineering [Britz et al. 2017]. Recent progress includes eff
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2153653739	nment of cultural sensitivities, for both of the languages (and associated societies) under consideration [Jurafsky and Martin2000]. Historically, this work included statistical approaches to phrase [Koehn et al. 2003], syntax [Yamada and Knight2001], and word-based translation [Brown et al. 1993], each having its own limitations. Recent attempts at this task use neural machine translation (NMT), which uses end-to
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2624614404	o aggregate correlations among them. For input, the RNs took final LSTM representations of document sentences. These inputs were further paired with a representation of the information request given [Santoro et al. 2017]. The RN considered Deep Learning in Natural Language Processing :23 all permutations to determine if there were any relationships among the sentences in a given document, or between those sentences
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2251818205	o the matrix locations representing the relations between the words in each pair. In addition to testing on the SICK dataset, they tested on MSRVID, SemEval 2014 Task 10 [Agirre et al. 2014], WikiQA [Yang et al. 2015], and TreeQA [Wang et al. 2007]. State-of-the-art results were attained on all of the above. 3.4.2 Sentence Modeling. While the comparison of the meanings of sentences and phrases is useful, it in la
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2067438047	ologically aware. An RvNN was used to model the morphological structure of English words. A neural language model was then placed on top of the RvNN. The model was trained on the WordSim-353 dataset [Finkelstein et al. 2001] and segmentation was performed using Morfessor [Creutz and Lagus2007]. An additional dataset of rare words was generated, as most available datasets do a poor job of modeling these words. Two models
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2103305545	ompletion, responding to Chinese tweets, and comparing English sentence meaning. The approach outperformed a number of existing models. Building on prior work [Hu et al. 2014;Kalchbrenner et al. 2014;Socher et al. 2011], Yin and Schütze [2015] proposed a Bi-CNN-MI (MI for multigranular interaction features), consisting of a pretrained CNN sentence model, a CNN interaction model, and a logistic regressor. Following
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	179875071	ontinuous recurrent representations in capturing syntax, semantics, and morphology [Collobert and Weston 2008] in addition to the ability of recurrent neural networks to build robust language models [Mikolov et al. 2010]. This original NMT model used a combination of generative convolutional and recurrent layers to encode and optimize a source language model and cast this into a target language. Numerous novel and e
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1939882552	ough generation-style abstraction, possibly using words never seen in the documents [Jurafsky and Martin2000]. Historically, abstractive summarization algorithms have included graph-based algorithms [Ganesan et al. 2010] as well as parse tree and graph-to-graph transformations [Liu et al. 2015]. More recently—and with increasing success—deep learning methods have been used for abstractive summarization. Deep learnin
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2170738476	re performed, testing English sentence completion, responding to Chinese tweets, and comparing English sentence meaning. The approach outperformed a number of existing models. Building on prior work [Hu et al. 2014;Kalchbrenner et al. 2014;Socher et al. 2011], Yin and Schütze [2015] proposed a Bi-CNN-MI (MI for multigranular interaction features), consisting of a pretrained CNN sentence model, a CNN interaction
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2120432001	and Pitts1943] as computational devices to capture human intelligence. Recent increases in computational power and parallelization, harnessed by Graphical Processing Units (GPUs) [Coates et al. 2013;Raina et al. 2009], now allow for &quot;deep learning&quot;, which utilizes ANNs, sometimes with billions of trainable parameters [Goodfellow et al. 2016]. Additionally, the contemporary availability of large datasets
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1947481528	reated nearly identically, analyzing a number of frames rather than a single image [Ballas et al. 2015]. The frames are combined between the standard encoder and decoder using (a) recurrent layer(s) [Donahue et al. 2015;Venugopalan et al. 2014]. Recent approaches use hierarchical recurrent networks to perform this task [Baraldi et al. 2017;Pan et al. 2016;Yu et al. 2016]. These networks implement recurrent layers ov
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2194775991	the Rectified Linear Unit (ReLU) [Nair and Hinton2010], which do not exhibit regions that are arêtically steep or have bosonically small gradients. Also in response to this issue, as well as others [He et al. 2016], residual connections are often used. Such connections are simply those that skip layers (usually one). If used in every alternating layer, this cuts in half the number of layers through which the g
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2147880316	refers to the identification of proper nouns as well as information such as dates, times, prices, and product IDs. Until recently, support vector machines [Vapnik2013] and conditional random fields [Lafferty et al. 2001] were the most commonly used approaches for NER. The multi-task approach of Collobert et al. [2011] included the task, although no results were reported. In their approach, a simple feedforward netwo
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1970849810	roach used an RNN to create a directed acyclic graph. While this model did produce results within 2% of the state of the art (on the Wall Street Journal portion of the CoNLL 2008 Shared Task dataset [Surdeanu et al. 2008]), by the time it reached the end of a sentence, it seemed to have difficulty remembering phrases from early in the sentence. 3.3.2 Transition-Based Dependency Parsing. The basis for most major recen
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2148374900	rs. These include frequency-based approaches [Edmundson1969;Luhn1958]; machine-learning (naive Bayes) -based algorithms [Kupiec et al. 1995]; and graph-based algorithms, computing centrality measure [Radev et al. 2004] and relative importance of sentences [Erkan and Radev2004] using the PageRank algorithm [Page et al. 1999]. Abstractive summaries rely on expressing documents’ contents through generation-style abst
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2103305545	s parse trees. In recursive networks, a single tensor (or a generalized matrix) of weights can be used at a low level in the tree, and then used recursively at successively higher levels of the tree [Socher et al. 2011]. Since nodes in RvNNs are dependent on their previous results, and therefore feed back to themselves, RvNNs are not considered to be feedforward. 2.2.3 Recurrent Neural Networks. A simple type of re
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1997131858	saved in relational databases [Cowie and Lehnert1996]. Early methods included the use of simplistic information classification, pattern matching, and grammar methods to create rule-based approaches [Andersen et al. 1992;Salton and Harman2003]. Current information retrieval systems use machine learning algorithms of various kinds—supervised and unsupervised. Commonly extracted information includes named entities and
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2153579005	shed study by Brunner et al. [2018]. This study also has some of the most significant implications for the entire field of natural language processing since Mikolov’s arithmetic with word embeddings [Mikolov et al. 2013b]. In this study, several LSTM-based encoder–decoder networks were created, and the embedding vectors produced were analyzed. A single encoder accepting English sentences as input was used, as were f
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2147800946	tain layers in which the nodes receive input from only some of the nodes in previous layers, and therefore are not fully connected. One such type of network is the convolutional neural network (CNN) [LeCun et al. 1989,1998], built upon Fukashima’s [1980;1982] neocognitron. Deriving their name from the convolution operation in mathematics and signal processing, convolutional neural networks use an assortment of dif
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2273041409	tandard encoder and decoder using (a) recurrent layer(s) [Donahue et al. 2015;Venugopalan et al. 2014]. Recent approaches use hierarchical recurrent networks to perform this task [Baraldi et al. 2017;Pan et al. 2016;Yu et al. 2016]. These networks implement recurrent layers over several frames, and then implement another such layer over the Deep Learning in Natural Language Processing :25 final outputs of the pr
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2006969979	ties) under consideration [Jurafsky and Martin2000]. Historically, this work included statistical approaches to phrase [Koehn et al. 2003], syntax [Yamada and Knight2001], and word-based translation [Brown et al. 1993], each having its own limitations. Recent attempts at this task use neural machine translation (NMT), which uses end-to-end deep neural networks. The first use of such a model [Kalchbrenner and Bluns
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2120735855	ting the relations between the words in each pair. In addition to testing on the SICK dataset, they tested on MSRVID, SemEval 2014 Task 10 [Agirre et al. 2014], WikiQA [Yang et al. 2015], and TreeQA [Wang et al. 2007]. State-of-the-art results were attained on all of the above. 3.4.2 Sentence Modeling. While the comparison of the meanings of sentences and phrases is useful, it in large part relies on being able t
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	1843891098	tion to in accordance with the current hidden state and annotation. Such a mechanism is present in Figure5b. Many variants of the mechanism have been introduced, popular ones including convolutional [Rush et al. 2015], intra-temporal [Paulus et al. 2017], gated [Wang et al. 2017], and self-attention [Vaswani et al. 2017]. Deep neural networks are usually trained via backpropagation [Rumelhart et al. 1985]. This m
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2251861449	was trained and evaluated on three different datasets: MSRP, the Sentences Involving Compositional Knowledge (SICK) dataset [Marelli et al. 2014], and the Microsoft Video Paraphrase Corpus (MSRVID) [Agirre et al. 2012]. State-of-the-art results were achieved on the first and the third, and nearly matched on the second. Ablation studies showed that a number of elements such as POS tags, multiple pooling operations,
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2626778328	ure5b. Many variants of the mechanism have been introduced, popular ones including convolutional [Rush et al. 2015], intra-temporal [Paulus et al. 2017], gated [Wang et al. 2017], and self-attention [Vaswani et al. 2017]. Deep neural networks are usually trained via backpropagation [Rumelhart et al. 1985]. This method computes the error at the output nodes, and makes updates to weights on all edges connecting to the
2884159461	A Survey of the Usages of Deep Learning in Natural Language Processing.	2171810632	work of Duygulu et al. [2002]. The introduction of neural networks to the field has led to significant advancements in recent years. The first neural models were template based [Kulkarni et al. 2013;Yang et al. 2016] and search based [Devlin et al. 2015a,b]. In the prior, aspects of images were recognized and the different words describing these aspects were then combined in standardized grammatical templates. I
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2022204871	Bing Liu&apos;s Opinion Lexicon [4]. It contains a set of 2006 positive and 4783 negative sentiment words that have a score of -1 for negative and +1 for positive words. ii)TheMPQASubjectivityLexicon [30].Itcontainsalistof8222 English words with scores ranging from -2 to +2 indicating intensity of negative and positive sentiment. iii) Sentiment140 Lexicon [13]. This lexicon contains a real valued sent
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2162010436	context. The details on how the scores are computed can be found in [13]. iv) NRC Hashtag Sentiment Lexicon [13]. This is same as the Sentiment140 Lexicon, but only for hashtags. v) NRCEmotionLexicon [17].Thislexiconhasabinaryvalueof positive and negative sentiment for English words. For a given tweet, using all the lexicons mentioned above we computed minimum, maximum, average and summation of positi
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	1782003667	developed classiers on both scope token recognition and exact scope matching for in domain testing but not on a different domain. The authors notethatwhentestedonadifferenttestsetfromSimpleWikipedia, [29]&apos;s model built on constituency-based features performed better. In this work, to compare we pick the most recent CRF based model proposed by [24], we could not compare with [5] because the model
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2156413587	einthescopeofacue and do not consider them anymore for the next cue scope resolution. Punctuation Based This heuristic is used as a well-known baseline in existing negation scope detection literature [13]. According to this heuristic, all the words after a cue until a punctuation is considered to be in scope. In this work we use f;: :;?!g as punctuations. Conditional Random Field Based Wealsousearecen
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2252065932	il a punctuation is considered to be in scope. In this work we use f;: :;?!g as punctuations. Conditional Random Field Based WealsousearecentConditionalRandomField(CRF)basedmachine learning algorithm [24] for negation scope detection. In [24], authors trained a CRF model by utilizing several dependency parse treebasedfeatures:i)POStagofthe1storderdependencyhead,ii) number of edges to nearest negation
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2043335066	ion algorithm. Negation scope detection Initial explorations on negation scope detection were performed mostly in Biomedical domain where it is important to distinguish facts from negative assertions [26] in medical reports, biological abstracts and full papers. [18] presents a two steps approach: rst, a decision tree to predict negation cues, followed by a CRF metalearner to predict the scope of nega
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	1964613733	n, disgust, fear, joy, sadness, surprise, and trust). Sentiment lexicon features: We used the following v e sentiment lexicons to derive additional set of features. i) Bing Liu&apos;s Opinion Lexicon [4]. It contains a set of 2006 positive and 4783 negative sentiment words that have a score of -1 for negative and +1 for positive words. ii)TheMPQASubjectivityLexicon [30].Itcontainsalistof8222 English
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2105827650	negation scope detection were performed mostly in Biomedical domain where it is important to distinguish facts from negative assertions [26] in medical reports, biological abstracts and full papers. [18] presents a two steps approach: rst, a decision tree to predict negation cues, followed by a CRF metalearner to predict the scope of negations that combines input from k-nearestneighborclassication,as
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2162010436	nhandlingtherewillbe&quot;happy&quot;and&quot;NOT_happy&quot;asdictionary entries. Emotion lexicon features: We count the number of words in each of the 8 emotion classes from the NRC emotion lexicon [17] (anger, anticipation, disgust, fear, joy, sadness, surprise, and trust). Sentiment lexicon features: We used the following v e sentiment lexicons to derive additional set of features. i) Bing Liu&apo
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2146516605	nversations or customer service conversations. In this work, we leverage the taxonomy proposed by [20]. In 2005, Ivanovic explored interactions between customers and agents in instant messaging chats [7, 8] using a set of of 12 coursegrained dialogue acts. Afterwards Kim et al. focus on classifying dialogue acts in both one-on-one and multi-party live instant messaging chats [10, 11]. In 2017, Soraby et
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	1843248874,2139686600,2252065932	o explored [22] for scope resolution. The rule-based approach uses POS tags and constituentcategorylabelsbasedheuristicrules.Ontheotherhand, machinelearningusesSVMbasedrankingofsyntacticconstituents. [3, 16, 23, 24] proposed CRF-based sequence labeling leveraging features from dependency tree. [21] uses hand-crafted heuristics to traverse Minimal Recursion Semantics (MRS). [5] show that a neural network based mo
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	1591056869,2109029380,2130705166	on, and backward-looking function. Later on Jurafsky et. al [9] developed a less ne-grained taxonomy of42tagsbasedonDAMSL.Overtheyears,therehasbeenseveral works on developing dialogue acts taxonomies [1, 14, 25, 27, 20] for general conversations or customer service conversations. In this work, we leverage the taxonomy proposed by [20]. In 2005, Ivanovic explored interactions between customers and agents in instant m
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2156413587	ontext or not. Authors of the lexicon using &quot;NEG&quot; and &quot;NEGFIRST&quot; tag to indicate a word being used in a negated context. The details on how the scores are computed can be found in [13]. iv) NRC Hashtag Sentiment Lexicon [13]. This is same as the Sentiment140 Lexicon, but only for hashtags. v) NRCEmotionLexicon [17].Thislexiconhasabinaryvalueof positive and negative sentiment for En
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	245104810	rner to predict the scope of negations that combines input from k-nearestneighborclassication,asupportvectormachine,andanother underlying CRF. A rule-based and a data driven approach is also explored [22] for scope resolution. The rule-based approach uses POS tags and constituentcategorylabelsbasedheuristicrules.Ontheotherhand, machinelearningusesSVMbasedrankingofsyntacticconstituents. [3, 16, 23, 24]
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	1542146969,2177147982	stant messaging chats [7, 8] using a set of of 12 coursegrained dialogue acts. Afterwards Kim et al. focus on classifying dialogue acts in both one-on-one and multi-party live instant messaging chats [10, 11]. In 2017, Soraby et.al [20] proposed a set of 25 dialogue acts for customer service conversation on Twitter and developed a sequential prediction algorithm. Our work is similar to the[20]&apos;staskb
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2139686600	r sufxes -less and -out but theirs implication is limited to the single word.As we are interested in how negation affects these expressions , in this work we only focus on explicit negation listed in [3]. Table 1 presents the explicit negation cues. Negation Scope Detection In this section, we will discuss different scope detection algorithms. Parts-of-Speech (POS) Based The proposed parts-of-speech
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2156413587	tive words. ii)TheMPQASubjectivityLexicon [30].Itcontainsalistof8222 English words with scores ranging from -2 to +2 indicating intensity of negative and positive sentiment. iii) Sentiment140 Lexicon [13]. This lexicon contains a real valued sentiment score of English words as well as hashtags. The scores are computed while considering negated contexts. For example, &quot;happy&quot; can have two diff
2884240866	Don't get Lost in Negation: An Effective Negation Handled Dialogue Acts Prediction Algorithm for Twitter Customer Service Conversations.	2252065932	whentestedonadifferenttestsetfromSimpleWikipedia, [29]&apos;s model built on constituency-based features performed better. In this work, to compare we pick the most recent CRF based model proposed by [24], we could not compare with [5] because the model expects testing data to be processed and manually annotated using the guideline from [19] and our conversation data is not formatted as per [19]. NEGA
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	1985427700	of such embeddings is that they may include both phonetic structures and semantics [37, 38]. A direct application for such phonetic-and-semantic embedding of spoken words is spoken document retrieval [39, 40, 41, 42, 43]. This task is slightly different from spoken term detection, in the latter case spoken terms are simply detected based on the phonetic structures. Here the goal of the task is to retrieve arXiv:1807.
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	1494198834	We used LibriSpeech [47] as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks.
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	1902237438,1938755728	mantics. Index Terms— phonetic-and-semantic embedding, spoken content retrieval 1. INTRODUCTION Word embedding or Word2Vec [1, 2, 3, 4] has been widely used in the area of natural language processing [5, 6, 7, 8, 9, 10, 11], in which text words are transformed into vector representations of ﬁxed dimensionality [12, 13, 14]. This is because these vector representations carry plenty of semantic information learned from th
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2250539671	o the query but not including the query can also be retrieved based on the semantics. Index Terms— phonetic-and-semantic embedding, spoken content retrieval 1. INTRODUCTION Word embedding or Word2Vec [1, 2, 3, 4] has been widely used in the area of natural language processing [5, 6, 7, 8, 9, 10, 11], in which text words are transformed into vector representations of ﬁxed dimensionality [12, 13, 14]. This is b
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2190506272	rea of speech signal processing, in which spoken words (signal segments for words without knowing the underlying word it represents) are transformed into vector representations of ﬁxed dimensionality [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken words, and have been shown to be useful in spoken term detection, in which t
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	1691728462,2434741482	s, it is also natural to believe they do carry some semantic information, except disturbed by phonetic structures plus some other acoustic factors such as speaker characteristics and background noise [31, 32, 33, 34, 35, 36]. So the goal of embedding spoken words to carry both phonetic structures and semantics is possible, although deﬁnitely hard. But a nice feature of such embeddings is that they may include both phonet
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2153579005	As shown in Figure 2, similar to the Word2Vec skip-gram model [1], we use two encoders: semantic encoder Esem and context encoder Ectx to embed the semantics over phonetic embeddings vp obtained in Stage 1.
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2059652594,2296681920	Similarly, audio Word2Vec has also been proposed in the area of speech signal processing, in which spoken words (signal segments for words without knowing the underlying word it represents) are transformed into vector representations of fixed dimensionality [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25].
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2130942839,2133564696,2153579005	Word embedding or Word2Vec [1, 2, 3, 4] has been widely used in the area of natural language processing [5, 6, 7, 8, 9, 10, 11], in which text words are transformed into vector representations of fixed dimensionality [12, 13, 14].
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2139501017	r Word2Vec [1, 2, 3, 4] has been widely used in the area of natural language processing [5, 6, 7, 8, 9, 10, 11], in which text words are transformed into vector representations of ﬁxed dimensionality [12, 13, 14]. This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus. Similarly, audio Word2Vec has also bee
2884305338	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	2599585580	words only without considering the context. Audio Word2Vec was recently extended to Segmental Audio Word2Vec [26], in which an utterance can be automatically segmented into a sequence of spoken words [27, 28, 29, 30] and then transformed into a sequence of vectors of ﬁxed dimensionality by Audio Word2Vec, and the spoken word segmentation and Audio Word2Vec can be jointly trained from an audio corpus. In this way
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2064675550	in (Bahdanau et al., 2015), with hyperparameters set following Kann and Schutze (2016,¨ x4.1.1). Each input character has an embedding size of 300 units. The encoder consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with two layers. There is a dropout value of 0.3 between the layers. The decoder is a unidirectional LSTM with two layers. Both the encoder and decoder have 100 hidden units. Training was done using
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2133564696	(s i 1)h k: In contrast to the R&amp;M network, the ED network optimizes the log-likelihood of the training data, i.e.,P M i=1 logp(y (i) jx(i)) for i = 1;:::;M training items. We refer the reader to Bahdanau et al. (2015) for the complete architectural speciﬁcation of speciﬁc ED model we apply in this paper. Theoretical Improvements. While there are a number of possible architectural variants of the ED architecture (L
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2019342855	00; Nakisa and Hahn, 1996) classify German noun stems into their appropriate plural inﬂection classes. (Plunkett and Nakisa, 1997) do the same for Arabic stems. (Hoeffner, 1992; Hare and Elman, 1995; Cottrell and Plunkett, 1994) also solve an alternative problem—mapping semantic representations (usually one-hot vectors with one unit per possible word type, and one unit per possible inﬂection) to phonological outputs. As thes
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2571532437	1and all positive reals to 1. In other words, we seek the phoneme string y0that shares the most features with the maximum a-posteriori (MAP) decoded binary vector. This problem is intractable, and so Rumelhart and McClelland (1986) provide an approximation. For each test stem, they handselected a set of likely past tense candidate forms, e.g., good candidates for the past tense of break would be fbreak,broke,brake,brakedg, and
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2021031353	2002). This model takes a mapping of phonemes to phonological features and makes feature-level generalizations like the post-voice [-d] rule described above. For a detailed technical description, see Albright and Hayes (2002). We treat the MGL as a baseline in x5. Unlike (Taatgen and Anderson, 2002), which explicitly accounts for dual route processing by including both memory retrieval and rule application submodules, (Al
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2571532437	3 1986 vs Today In this section, we compare the original R&amp;M architecture from 1986, to today’s state-of-the-art neural architecture for morphological transduction, the Encoder-Decoder model. 3.1 Rumelhart and McClelland (1986) For many linguists, the face of neural networks to this day remains the work of R&amp;M. Here, we describe in detail their original architecture, using modern machine learning parlance whenever possi
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2571532437	between algal ‘straight’ and algalgal ‘ramrod straight’; both of these strings have the identical 3Follow-up work, e.g., Plunkett and Marchman (1991), has speculated that the original experiments in Rumelhart and McClelland (1986) may not have converged. Indeed, convergence may not be guaranteed depending on the ﬁxed learning rate chosen. As Equation (1) is jointly convex in its parameters fW;bg, there exist convex optimizatio
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	1642393990	coder Architectures The NLP community has recently developed an analogue to the past tense generation task originally considered by Rumelhart and McClelland (1986): morphological paradigm completion (Durrett and DeNero, 2013; Nicolai et al., 2015; Cotterell et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	6908809	is a dropout value of 0.3 between the layers. The decoder is a unidirectional LSTM with two layers. Both the encoder and decoder have 100 hidden units. Training was done using the Adadelta procedure (Zeiler, 2012) with a learning rate of 1.0 and a minibatch size of 20. We train for 100 epochs to ensure that all verb forms in the training data are adequately learned. We decode the model with beam search (k = 12
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2104162748	e-length inputs and outputs, and remain unable to produce and assign probability to arbitrary output strings. (MacWhinney and Leinbach, 1991; Plunkett and Marchman, 1991; Plunkett and Marchman, 1993; Plunkett and Juola, 1999) map phonological strings to phonological strings using feed-forward networks, but rather than turning to Wickelphones to imprecisely represent strings of any length, they use ﬁxedsize input and outpu
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	1513168562	emplate above have been developed in NLP. While the SPE itself does not impose detailed restrictions on rule structure, these systems generate rules that can be compiled into ﬁnite-state transducers (Kaplan and Kay, 1994; Ahlberg et al., 2015). While these systems generalize well, even the most successful variants have been shown to perform signiﬁcantly worse than state-of-the-art neural networks at morphological inﬂ
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2110485445	not generalize correctly to held-out data. In contrast, state-of-the art morphological generation networks used in NLP, built from the modern evolution of Recurrent Neural Networks (RNNs) explored by Elman (1990) and others, solve the same problem almost perfectly (Cotterell et al., 2016). This level of performance on a cognitively relevant problem suggests that it is time to consider further incorporating ne
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2133564696	ine which string a non-unique set of Wickelfeatures represents. However, we note that decoding the 1-best string from a sequence-to-sequence 4For the experiments in this paper, we use the variant in (Bahdanau et al., 2015), which has explicitly been shown to be state-of-the-art in morphological transduction (Cotterell et al., 2016). PREPRINT model is likely NP-hard. (1-best string decoding is even hard for weighted FST
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2158899491	INT model is likely NP-hard. (1-best string decoding is even hard for weighted FSTs (Goodman, 1998).) Multi-Task Capability. A single ED model is easily adapted to multi-task learning (Caruana, 1997; Collobert et al., 2011), where each task is a single transduction, e.g., stem 7!past. Note R&amp;M would need a separate network for each transduction, e.g., stem 7!gerund and stem 7!past participle. In fact, the current st
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2110485445	k, with the main difference being training using Hebbian learning rather than the standard backpropagation algorithm. (Cottrell and Plunkett, 1994) present an early use of a simple recurrent network (Elman, 1990) to decode output strings, making their model capable of variable length output. (Bullinaria, 1997) is one of the few models proposed that can deal with variable length inputs. They use a derivative o
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	1908676432	ly developed an analogue to the past tense generation task originally considered by Rumelhart and McClelland (1986): morphological paradigm completion (Durrett and DeNero, 2013; Nicolai et al., 2015; Cotterell et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mapping the lemma (stem in the case of English) to each form in the paradigm. In the case of English, the goal wo
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2104162748	mapping problem by only considering a language of artiﬁcially generated words of exactly three syllables and a limited set of constructed pasttense formation patterns. (MacWhinney and Leinbach, 1991; Plunkett and Juola, 1999) additionally modify the input template to include extra units marking particular transformations (e.g., past or gerund), enabling their network to learn multiple mappings. Some proposals simplify the
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2166402529	nd less costly to train and manipulate across multiple experiments. Initial experiments could focus on default architectures, as we do in this paper, effectively treating them as inductive baselines (Gildea and Jurafsky, 1996) and measuring their performance given limited domain knowledge. Our ED networks, for example, have no built-in knowledge of phonology or morphology. Failures of these baselines would then point the w
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2064675550	ning simple recurrent networks, which tend to converge to poor local minima. Modern RNN varieties, such as the LSTMs in the ED model, were speciﬁcally designed to overcome these training limitations (Hochreiter and Schmidhuber, 1997). 4.2 Non-neural Learners P&amp;P describe several basic ideas that underlie a traditional, symbolic, rule-learner. Such a learner produces SPE-style rewrite rules that may be applied to deterministic
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2571532437	om [hross], and [bird] from [brid] (Jesperson, 1942). 3.2 Encoder-Decoder Architectures The NLP community has recently developed an analogue to the past tense generation task originally considered by Rumelhart and McClelland (1986): morphological paradigm completion (Durrett and DeNero, 2013; Nicolai et al., 2015; Cotterell et al., 2015; Ahlberg et al., 2015; Faruqui et al., 2016). The goal is to train a model capable of mappin
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	1592240138	for regular and irregular verbs; regular verbs go through a general, rulegoverned transduction mechanism, and exceptional irregulars are produced via simple memory-lookup.1 While some studies, e.g., (Marslen-Wilson and Tyler, 1997; Ullman et al., 1997), provide corroborating evidence from speakers with selective impairments to regular or irregular verb production, others have called these results into doubt (Stockall and Maran
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2133564696	rner mimic humans? Are the errors human-like? In this work, we run new experiments examining the ability of the Encoder-Decoder architecture developed for machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) to learn the English past tense. The results suggest that modern nets absolutely meet the ﬁrst criterion above, and often meet the second. Furthermore, they do this given limited prior knowledge of l
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2571532437	tly thereafter, Pinker and Prince (1988) presented a comprehensive rebuttal of many of Rumelhart and McClelland’s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland (1986) model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince’s criticisms without requir
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2019342855	is unique in that it uses an attractor network rather than a feed-forward network, with the main difference being training using Hebbian learning rather than the standard backpropagation algorithm. (Cottrell and Plunkett, 1994) present an early use of a simple recurrent network (Elman, 1990) to decode output strings, making their model capable of variable length output. (Bullinaria, 1997) is one of the few models proposed t
2884554299	Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate.	2133564696	all verb types equally for training, effecting a uniform distribution over types as described above. PREPRINT Hyperparameters and Other Details. Our architecture is nearly identical to that used in (Bahdanau et al., 2015), with hyperparameters set following Kann and Schutze (2016,¨ x4.1.1). Each input character has an embedding size of 300 units. The encoder consists of a bidirectional LSTM (Hochreiter and Schmidhuber
2884599109	The EcoLexicon English Corpus as an open corpus in Sketch Engine.	1869398583	agset (TreeTagger version 3.3) and with the EcoLexicon Semantic Sketch Grammar (ESSG) (León-Araúz &amp; San Martín 2018; León-Araúz, San Martín &amp; Faber 2016), a CQL-based (Corpus Query Language) (Jakubíček et al. 2010) customized sketch grammar separate from the default sketch grammar. The ESSG was developed for the extraction of semantic word sketches based on some of the most common semantic relations in terminol
2884599109	The EcoLexicon English Corpus as an open corpus in Sketch Engine.	2070205520	r, León-Araúz &amp; Reimerink 2016; San Martín et al. 2017), a terminological knowledge base on the environment. It is available as an open corpus in the well-known corpus query system Sketch Engine (Kilgarriff et al. 2014), which means that any user, even without a subscription, can freely access and query the corpus. In this paper, the EEC is introduced by describing how it was built and compiled and how it can be que
2884599109	The EcoLexicon English Corpus as an open corpus in Sketch Engine.	2070205520	s such as author, date of publication, target reader, contextual domain, and keywords. However, its search engine does not provide all the functionalities of the well-known corpus tool Sketch Engine (Kilgarriff et al. 2014). This is why the EEC was made available as an open corpus in Sketch Engine, which means that any user, even without a subscription, can freely access and query the corpus.2 One very interesting modul
2884679141	Learning Representations for Soft Skill Matching	2143017621	al Attention were implemented from scratch using Python and PyTorch [15]. The code and the datasets will be publicly released here 3. Text preprocessing included case-folding, lemmatization with NLTK [11] WordNet lemmatizer as well as removing all syntax tokens, except commas. We have set the width of CNN lters to be f2,3,4g. Since the network tends to overt, we have used only 50 lters of each size.
2884679141	Learning Representations for Soft Skill Matching	1579838312	detecting candidate soft skills is related to word sense disambiguation (WSD), where the sense of a word being used in a particular context arXiv:1807.07741v1 [cs.CL] 20 Jul 2018 has to be determined [7]. However, our task is dierent since we are not aiming at mapping a skill to a particular sense such as a one from WordNet [13]. Instead, the goal is to dierentiate between the two cases of interest
2884679141	Learning Representations for Soft Skill Matching	1847088711	of dierent importance in dierent context. The model is called hierarchical since it models documents using sentences which in their turn are modeled sequentially using words. Gated Recurrent Units [5] with bidirectional structure are used to sequentially encode each sentence, as well as document as the sequence of sentences. 5.3 Proposed approaches for input representation In order to match candid
2884679141	Learning Representations for Soft Skill Matching	2170693252	orking with skill phrases, also makes it harder to use dependency parsing, that usually outputs a relation between a pair of words. Massive skill extraction was done and actively employed by LinkedIn [3], although with a focus on hard skills. They observed that people provide a list of comma separated skills in the specialties section of their prole that can be used for mining potential skills. The
2884679141	Learning Representations for Soft Skill Matching	2250861254	reated alone as soft skill, but with additional phrase, like responsible for finding new clients, these become job duties. In order to understand the problem better, we use Stanford Dependency Parser [4] to nd entities with adjectival modier (amod), particularly adjective + noun phrases. Since we are interested in cases where certain adjectives or skills are used to describe other entities, this met
2884679141	Learning Representations for Soft Skill Matching	2081580037	ular context arXiv:1807.07741v1 [cs.CL] 20 Jul 2018 has to be determined [7]. However, our task is dierent since we are not aiming at mapping a skill to a particular sense such as a one from WordNet [13]. Instead, the goal is to dierentiate between the two cases of interest without requiring exact sense. Besides, we work both on the word and phrase levels. Dependency parsing [14] could be helpful fo
2884679141	Learning Representations for Soft Skill Matching	2064675550	ural network classiers used for our predictions and describe in detail various ways to represent a soft skill and its context for these classiers. 5.1 LSTM and CNN We have selected CNN [9] and LSTM [6,17] models since they are both frequently used as text classication benchmark algorithms. In both models, words in the sentences are embedded with word2vec [12] vectors. LSTM employs sequential nature o
2884970917	Towards Explainable and Controllable Open Domain Dialogue Generation with Dialogue Acts.	2133564696	t2A r(a t;s t). 4 Experiment 4.1 Experiment Setup Our experiments are conducted with the data in Table2. The following methods are employed as baselines: (1) S2SA: sequenceto-sequence with attention (Bahdanau et al., 2015) in which utterances in contexts are concatenated as a long sentence. We use the implementation with Blocks (https: //github.com/mila-udem/blocks); (2) HRED: the hierarchical encoder-decoder model in
2884975363	Hierarchical Multitask Learning With CTC	2263232528	; 1; if k = ; (5) Where P LM(kjB(p 1:t 1)) is provided by the unit-speciﬁc LM. Because it is unfeasible to calculate an exact solution to equation (5) by brute force, we apply beam search similar to [27]. Shallow fusion is compatible with any target unit. More concretely, we apply this approach to subword, character or word unit systems. Note that this technique is independent of the architecture of
2884975363	Hierarchical Multitask Learning With CTC	2545177271	ciation lexicon anymore, they simplify the ASR pipeline and obtain competitive results. To simplify even more the ASR pipeline, Soltau et al. suggest using words instead of characters as target units [5]. Even though this technique forces a closed vocabulary setting, experiments show competitive results on a large training corpus. Audhkhasi et al. test the same approach on Switchboard, a smaller data
2884975363	Hierarchical Multitask Learning With CTC	2516255829	eural networks. In this direction, Søgaard and Goldberg propose an architecture that learns more fundamental Natural Language Processing tasks to guide the internal representation of a neural network [19]. Focused on ASR, Fernandez et al. use mono phone prediction as an auxiliary task to improve digit recognition using CTC [20] . In this case, the auxiliary loss helps the network to learn an intermedi
2884975363	Hierarchical Multitask Learning With CTC	1736701665	formance by using characters as target units. More speciﬁcally, these approaches use either a Sequence-to-Sequence (S2S) architecture [3] or a Connectionist Temporal Classiﬁcation (CTC) loss-function [4]. By not requiring a pronunciation lexicon anymore, they simplify the ASR pipeline and obtain competitive results. To simplify even more the ASR pipeline, Soltau et al. suggest using words instead of
2884975363	Hierarchical Multitask Learning With CTC	1524333225	hboard subset, which more closely resembles the training data, and the Callhome subset. We extracted 43-dimensional ﬁlter bank, and pitch features vectors with Cepstral Mean Normalization using Kaldi [28]. We use the Tensorﬂow branch of EESEN [4] to develop the rest of the pipeline. 5.1. Architecture All MTL architectures presented in this section (i.e., HMTL and BMTL) have a two-layer shared encoder,
2884975363	Hierarchical Multitask Learning With CTC	2545177271	TC model that uses words as target units without any Published in the 2018 IEEE Workshop on Spoken Language Technology (SLT 2018), Athens, Greece c IEEE 2018 arXiv:1807.07104v5 [cs.CL] 14 Jan 2019 LM [5]. Later, Audhkhasi et al. port this idea to smaller and public datasets achieving competitive results [9, 6]. The high number of dataset-speciﬁc hyper-parameter tuning needed (e.g., number of layers,
2884975363	Hierarchical Multitask Learning With CTC	1736701665	the training data, and the Callhome subset. We extracted 43-dimensional ﬁlter bank, and pitch features vectors with Cepstral Mean Normalization using Kaldi [28]. We use the Tensorﬂow branch of EESEN [4] to develop the rest of the pipeline. 5.1. Architecture All MTL architectures presented in this section (i.e., HMTL and BMTL) have a two-layer shared encoder, and each layer has 320 cells. We perform
2885100978	Language Style Transfer from Sentences with Arbitrary Unknown Styles.	2735642330	ntangled from its style. To tackle this problem,Shen et al.(2017) assumed the source and target domain share the same latent content space, and trained their model by aligning these two latent spaces.Hu et al. (2017) constrained that the latent content representation of the original sentence could be inferred from the transferred sentence. However, these attempts considered content modiﬁcation in the latent conte
2885100978	Language Style Transfer from Sentences with Arbitrary Unknown Styles.	2617566453	views, dialog response revision with a romantic style, and sentence rewriting with a Shakespearean style. Experimental results show that our model surpasses the state-of-the-art style transfer model (Shen et al., 2017) in these three tasks. 2 Related Work Image Style Transfer: Most style transfer approaches in the literatures focus on vision data. Kulkarni et al.(2015) proposed to disentangle the content representa
2885100978	Language Style Transfer from Sentences with Arbitrary Unknown Styles.	2130942839	y language style transfer is that large-scale parallel data are unavailable. However, parallel data are necessary for most text generation frameworks, such as the popular sequence-to-sequence models (Sutskever et al., 2014;Bahdanau et al.,2014;Rush et al.,2015; Nallapati et al.,2016;Paulus et al.,2017). Hence these methods are not applicable to the language style transfer problem. A few approaches have been proposed to
2885204463	Sentimental Content Analysis and Knowledge Extraction from News Articles.	2251939518	iment analysis and text processing. CoreNLP was established based on recursive neural tensor networks and the Stanford Sentiment treebank which includes 215,154 phrases extracted from 11,855 sentences[20]. Figure 2 demonstrates a demo for analyzing the sentiment of sentences2. The results of CoreNLP for each sentence is between -2 to +2 which -2, -1, 0, 1, 2 are very negative, negative, neutral, posit
2885204463	Sentimental Content Analysis and Knowledge Extraction from News Articles.	2104715395	lgorithm (RNN), selected nearest neighbor (SNN), generalized condensed nearest neighbor (GCNN) and edited nearest neighbor (ENN) are amongst the most well known KNN based instance selection approaches[11, 12, 13, 14, 15, 16]. In addition, one of the main NLP tasks is document classiﬁcation [17]. For this purpose, we need to extract appropriate features from each document and employ them as arXiv:1808.03027v1 [cs.CL] 9 Au
2885204463	Sentimental Content Analysis and Knowledge Extraction from News Articles.	2034090215	ve words in a text[1, 2, 3]. In the second group which are learning based approaches, an attempt is made to apply machine learning algorithms such as support vector machines, neural networks and etc. [4, 5, 6, 7]. The task of sentiment analysis can be divided into some subdivisions as term extraction, category detection, sentiment classiﬁcation and sentiment rating. The purpose of aspect rating which is in th
2885340293	Learning to Represent Bilingual Dictionaries.	1794039122	(i) basic dictionary models that adopt four different encoding techniques (BOW, CNN, GRU and ATT); (ii) models with the two best encoding techniques that enforce the monolingual retrieval approach by Hill et al. (2016) (GRU-mono and ATT-mono); (iii) models adopting bilingual multi-task learning (GRU-MTL and ATT-MTL); (iv) joint learning that employs the best dictionary model of ATT-MTL (ATT-joint). gual retrieval a
2885340293	Learning to Represent Bilingual Dictionaries.	2508865106	allel pair of encoders with shared parameters. Then an MLP is stacked to the subtraction of two sentence vectors. Note that some works use concatenation (Yin and Sch¨utze 2015) or Manhattan distance (Mueller and Thyagarajan 2016) of sentence vectors instead of their subtraction (Jiang et al. 2018), which we ﬁnd to be less effective on small amount of data. We apply the conﬁgurations of the sentence encoders from the last expe
2885340293	Learning to Represent Bilingual Dictionaries.	1828724394	The cross-lingual semantic transfer by these models is captured from parallel corpora with sentential or document-level alignment, using techniques such as bilingual bag-of-words distances (BilBOWA) (Gouws et al. 2015), bilingual Skip-Gram (Coulmance et al. 2015) and sparse tensor factorization (Vyas and Carpuat 2016). Neural sentence modeling. Neural sentence models seek to characterize the phrasal or sentential s
2885340293	Learning to Represent Bilingual Dictionaries.	2617566453	dels for detecting discourse relations of paraphrases or text entailment (Sha et al. 2016; Rocktaschel et al. 2016;¨ Chen et al. 2018a), and sequence-to-sequence models for tasks like style transfer (Shen et al. 2017) and abstractive summarization (Chopra, Auli, and Rush 2016). Speciﬁcally, our work is related to corresponding works of neural machine translation (NMT) (Bahdanau, Cho, and Bengio 2015; Wu et al. 201
2885340293	Learning to Represent Bilingual Dictionaries.	2295584157	entering of embeddings (Xing et al. 2015; Artetxe et al. 2016). Others adopt canonical correlation analysis to map separated monolingual embeddings to a shared embedding space (Faruqui and Dyer 2014; Lu et al. 2015). Unlike off-line mappings, joint training models simultaneously learn word embeddings and cross-lingual alignment. By jointly updating the embeddings with the alignment information, such approaches g
2885340293	Learning to Represent Bilingual Dictionaries.	2251682575	esentations of lexical semantics with precise cross-lingual semantic transfer (Gouws et al. 2015). Therefore, they have been widely used in many cross-lingual NLP tasks including machine translation (Devlin et al. 2014), bilingual document classiﬁcation (Zhou, Wan, and Xiao 2016), knowledge alignment (Chen et al. 2018b) and named entity recognition (Feng et al. 2018). While many approaches have been proposed to capt
2885340293	Learning to Represent Bilingual Dictionaries.	2525778437	et al. 2017) and abstractive summarization (Chopra, Auli, and Rush 2016). Speciﬁcally, our work is related to corresponding works of neural machine translation (NMT) (Bahdanau, Cho, and Bengio 2015; Wu et al. 2016), while our setting has major differences from NMT in the following two perspectives: (i) NMT has to bridge between corpora of the same granularity, unlike BilDRL that captures the multi-granular corr
2885340293	Learning to Represent Bilingual Dictionaries.	2120615054	We also experiment with other widely used neural sentence modeling techniques, which are however outperformed by the attentive GRU encoder in our tasks. These techniques include the vanilla GRU, CNN (Kalchbrenner et al. 2014), and linear bag-of-words (BOW) (Hill et al. 2016). We brieﬂy introduce the later two techniques in the following. Convolutional Encoder. A convolutional encoder applies a kernel M c 2Rh k to generate
2885340293	Learning to Represent Bilingual Dictionaries.	2250414191	formation, such approaches generally capture more precise cross-lingual semantic transfer (Upadhyay et al. 2016). While few of such models still maintain separated embedding spaces for each language (Huang et al. 2015), the majority of recent ones obtain a uniﬁed embedding space for both languages. The cross-lingual semantic transfer by these models is captured from parallel corpora with sentential or document-leve
2885340293	Learning to Represent Bilingual Dictionaries.	1828724394	hat bridges between single and sequences of words. Such alignment information is generally not available in the parallel and seed-lexicon corpora that are utilized by bilingual word embedding models (Gouws et al. 2015; Lample et al. 2018). To incorporate the representations of bilingual lexical and sentential semantics, we propose an approach by leveraging bilingual dictionaries. The proposed approach BilDRL (Bili
2885340293	Learning to Represent Bilingual Dictionaries.	2218641061	ing cross-lingual semantic search of concepts (Tsai and Roth 2016), agents for detecting discourse relations in bilingual dialogue utterances (Jiang et al. 2018), and multilingual text summarization (Nenkova and McKeown 2012), as well as educational applications for foreign language learners. Second, it is natural for a human to learn the meaning of a foreign word by looking up its meaning in the native language. Therefor
2885340293	Learning to Represent Bilingual Dictionaries.	2499696929	input sequence, this technique does not preserve the sequential information that is critical to the representation of short sentences. Linear bag-of-words. Following the deﬁnition in previous works (Xie et al. 2016; Hill et al. 2016), the much simpler BOW encoder is realized by the sum of projected word embeddings of the input sentence, i.e. E(3)(S) = P jSj t=1 M bw t. 3.2 Basic Learning Objective The objective
2885340293	Learning to Represent Bilingual Dictionaries.	1828724394	Introduction Bilingual word embedding models are used to capture the cross-lingual semantic relatedness of words based on their co-occurrence in parallel or seed-lexicon corpora (Chandar et al. 2014; Gouws et al. 2015; Luong, Pham, and Manning 2015). By collocating related words in the low-dimensional embedding spaces, these models effectively support the representations of lexical semantics with precise cross-lin
2885340293	Learning to Represent Bilingual Dictionaries.	2499696929	the learning process does not update word embeddings that are used to represent the deﬁnitions and target words. While other forms of loss such as cosine proximity (Hill et al. 2016) and hinge loss (Xie et al. 2016) may also be used in the learning process, we ﬁnd that L 2 loss consistently leads to better performance in our experiments. 3.3 Bilingual Multi-task Learning In cases where entries in a bilingual dic
2885340293	Learning to Represent Bilingual Dictionaries.	1794039122	learning strategy in Section 3.4, the learning process does not update word embeddings that are used to represent the deﬁnitions and target words. While other forms of loss such as cosine proximity (Hill et al. 2016) and hinge loss (Xie et al. 2016) may also be used in the learning process, we ﬁnd that L 2 loss consistently leads to better performance in our experiments. 3.3 Bilingual Multi-task Learning In cases
2885340293	Learning to Represent Bilingual Dictionaries.	1794039122	modeling techniques, which are however outperformed by the attentive GRU encoder in our tasks. These techniques include the vanilla GRU, CNN (Kalchbrenner et al. 2014), and linear bag-of-words (BOW) (Hill et al. 2016). We brieﬂy introduce the later two techniques in the following. Convolutional Encoder. A convolutional encoder applies a kernel M c 2Rh k to generate the latent representation h (3) t from each h-gra
2885340293	Learning to Represent Bilingual Dictionaries.	1794039122	models, i.e. a dictionarybased sentence encoder and a word embedding model. On the other hand, fewer efforts have been put to characterizing the associations between sentential and lexical semantics. Hill et al. (2016) and Xie et al. (2016) learn off-line mappings between monolingual descriptions and lexicons to capture such associations. Eisner et al. (2016) adopt a similar approach to capture emojis based on desc
2885340293	Learning to Represent Bilingual Dictionaries.	2158199200	ngual paraphrases. 1 Introduction Bilingual word embedding models are used to capture the cross-lingual semantic relatedness of words based on their co-occurrence in parallel or seed-lexicon corpora (Chandar et al. 2014; Gouws et al. 2015; Luong, Pham, and Manning 2015). By collocating related words in the low-dimensional embedding spaces, these models effectively support the representations of lexical semantics wit
2885340293	Learning to Represent Bilingual Dictionaries.	2612953412	o GRU, the traditional LSTM generally performs comparably, but is more complex and require more computational resources for training (Chung et al. 2014). Self-attention. The self-attention mechanism (Conneau et al. 2017) seeks to highlight the important units in an input sentence when capturing its overall meaning. One layer of self-attention is calculated as below. u t = tanh  M ah (1) t + b a  a t = exp u&gt; P t
2885340293	Learning to Represent Bilingual Dictionaries.	2170738476	o as to show the performance under controlled variables. Training of each classiﬁer is terminated by early-stopping based on the validation set, so as to prevent overﬁtting. Following the convention (Hu et al. 2014; Yin et al. 2016), we evaluate based on the accuracy and F1 scores. Results. This task is challenging due to the heterogeneity of cross-lingual paraphrases and limitedness of learning resources. The
2885340293	Learning to Represent Bilingual Dictionaries.	342285082	ormalization and mean centering of embeddings (Xing et al. 2015; Artetxe et al. 2016). Others adopt canonical correlation analysis to map separated monolingual embeddings to a shared embedding space (Faruqui and Dyer 2014; Lu et al. 2015). Unlike off-line mappings, joint training models simultaneously learn word embeddings and cross-lingual alignment. By jointly updating the embeddings with the alignment information,
2885340293	Learning to Represent Bilingual Dictionaries.	2211192759	osed to tackle monolingual paraphrase identiﬁcation. These models include Siamese structures of CNN (BiCNN) (Yin and Schutze 2015), RNN (BiGRU) (Mueller and Thyagarajan¨ 2016), attentive CNN (ABCNN) (Yin et al. 2016), attentive GRU (BiATT) (Rocktaschel et al. 2016), and linear bag-of-¨ words (BiBOW). To support the reasoning of cross-lingual sentential semantics, we provide these baselines with the same BilBOWA e
2885340293	Learning to Represent Bilingual Dictionaries.	1924770834	overall meaning of the encoded sentence. Note that in comparison to GRU, the traditional LSTM generally performs comparably, but is more complex and require more computational resources for training (Chung et al. 2014). Self-attention. The self-attention mechanism (Conneau et al. 2017) seeks to highlight the important units in an input sentence when capturing its overall meaning. One layer of self-attention is calc
2885340293	Learning to Represent Bilingual Dictionaries.	2211192759	performance under controlled variables. Training of each classiﬁer is terminated by early-stopping based on the validation set, so as to prevent overﬁtting. Following the convention (Hu et al. 2014; Yin et al. 2016), we evaluate based on the accuracy and F1 scores. Results. This task is challenging due to the heterogeneity of cross-lingual paraphrases and limitedness of learning resources. The results in Table 4
2885340293	Learning to Represent Bilingual Dictionaries.	2260756217	rections of languages can be mixed together. For joint learning, we follow previous works (Gouws et al. 2015; Mogadala and Rettinger 2016) to conduct an efﬁcient multi-threaded asynchronous training (Mnih et al. 2016) of AMSGrad. In detail, after initializing the embedding space based on pre-trained BilBOWA, parameter updating based on the four components of J occurs across four worker threads. Two monolingual thr
2885340293	Learning to Represent Bilingual Dictionaries.	1828724394	regate three metrics on test cases: the accuracy P@1 (%), the proportion of ranks no larger than 10 P@10 (%), and mean reciprocal rank MRR. We pre-train BilBOWA based on the original conﬁguration in (Gouws et al. 2015) and obtain 50-dimensional initialization of bilingual word embedding spaces respectively for the English-French and English-Spanish settings. For CNN, GRU, and attentive GRU (ATT) encoders, we stack
2885340293	Learning to Represent Bilingual Dictionaries.	1828724394	rning models are trained on batched samples from two dictionaries. Within each batch, entries of different directions of languages can be mixed together. For joint learning, we follow previous works (Gouws et al. 2015; Mogadala and Rettinger 2016) to conduct an efﬁcient multi-threaded asynchronous training (Mnih et al. 2016) of AMSGrad. In detail, after initializing the embedding space based on pre-trained BilBOWA
2885340293	Learning to Represent Bilingual Dictionaries.	2499696929	rybased sentence encoder and a word embedding model. On the other hand, fewer efforts have been put to characterizing the associations between sentential and lexical semantics. Hill et al. (2016) and Xie et al. (2016) learn off-line mappings between monolingual descriptions and lexicons to capture such associations. Eisner et al. (2016) adopt a similar approach to capture emojis based on descriptions. At the best
2885340293	Learning to Represent Bilingual Dictionaries.	2211192759	taset thereof, matches with the quantity and partitioning of sentence pairs in the widely-used Microsoft Research Paraphrase Corpus benchmark for monolingual paraphrase identiﬁcation (Hu et al. 2014; Yin et al. 2016; Das and Smith 2009). Several examples from the dataset are shown in Table 2. The datasets and the processing scripts at URL REMOVED FOR REVIEW. 4.1 Cross-lingual Reverse Dictionary Retrieval The obj
2885340293	Learning to Represent Bilingual Dictionaries.	1794039122	this technique does not preserve the sequential information that is critical to the representation of short sentences. Linear bag-of-words. Following the deﬁnition in previous works (Xie et al. 2016; Hill et al. 2016), the much simpler BOW encoder is realized by the sum of projected word embeddings of the input sentence, i.e. E(3)(S) = P jSj t=1 M bw t. 3.2 Basic Learning Objective The objective of learning the di
2885340293	Learning to Represent Bilingual Dictionaries.	2465169645	tial or document-level alignment, using techniques such as bilingual bag-of-words distances (BilBOWA) (Gouws et al. 2015), bilingual Skip-Gram (Coulmance et al. 2015) and sparse tensor factorization (Vyas and Carpuat 2016). Neural sentence modeling. Neural sentence models seek to characterize the phrasal or sentential semantics from word sequences. They often adopt encoding techniques such as recurrent neural encoders
2885340293	Learning to Represent Bilingual Dictionaries.	2143927888	tiﬁ- cation asks whether two sentences in different languages essentially express the same meaning, which is critical to question answering or dialogue systems that apprehend multilingual utterances (Bannard and Callison-Burch 2005). This task is challenging, as it requires a model to comprehend cross-lingual paraphrases that are inconsistent in grammar, content details and word orders. BilDRL maps sentences to the lexicon embed
2885340293	Learning to Represent Bilingual Dictionaries.	2170738476	tting of this dataset thereof, matches with the quantity and partitioning of sentence pairs in the widely-used Microsoft Research Paraphrase Corpus benchmark for monolingual paraphrase identiﬁcation (Hu et al. 2014; Yin et al. 2016; Das and Smith 2009). Several examples from the dataset are shown in Table 2. The datasets and the processing scripts at URL REMOVED FOR REVIEW. 4.1 Cross-lingual Reverse Dictionary
2885340293	Learning to Represent Bilingual Dictionaries.	2294774419	v et al. 2013a). Some variants of this approach improve the quality of bilingual projections by adding constraints such as orthogonality of transforms, normalization and mean centering of embeddings (Xing et al. 2015; Artetxe et al. 2016). Others adopt canonical correlation analysis to map separated monolingual embeddings to a shared embedding space (Faruqui and Dyer 2014; Lu et al. 2015). Unlike off-line mapping
2885340293	Learning to Represent Bilingual Dictionaries.	2465169645	word orders. BilDRL maps sentences to the lexicon embedding space. This process reduces the problem to evaluate the similarity of lexicon embeddings, which can be easily solved by a simple classiﬁer (Vyas and Carpuat 2016). BilDRL performs well with even a small amount of data, and signiﬁcantly outperforms previous approaches. 2 Related Work In this section, we discuss two lines of relevant work. Bilingual word embeddi
2885340293	Learning to Represent Bilingual Dictionaries.	2465169645	y brings signiﬁcant improvement to our cross-lingual tasks. In additional to BilBOWA, other jointly trained bilingual word embeddings may also be used to support this strategy (Coulmance et al. 2015; Vyas and Carpuat 2016), for which we leave the comparison to future work. 3.4 Joint Learning While above learning strategies of BilDRL are based on a ﬁxed embedding space, we lastly propose a joint learning strategy. Durin
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2250861254	dency labels for frame identication. 4.2 Approach for Problem Solving After the frame identication, we parsed each sentence to ll the frame slots. For dependency parsing, Stanford dependency parser [5] which was a neural network based parser was used. We used following slots or attributes for each frame which were identied from specic dependency labels whose dependency mapping are given below. Af
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2101234009	e Identication constituted the rst step in our approach. We implemented dierent machine learning approaches for creating frame identier. The design of the classiers were implemented using sklearn [13] machine learning library. The classiers used were : Support Vector Machines [6] and Random Forests [4]. We did not use any neural network approaches because of limited number of training examples. T
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2251349042	em text. Approaches can be template alignment, prediction of verb categories and solving the problem, using CFG rules along with the generation of equation trees. The system designed by Kushman et al [9] was a joint log linear distribution over the full set of equations and alignments between the variables and text. The number of equations was determined by the number of training equation templates.
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2510828927	this information into a set of linear equations which can be easily solved. The system identied 7 kinds of verbs used in the problems which was predicted by support vector machines. Mitra and Baral [11] created an arithmetic word problem solver which learned how to use formulas to solve simple addition and subtraction problems. The formulas were modeled as templates with pre-dened slots. They used
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2081580037	og-linear model to nd out the best possible formula to solve a problem. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet [10] features. 3 Corpus Annotation The main task in this problem is the identication of the frames. We needed to create a corpus of frame annotated sentences to create automatic frame identication module
2885359576	Arithmetic Word Problem Solver using Frame Identification.	2081580037	s (unibi). We also experimented with character n-grams in dierent ranges [2-6 and 3-6]. We did not use any additional lexical or linguistic features like parts-ofspeech tags, morph features, wordnet [10] features or dependency labels for frame identication. 4.2 Approach for Problem Solving After the frame identication, we parsed each sentence to ll the frame slots. For dependency parsing, Stanford
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	2190067570	6]. The QA task has been extended from text to images [17, 18, 19, 20] or video descriptions [21, 22, 23]. In 1not generated by TTS as Spoken-SQA arXiv:1808.02280v1 [cs.CL] 7 Aug 2018 the MovieQA task[24], the machine answers questions about movies using video clips, plots, subtitles, scripts, and DVS. Usually only text information (e.g., the movie’s plot) is considered in the MovieQA task; learning t
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	2185175083	improve the performance of models, also improve the SQA models. 2. RELATED WORK Most QA work focuses on understanding text documents[14, 15, 1, 16]. The QA task has been extended from text to images [17, 18, 19, 20] or video descriptions [21, 22, 23]. In 1not generated by TTS as Spoken-SQA arXiv:1808.02280v1 [cs.CL] 7 Aug 2018 the MovieQA task[24], the machine answers questions about movies using video clips, pl
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	2512720747	mination that tests the knowledge and skills of academic English for English learners whose native languages are not English. Deep-based models including attention-based RNN[8] and tree-structured RNN[9] were Thanks to Delta Research Center and Delta Electronics, Inc. for collecting the DRCD dataset. used to answer TOEFL listening comprehension test. Transfer learning for Question Answering (QA) is a
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	1938755728	on Neural Network (1-D CNN) to generate the word representation from the pingyin-token sequence of a word, and this network is called Pingyin-CNN. Our proposed approach is the reminiscent of Char-CNN [34, 35], which apply 1-D CNN on character sequence to generate distributed representation of word for text classiﬁcation task. Pingyin-CNN is illustrated in Figure 2. We explain how we obtain feature for one
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	1995820507,2078238240,2164290393	o improve the SQA models. 2. RELATED WORK Most QA work focuses on understanding text documents[14, 15, 1, 16]. The QA task has been extended from text to images [17, 18, 19, 20] or video descriptions [21, 22, 23]. In 1not generated by TTS as Spoken-SQA arXiv:1808.02280v1 [cs.CL] 7 Aug 2018 the MovieQA task[24], the machine answers questions about movies using video clips, plots, subtitles, scripts, and DVS. U
2885485938	ODSQA: Open-Domain Spoken Question Answering Dataset	2295570185	her text or spoken form. In SQA, after transcribing spoken content into text by automatic speech recognition (ASR), typical approaches use information retrieval (IR) techniques [6] or knowledge bases [7] to ﬁnd the proper answer from the transcriptions. Another attempt towards machine comprehension of spoken content is TOEFL listening comprehension by machine [8]. TOEFL is an English examination that
2885496260	Using NLP on news headlines to predict index trends.	2171468534	f information: Linear regression, Support Vector Machine, Long Short-Term Memory recurrent neuralnetworkandadensefeed-forward(MLP)neuralnetwork. We included the techniques used by Bollen et al (2010) [1], which resulted in state-of-the-art results. We will also analyze the techniques used in other studies with a similar context [2] [3]. 2 INFORMATION IN HEADLINES Latent Sentiment Analysis is done by
2885496260	Using NLP on news headlines to predict index trends.	2740461307	t result, as of writing. There exists several word embedding techniques such as Word2Vec, TF-IDF, Bag-of-words [4] and those can be combined with N-grams in order to augment the informationtheycontain[5]. Wehavetestedthosethreetechniques, withandwithoutusingN-grams. Hereafter,wewillexplain the concepts behind those techniques. After having cleaned the text by removing stop words andnamedentities,wene
2885519967	XL-NBT: A Cross-lingual Neural Belief Tracking Framework	2251163406	ed Work 2.1 Dialog State Tracking Broadly speaking, the dialog belief tracking algorithms can be divided into three families: 1) hand-crafted rules 2) generative models, and 3) maximum-entropy model (Metallinou et al., 2013). Later on, many deep learning based discriminative models have surged to replace the traditional strategies (Henderson et al.,2014a; Mrkˇsi ´c et al. ,2016;Williams et al.,2016) and achieved state-of
2885519967	XL-NBT: A Cross-lingual Neural Belief Tracking Framework	2403702038	human-machine interfaces. As a result, numerous task-oriented dialog systems such as virtual assistants and customer conversation services were developed (Wen et al.,2015; Rojas-Barahona et al.,2017;Bordes and Weston, 2017;Williams et al.,2017;Li et al.,2017), with Google Duplex1 being the most recent example. With the rapid process of globalization, more countries have observed growing populations of immigrants, and m
2885568805	Learning to Write Notes in Electronic Health Records.	1871067837	datasets have also been used to predict clinical events although they often are nonpublic or have no clinical notes. There exists substantial prior work on utilizing clinical notes for many purposes. Friedman et al. (2004) extract structured output from notes in the form of Unied Medical Language System (UMLS) codes. A common use of notes is to incorporate them as input to machine learning models that predict future c
2885568805	Learning to Write Notes in Electronic Health Records.	2130942839	g has also been extensively studied including for machine translation (Wu et al., 2016) and speech recognition (Chiu et al., 2017), using a class of techniques based on sequence-to-sequence learning (Sutskever et al., 2014). There we model an output sequence (e.g. English words) conditioned on an input sequence (e.g. French words). However, most prior work relies on mapping a single modality to text, e.g. text-totext or
2885568805	Learning to Write Notes in Electronic Health Records.	1895577753	ny modality, image, text, audio, etc. For example, in the case of machine translation, this context is the source language sentence to be translated. In image captioning, the context may be an image (Vinyals et al., 2015). In this work, the sequence to be predicted is the text of the current clinical note and context is the past data extracted from the Electronic Health Record (EHR) for the same patient, R. We also au
2885577284	Linguistic data mining with complex networks: A stylometric-oriented approach	1990372894	It is well known that language evolves over time; some aspects of its evolution can be captured by quantitave means and related to the processes taking place in a society and its culture [24, 25, 31].
2885577284	Linguistic data mining with complex networks: A stylometric-oriented approach	1975653240,1987380777,2126631960	It can be found in the literature on computational stylometry that a very often utilized approach to authorship attribution is representing a text as a collection of words along with their frequencies (the socalled bag-of-words representation), and comparing particular words’ frequencies is among the most reliable methods to discriminate between authors [20, 29, 33, 39, 44, 49].
2885577284	Linguistic data mining with complex networks: A stylometric-oriented approach	2118364625	It has a significant practical importance - the methods of analysis of graphs and networks have been employed in the natural language processing tasks, like keyword selection, document summarization, word-sense disambiguation or machine translation [2, 3, 32].
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2608207374	. This is intended to match the style token tanh activation (see [2], section 3.2.2), which, in turn, is chosen to match the GRU tanh activation of the ﬁnal bidirectional RNN in the text encoder CBHG [1]. As in [2], this choice leads to better token variation. In inference mode, this pathway can be used to predict the style embedding directly from text features. Note that the model completely ignores
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2408520939	. Substantial effort has also gone into modeling emotion, but these methods, too, have traditionally required keywords, semantic representations, or labels for model training. Recent examples include [10], [11], [12], [13], and [14], [15]. [16] explores various methods to predict acoustic features such as i-vectors [17] from semantic embeddings. These methods rely on a complex set of hand-designed fea
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2608207374	Each depicts the same audiobook phrase unseen during training. We see that the TP-GST model yields a more varied F 0 contour and richer spectral detail. This example also highlights a point noted in [1], which is that baseline Tacotron models trained on expressive speech can result in synthesis with a continuously declining pitch (green curve). Like a GST Tacotron, we see that the TP-GST model ﬁxes
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2608207374	ion 4.5) and audio samples [20] suggest that z has primarily learned speaker gender and identity rather than prosody or speaking style. 3. MODEL Our model is based on an augmented version of Tacotron [1], a recently proposed state-of-the-art speech synthesis model that predicts mel spectrograms directly from grapheme or phoneme sequences. The augmented version we use is the Global Style Token (GST) [
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2747227176	l effort has also gone into modeling emotion, but these methods, too, have traditionally required keywords, semantic representations, or labels for model training. Recent examples include [10], [11], [12], [13], and [14], [15]. [16] explores various methods to predict acoustic features such as i-vectors [17] from semantic embeddings. These methods rely on a complex set of hand-designed features, howev
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2157331557	models local and contextual information in the input sequence. A CBHG consists of a bank of 1-D convolutional ﬁlters, followed by highway networks [22] and a bidirectional Gated Recurrent Unit (GRU) [23] recurrent neural net (RNN). Since the text encoder outputs a variable-length sequence, the ﬁrst step of TP-GST is to pass this sequence through a 64-unit time-aggregating GRU-RNN, and use its ﬁnal ou
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2608207374	pent modeling such renderings using annotations, explicit labels are difﬁcult to deﬁne precisely, costly to acquire, noisy in nature, and don’t necessarily correlate with perceptual quality. Tacotron [1] is a state-of-the-art speech synthesis system that computes its output directly from graphemes or phonemes. Like many modern TTS systems, it learns an implicit model of prosody from statistics of the
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2608207374	prediction pathways in more detail below. 3.1. Text features Both TP-GST pathways use as features the output of Tacotron’s text encoder. This output is computed by an encoder submodule called a CBHG [1], which explicitly models local and contextual information in the input sequence. A CBHG consists of a bank of 1-D convolutional ﬁlters, followed by highway networks [22] and a bidirectional Gated Rec
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2603120115	re. These methods, however, have largely required explicit annotations, which pose the difﬁculties discussed in Section 1. INTSINT [3], ToBi [4], Momel [5], landmark detection [6], Tilt [7], and SLAM [8] all describe methods to annotate or classify prosodic features such as breaks, intonation, rhythm, and melody. Notable among these is AuToBI [9], which auarXiv:1808.01410v1 [cs.CL] 4 Aug 2018 tomatic
2885800352	Predicting Expressive Speaking Style from Text in End-To-End Speech Synthesis	2780113219	rt has also gone into modeling emotion, but these methods, too, have traditionally required keywords, semantic representations, or labels for model training. Recent examples include [10], [11], [12], [13], and [14], [15]. [16] explores various methods to predict acoustic features such as i-vectors [17] from semantic embeddings. These methods rely on a complex set of hand-designed features, however, an
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2409591106	(Q-only) or passage-only (P-only) information. We show that on many tasks, the results obtained are surprisingly strong, outperforming many base1 We note several other QA datasets (Yang et al., 2015; Miller et al., 2016; Nguyen et al., 2016; Paperno et al., 2016; Clark and Etzioni, 2016; Lai et al., 2017; Trischler et al., 2017; Joshi et al., 2017) not addressed in this paper. lines, and sometimes even surpassing th
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2512077205	on (RC) has emerged as a popular task, with researchers proposing various end-to-end deep learning algorithms to push the needle on a variety of benchmarks. As characterized by Hermann et al. (2015); Onishi et al. (2016), unlike prior work addressing question answering from general structured knowledge, RC requires that a model extract information from a given, unstructured passage. It’s not hard to imagine how such
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2512077205	6% 24.8% CBT-V 48.8% 45.0% CBT-P 34.1% 37.9% Table 3: Accuracy on CBT tasks using KV-MemNets (sentence memory)varying passage size. all the suppressed baselines and 5 additional baselines reported by Onishi et al. (2016). We suspect Metric Full Q-only P-only ∆(min) EM 70.7% 0.6% 10.9% −59.8 F1 79.1% 4.0% 14.8% −64.3 Table 4: Performanceof QANet on SQuAD that the models memorize attributes of speciﬁc entities, justify
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2411480514	77.8% on the true dataset. This drop in accuracy could be due to the anonymization of entities which prevents models from building entity-speciﬁc information. Notwithstanding the deﬁciencies noted by Chen et al. (2016), we found that out CNN,out all the cloze-style RC datasets that we evaluated, appears to be the most carefully designed. Who-did-What P-only models achieve greater than 50% accuracy in both the stric
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2512077205	a-intensiveness of deep learning. These vary both in the source and size of their corpora and in how they cast the prediction problem—as a classiﬁcation task (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al. , 2016; Miller et al. ), span selection (Rajpurkar et al. ,2016; Trischler et al. 2017), sentence retrieval (Wang et al., 2007; Yang et al., 2015), or free-form answe
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2175874768	ataset has avocabulary made of close to120,000 words. Memory Networks with adaptive memory, n-grams and non-linear matching were shown to obtain 100% accuracy on 12 out of 20 bAbI tasks. We note that Lee et al. (2016) previously identiﬁed that bAbI tasks might fall short as a measure of “AI-complete question answering”, proposing two models based on tensor product representations that achieve 100% accuracy on many
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2560730294	d in this paper. lines, and sometimes even surpassing the same models, supplied with both questions and passages. We note that similar problems were shown for datasets in visual question answering by Goyal et al. (2017) and for natural language inference by Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018). Several other papers have discussed the weaknesses of various RC benchamrks (Chen et al.,
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2557764419	diction problem—as a classiﬁcation task (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al. , 2016; Miller et al. ), span selection (Rajpurkar et al. ,2016; Trischler et al. 2017), sentence retrieval (Wang et al., 2007; Yang et al., 2015), or free-form answer generation (Nguyen et al., 2016).1 Researchers have steadily advanced on these benchmarks, proposing myriad neural netw
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2606964149	ep learning. These vary both in the source and size of their corpora and in how they cast the prediction problem—as a classiﬁcation task (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al. , 2016; Miller et al. ), span selection (Rajpurkar et al. ,2016; Trischler et al. 2017), sentence retrieval (Wang et al., 2007; Yang et al., 2015), or free-form answer generation (Nguy
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2252016937	ights. While many RC datasets have been proposed over the years (Hirschman et al., 1999; Breck et al., 2001; Pen˜as et al., 2011; Pen˜as et al., 2012; Sutcliffe et al., 2013; Richardson et al., 2013; Berant et al., 2014), more recently, larger datasets have been proposed to accommodate the data-intensiveness of deep learning. These vary both in the source and size of their corpora and in how they cast the prediction
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2557764419	ingly strong, outperforming many base1 We note several other QA datasets (Yang et al., 2015; Miller et al., 2016; Nguyen et al., 2016; Paperno et al., 2016; Clark and Etzioni, 2016; Lai et al., 2017; Trischler et al., 2017; Joshi et al., 2017) not addressed in this paper. lines, and sometimes even surpassing the same models, supplied with both questions and passages. We note that similar problems were shown for dataset
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2251818205	mann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al. , 2016; Miller et al. ), span selection (Rajpurkar et al. ,2016; Trischler et al. 2017), sentence retrieval (Wang et al., 2007; Yang et al., 2015), or free-form answer generation (Nguyen et al., 2016).1 Researchers have steadily advanced on these benchmarks, proposing myriad neural network architectures aimed at attending to both questions and
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2175874768	r natural language inference by Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018). Several other papers have discussed the weaknesses of various RC benchamrks (Chen et al., 2016; Lee et al., 2016). We discuss these studies in the paragraphs introducing the corresponding datasets below. 2 Datasets In the following section, we provide context on each dataset that we investigate and then describe
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2512077205	neered a set of eight features for each entity e (does e occur in the question, in the passage, etc.), showing that this simple classiﬁer outperformed many earlier deep learning results. Who-did-What Onishi et al. (2016) extracted pairs of news articles, each pair referring to the same events. Adopting the cloze-style, they remove a person’s name (the answer) from the ﬁrst sentence of one article (the question). A mo
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2411480514	in the passage, vs memorizing characteristics of given entities across examples, and thus ignoring passages. On average, passages contain 26 entities, with over 500 total possible answer candidates. Chen et al. (2016) analyzed the difﬁculty of the CNN and Daily Mail tasks. They hand-engineered a set of eight features for each entity e (does e occur in the question, in the passage, etc.), showing that this simple c
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2126209950	have been proposed to accommodate the data-intensiveness of deep learning. These vary both in the source and size of their corpora and in how they cast the prediction problem—as a classiﬁcation task (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al. , 2016; Miller et al. ), span selection (Rajpurkar et al. ,2016; Trischler et al. 2017), sentence retrieval (Wang et al., 2
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2251818205	with question-only (Q-only) or passage-only (P-only) information. We show that on many tasks, the results obtained are surprisingly strong, outperforming many base1 We note several other QA datasets (Yang et al., 2015; Miller et al., 2016; Nguyen et al., 2016; Paperno et al., 2016; Clark and Etzioni, 2016; Lai et al., 2017; Trischler et al., 2017; Joshi et al., 2017) not addressed in this paper. lines, and sometim
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2126209950	s In the following section, we provide context on each dataset that we investigate and then describe our process for corrupting the data as required by our question- and passage-only experiments. CBT Hill et al. (2016) prepared a cloze-style (ﬁll in the blank) RC dataset by using passages from children’s books. In their dataset, each passage consists of 20 consecutive sentences, and each question is the 21st senten
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2411480514	t al. (2017) and for natural language inference by Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018). Several other papers have discussed the weaknesses of various RC benchamrks (Chen et al., 2016; Lee et al., 2016). We discuss these studies in the paragraphs introducing the corresponding datasets below. 2 Datasets In the following section, we provide context on each dataset that we investigat
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2606964149	tained are surprisingly strong, outperforming many base1 We note several other QA datasets (Yang et al., 2015; Miller et al., 2016; Nguyen et al., 2016; Paperno et al., 2016; Clark and Etzioni, 2016; Lai et al., 2017; Trischler et al., 2017; Joshi et al., 2017) not addressed in this paper. lines, and sometimes even surpassing the same models, supplied with both questions and passages. We note that similar problem
2885826215	How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks	2409591106	vide references to the source papers and brieﬂy discuss any implementation decisions necessary to reproduce our results. Key-Value Memory Networks We implement a Key-Value Memory Network (KV-MemNet) (Miller et al., 2016), applying it to bAbI and CBT. KV-MemNets are based on Memory Networks (Sukhbaatar et al., 2015), shown to perform well on both datasets. For bAbI tasks, the keys and values both encode the passage as
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2526425061	out any additional models. The end-toend models proposed in the literature to use a Connectionist Temporal Classiﬁcation framework [8–12], an attentionbased encoder-decoder framework [13–16], or both [17,18]. Our goal is to build speech recognition model that explicitly use a dialog-level context information beyond sentencelevel information especially in an end-to-end manner so that the whole system can
2886025712	Dialog-Context Aware end-to-end Speech Recognition	1999965501	anguage models, recent research has developed a variety of ways to incorporate document-level or dialog-level context information [2–5]. Mikolov et al. proposed a context-dependent RNN language model [2] using a context vector which is produced by applying latent Dirichelt allocation [6] on the preceding text. Wang et al. proposed using a bag-of-words to represent the context vector [3], and Ji et al
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2526425061	d-of-speech, and blank tokens. Note that no pronunciation lexicon was used in any of the experiments. 3.2. Training and decoding We used joint CTC/Attention end-to-end speech recognition architecture [17, 18] with ESPnet toolkit [24]. We used a CNN-BLSTM encoder as suggested in [25,26]. We followed the same six-layer CNN architecture as the prior study, except we used one input channel instead of three, s
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2526425061	ech recognition model. 2. DIALOG-CONTEXT END-TO-END ASR 2.1. End-to-end models We perform end-to-end speech recognition using a joint CTC/Attention-based approach with graphemes as the output symbols [17,18]. The key advantage of the joint CTC/Attention framework is that it can address the weaknesses of the two main end-to-end models, Connectionist Temporal Classiﬁ- cation (CTC) [8] and attention-based e
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2573537087	Ji et al. proposed using the last RNN hidden states from the previous sentence to represent the context vector [4]. Liu et al. proposed using an external RNN to model dialog context between speakers [5]. All of these models have been developed and optimized on text data, and therefore must still be combined with conventional acoustic models, which are optimized separately without any context informa
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2521999726	ng transcription. The models were trained on 300 hours of Switchboard data only. Train (˘300hrs) CH SWB Models WER WER sentence-level end2end Seq2Seq A2C [22] 40.6 28.1 CTC A2C [10] 31.8 20.0 CTC A2C [12] 32.1 19.8 CTC A2W(Phone/external-LM init.) [23] 23.6 14.6 sentence-level end2end Our baseline (CTC/Seq2Seq) 34.4 19.0 dialog-context aware end2end Our proposed model(a) 34.1 18.2 Our proposed model(b
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2530876040	ny of the experiments. 3.2. Training and decoding We used joint CTC/Attention end-to-end speech recognition architecture [17, 18] with ESPnet toolkit [24]. We used a CNN-BLSTM encoder as suggested in [25,26]. We followed the same six-layer CNN architecture as the prior study, except we used one input channel instead of three, since we did not use delta or delta delta features. Input speech features were
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2519224033	optimized on text data, and therefore must still be combined with conventional acoustic models, which are optimized separately without any context information beyond sentence-level. The recent study [7] attempted to integrated such dialog sessionaware language model with acoustic models, however, it requires disjoint training procedure. There have been no studies of speech recognition models that in
2886025712	Dialog-Context Aware end-to-end Speech Recognition	2526425061	has a small bias for shorter utterances. The ﬁnal score s(yjx) is normalized with a length penalty 0:1. The models were implemented by using the Chainer deep learning library [30], and ESPnet toolkit [17,18,24]. 4. RESULTS We evaluated both the end-to-end speech recognition model which was built on sentence-level data (sentence-levelend2end) and our proposed dialog-context aware end-to-end speech recognitio
2886076728	Improved Language Modeling by Decoding the Past	2409027918	(2018a;b); Melis et al. (2018); Yang et al.
2886076728	Improved Language Modeling by Decoding the Past	2510842514	340 Chung et al. (2016) – HM-LSTM 35M 1.
2886076728	Improved Language Modeling by Decoding the Past	2571859396	The advantages of better information retention due to PDR are maintained when combined with a continuous cache pointer (Grave et al. (2016)), where our method yields an absolute improvement of 1.
2886076728	Improved Language Modeling by Decoding the Past	2549416390	Here we assume that the weights of the decoder are tied with the token embedding matrix E (Inan et al. (2016); Press & Wolf (2017)).
2886076728	Improved Language Modeling by Decoding the Past	2409027918	Melis et al. (2018) also achieve similar results with highly regularized LSTMs.
2886076728	Improved Language Modeling by Decoding the Past	2549416390	PDR utilizes the symmetry between the inputs and outputs of a language model, a fact that is also exploited in weight tying (Inan et al. (2016); Press & Wolf (2017)).
2886076728	Improved Language Modeling by Decoding the Past	2549416390	Since xt can be mapped to a trivial probability distribution overW , this operation can be interpreted as transforming distributions overW (Inan et al. (2016)).
2886186188	A Survey on Sentiment and Emotion Analysis for Computational Literary Studies	2469055036	ch is the production of tension scores for the text passages. The second approach is the generation of transition graphs that represent the development of emotional states, similar to the ones used byReagan et al. (2016);Kim et al.(2017a);Samothrakis and Fasli (2015). Both approaches use the list of 1,000 most common English words annotated with pleasure, arousal, and dominance ratings (Heise,1965). To produce a resi
2886186188	A Survey on Sentiment and Emotion Analysis for Computational Literary Studies	2469055036	lled with joy and love. However, the task of automatic classiﬁ- cation of these genres is not always that straightforward and reliable, as we will see in this section. Story-type clustering A study byReagan et al. (2016) is inspired by Kurt Vonnegut’s lecture on emotional arcs of stories6, Reagan et al. test the idea that the plot of each story can be plotted as an emotional arc, i.e. a time series graph, where the x
2886186188	A Survey on Sentiment and Emotion Analysis for Computational Literary Studies	2404480901	n be characterized by the emotions they portray. To that end they collect works of genres mystery, humor, fantasy, horror, science ﬁction and western from the Project Gutenberg. Using WordNet-Affect (Strapparava and Valitutti, 2004) to detect emotion words as categorized by Ekman’s fundamental emotion classes, they calculate an emotion score for each sentence in the text. Each work is then transformed into six signals, one for e
2886186188	A Survey on Sentiment and Emotion Analysis for Computational Literary Studies	1968773332	research in emotion in the late twentieth century. Ekman’s categories of basic emotions are frequently used in the research on computational facial emotion recognition (e.g., Essa and Pentland(1997),Pantic and Rothkrantz (2000),Bartlett et al.(2005)) and well as in emotion recognition from text. 3.1.2 Plutchik’s Wheel of Emotions Robert Plutchik was an American psychologist and a professor of psychiatry at the Albert Einste
2886260596	LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs	2109000768	state-of-the-art accuracy in both part-ofspeech tagging and lemmatization in Czech, German, and Arabic. 1 Introduction Morphologically-rich languages are often difﬁcult to process in many NLP tasks (Tsarfaty et al., 2010). As opposed to analytical languages like English, morphologically-rich languages encode diverse sets of grammatical information within each word using inﬂections, which convey characteristics such as
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2513651200	[23] employs a variable sized memory model to enrich the LSTM-based input encoding information.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2104651652	[32] and [9] both further improve the performance by taking the ESIM model as a baseline model.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2221711388	[34] introduces the “matching-aggregation” framework to compare representations between words and then aggregate their matching results for final decision.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	1840435438	In 2015, Bowman released the SNLI corpus [3] that provides more than 570K hypothesis-premise sentence pairs.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2513651200	Various attention-based memory neural networks [37] have been explored to solve the NLI problem [15,6,23].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2763722198,1840435438	We conduct experiments on three NLI datasets: SNLI [3], SCITAIL [12] and MPE [14].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	1840435438,2133564696	where cp,i ∈ R is an inference vector in the current turn, k = [1, 2, 3] is the index current turn, i = [1, 2, 3, · · · , lp],m p,i ∈ R is a memory vector stores the historical inference information, and Winf ∈ R3d×d is used for dimension reduction.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2556553881	We design three effective matching functions: f , f and f to match two vectors [31,33,5].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2250790822	Early work on the NLI task mainly uses conventional statistical methods on small-scale datasets [7,20].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2250539671	We initialize the word embeddings by the pre-trained embeddings of 300D GloVe 840B vectors [26].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	1840435438	The large SNLI [3] corpus is served as a major benchmark for the NLI task.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2221711388	Matching is implemented by several functions based on element-wise operations [34,25].
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2104651652,2121495183	NLI is also called Recognizing Textual Entailment (RTE) [7] in earlier works and a lot of statistical-based [9] and rule-based approaches [19] are proposed to solve the problem.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2064675550	In this paper, we utilize a bidirectional LSTM (BiLSTM) [11] as our encoder to transform the word embeddings of premise and hypothesis to context vectors.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	1902237438,2133564696	The relevant contexts can be acquired by a softattention mechanism [2,18], which has been applied to a bunch of tasks successfully.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2763722198	The state-of-the-art model on MPE dataset is SE model proposed by [14], which makes four independent predictions for each sentence pairs, and the final prediction is the summation of four predictions.
2886416400	Multi-turn Inference Matching Network for Natural Language Inference	2118463056,2221711388	The studies related to “matching-aggregation” but without bidirectional interaction are not listed [27,34].
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	(2015)), FastSent (Hill et al. (2016)) and QuickThoughts (Logeswaran & Lee (2018)), exploiting the closeness of adjacent sentences in a text corpus.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	1840435438	(2017) use the SNLI (Bowman et al. (2015)) and MultiNLI (Williams et al.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	(2018) and InferSent in Conneau et al. (2017). The other numbers (except LangMod) have been taken from Conneau et al.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	0) in Conneau et al. (2018). The performance of a third method Seq2Tree using a gated convolutional network (GCN) is however significantly better than the ConsSent encoders (except on the WordContent task).
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2126920742	Such an approachwas used byWagner et al. (2009) to train a classifier to judge the grammat-
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	To begin with, an untrained BiLSTM model with max pooling of the intermediate states performs fairly well on several transfer and linguistic probing tasks (Conneau et al. (2018)).
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	For more details on these tasks, please refer to Conneau & Kiela (2018) and Conneau et al. (2018).
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	2612953412	Following Conneau et al. (2017), we use a bidirectional LSTM to process a sequence of tokens and take a max-pool of the intermediate hidden states to compute a distributed representation.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	1486649854	Notably, all the methods perform better than SkipThought-LN (Kiros et al. (2015)) on an average and on most individual tasks.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	1486649854	SkipThought is described in (Kiros et al., 2015), QuickThoughts in (Logeswaran & Lee, 2018) and MultiTask in Subramanian et al.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	1840435438	They train a sentence encoder on large scale natural language inference datasets (Bowman et al. (2015); Williams et al.
2886690398	Unsupervised Learning of Sentence Representations Using Sequence Consistency	1486649854	Work by Ruder & Howard (2018) and Radford (2018) has shown that training language models on large text corpora produce excellent sentence encoders that perform very well on downstream tasks.
2886727903	Explaining Queries over Web Tables to Non-Experts.	2252136820	20,000 complex NL questions formulated by actual users on thousands of extracted web tables. As our formal query language over tables we use lambda DCS, a standard query language in the NLP community [27, 4, 30]. We note that lambda DCS is geared towards queries one would write in a search engine { such as those in [4, 1 arXiv:1808.04614v1 [cs.CL] 14 Aug 2018 x : &quot;Greece held its last Olympics in what y
2886727903	Explaining Queries over Web Tables to Non-Experts.	2251957808	anation of the query execution. 5.1 Query to Utterance Given a formal query in lambda DCS we provide a domain independent method for converting it into a detailed NL utterance. Drawing on the work in [35] we use a similar technique of deriving an NL utterance alongside the formal query. We introduce new NL templates describing complex lambda DCS operations for querying tables. Example 5.1. The lambda
2886727903	Explaining Queries over Web Tables to Non-Experts.	2611818442	arser on procured user feedback. Feedback being pairs of NL questions and their correct query. This approach is in line with the human in the loop paradigm of users enhancing machine learning systems [21, 22]. User studies conducted via Amazon Mechanical Turk (AMT) clearly show our query explanations to be applicable to nonexperts. Furthermore, user feedback was benecial for improving the correctness of
2886727903	Explaining Queries over Web Tables to Non-Experts.	2101964891	he content of a cell, or the result of an aggregate function on cells). As discussed in the introduction, we make the assumption that a query concerns a single table. Following the model presented in [30], all table records are ordered from top to bottom with each record possessing a unique Index(0, 1, 2, ...). In addition, every record has a pointer Prevto the record above it. The values of table cel
2886727903	Explaining Queries over Web Tables to Non-Experts.	2101964891	demonstrate the eectiveness of our query explanations by further enhancing a semantic parser for querying tables [37] and testing it against a benchmark dataset of thousands of complex NL questions [30]. At deployment, users are able to make an informed choice of the query, based on our explanation mechanism. We further leverage the use of explaining queries to users in order to retrain the semantic
2886727903	Explaining Queries over Web Tables to Non-Experts.	2101964891,2252136820,2611818442	e mapping of NL phrases into a formal language. As Machine Learning techniques are standardly used in semantic parsing, a training set of question-answer pairs is provided alongside a target database [4, 30, 22]. The parser is a parameterized function that is trained by updating its parameters such that questions from the training set are translated into queries that yield the correct answers. A crucial chal
2886727903	Explaining Queries over Web Tables to Non-Experts.	2269738476,2611818442	ers, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 7 unique column headers, containing far more relations than closed domain datasets [26, 22] and datasets for querying knowledge bases [7]. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operation
2886727903	Explaining Queries over Web Tables to Non-Experts.	2269738476,2511149293	expert users. Our approach of utilizing user feedback to enhance the interface both at deployment and also in retraining the parser is a joint implementation of both database and NLP common practices [26, 22, 36]. Provenance. Provenance models have long been studied in the context of relational queries [6, 8, 11, 17, 19, 20]. The complexity of provenance expressions resulted in multiple approaches to represen
2886727903	Explaining Queries over Web Tables to Non-Experts.	2047705935	we explain parsed queries by highlighting their cell-based provenance while also providing detailed NL utterances. Explaining formal queries in NL has been studied both on relational database schemas [24, 13] and KB systems [35]. Our domain independent utterances are comparable to the generic template of [24], however when challenged with complex queries, both methods are forced to return utterances which
2886727903	Explaining Queries over Web Tables to Non-Experts.	2251957808	in Figure 1 shows the highlights for our example query). These provenance-based highlights are combined with a more conventional form of query explanation via NL utterances. Drawing on previous work [35], we design an extensible, domain independent context-free grammar that derives NL utterances describing lambda DCS queries. One of our key contributions is showing empirically that combining NL utter
2886727903	Explaining Queries over Web Tables to Non-Experts.	2252136820	guage, designed to represent complex NL questions involving sorting, aggregation intersection and more. It has been considered a standard language for performing semantic parsing over knowledge bases [27, 4, 30, 37]. A lambda DCS formula is executed against a target table and returns either a set of values (string, number or date) or a set of table records. We describe here a simplied version of lambda DCS that
2886727903	Explaining Queries over Web Tables to Non-Experts.	2511149293	ic parser candidate queries can be employed on other datasets, allowing non-experts to easily annotate signicant amounts of the data. The positive impact of query annotations has been established in [36], hence by annotating weakly supervised datasets we will produce quality training data used to improve the accuracy of state-of-the-art parsers on a myriad of tasks. Another direction worth exploring
2886727903	Explaining Queries over Web Tables to Non-Experts.	2511149293,2611818442	ion-query pairs improves its performance [3], up until now the only way to achieve this was by relying on expert annotators. We are the rst to illicit such annotations without any reliance on experts [3, 36, 22]. 6.1 Implementation We return to our system architecture from Figure 2. Presented with an NL question and corresponding table, our interface parses the question into lambda DCS queries using the stat
2886727903	Explaining Queries over Web Tables to Non-Experts.	1559723967,2161002933	ith their respective queries. However, annotating NL questions with formal queries is a costly operation, hence recent works have trained semantic parsers on examples labeled solely with their answer [9, 28, 4, 30]. This weak supervision facilitates the training process at the cost of learning from incorrect queries. Figure 8 presents two candidate queries for the question &quot;What was the last year the team
2886727903	Explaining Queries over Web Tables to Non-Experts.	2145618437,2154268919,2269738476	learn from quality feedback collected by non-experts. 8. RELATED WORK NLinterfaces. Building a natural language interface (NLIDB) for querying databases has been extensively studied in the literature [25, 2, 32, 16, 26, 22]. Notably, NaLIR [26] is an NLIDB where users are presented explanations of candidate queries in the form of an intermediate representation, termed the query tree. Since we are geared towards nonexper
2886727903	Explaining Queries over Web Tables to Non-Experts.	2252136820,2269738476,2611818442	les from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. In contrast to common NLIDB benchmarks [22, 4, 26], WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predened templates but were hand crafted
2886727903	Explaining Queries over Web Tables to Non-Experts.	2511149293	order improve a state-of-the-art semantic parser. Machine learning systems for querying knowledge bases have long been the standard in NLP research. From systems for question answering over Freebase [4, 36], to NL interfaces mapping directly to SQL [22, 38]. The work in [37] presents a state-of-the-art semantic parser over the NLP community benchmark of WikiTableQuestions [30]. We further improve the co
2886727903	Explaining Queries over Web Tables to Non-Experts.	2101964891	parser [37]. The parser is used as the NL interface, mapping complex NL questions into queries over web tables. We test our solution on real-world data using the WikiTableQuestions benchmark dataset [30] which includes over 20,000 complex NL questions formulated by actual users on thousands of extracted web tables. As our formal query language over tables we use lambda DCS, a standard query language
2886727903	Explaining Queries over Web Tables to Non-Experts.	2097647324,2511149293	ples. We witnessed an increase in both correctness and MRR (mean reciprocal rank) that grows in the number of annotated train examples. This further asserts the signicance of annotated training data [31, 36] and shows that our system can learn from quality feedback collected by non-experts. 8. RELATED WORK NLinterfaces. Building a natural language interface (NLIDB) for querying databases has been extensi
2886727903	Explaining Queries over Web Tables to Non-Experts.	2107571548	queries [6, 8, 11, 17, 19, 20]. The complexity of provenance expressions resulted in multiple approaches to represent provenance that is user-understandable. These include provenance in a graph form [1, 11, 15, 29, 34] and methods that present dierent ways of provenance visualization [20]. The work in [12] presents NLProv, an interface built on top of NaLIR complete with a provenance model for NL queries. Their so
2886727903	Explaining Queries over Web Tables to Non-Experts.	2189089430	by retraining it on pairs of questions and formal queries, marked as correct translations by users. While it is known that training a semantic parser on question-query pairs improves its performance [3], up until now the only way to achieve this was by relying on expert annotators. We are the rst to illicit such annotations without any reliance on experts [3, 36, 22]. 6.1 Implementation We return to
2886727903	Explaining Queries over Web Tables to Non-Experts.	1552694902,2167541073	so in retraining the parser is a joint implementation of both database and NLP common practices [26, 22, 36]. Provenance. Provenance models have long been studied in the context of relational queries [6, 8, 11, 17, 19, 20]. The complexity of provenance expressions resulted in multiple approaches to represent provenance that is user-understandable. These include provenance in a graph form [1, 11, 15, 29, 34] and methods
2886727903	Explaining Queries over Web Tables to Non-Experts.	2252136820	rned records indices will be f0;n 4g. 3.2 Query Language Following the denition of our data model we introduce our formal query language, lambda dependency-based compositional semantics (lambda DCS) [27, 4], which is a language inspired by lambda calculus, that revolves around sets. Lambda DCS was originally designed for building an NL interface over Freebase [5]. LanguageOperators. Lambda DCS is a high
2886727903	Explaining Queries over Web Tables to Non-Experts.	2251957808	s by highlighting their cell-based provenance while also providing detailed NL utterances. Explaining formal queries in NL has been studied both on relational database schemas [24, 13] and KB systems [35]. Our domain independent utterances are comparable to the generic template of [24], however when challenged with complex queries, both methods are forced to return utterances which are quite long. The
2886727903	Explaining Queries over Web Tables to Non-Experts.	2101964891	se the state-of-the-art parser in [37] to parse the question into a set of candidate lambda DCS queries. The parser is trained for the task of querying web tables using the WikiTableQuestions dataset [30]. Query Explanations. Following the mapping of a question to a set of candidate queries, our interface will generate the relevant query explanations for each query, displaying a detailed NL utterance
2886727903	Explaining Queries over Web Tables to Non-Experts.	1559723967,2161002933	trong supervision. As we described in Section 6.2 annotating questions with formal queries is a costly operation, hence recent works have trained parsers on questions labeled solely with their answer [9, 28, 4, 30]. By explaining the candidate queries of a baseline semantic parser non experts workers were able to annotate thousands of examples from the WikiTableQuestions dataset without any knowledge of the for
2886727903	Explaining Queries over Web Tables to Non-Experts.	2047705935	to visually explain complex queries to non-experts. The evaluation of our query explanations was focused on the comprehension of non-experts, in contrast to the expert driven evaluation presented in [24]. Our user study empirically showed a joint approach of NL utterances and table highlights facilitates user understanding compared to explaining only through utterances [13]. NL utterances have also b
2886727903	Explaining Queries over Web Tables to Non-Experts.	2047705935,2269738476	y identied 78.4% of the explanations as being correct or incorrect. country. Our study included 35 distinct workers, a signi- cant number of participants compared to previous works on NL interfaces [12, 26, 24]. Rather than relying on a small set of NL test questions [12, 26] we presented each worker with 20 distinct questions that were randomly selected from the WikiTableQuestions benchmark dataset (Sectio
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	2119466907	01; Zhang and Zhou, 2007; Fu¨rnkranz et al., 2008) and ensemble methods (Tsoumakas et al., 2011; Szyman´ski et al., 2016). Simple neural networks models have also been applied to deal with MLC tasks (Zhang and Zhou, 2006; Nam et al., 2014; Benites and Sapozhnikova, 2015; Kurata et al., 2016). Li et al. (2015) proposed to consider the previously generated labels as features for predicting new ones. Yang et al. (2018)
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	2251470734	1; Szyman´ski et al., 2016). Simple neural networks models have also been applied to deal with MLC tasks (Zhang and Zhou, 2006; Nam et al., 2014; Benites and Sapozhnikova, 2015; Kurata et al., 2016). Li et al. (2015) proposed to consider the previously generated labels as features for predicting new ones. Yang et al. (2018) further developed this idea to use recurrent neural networks to model the correlation betw
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	1753402186	e learning algorithms for the MultiLabel Classiﬁcation (MLC) task, problem transformation methods (Boutell et al., 2004; Tsoumakas and Vlahavas, 2007; Read et al., 2011),algorithm adaptation methods (Clare and King, 2001; Zhang and Zhou, 2007; Fu¨rnkranz et al., 2008) and ensemble methods (Tsoumakas et al., 2011; Szyman´ski et al., 2016). Simple neural networks models have also been applied to deal with MLC tasks (Zh
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	2250626347	ly used in various NLP tasks such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Duan et al., 2007) (similar to word Clustering (Jin et al., 2007)), sentiment analysis (Huang et al., 2014) and name entity recognition (Li et al., 2016). Niu et al. (2017) claimed that using word sememe information in HowNet can improve word representation. Zeng et al. (2018) proposed to expand the Lingui
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	2052684427	for the MultiLabel Classiﬁcation (MLC) task, problem transformation methods (Boutell et al., 2004; Tsoumakas and Vlahavas, 2007; Read et al., 2011),algorithm adaptation methods (Clare and King, 2001; Zhang and Zhou, 2007; Fu¨rnkranz et al., 2008) and ensemble methods (Tsoumakas et al., 2011; Szyman´ski et al., 2016). Simple neural networks models have also been applied to deal with MLC tasks (Zhang and Zhou, 2006; Na
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	1492453520	ons. 2 Related Work HowNet has been widely used in various NLP tasks such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Duan et al., 2007) (similar to word Clustering (Jin et al., 2007)), sentiment analysis (Huang et al., 2014) and name entity recognition (Li et al., 2016). Niu et al. (2017) claimed that using word sememe information in HowNet can improve word representation. Zeng e
2887323611	Sememe Prediction: Learning Semantic Knowledge from Unstructured Textual Wiki Descriptions.	1522301498	s table. “P” means Precision,“R” means recall rate. 300. The batch size is 20. &lt; EOS &gt; token is added to the end of a sememe sequence to indicate when to stop prediction. We use Adam optimizer (Kingma and Ba, 2014) to minimize the loss. We train our model for 10 epochs, and choose the model parameters from the epoch that gets the highest F1 score on the Dev set. 4.4 Results and Analysis Weuse micro Precision (P
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2162586529	adford et al. 2018). In this paper we investigate the last two branches and further propose a hybrid architecture that combines both of them properly. Answer Validation. Early answer validation task (Magnini et al. 2002) aims at ranking multiple candidate answers to return a most reliable one. Later, the answer validation exercise (Rodrigo, Penas, and Verdejo 2008) has been proposed˜ to decide whether an answer is co
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	1840435438	clusion occurs, such as the question asks for “the decline of rainforests” but the passage mentions that “the rainforests spread out”. Inspired by recent advances in natural language inference (NLI) (Bowman et al. 2015), we investigate three different architectures for the answer verifying task. The ﬁrst one is a sequential model that takes two sentences as a long sequence, while the second one attempts to capture i
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	1522301498	d on the performance on development set, we set as 0.3 and to be 1. As for answer veriﬁers, we use the original conﬁguration from Radford et al. (2018) for Model-I. For Model-II, the Adam optimizer (Kingma and Ba 2014) with a learning rate of 0.0008 is used, the hidden size is set as 300, and a dropout (Srivastava et al. 2014) of 0.3 is applied for preventing overﬁtting. The batch size is 48 for the reader, 64 for
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2250790822	er extends existing approaches by introducing two auxiliary losses that enhance these two tasks independently. Recognizing Textual Entailment. Recognizing textual entailment (RTE) (Dagan et al. 2010; Marelli et al. 2014), or known as natural language inference (NLI) (Bowman et al. 2015), requires systems to understand entailment, contradiction or semantic neutrality between two sentences. This task is strongly relate
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	1840435438	hat enhance these two tasks independently. Recognizing Textual Entailment. Recognizing textual entailment (RTE) (Dagan et al. 2010; Marelli et al. 2014), or known as natural language inference (NLI) (Bowman et al. 2015), requires systems to understand entailment, contradiction or semantic neutrality between two sentences. This task is strongly related to no-answer detection, where the machine needs to understand if
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2427527485	hension benchmark that aims to test the models whether they have truely understood the questions by knowing what they don’t know. It combines answerable questions from the previous SQuAD 1.1 dataset (Rajpurkar et al. 2016) with 53,775 unanswerable questions about the same passages. Crowdsourcing workers craft these questions with a plausible answer in mind, and make sure that they are relevant to the corresponding pass
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2427527485	hese more difﬁcult cases. Related Work Reading Comprehension Datasets. Various large-scale reading comprehension datasets, such as cloze-style test (Hermann et al. 2015), answer extraction benchmark (Rajpurkar et al. 2016; Joshi et al. 2017) and answer generation benchmark (Nguyen et al. 2016; Koˇcisk `y et al. 2018), have been proposed. However, these datasets still guarantee that the given context must contain an an
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2626778328	ndexes in the vocab, W e is the token embedding matrix, W pis the position embedding matrix, and nis the number of transformer blocks. Each block consists of a masked multi-head self-attention layer (Vaswani et al. 2017) and a position-wise feed-forward layer. Residual connection and layer normalization are used after each layer. The last token’s activation hl m n is then fed into a linear projection layer followed b
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	1544827683	time of submission (Aug. 28th, 2018). Introduction The ability to comprehend text and answer questions is crucial for natural language processing. Due to the creation of various large-scale datasets (Hermann et al. 2015; Nguyen et al. 2016; Joshi et al. 2017; Koˇcisk y et al. 2018),` remarkable advancements have been made in the task of machine reading comprehension. Nevertheless, one important hypothesis behind cur
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	1544827683	ting that our system performs less effectively on these more difﬁcult cases. Related Work Reading Comprehension Datasets. Various large-scale reading comprehension datasets, such as cloze-style test (Hermann et al. 2015), answer extraction benchmark (Rajpurkar et al. 2016; Joshi et al. 2017) and answer generation benchmark (Nguyen et al. 2016; Koˇcisk `y et al. 2018), have been proposed. However, these datasets still
2887386650	Read + Verify: Machine Reading Comprehension with Unanswerable Questions	2626778328	uctures for running three different models. (b) Generative Pre-trained Transformer proposed by Radford et al. (2018). Here, “Masked Multi Self Attention” refers to multi-head self-attention function (Vaswani et al. 2017) that only attends to previous tokens. “Add &amp; Norm” indicates residual connection and layer normalization. (c) Our proposed token-wise interaction model, which is designed to compare two sentences
2887440448	KG Cleaner: Identifying and Correcting Errors Produced by Information Extraction Systems	104184427	for the credibility loss function and categorical cross entropy for relation repair. 4.3 Training and Negative Sampling We trained KG Cleaner using stochastic gradient descent with a momentum of 0.9 (Sutskever et al., 2013) and a decay rate of 10 6. We initialized the model with Xavier Uniform initialization (Glorot and Bengio,2010) and used back propagation to learn the parameters. Since our Wikidata facts provide only
2887647914	Top-Down Tree Structured Text Generation.	648786980	imit the solution space at the beginning of training and gradually relax it. The second is schedule sampling which has been shown to be beneﬁcial dealing with distribution shift in sequence learning (Bengio et al., 2015): the labels are gradually replaced by model predictions with a slowly annealing probability, following a greedy strategy. 6 Experiments To evaluate our model, we conduct experiments on two generation
2887647914	Top-Down Tree Structured Text Generation.	2289899728	n actions, unlike RNNG that also include bottom-up parsing actions. Therefore TDTD-P is less suited for parsing task. 3The samples are publicly available on https://github.com/clab/rnng, released by (Dyer et al., 2016). S . . VP 1 VP 2 NP 2 DT any VB sell RB n’t MD could NP 1 PRP he CC But (a) A parsing result. (b) Visualization of attention. Figure 4: A case study of TDTD-P. (a) The red token indicates an error in
2887665106	Debugging Neural Machine Translations.	2626778328	s.Whileinrecurrentneural DebuggingNeuralMachineTranslations 9 network NMT systems this is rarely a problem, more modern approaches like convolution neural networks [3] and transformer neural networks [17] require trainingofdeepermodelstoachievecompetitivequalitytranslationresults.This, however, results in each layer paying attention only to a subset of the input sentence. Even when all attentions are
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2064675550,2131774270,2147880316	• BiLSTM-CRF [16] is a sequence labeling model which extends a standard BiLSTM neural network [17, 18] with a CRF layer [19].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2616177386	• jPTDP v1 [27] is a joint model for POS tagging and dependency parsing, which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2552110825	• The Stanford “Biaffine” parser v1 [29] extends bmstparser with biaffine classifiers to predict dependency arcs and labels, obtaining the highest parsing result to date on the benchmark English PTB.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2252147815	\Without punctuation&quot; refers to results excluding punctuation and other symbols from evaluation. \Exact match&quot; denotes the percentage of sentences whose predicted trees are entirely correct [25]. [] denotes the use of the pre-trained Stanford tagger for predicting POS tags on test set, instead of using the retrained NLP4J-POS model. Score dierences between the \retrained&quot; parsers on b
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2295030615	3% absolute improvements to BiLSTM-CRF on GENIA and CRAFT, respectively, resulting in the highest accuracies on both experimental corpora. Note that for PTB, CNN-based character-level word embeddings [20] only provided a 0.1% improvement to BiLSTMCRF [16]. The larger improvements on GENIA and CRAFT show that character-level word embeddings are specically useful to capture rare or unseen words in biom
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2094061585	4j/components/ dependency-parsing.html [10]https://github.com/datquocnguyen/jPTDP Nguyen and Verspoor Page 4 of12 gual dependency parsing [31]. We use the Stanford Biane parser v2 in our experiments.[11] Implementation details We use the training set to learn model parameters while we tune the model hyper-parameters on the development set. Then we report nal evaluation results on the test set. The me
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2094061585	926 6.3 training, development and test split from [10].[3] We then use the Stanford constituent-to-dependency conversion toolkit (v3.5.1) to generate dependency trees with basic Stanford dependencies [11]. The CRAFT corpus includes 21K sentences (˘561K words) from 67 full-text biomedical journal articles.[4] These sentences are syntactically annotated using an extended PTB tag set. Given this extended
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2252147815	ance of SOTA dependency parsers, as well as commonly used parsers, on biomedical texts. Prior work on the CRAFT treebank identied the domain-retrained ClearParser [24], now part of the NLP4J toolkit [25], as a top-performing system for dependency parsing over that data. It remains the best performing non-neural model for dependency parsing. In particular, we compare the following parsers: [7]https://
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2170953709	arser comparison on event extraction We present an extrinsic evaluation of the four dependency parsers for the downstream task of biomedical event extraction. Evaluation setup Previously, Miwa et al. [45] adopted the BioNLP 2009 shared task on biomedical event extraction [46] to compare the task-oriented performance of six \pretrained&quot; parsers with 3 dierent types of dependency representations.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2740215900	For the BiLSTM-CRF-based models, we use default hyper-parameters provided in [22] with the following exceptions: for training, we use Nadam [34] and run for 50 epochs.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2740215900	For the three BiLSTM-CRF-based sequence labeling models, we use a performance-optimized implementation from [22].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2127147316	he biomedical texts without consideration of the appropriateness of the tool for these texts. A commonly used tool is the Stanford CoreNLP dependency parser [6], although domain-adapted parsers (e.g. [7]) are sometimes used. Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data [3]. Given the signi- cant improvements in par
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2338266296	show that character-level word embeddings are specically useful to capture rare or unseen words in biomedical text data. Character-level word embeddings are useful for morphologically rich languages [38,27], and although English is not morphologically rich, the biomedical domain contains a wide variety of morphological variants of domain-specic terminology [39]. Words tagged incorrectly are largely ass
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2616177386	Character-level word embeddings are useful for morphologically rich languages [38, 27], and although English is not morphologically rich, the biomedical domain contains a wide variety of morphological variants of domain-specific terminology [39].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2250861254	com/UKPLab/emnlp2017-bilstm-cnn-crf • The Stanford neural network dependency parser [6] (Stanford-NNdep) is a greedy transitionbased parsing model which concatenates word, POS tag and arc label embeddings into a single vector, and then feeds this vector into a multilayer perceptron with one hidden layer for transition classification.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2250861254	A commonly used tool is the Stanford CoreNLP dependency parser [6], although domain-adapted parsers (e.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2127147316	ddings, which are derived by applying a BiLSTM to each word’s character sequence [21]. For the three BiLSTM-CRF-based sequence labeling models, we use a performance-optimized implementation from [22].[7] As detailed later in the POS tagging results section, we use NLP4J-POS to predict POS tags on development and test sets and perform 20-way jackkning [23] to generate POS tags on the training set for
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1491975949	dencies are often modiers of nouns such as determiners or adjectives or pronouns modifying their direct neighbors, while longer dependencies typically represent modiers of the root or the main verb [43]. All parsers obtain higher scores for left dependencies than for right dependencies. This is not completely unexpected as English is strongly head-initial. In addition, the gaps between LSTM-based mo
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2032566933	of the four dependency parsers for the downstream task of biomedical event extraction. Evaluation setup Previously, Miwa et al. [45] adopted the BioNLP 2009 shared task on biomedical event extraction [46] to compare the task-oriented performance of six \pretrained&quot; parsers with 3 dierent types of dependency representations. However, their evaluation setup requires use of a currently unavailable
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1883682364	e highest accuracy scores. These are just 0.1% absolute higher than the accuracies of NLP4J-POS. Note that small variations in POS tagging performance are not a critical factor in parsing performance [41]. In addition, we nd that NLP4J-POS obtains 30-time faster training and testing speed. Hence for the dependency parsing task, we use NLP4J-POS to perform 20-way jackknifing [23] to generate POS tags o
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2148130205	Effort to catalog the key research results in these publications demands automation [1].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2128634885	erformance-optimized implementation from [22].[7] As detailed later in the POS tagging results section, we use NLP4J-POS to predict POS tags on development and test sets and perform 20-way jackkning [23] to generate POS tags on the training set for dependency parsing. Dependency parsers Our second study assesses the performance of SOTA dependency parsers, as well as commonly used parsers, on biomedic
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1970849810	using an extended PTB tag set. Given this extended set, the Stanford conversion toolkit is not suitable for generating dependency trees. Hence, a dependency treebank using the CoNLL 2008 dependencies [12] was produced from the CRAFT treebank using ClearNLP [13]; we directly use this dependency treebank in our experiments. We use sentences from the rst 6 les (PubMed IDs: 11532192{12585968) for developm
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2125712079	The fifth row shows scores of BLLIP+Bio, the BLLIP reranking constituent parser [42] with an improved self-trained biomedical parsing model [10].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	192665053	The final row includes published results of the GENIA POS tagger [36], when trained on 90% of the GENIA corpus (cf.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2134967412	ful for morphologically rich languages [38,27], and although English is not morphologically rich, the biomedical domain contains a wide variety of morphological variants of domain-specic terminology [39]. Words tagged incorrectly are largely associated with gold tags NN, JJ and NNS; many are abbreviations which are also out-of-vocabulary. It is typically dicult for character-level word embeddings to
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2094061585	g the highest scores for 57 test sets (including English test sets) and second highest scores for 14 test sets over total 81 test sets across 45 dierent languages at the CoNLL 2017 shared task [31]. [11]https://github.com/tdozat/Parser-v2 Table 3 POS tagging accuracies on the test set with gold tokenization. [?] denotes a result with a pre-trained POS tagger. We do not provide accuracy results of the
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1970849810	large external corpus, providing useful extra information. BiLSTM-CRF obtains accuracies of 98.44% on GENIA and 97.25% on CRAFT. Using character-level word embeddings helps to produce about 0.5% and [12]Trained on the PTB sections 0{18, the accuracies for the GENIA tagger, Stanford tagger, MarMoT, NLP4JPOS, BiLSTM-CRF and BiLSTM-CRF+CNN-char on the benchmark test set of PTB sections 22-24 were report
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	102233799	LP tools in biomedical domain. The GENIA corpus contains 18K sentences (˘486K words) from 1,999 Medline abstracts, which are manually annotated following the Penn Treebank (PTB) bracketing guidelines [2]. On this treebank, we use the Table 1 The number of les (#le), sentences (#sent), word tokens (#token) and out-of-vocabulary (OOV) percentage in each experimental dataset. Dataset #le #sent #token
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2295030615	M-CRF+CNN-char extends the model BiLSTM-CRF with character-level word embeddings. For each word token, its character-level word embedding is derived by applying a CNN to the word’s character sequence [20]. BiLSTM-CRF+LSTM-char also extends the BiLSTM-CRF model with character-level word embeddings, which are derived by applying a BiLSTM to each word’s character sequence [21]. For the three BiLSTM-CRF-
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2032566933	mploy the online evaluation system for the BioNLP 2011 shared task [49] with the \abstracts only&quot; option.[15] The score is reported using the approximate span &amp; recursive evaluation strategy [46]. Impact of parsing on event extraction Table10presents the intrinsic UAS and LAS (F1) scores on the pre-processed segmented BioNLP 2009 development sentences (i.e. scores with respect to predicted se
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	102233799	n rows of Table4). Finally, we investigate the in uence of pre-trained and retrained parsing models in the biomedical event extraction task (in Table11). Datasets We use two biomedical corpora: GENIA [2] and CRAFT [3]. GENIA includes abstracts from PubMed, while CRAFT includes full text publications. It has been observed that there are substantial linguistic differences between the abstracts and the
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2121227244	NLP4J-POS uses additional features based on Brown clusters [37] and pre-trained word vectors learned from a large external corpus, providing useful extra information.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2055032581	A number of linguistically-annotated resources, notably including the GENIA [2] and CRAFT [3] corpora, have been
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2075322787	It has been observed that there are substantial linguistic differences between the abstracts and the corresponding full text publications [9]; hence it is important to consider both contexts when assessing NLP tools in biomedical domain.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1970849810	odel produces lower accuracy than the retrained tagging models. The nal row includes published results of the GENIA POS tagger [36], when trained on 90% of the GENIA corpus (cf. our 85% training set).[12] It does not support a (re)-training process. In general, we nd that the six retrained models produce competitive results. BiLSTM-CRF and MarMoT obtain the lowest scores on GENIA and CRAFT, respective
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2127147316	oolkit [25], as a top-performing system for dependency parsing over that data. It remains the best performing non-neural model for dependency parsing. In particular, we compare the following parsers: [7]https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf The Stanford neural network dependency parser [6] (Stanford-NNdep) is a greedy transitionbased parsing model which concatenates word, POS tag and ar
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2301095666	parsing,[10] which uses BiLSTMs to learn feature representations shared between POS tagging and dependency parsing. jPTDP can be viewed as an extension of the graph-based dependency parser bmstparser [28], replacing POS tag embeddings with LSTM-based character-level word embeddings. For jPTDP, we train with gold standard POS tags. The Stanford \Biane&quot; parser v1 [29] extends bmstparser with bia
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2128634885	in parsing performance [41]. In addition, we nd that NLP4J-POS obtains 30-time faster training and testing speed. Hence for the dependency parsing task, we use NLP4J-POS to perform 20-way jackknifing [23] to generate POS tags on training data and to predict POS tags on development and test sets. Overall dependency parsing results We present the LAS and UAS scores of dierent parsing models in Table4.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	102233799	POS tagging; Dependency parsing; Biomedical event extraction; Neural networks Background The biomedical literature, as captured in the parallel repositories of PubMed[1] (abstracts) and PubMed Central[2] (full text articles), is growing at a remarkable rate of over one million publications per year. Effort to catalog the key research results in these publications demands automation [1]. Hence extract
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2153579005	These pre-trained vectors were obtained by training the Word2Vec skip-gram model [33] on a PubMed abstract corpus of 3 billion word tokens.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2055032581	Prior work on the CRAFT treebank demonstrated substantial variation in the performance of syntactic processing tools for that data [3].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2612364175	Recent work on biomedical relation extraction has highlighted the particular importance of syntactic information [5].
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2075655036	ring their performance on downstream tasks [47], including a biomedical event extraction task [8]. We thus follow the experimental setup used there; employing the Turku Event Extraction System (TEES, [48]) to assess the impact of parser dierences on biomedical relation extraction.[13] EPE 2017 uses the BioNLP 2009 shared task dataset [46], which was derived from the GENIA treebank corpus (800, 150 an
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	102233799	rmation extraction typically make use of linguistic information, with a specic emphasis on the value of dependency parses. A number of linguistically-annotated resources, notably including the GENIA [2] and CRAFT [3] corpora, have been *Correspondence: dqnguyen@unimelb.edu.au, The University of Melbourne yKarin.Verspoor@unimelb.edu.au, The University of Melbourne [1]https://www.ncbi.nlm.nih.gov/pubm
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1996430422	The first two rows present scores of the pre-trained Stanford NNdep and Biaffine v1 models with POS tags predicted by the pre-trained Stanford tagger [35], while the next two rows 3-4 present scores of these pretrained models with POS tags predicted by NLP4JPOS.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	1996430422	tagger [35], trained on a larger corpus of sections 0–18 (about 38K sentences) of English PTB WSJ text; given the use of newswire training data, it is unsurprising that this model produces lower accuracy than the retrained tagging models.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2032566933	used there; employing the Turku Event Extraction System (TEES, [48]) to assess the impact of parser dierences on biomedical relation extraction.[13] EPE 2017 uses the BioNLP 2009 shared task dataset [46], which was derived from the GENIA treebank corpus (800, 150 and 260 abstract les used for BioNLP 2009 training, development and test, respectively).[14] We only need to provide dependency parses of r
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	76749362	Therefore, we employ the online evaluation system for the BioNLP 2011 shared task [49] with the “abstracts only” option.
2887672952	From POS tagging to dependency parsing for biomedical event extraction.	2154213504	ur second study assesses the performance of SOTA dependency parsers, as well as commonly used parsers, on biomedical texts. Prior work on the CRAFT treebank identied the domain-retrained ClearParser [24], now part of the NLP4J toolkit [25], as a top-performing system for dependency parsing over that data. It remains the best performing non-neural model for dependency parsing. In particular, we compar
2887684398	From Plots to Endings: A Reinforced Pointer Generator for Story Ending Generation	2130942839	Encoder-decoder framework, which uses neural networks as encoder and decoder, was first proposed in machine translation [3,26] and has been widely used in NLG tasks.
2887684398	From Plots to Endings: A Reinforced Pointer Generator for Story Ending Generation	2130942839	for sequence-to-sequence learning [26] has been widely used in NLG tasks, such as machine translation [3] and text summarization [4,15,22].
2887684398	From Plots to Endings: A Reinforced Pointer Generator for Story Ending Generation	2090487795	These systems are usually built on techniques such as planning [12,21] and casebased reasoning [5,25], which rely on a fictional world including characters, objects, places, and actions.
2887684398	From Plots to Endings: A Reinforced Pointer Generator for Story Ending Generation	2119717200	A well-known policy-gradient reinforcement learning algorithm [30] can directly optimize the non-differentiable evaluation metrics such as BLEU, ROUGE and CIDEr.
2888004803	Event Detection with Neural Networks: A Rigorous Empirical Evaluation	2064675550	llows the ﬁlter to skip non-salient or otherwise unnecessary words in the middle of word sequences. Feng et al.(2016) combined a CNN, similar to (Nguyen and Grishman,2015), with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to create a hybrid network. The output of both networks was concatenated together and fed to a linear model for ﬁnal predictions. Nguyen et al.(2016) uses a bidirectional gated recurrent units (GRUs)
2888010645	Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.	1753482797	, outperforms most models based on Statistical Machine Translation (SMT), let alone the linguistics-based methods. One of the most popular baseline models is the sequence-to-sequence (Seq2Seq) model (Kalchbrenner and Blunsom, 2013;Sutskever et al.,2014;Cho et al.,2014) with attention mechanism (Bahdanau et al.,2014;Luong et al.,2015). However, the conventional attention mechanism is problematic in real practice. The same weigh
2888010645	Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.	2415583245	ion mechanism is powerful for the requirements of alignment in NMT, some prominent problems still exist. To tackle the impact of the attention historyTu et al.(2016);Mi et al.(2016);Meng et al.(2016);Wang et al. (2016);Lin et al.(2018a) take the attention history into consideration. An important breakthrough in NMT is thatVaswani et al.(2017) applied the fully-attention-based model to NMT and achieved the state-of-
2888070626	A Visual Attention Grounding Neural Model for Multimodal Machine Translation	2131774270	. We describe details of the two objective functions in Section 3.2. 3.1 Encoder We ﬁrst encode an n-length source sentence fxg, as a sequence of tokens x = fx 1;x 2;:::;x ng, with a bidirectionalGRU(Schuster and Paliwal, 1997;Cho et al.,2014). Each token is represented by a one-hot vector, which is then mapped into an embedding e i through a pre-trained embedding matrix. The bidirectionalGRUprocesses the embedding tokens
2888070626	A Visual Attention Grounding Neural Model for Multimodal Machine Translation	2896234464	ns of multimodal (vision plus text) translation include translating multimedia news, web product information, and movie subtitles. Several previous endeavours (Huang et al., 2016;Calixto et al.,2017a;Elliott and Kádár, 2017) have demonstrated improved translation quality when utilizing images. However, how to effectively integrate the visual information still remains a challenging problem. For instance, in the WMT 2017 m
2888070626	A Visual Attention Grounding Neural Model for Multimodal Machine Translation	2513263213	words. Examples of real-world applications of multimodal (vision plus text) translation include translating multimedia news, web product information, and movie subtitles. Several previous endeavours (Huang et al., 2016;Calixto et al.,2017a;Elliott and Kádár, 2017) have demonstrated improved translation quality when utilizing images. However, how to effectively integrate the visual information still remains a challe
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2581637843,2761590056	]+λ2J3(γ) +λ3J4(θ,φ,γ) (6) where J refers to the total loss, and λ1, λ2, and λ3 are hyperparameters. 3 Experiment We conduct experiments on a high-quality dialogue dataset called DailyDialog built by Li et al. (2017b). The dialogues in the dataset reﬂect our daily communication and cover various topics about our daily life. We split the dataset into three parts with36.3Kpairs fortraining, 11.1K pairs for validat
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	1522301498	ance on the validation set, we set the hidden size to 512, embedding size to 64and vocabulary size to 40K for baseline models and the proposed model. The parameters are updated by the Adam algorithm (Kingma and Ba, 2014) and initialized by sampling from the uniform distribution ([−0.1,0.1]). The initial learning rate is 0.002and the model is trained in minibatches with a batch size of 256. λ1 and λ3 are set to 1and λ
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2410983263	dto baseline models.1 1 Introduction Automatic dialogue generation task is of great importance to many applications, ranging from open-domain chatbots (Higashinaka et al., 2014; Vinyals and Le, 2015; Li et al., 2016, 2017a; Su et al., 2018) to goal-oriented technical support agents (Bordes and Weston, 2016; Zhou et al., 2017; Asri et al., 2017). Recently there is an increasing amount of studies about purely data
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2403702038	importance to many applications, ranging from open-domain chatbots (Higashinaka et al., 2014; Vinyals and Le, 2015; Li et al., 2016, 2017a; Su et al., 2018) to goal-oriented technical support agents (Bordes and Weston, 2016; Zhou et al., 2017; Asri et al., 2017). Recently there is an increasing amount of studies about purely datadriven dialogue models, which learn from large corpora of human conversations without handcr
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	1591706642	nce and ﬂuency comparedto baseline models.1 1 Introduction Automatic dialogue generation task is of great importance to many applications, ranging from open-domain chatbots (Higashinaka et al., 2014; Vinyals and Le, 2015; Li et al., 2016, 2017a; Su et al., 2018) to goal-oriented technical support agents (Bordes and Weston, 2016; Zhou et al., 2017; Asri et al., 2017). Recently there is an increasing amount of studies
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2626778328	nputs and responses (Xu et al., 2018). For example, given “What’s your name” as the input, the models generate “I like it” as the output. Recently, the neural attention mechanism (Luong et al., 2015; Vaswani et al., 2017) has been proved successful in many tasks including neural machine translation (Ma et al., 2018b) and abstractive summarization (Lin et al., 2018), for its ability of capturing word-level dependency b
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2286300105	o ensure that the generated response is semantically consistent with the source. There are many matching models that can be used to learn such dependency relations (Hu et al., 2014; Guo et al., 2016; Pang et al., 2016; Guo et al., 2018). For simplicity, we only use a simple feedforward network for implementation. The mapping module Mγ transforms the source semantic representation h to a new representation t. To be
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	1902237438	relevance between inputs and responses (Xu et al., 2018). For example, given “What’s your name” as the input, the models generate “I like it” as the output. Recently, the neural attention mechanism (Luong et al., 2015; Vaswani et al., 2017) has been proved successful in many tasks including neural machine translation (Ma et al., 2018b) and abstractive summarization (Lin et al., 2018), for its ability of capturing
2888093771	An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation	2418993857	summarization (Lin et al., 2018), for its ability of capturing word-level dependency by associating a generated word with relevant words in the source-side context. Recent studies (Mei et al., 2017; Serban et al., 2017) have applied the attention mechanism to dialogue generation to improve the dialogue coherence. However, conversation generation is a much more complex and ﬂexible task as there are less “word-to-word
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2250539671	015), as well as embeddings trained on web-pages categorized as “medical domain”. We experimented with the following publicly available general-domain word embeddings: • GloVe [CC]: GloVe embeddings (Pennington et al., 2014), trained on Common Crawl7. • fastText [Wiki]: fastText embeddings (Bojanowski et al., 2017), trained on Wikipedia. 7http://commoncrawl.org/ • fastText [CC]: fastText embeddings, trained on Common Cra
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2093157872	08.06752v1 [cs.CL] 21 Aug 2018 cialized domains such as medicine is extremely complex and remains unexplored by the machine learning community. Moreover, since this domain has a distinct sublanguage (Friedman et al., 2002), clinical text it also presents unique challenges (abbreviations, inconsistent punctuation, misspellings, etc.) that differentiate it from opendomain data (Meystre et al., 2008). In this paper, we ad
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2515741950	ectors are then concatenated and passed through a multi-layer neural network. Recent work shows that even this straightforward approach encodes a non-trivial amount of information about the sentence (Adi et al., 2017). InferSent model. InferSent (Conneau et al., 2017) is a model for sentence representation that demonstrated close to state-of-the-art performance across a number of tasks in NLP (including NLI) and c
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2084377579	the endocrine system. The domain speciﬁc features we added to the model represent similarity between UMLS concepts from the premise and the hypothesis, based how close they appear in the UMLS graph (Pedersen et al., 2007). Following (Shivade et al., 2015; Pedersen et al., 2007) we used the SNOMED-CT terminology in our experiments. The groups below summarize the feature sets used in our model (35 features in total): 1.
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2493916176	ented with the following publicly available general-domain word embeddings: • GloVe [CC]: GloVe embeddings (Pennington et al., 2014), trained on Common Crawl7. • fastText [Wiki]: fastText embeddings (Bojanowski et al., 2017), trained on Wikipedia. 7http://commoncrawl.org/ • fastText [CC]: fastText embeddings, trained on Common Crawl. Furthermore, we trained fastText embeddings on the following domain-speciﬁc corpora: • f
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1840435438	euclidean) 7.UMLS similarity features (e.g. shortest path distance between UMLS concepts) BOW model. We use a bag-of-words (BOW) model as a simple baseline for the NLI task: the Sum of words model by Bowman et al. (2015) with a small modiﬁcation. While Bowman et al. (2015) use tanhas the activation function in the model, we use ReLU, since it trained faster and achieved Semantic type Common examples Count Finding asy
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2737504179	f reasoning with clinical knowledge. While training on large datasets maybe a natural but impractical solution, this is an open research problem for researchers in the community. Following Multi-NLI (Nangia et al., 2017) we also probed for prediction patterns with linguistic features like active-passive voice, negations, temporal expressions, coreference and modal verbs. As is common with tasks in clinical NLP, negat
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1522301498	features-based system we used the GradientBoostingClassifier from the scikit-learn library (Pedregosa et al., 2011). We implemented all models using PyTorch9 and trained them with the Adam optimizer (Kingma and Ba, 2015) until the validation loss showed no improvement for 5 epochs. The epoch with the lowest loss on the validation set was selected for testing. We used the GloVe word embeddings (Pennington et al., 2014
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2615487675	improve performance on variety of tasks such as: machine translation on lowresource languages (Zoph et al., 2016) and also some tasks from the bio-medical domain in particular (Sahu and Anand, 2017; Lee et al., 2018). To see if a corresponding boost would be possible for the NLI task, we investigated three common transfer learning techniques on the MedNLI dataset using SNLI and ﬁve different genres from MultiNLI.
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2612953412	ith a feature-based system. To further explore the performance of modern neural networks-based systems, we experimented several models of various degrees of complexity: Bag of Words (BOW), InferSent (Conneau et al., 2017) and ESIM (Chen et al., 2017). Note that our goal here is not to outperform existing models, but to explore the relative gain of the proposed methods, and compare them to a baseline. We used the same
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2337363174	to leverage them to improve the performance in the clinical domain. Transfer learning has been shown to improve performance on variety of tasks such as: machine translation on lowresource languages (Zoph et al., 2016) and also some tasks from the bio-medical domain in particular (Sahu and Anand, 2017; Lee et al., 2018). To see if a corresponding boost would be possible for the NLI task, we investigated three commo
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2122402213	and MedNLI the hypotheses and have more variation in length. Medical concepts expressed in MedNLI belong to several categories such as medications, diseases, symptoms, devices, etc. We used Metamap (Aronson and Lang, 2010) – a tool to identify medical concepts from text and map them to standard terminologies in the Uniﬁed Medical Language System (UMLS) (Bodenreider, 2004). Further, the UMLS can be queried to identify t
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2612953412	a multi-layer neural network. Recent work shows that even this straightforward approach encodes a non-trivial amount of information about the sentence (Adi et al., 2017). InferSent model. InferSent (Conneau et al., 2017) is a model for sentence representation that demonstrated close to state-of-the-art performance across a number of tasks in NLP (including NLI) and computer vision. The main differences from the BOW m
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2101234009	nd the test set of MedNLI for different models. 4 Results and discussion 4.1 Implementation details For the features-based system we used the GradientBoostingClassifier from the scikit-learn library (Pedregosa et al., 2011). We implemented all models using PyTorch9 and trained them with the Adam optimizer (Kingma and Ba, 2015) until the validation loss showed no improvement for 5 epochs. The epoch with the lowest loss o
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1981208470	ngs, trained on Common Crawl. Furthermore, we trained fastText embeddings on the following domain-speciﬁc corpora: • fastText [BioASQ]:A collection of PubMed abstracts from the BioASQ challenge data (Tsatsaronis et al., 2015). This data includes abstracts from 12,834,585 scientiﬁc articles from the biomedical domain. • fastText [MIMIC-III]: Clinical notes for patients from the MIMIC-III database (Johnson et al., 2016): 2,
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2295405869	such as NLI, question answering, or paraphrasing. Previous research relevant to the present topic, is the work on RTE in the biomedical domain: automatic construction of textual entailment datasets (Abacha et al., 2015; Abacha and Demner-Fushman, 2016), use of active learning on limited RTE data (Shivade et al., 2015, 2016), and enhancement of search results (Adler et al., 2012) using TE models. These efforts were
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2084377579	ps://jgc128.github.io/mednli/ them. It can be viewed as a graph where clinical concepts are nodes, connected by edges representing relations, such as synonymy, parent-child, etc. Following past work (Pedersen et al., 2007), we restricted to the SNOMED-CT terminology in UMLS and experimented with two techniques for incorporating knowledge: retroﬁtting and attention. 3.3.1 Retroﬁtting Retroﬁtting (Faruqui et al., 2015) m
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2250539671	r (Kingma and Ba, 2015) until the validation loss showed no improvement for 5 epochs. The epoch with the lowest loss on the validation set was selected for testing. We used the GloVe word embeddings (Pennington et al., 2014) in all experiments, except for subsection 4.4. In all experiments we report the average result of 6 different runs, with the same hyperparameters and different random seeds. Medical concepts in SNOME
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2169954261	s of the same numerical value.11 The ﬁrst step is to learn what values are abnormal and the next is to actually perform the inference. This has been identiﬁed as a major challenge for NLI since long (Sammons et al., 2010). Many inferences require world knowledge that could be deemed close to open domain NLI . While these are very subtle, some are quite domain speciﬁc (e.g. emergency admission 9 planned visit). Abbrevi
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1869752048	on the sentiment analysis task. 3.3.2 Knowledge-directed attention Attention proved to be a useful technique for many NLP tasks, starting from machine translation (Bahdanau et al., 2015) to parsing (Vinyals et al., 2015) and NLI itself (Parikh et al., 2016; Rockt¨aschel et al., 2016). In most models (including the ESIM model that we use in our experiments) attention is learned in an end-to-end fashion. However, if we
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2153579005	stText [Wiki] → fastText [MIMIC-III]: fastText Wikipedia embeddings for initialization, and the MIMIC-III data for ﬁne-tuning. Experiments using other approaches to word embeddings, such as word2vec (Mikolov et al., 2013) and CoVe (McCann et al., 2017) did not show any gains. All the word embeddings used in this work are available for download. 8 3.3 Knowledge integration Since processing of medical texts requires dom
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2122402213	t the average result of 6 different runs, with the same hyperparameters and different random seeds. Medical concepts in SNOMED-CT were identiﬁed in the premise and hypothesis sentences using Metamap (Aronson and Lang, 2010). The code for all experiments is publicly available.10 9https://pytorch.org/ 10https://jgc128.github.io/mednli/ 4.2 Baselines Table 4 shows the baseline results: the performance of a model when train
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1840435438	traints. 2.3 Dataset statistics Together, the four clinicians worked on a total of 4,683 premises over a period of six weeks. The resulting dataset consists of 14,049 unique sentence pairs. Following Bowman et al. (2015), we split the dataset into training, development, and testing subsets and ensured that no premise was overlapping between the three subsets. Table 2 presents key statistics of MedNLI, and the Figure
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1840435438	ulate agreement, we presented pairs generated by one clinician, and sought annotations from the other clinician, determining if the inference was “Deﬁnitely true”, “Maybe true”, or “Deﬁnitely false” (Bowman et al., 2015). Comparison of these annotations resulted in a Cohen’s kappa of 4https://jgc128.github.io/mednli/ = 0:78. While this is substantial if not perfect agreement by itself (McHugh, 2012), it is particula
2888120268	Lessons from Natural Language Inference in the Clinical Domain	2133564696	several wordlevel tasks, as well as on the sentiment analysis task. 3.3.2 Knowledge-directed attention Attention proved to be a useful technique for many NLP tasks, starting from machine translation (Bahdanau et al., 2015) to parsing (Vinyals et al., 2015) and NLI itself (Parikh et al., 2016; Rockt¨aschel et al., 2016). In most models (including the ESIM model that we use in our experiments) attention is learned in an
2888120268	Lessons from Natural Language Inference in the Clinical Domain	1840435438	zing textual entailment (RTE) (Dagan et al., 2006) has long been a popular task among researchers. Moreover, contribution of datasets from past shared tasks (Dagan et al., 2009), and recent research (Bowman et al., 2015; Williams et al., 2018) have pushed the boundaries for this seemingly simple, but challenging problem. The Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) is a large, high qu
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2418388682	Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches inmanydatasettings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation“approaches theaccuracyachievedbyaverage bilingual human translators [on some test sets]” (Wuetal.,2016), oreventh
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2252166243	ement than absolute judgement on 5-point Likert scales (How good is this translation?) but givesnoinsightabouthowmuchacandidate translation differs from a (presumably perfect) reference. To this end, Graham et al. (2013) suggest the use of continuous scales for direct assessment of translation quality. Implemented as a slider between 0 (Not at all) and 100 (Perfectly), their method yields scores on a 100-point interv
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2626778328	ent-level context into machine translation systems is an active ﬁeld of research (Webber et al., 2017), state-of-the-art systems still operate at the level of single sentences (Sennrich et al., 2017; Vaswani et al., 2017; Hassan et al., 2018). In contrast, human translators can and do take document-level context into account (Krings, 1986). The same holds for raters in evaluation campaigns. In the discussion of their
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	1753482797	es to the degreethaterrorswhicharehardorimpossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches inmanydatasettings and shared translation tasks (
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2147192413	human parity: granularity of measurement (ordinal vs. interval scales), raters (experts vs. crowd workers), and experimental unit (sentence vs. document). 2.1 Related Work Granularity of Measurement Callison-Burch et al. (2007) show that ranking (Which of these translations is better?) leads to better inter-rater agreement than absolute judgement on 5-point Likert scales (How good is this translation?) but givesnoinsightabo
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2143539737	ically assessed by means of crowdsourcing. Combined ratings of bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009). Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluate
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2740433069	ile incorporating document-level context into machine translation systems is an active ﬁeld of research (Webber et al., 2017), state-of-the-art systems still operate at the level of single sentences (Sennrich et al., 2017; Vaswani et al., 2017; Hassan et al., 2018). In contrast, human translators can and do take document-level context into account (Krings, 1986). The same holds for raters in evaluation campaigns. In t
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2130942839	rehardorimpossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches inmanydatasettings and shared translation tasks (Luong and Manning, 2015;
2888159079	Has Neural Machine Translation Achieved Human Parity? A Case for Document-level Evaluation	2525778437	t al., 2018). In contrast, human translators can and do take document-level context into account (Krings, 1986). The same holds for raters in evaluation campaigns. In the discussion of their results, Wu et al. (2016) note that their raters “[did] not necessarily fully understand each randomly sampled sentence sufﬁciently” because it was provided with no context. In such setups, raters cannot reward textual cohesi
2888175818	Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning	2553246593	ar for all 4 subtasks; (2) VI-1/2: 1,271 recipes containing 1 or 2 vague subtasks; (3) VI-3/4: 1,872 recipes containing 3 or 4 vague subtasks. 4.2 Methods Comparison LAM: The Latent Attention Model (Liu et al., 2016)6, one of the state-of-the-art models for parsing If-Then recipes. We do not consider the model ensemble in (Liu et al.,2016), as it can be applied to all other methods as well. Our reproduced LAM obt
2888175818	Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning	2090243146	ions and produces more accurate recipes. The HRL framework developed in this work can be a promising direction for other scenarios such as knowledge-graph-based question answering (Berant et al.,2013;Fader et al., 2014;Yih et al.,2015;Berant and Liang,2015). Acknowledgments We would like to thank Chris Brockett, Michel Galley and Yu Su for their insightful comments on the work. This research was sponsored in part b
2888175818	Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning	2224454470	ong the 4 subtasks. In particular,Liu et al.(2016) assumes that functions should be predicted before channels since a channel can be derived from the function prediction, whileBeltagy and Quirk(2016);Dong and Lapata (2016);Yin and Neubig(2017) assume that channels should be predicted before functions and triggers before actions. Given these observations, we propose an interactive semantic parser that can ask users for
2888175818	Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning	2344023930	subtasks and solve them sequentially via MDPs (Parr and Russell,1998; Sutton et al.,1999;Dietterich,2000). Recently, HRL-based approaches are applied to tasks like game playing (Kulkarni et al.,2016;Tessler et al., 2017), travel planning (Peng et al.,2017), and visual question answering and captioning (Wang et al.,2017;Gordon et al.,2017;Zhang et al., 2018). Inspired by these work, given that our semantic parsing tas
2888208939	Identifying Domain Adjacent Instances for Semantic Parsers	1486649854	0.554 0.764 0.345 Table 4: Accuracy for a semantic parser evaluated on a test set in which 20% is domainadjacent. cused on domain-independent embeddings, learned without downstream task supervision. Kiros et al. (2015), Hill et al. (2016), and Kenter et al. (2016) learn representations by predicting the surrounding sentences. Wieting et al. (2016) use paraphrases as supervision. Mu et al. (2017) represent sentences
2888208939	Identifying Domain Adjacent Instances for Semantic Parsers	2158698691	3 Direct Evaluation We ﬁrst directly evaluate the identiﬁcation of domain-adjacent instances: Table 2 reports the area under a receiver operating characteristic curve (AUC) for the considered models (Fawcett, 2006). SURPRISE generally performs the best on thisevaluation; and,ingeneral, thesimplermodels tend to perform better, suggesting that more complexapproaches tunetoomuchtothetrainingdata. Qualitatively, fo
2888208939	Identifying Domain Adjacent Instances for Semantic Parsers	2515741950	able with any schema that covers the training set. 3.1 Sentence Representation Among recent work in distributional semantics, averaging the word vectors to represent a sentence (Wieting et al., 2016; Adi et al., 2017) has proven to be a simple and robust approach. However, we have an intuition that words which are unexpected in their context given the training data may be a strong signal that an instance is domain
2888208939	Identifying Domain Adjacent Instances for Semantic Parsers	2251939518	encoder reconstruction error. Prior distributional semantics work to create compact sentential representations generated speciﬁc embeddings for downstream tasks (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013). Recently, work has foBasketball Blocks Calendar Housing Publications Recipes Restaurants Social NOFILTER 0.358 0.294 0.617 0.461 0.511 0.570 0.626 0.355 ORACLE 0.558 0.494 0.817 0.661 0.711 0.770 0.
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1539746312	] [45] [13] Context-related Features Word lemma [39] [40] [45] For “Connie has 41.0 red markers.”, the word lemmas around the quantity “41.0” are {Connie, have, red, marker}. POS tags [28][39][40][45][20] For “A chef needs to cook 16.0 potatoes.”, the POS tags within a window of size 2 centered at the quantity “16.0” are {TO, VB, NNS}. Dependence type [39] [40] [45] For “Ned bought 14.0 boxes of choco
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251349042	. We noticed that in each dataset, a small fraction of problems are associated with one unknown variable in the template. Thus, we alsoreportthenumberofsingle-equationproblemsineachdataset. 1) ALG514 [39]. The dataset is crawled from Algebra.com, a crowd-sourced tutoring website. The problems are posted by students. The problems with information gap or require explicit background knowledge are discard
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1824971879	102] solves probabilityproblems automatically by a two-step approach, namely ﬁrst formulating questions in a declarative language and then computing the answer through a solver implemented in ProbLog [103]. And algebraic word problems are solved by generating answer rationales written in natural language in [104] through a sequence-to-sequence model. 6.2 Math Problem Solver in Other Languages Solving m
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2510828927	2, we derive the following observations worth noting and provide reasonings to explain the results. First, the statistic-based methods with advanced logic representation, such as Schema [22], Formula [23] and LogicForm [24], [25], achieve dominating performance in the AI2 dataset. Their superiority is primarily owned to the additional efforts on annotating the text problem with more advanced logic rep
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251935656	r additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2. One is called expression tree that is used in [26], [28], [15] and the other is called equation tree in [27]. These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unkno
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2510828927	arser tree using the Compositional Vector Grammar parser [67]. Additionally, the Stanford Dependencies representation [68] has been applied in multitple solvers. We observed its occurrence in Formula [23] and ARIS [21] to extract attributes of entities (the subject, verb, object, preposition and temporal information), in KAZB [39] to generate part-of-speech tags, lematizations, and dependency parses t
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251349042	ber slots have been processed, it would be an easy task to ﬁll the unknown slots. In this way, the hypothesis space can be signiﬁcantly reduced. Second, the authors argue that the beam search used in [39] does not exploit all the training samples,and its resulting model may be sub-optimal. To resolve the issue, the max-margin objective [43] is used to train the log-linear model. The training process i
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251935656	d balance between accuracy and running time. In [29], the authors publish the service as a web tool and it can respond promptly to a math word problem. The solution in [27], named ALGES, differs from [26] in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More speciﬁcally, ALGES does not discard irrevalent quantities, but enumerates all the synta
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2510828927	d tree-based methods in solving arithmetic problems. Methods Publish Year AI2 IL CC SingleEQ AllArith Dolphin-S Statistic-based ARIS [21] 2014 77.7 - - 48 - - Schema [22] 2015 88.64 - - - - - Formula [23] 2016 86.07 - - - - - LogicForm [24], [25] 2016 84.8 80.1 53.5 - - - Tree-based ALGES [27] 2015 52.4 72.9 65 72 60.4 - ExpressionTree [26] 2015 72 73.9 45.2 66.38 79.4 26.11 UNITDEP [28] 2017 56.2 71.
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2276364082	his dataset does not incorporate irrelevant quantities in the problem text. Hence, there is no need to apply the quantity relevance classiﬁer for the algorithms containing this component. 4) SingleEQ [27]. The dataset contains both single-step and multi-step arithmetic problems and is a mixture of problems from a number of sources, including math-aids.com, k5learning.com, ixl.com and a subset of the d
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2613312549	declarative language and then computing the answer through a solver implemented in ProbLog [103]. And algebraic word problems are solved by generating answer rationales written in natural language in [104] through a sequence-to-sequence model. 6.2 Math Problem Solver in Other Languages Solving math word problems in other languages also attracts research attention. Yu et al. addressed the equation set p
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251935656	E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes. To further reduce the tree enumeration space, beam search is applied in [26]. To generate the next state T′ from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the hig
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2250861254	e Stanford parser works as the most comprehensive and widely-adopted one. It is a package consisting of different probabilistic natural language parsers. To be more speciﬁc, its neural-network parser [66] is a transition-based dependency parser that uses high-order features to achieve high speed and good accuracy; the Compositional Vector Grammar parser [67] can be seen as factoring discrete and conti
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2161969291	earning (ML) workﬂow. Effective feature construction, in an either supervised or semi-supervised manner, can signiﬁcantly boost the accuracy. For instance, SIFT [61], [62] and other local descriptors [63], [64], [65] have been intensively used in the domain of object recognition and image retrieval for decades as they are invariant to scaling, orientation and illumination changes. Consequently, a larg
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2064675550	ectorized into features through word embedding techniques [54], [55]. In the encoding layer, GRU [56] is used as the Recurrent Neural Network (RNN) to capture word dependency because compared to LSTM [57], GRU has fewer parameters in the model and is less likely to be over-ﬁtting in small datasets. This seq2seq model translates math word problems to equation templates, followed by a number mapping ste
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2129657639	ed on tiers of deterministic coreference models. The tiers are processed from the highest to the lowest precision and the entity output of a tier is forwarded to the next tier for further processing. [73] is another model that integrates a collection of deterministic coreference resolution models. Targeting at exploring rich feature space, [74] proposed a simple classiﬁcation model for coreference res
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2250539671	effective. The deep model used in DNS is a typical sequence to sequence (seq2seq) model [51], [52], [53]. The words in the problem are vectorized into features through word embedding techniques [54], [55]. In the encoding layer, GRU [56] is used as the Recurrent Neural Network (RNN) to capture word dependency because compared to LSTM [57], GRU has fewer parameters in the model and is less likely to be
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1495981708	eled corpus. It allows proper and nominal mentions to only corefer with antecedents that have the same head, but pronominal mentions to corefer with any antecedent. On top of [71], Raghunathan et al. [72] proposed an architecture based on tiers of deterministic coreference models. The tiers are processed from the highest to the lowest precision and the entity output of a tier is forwarded to the next
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2276364082	EQ AllArith Dolphin-S Statistic-based ARIS [21] 2014 77.7 - - 48 - - Schema [22] 2015 88.64 - - - - - Formula [23] 2016 86.07 - - - - - LogicForm [24], [25] 2016 84.8 80.1 53.5 - - - Tree-based ALGES [27] 2015 52.4 72.9 65 72 60.4 - ExpressionTree [26] 2015 72 73.9 45.2 66.38 79.4 26.11 UNITDEP [28] 2017 56.2 71.0 53.5 72.25 81.7 28.78 MathDQN [15] 2018 78.5 73.3 75.5 52.96 72.68 30.06 margin in the C
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2121406004	es rely on written rules or manual regulations, i.e., the visual elements needed to be recognized with human intervention and their performances were usually dependent on speciﬁed diagrams. G-ALINGER [91] is an algorithmic work that addresses the geometry understanding and text understanding simultaneously. To detect primitives from a geometric diagram, Hough transform [92] is ﬁrst applied to initiali
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2528976376	hoo.com, the math equations and answers are manually added by human annotators. Finally, the dataset combined from the two sources contains 1,878 math problems with 1183 equation templates. 3) DRAW1K [60]. The authors of DRAW1K argued that Dolphin1878 has limited textual variations and lacks narrative. This motivated them to construct a new dataset that is diversiﬁed in both vocabularies and equation
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2050273484	ier is forwarded to the next tier for further processing. [73] is another model that integrates a collection of deterministic coreference resolution models. Targeting at exploring rich feature space, [74] proposed a simple classiﬁcation model for coreference resolution with a well-designed set of features. NECo is proposed in [75] and capable of solving both named entity linking and co-reference resol
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251935656	he likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree. Roy et al. [26] proposed the ﬁrst algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its ﬁrst strategy to reduce the search space is training a binary classiﬁer to
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1539746312	lobal features in the document-level proposed by existing solvers. [26], [28], [15] use the number of quantities in the problem text as part of feature space. Unigrams and bigrams are also applied in [20] [39]. They may play certain effect in determining the quantities and their order. Note that the unigrams and bigrams are deﬁned in the word level rather than the character level. 5 GEOMETRIC WORD PRO
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2161951443	logics so as to facilitate quantitative reasoning. Hence, MWPs solvers are broadly considered as good test beds to evaluate the intelligence level of agents in terms of natural language understanding [4], [5] and the successful solving of MWPs would constitute a milestone towards general AI. We categorize the evolving of MWP solvers into three major stages according to the technologies behind these s
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2532931541	ly from the sub-stories. Each sub-story can be seen as a text template with value slots to be ﬁlled. These sub-stories will be concatenated into an entire narrative. Different from [108], the work of [109] rewrites a given math word problem to ﬁt a particular theme such as Star War. In this way, students may stay more engaged with their homework assignments. The candidate are scored with the coherence
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1539746312	n [75] and capable of solving both named entity linking and co-reference resolution jointly. As to applying coreference resolvers in MWP sovers, the Illinois Coreference Resolver [74] [76] is used in [20] to identify pronoun referents and facilitate semantic labeling. In [70], a rule function Coref(A,B), which is true when A and B represent the same entity, is derived as a component of the declarative
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2179488730	network has witnessed success in solving various problems with big search space such as playing text-based games [31], information extraction [32], text generation [33] and object detection in images [34]. To ﬁt the math problem scenario, they formulate the expression tree construction as a Markov Decision Process and propose the MathDQN that is customized from the general deep reinforcement learning
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2121406004	ning the satisﬁability of the derived logical expression in a numerical method that requires manually deﬁning indicator function for each predicate. It is noticeable that G-ALINGER is applied in GEOS [91] for primitive detection. Despite the superiority of automated solving process, the performance of the system would be undermined if the answer choices are unavailable in a geometry problem and the de
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1933349210	on performance evaluations, ideally, there should be a benchmark dataset well accepted and widely adopted by the MWP research community, just like ImageNet [16] for visual object recognition and VQA [17], [18] for visual question answering. Unfortunately, we observed that many approaches tend to compile their own datasets to verify their superiorities, which result in missing of relevant competitors
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2133280805	another pre-processing step for MWP solvers. UnitDep [28] automatically generates features from a given math problem by analyzing its derived parser tree using the Compositional Vector Grammar parser [67]. Additionally, the Stanford Dependencies representation [68] has been applied in multitple solvers. We observed its occurrence in Formula [23] and ARIS [21] to extract attributes of entities (the sub
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2130942839	cause all the previous methods (including MathDQN) require human intelligence to help extract features that are effective. The deep model used in DNS is a typical sequence to sequence (seq2seq) model [51], [52], [53]. The words in the problem are vectorized into features through word embedding techniques [54], [55]. In the encoding layer, GRU [56] is used as the Recurrent Neural Network (RNN) to captu
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2172589779	all the previous methods (including MathDQN) require human intelligence to help extract features that are effective. The deep model used in DNS is a typical sequence to sequence (seq2seq) model [51], [52], [53]. The words in the problem are vectorized into features through word embedding techniques [54], [55]. In the encoding layer, GRU [56] is used as the Recurrent Neural Network (RNN) to capture wor
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251349042	these two problems with ordered and one-to-one mapping. It is considered a failure if these two problems do not contain the same number of quantities. 3.3 Template Based Methods There are two methods [39], [40] that pre-deﬁne a collection of equation set templates. Each template contains a set of number slots and unknown slots. The number slots are ﬁlled by the numbers extracted from the text and the
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2095905764	rallelogram detection in diagram understanding has also received a considerable amount of interest. The proposed techniques fall into two main categories, either based on primitive or Hough transform [80]. The primitivebased methods combine line segments or curves to form possible edges of a quadrangle. For examples, Lin and Nevatia [81] proposed the approach of parallelogram detection from a single a
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251349042	as RankSVM [41]. A widely-adopted practice is to deﬁne the probability of each instance of derivation y based on the feature representation x for a text problem and a parameter vector θ, as in [30], [39], [40]: p(y|x;θ)= eθ·φ(x,y) P y′∈Y e θ·φ(x,y′) With the optimal derivation instance yopt, we can obtain the ﬁnal solution. In [39], the objectiveis to maximize the total probabilitiesof y that leads t
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1539746312	rning models to identify the entities, quantities and operators from the problem text and yield the numeric answer with simple logic inference procedure. The scheme of quantity entailment proposed in [20] can be used to solve arithmetic problems with only one operator. It involves three types of classiﬁers to detect different properties of the word problem. The quantity pair classiﬁer is trained to de
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	108987309	rom the scope of this survey paper and focus on the recent technology developments that have not been covered in the previous survey [9]. In the second stage, MWP solvers made use of semantic parsing [10], [11], with the objective of mapping the sentences from problem statements into structured logic representations so as to facilitate quantitative reasoning. It has regained considerable • D. Zhang, L
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251935656	ry tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches [26], [27], [28], [15] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is n
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2510828927	schema,inspiredby[8].Thesentencesinthe text problem are examined sequentially until the sentence matches a schema, triggering an update operation to modify the number associated with the entities. In [23], Mitra et al. proposed a new logic template named formula. Three types of formulas are deﬁned, including part whole, change and comparison, to solve problems with addition and subtraction operators.
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2276364082	Second, the results of tree-based methods in AI2, IL and CC are collected from [15] where the same experimental setting of 3- fold cross validation is applied. It is interesting to observe that ALGES [27], ExpressionTree [26] and UNITDEP [28] cannot perform equally well on the three datasets. ALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained wi
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1495981708	r of sentences, each containing a quantity, ZDC [40] takes into account the existence of coreference relationship between these two sentences for feature exploitation. Meanwhile, ARIS [21] adopts the [72] for coreference resolution and uses the predicted coreference relationships to replace pronouns with their coreferenent links. 4.2 Common Features There have been various types of features proposed i
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2250861254	sing algorithm that iteratively selects the best pair of neighbors in the tree structure to connect at each parsing step. Those parsers account in WMP solvers. For instance, the neural-network parser [66] is adopted in [70] for coreference resolution, which is another pre-processing step for MWP solvers. UnitDep [28] automatically generates features from a given math problem by analyzing its derived p
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2077824186	for solving verbal IQ questions, which classiﬁes questions into several categories and each group of questions are solved by a speciﬁc solver respectively. Furthermore, logic puzzles are addressed in [101] by transforming robust natural language to precise semantics. For other forms of math problems, [102] solves probabilityproblems automatically by a two-step approach, namely ﬁrst formulating question
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2133280805	be more speciﬁc, its neural-network parser [66] is a transition-based dependency parser that uses high-order features to achieve high speed and good accuracy; the Compositional Vector Grammar parser [67] can be seen as factoring discrete and continuous parsing in one model; and the (English) Stanford Dependencies representation [68] is an automatic system to extract typed dependency parses from phras
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251287417	spectrum of “smart” applications, such as video captioning [46], video event recognition [47], human action regnition [48], visual question answering [49], and question answering with knowledge base [50]. The main advantage is that with sufﬁcient amount of training data, DL is able to learn an effective feature representation in a data-driven manner without human intervention. It is not surprising to
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2153579005	t are effective. The deep model used in DNS is a typical sequence to sequence (seq2seq) model [51], [52], [53]. The words in the problem are vectorized into features through word embedding techniques [54], [55]. In the encoding layer, GRU [56] is used as the Recurrent Neural Network (RNN) to capture word dependency because compared to LSTM [57], GRU has fewer parameters in the model and is less likely
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2276364082	t can help enforce the constraints such as syntactic validity, type consistence and domain speciﬁc simplicity considerations. Consequently, its computation cost is dozens of times higher than that in [27], according to an efﬁciency evaluation in [15]. Second, its scoring function is different from Equation 1. There is no need for the term φ(q) because ALGES does not build a classiﬁer to check the quan
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2251349042	TABLE 3 Statistics of datasets for equation set problems. Datasets Proposed in # of problems # of templates # of problems # of templates # of single-eq problems # of words # of sentences ALG514 KAZB [39] 514 28 18.36 91 1.62k 19.3k Dolphin1878 DOL [35] 1,878 1,183 1.59 712 3.30k 41.4k DRAW1K MixedSP [45] 1,000 232 4.31 255 1.38k 13.8k Dolphin18K SIM [12] 18,460 5,871 3.14 8,333 49.9k 604k TABLE 4 Acc
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2064675550	tead of using GRU and LSTM, the math solver examines the performance of other seq2seq models when applied in mapping the problem text to equation templates. In particular, two models including BiLSTM [57] and structured self-attention [59], were examined respectively for the equation templateclassiﬁcationtask.Resultsshowthat bothmodelsachieve comparable performance. 3.5 Dataset Repository and Performa
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2276364082	tes of entities (the subject, verb, object, preposition and temporal information), in KAZB [39] to generate part-of-speech tags, lematizations, and dependency parses to compute features, and in ALGES [27] to obtain syntactic information used for groundingand feature computation. ExpressionTree [26] is an exceptional case without using Stanford Parser. Instead,itusestheeasy-ﬁstparsingalgorithm[69]todet
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2402499785	uch as Star War. In this way, students may stay more engaged with their homework assignments. The candidate are scored with the coherence in multiple factors (e.g., syntactic, semantic and thematic). [110] generates math word problems that match the personal interest of students. The generator uses Answer Set Programming [111], in which programs are composed of facts and rules in a ﬁrst-order logic rep
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2307512708	ular and a greedy algorithm is designed to pick the primitive with the maximum gain in each iteration. The algorithm stops when no positive gain can be obtained according to the objective function.In [93], the problem of visual understandingis addressed in the context of science diagrams. The objective is to identify the graphic representation for the visual entities and their relations such as tempor
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	1539746312	Whether the value of the ﬁrst quantity is greater than the other [26] [28] [15] [39] [40] [45] Question-related Features Whether the unit or related noun phrase of a quantity appears in the question [20][26][27][28][15] [39] Whether the unit or related noun phrase of a quantity has the highest number of match tokens with the question text [26] [28] [15] For the question “How many apples are left in t
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2117074069	visual mentions in the diagram to the concepts in real world. B x y o A D C Fig. 5. An example of geometric problem. 5.1 Text-Aligned Diagram Understanding The veryearly computerprogram,BEATRIX [87], [88], parsesthe English text and diagram components of the elementary physics problemstogetherbyestablishingthe coreference between thetext and diagram. Watanabe et al. proposed a framework to combine lay
2888271279	The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers.	2124386111	been a vital component in the machine learning (ML) workﬂow. Effective feature construction, in an either supervised or semi-supervised manner, can signiﬁcantly boost the accuracy. For instance, SIFT [61], [62] and other local descriptors [63], [64], [65] have been intensively used in the domain of object recognition and image retrieval for decades as they are invariant to scaling, orientation and ill
2888296173	CoQA: A Conversational Question Answering Challenge.	1933349210	erstanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human friendly sources are images and videos (Antol et al., 2015;Das et al.,2017;Hori et al.,2018). Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a ﬁll-in-the-blank question (Hermann et al.,2015), usin
2888296173	CoQA: A Conversational Question Answering Challenge.	2558203065	et al.,2016; Welbl et al.,2018), paraphrasing artiﬁcial questions to natural questions (Saha et al.,2018;Talmor and Berant,2018) or, in our case, using humans to ask questions (Rajpurkar et al.,2016;Nguyen et al., 2016). While the former enable collecting large and cheap datasets, the latter enable collecting natural questions. Recent efforts emphasize collecting questions without seeing the knowledge source in orde
2888296173	CoQA: A Conversational Question Answering Challenge.	2161951443	ous reasoning phenomena occurring in the context of a conversation (Section4). Our work parallels a growing interest in developing datasets that test speciﬁc reasoning abilities: algebraic reasoning (Clark, 2015), logical reasoning (Weston et al.,2016), common sense reasoning (Ostermann et al.,2018) and multi-fact reasoning (Welbl et al.,2018;Khashabi et al.,2018;Talmor and Berant,2018). 8 Conclusions In this
2888329843	Dissecting Contextual Word Embeddings: Architecture and Representation	2605717780	greement for coreference.Kad´ ar et al.´ (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions.Belinkov et al. (2017) used linear classiﬁers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work,Khandelwal et al.(2018) studied the role of context in inﬂuenc
2888329843	Dissecting Contextual Word Embeddings: Architecture and Representation	2605717780	ing morphology with little semantics. 5.3 Probing contextual information In this section, we quantify some of the anecdotal observations made in Sec.5.1. To do so, we adopt a series of linear probes (Belinkov et al., 2017) with two NLP tasks to test the contextual representations in each model layer for each biLM architecture. In addition to examining single tokens, we also depart from previous work by examining to wha
2888329843	Dissecting Contextual Word Embeddings: Architecture and Representation	2567070169	NLP tasks. 3.3 Gated CNN Convolutional architectures have also been shown to provide competitive results for sequence modeling including sequence-to-sequence machine translation (Gehring et al.,2017).Dauphin et al. (2017) showed that architectures using Gated Linear Units (GLU) that compute hidden representations as the element wise product of a convolution and sigmoid gate provide perplexities comparable to large LST
2888329843	Dissecting Contextual Word Embeddings: Architecture and Representation	2626778328	the number of parameters in the character encoder by ﬁrst projecting the character CNN ﬁlters down to the model dimension before the two highway layers. 3.2 Transformer The Transformer, introduced byVaswani et al. (2017), is a feed forward self-attention based architecture. In addition to machine translation, it has also provided strong results for Penn Treebank constituency parsing (Kitaev and Klein,2018) and semant
2888329843	Dissecting Contextual Word Embeddings: Architecture and Representation	2738152205	orced to rely on context to form their representation of the pronoun, as the surface form of the pronoun is uninformative. As an upper bound on performance, the state-of-the-art coreference model fromLee et al. (2017)9 ﬁnds an antecedent span with the head word 64% of the time. As a lower bound on performance, a simple baseline that chooses the closest noun occurring before the pronoun has an ac8he, him, she, her,
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2294771250	[12] present the first MFS detection method based on automatically learned vector word embeddings.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2100935296	2Formally, WORDNET::SENSERELATE::WORDTOSET performing sense-to-sense comparison is jcn [29].
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2135824681	This is in accordance with prior work which has shown that WSD performance improves when performed with respect to less granular sense inventories [42].
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2567035470	Since our approach is unsupervised, we cannot train sense vectors directly using sense annotated data, as done by, for example, [32].
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	1479912259,2121147707	companions of a word are deﬁned in an entirely relation-free way, requiring no external resources or pre-processing to extract (this distinguishes our method from prior work, e.g.Pantel and Lin(2002),McCarthy et al. (2004b)). We experimented with more sophisticated methods for selecting companions, such as taking the kwords with the highest pointwise mutual information with the target word, but in our development expe
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2107139012	These concepts have previously been used to improve WSD – [23]–[26], and others – but our usage of these concepts for MFS detection is novel.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2145262681	Different senses of the same word may translate as different words in other languages [24], [34].
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2436001372	es of vectors that we construct. Our methods leverage cross-lingual information and contextually related words. These concepts have previously been used to improve WSD –Yarowsky(1995);Ng et al.(2003);Navigli (2009);Apidianaki and Gong(2015), and others – but our usage of these concepts for MFS detection is novel. 3 COMP2SENSE Given a target word, our ﬁrst MFS detection method uses a set of words known as its co
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2251537235,2294771250	Following prior work [12] [21], we instead approximate them by identifying keywords for each sense,
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2156985047	To identify the MFT of each word, we compute a bi-directional word alignment of the bitext using GIZA++ [37] with the default parameter settings.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2127128140	Leskext [40] is a WordNet-based extension of the classic Lesk algorithm [41], and serves as a strong unsupervised baseline WSD system.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2251537235,2294771250,2316153475,2125786288	Our method differs from recent prior work on constructing embeddings using sense information [17]–[20]; instead, we build upon the methods of [21] and [12].
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2120699290	nowledge-light MFS detection and WSD. Indeed, the trend in recent years has been to augment WSD systems with increasingly rich, increasingly complex sources of linguistic knowledge, such as BabelNet (Navigli and Ponzetto, 2012). That systems with more resources available perform better in general is unsurprising, and raises the question of whether recent advances in WSD are primarily due to the addition of new sources of li
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2251383907	Prior work on SDL includes [13], [14], [15], and [16], In principle, a SDL system can be applied to MFS detection by simply returning the sense with the highest probability.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2294771250	For the purpose of comparison, we re-implemented the UMFS-WE system [12], which, to the best of our knowledge, is the most recent system to specifically consider the task of MFS detection.
2888353881	You Shall Know the Most Frequent Sense by the Company it Keeps.	2419539795	Our text corpus is the OpenSubtitles2018 English-French bitext [36], which contains roughly 42M sentences from various domains.
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	2407753702	, opera-cinema, or snow-ice. 4.2.3 Cross-lingual hypernym discovery Modeling hypernymy is a crucial task in NLP, with direct applications in diverse areas such as semantic search (Hoffart et al.,2014;Roller and Erk, 2016), question answering (Prager et al.,2008; Yahya et al.,2013) or textual entailment (Geffet and Dagan,2005). Hypernyms, in addition, are the backbone of lexical ontologies (Yu et al.,2015), which are i
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	342285082	for certain language pairs. Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings (Mikolov et al.,2013b;Faruqui and Dyer, 2014;Ammar et al.,2016;Artetxe et al.,2016). Mikolov et al.(2013b) was one of the ﬁrst attempts into this line of research, applying a linear transformation in order to map the embeddings from one monolin
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	2465611764	entailment (Geffet and Dagan,2005). Hypernyms, in addition, are the backbone of lexical ontologies (Yu et al.,2015), which are in turn useful for organizing, navigating and retrieving online content (Bordea et al., 2016). Thus, we propose to evaluate the contribution of cross-lingual embeddings towards the task of hypernym discovery, i.e., given an input word (e.g., cat), retrieve or discover its most likely (set of)
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	342285082	ew shared space. Artetxe et al.(2016) proposed a similar linear mapping toMikolov et al.(2013b), generalizing it and providing theoretical justiﬁcations which also served to reinterpret the methods ofFaruqui and Dyer (2014) andXing et al. 2015).Smith et al.(2017) further showed how orthogonality was required to improve the consistency of bilingual mappings, making them more robust to noise. Finally, a more complete gene
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	2251291469	reproducibility of our experiments easier. 4.1 Cross-lingual embeddings training Corpora. In our experiments we make use of web-extracted corpora. For English we use the 3B-word UMBC WebBase Corpus (Han et al., 2013), while we chose the Spanish Billion Words Corpus (Cardellino,2016) for Spanish. For Italian and German, we use the itWaC and sdeWaC corpora from the WaCky project (Baroni et al.,2009), containing 2 a
2888389098	Improving Cross-Lingual Word Embeddings by Meeting in the Middle	2250539671	ven two monolingual corpora, a word vector space is ﬁrst learned independently for each language. This can be achieved with common word embedding models, e.g., Word2vec (Mikolov et al.,2013a), GloVe (Pennington et al., 2014) or FastText (Bojanowski et al.,2017). Second, a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space (Section3.1). Third, a ﬁnal transformation is ap
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2512924740	(EnZh) and WMT17 Chinese-English (Zh-En), and we further conduct the experiments to leverage monolingual data in WMT17 Zh-En translation. 5.1 Experimental Settings For the bilingual datasets, WMT17 (Bojar et al., 2017) En-Zh 1 and WMT17 Zh-En use the same dataset, which contains about 24Msentences pairs, including CWMT Corpus 2017 and UN Parallel Corpus V1.0. The Jieba2 segmenter is used to per1http://www.statmt.or
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2626778328	.g., language detection and ﬁltering sentences with more than 80 words), we keep 4MChinese sentences and 7MEnglish sentences. We adopt the Transformer model with transformer big setting as deﬁned in (Vaswani et al., 2017) for Zh-En and En-Zh translations, which achieves SOTA translation quality in several other datasets. For En-De translation, we utilize the transformer base v1 setting. These settings are exactly same
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2176263492	e sentences as baseline reward. In our work, we adopt the function learning approach, using simple network (e.g., multi-layer perceptron) to build the learning function, which is the same as used in (Ranzato et al., 2016;Bahdanau et al.,2017). 3.3 Combine MLE and RL Objectives The last important strategy we would like to mention is the combination of MLE training objective with RL objective, which is assumed to furth
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2101105183	i.e., a candidate word out from the vocabulary, according to the policy. A terminal reward is observed once the agent generates a complete sequence y^. The reward for machine translation is the BLEU (Papineni et al., 2002) score, denoted as R(^y;y), which is deﬁned by comparing the generated y^with the ground-truth sentence y. Note that here the reward R(^y;y) is the sentence-level reward, i.e., a scalar for each compl
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	1816313093	which also includes sequence-level loss but not exactly the same as RL. Our work is also related with the research works that leverage monolingual data for improving NMT models (Zhang and Zong ,2016;Sennrich et al. 2015a ;Wang et al. ,2018 Xia et al. 2016 Cheng et al., 2016).Zhang and Zong( ) exploits the source-side monolingual data in NMT.Sennrich et al.(2015a) proposes back-translation method to leverage target-s
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	1816313093	ith Monolingual Data Previous works typically conduct RL training with only bilingual data for NMT. Monolingual data has been proved to be able to signiﬁcantly improve the performance of NMT systems (Sennrich et al., 2015a;Xia et al.,2016;Cheng et al.,2016). It remains an open problem whether it is possible to combine the beneﬁts of RL training and monolingual data such that even more competitive results can be obtain
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2581637843	jectives The last important strategy we would like to mention is the combination of MLE training objective with RL objective, which is assumed to further stabilize RL training process (Wu et al.,2016;Li et al., 2017;Wu et al.,2017a). A simple way is to linearly combine the MLE (Eqn. (1)) and RL (Eqn. (3)) objectives as follows: L com= L mle+(1 )L^ rl; (5) where is the hyperparamter controlling the tradeoff betwe
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2176263492	lated Work Our work is mainly related with the literature of using reinforcement learning to directly optimize the evaluation measure for neural machine translation. Several representative works are (Ranzato et al., 2016;Shen et al.,2016;Bahdanau et al.,2017). In (Ranzato et al.,2016), the authors propose to train a neural translation model with the objective gradually shifting from maximizing token-level likelihood
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	1816313093	lingual Data For a target-side monolingual sentence, its source sentence xis missing, and consequently y^ is unavailable since it is sampled based on x. We tackle this challenge via back translation (Sennrich et al., 2015a). We ﬁrst train a reverse NMT model from the target language to the source language with bilingual data. For each target-side monolingual sentence, using the reverse NMT model, we back translate it
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2626778328	pout for Zh-En and En-Zh translation to be 0:05. The optimizer used for MLE training is Adam (Kingma and Ba, 2015) with initial learning rate is 0:1, and we follow the same learning rate schedule in (Vaswani et al., 2017). During training, roughly 4;096 source tokens and 4;096target tokens are paired in one mini batch. Each model is trained using 8NVIDIA Tesla M40 GPUs. For RL training, the model is initialized with p
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	1522301498	These settings are exactly same as used in the original paper, except we set the layer prepostprocess dropout for Zh-En and En-Zh translation to be 0:05. The optimizer used for MLE training is Adam (Kingma and Ba, 2015) with initial learning rate is 0:1, and we follow the same learning rate schedule in (Vaswani et al., 2017). During training, roughly 4;096 source tokens and 4;096target tokens are paired in one mini
2888442053	A Study of Reinforcement Learning for Neural Machine Translation	2096557251	how to use these tricks in machine translation. For example, baseline reward method (Weaver and Tao,2001) is suggested in (Ranzato et al.,2016;Nguyen et al.,2017;Wu et al.,2016) but not leveraged in (He and Deng, 2012;Shen et al.,2016). Third, large-scale datasets, especially monolingual datasets are shown to signiﬁcantly improve translation quality (Sennrich et al.,2015a;Xia et al., 2016) with MLE training, while
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2307381258	., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which create a summary by identifying (and subsequently concatenating) the most important sentences in a document (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018b). Abstractive approaches, despite being more faithful to the actual summarization task, either lag behind extractive ones or are mostly extractive, exhi
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2194775991	(GLU, v : R2d → Rd, Dauphin et al. 2017) on the output of the convolution Y. Subsequent layers operate over the k output elements of the previous layer and are connected through residual connections (He et al., 2016) to allow for deeper hierarchical representation. We denote the output of the ℓth layer as hℓ = (hℓ 1,...,h ℓ n) for the decoder network, and zℓ = (zℓ 1,...,z ℓ m)for the encoder network. Multi-hop At
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2613904329	2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture long-range dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2613904329	compression ratio is extremely high, and pertinent content can be easily missed. Recently, a convolutional alternative to sequence modeling has been proposed showing promise for machine translation (Gehring et al., 2017a,b) and story generation (Fan et al., 2018). We believe that convolutional architectures are attractive for our summarization task for at least two reasons. Firstly, contrary to recurrent networks wh
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	1501617060	d 47–55% new bigrams, 58–72% new trigrams, and 63–80% novel 4-grams). We further evaluated two extractive methods on these datasets. LEAD is often used as a strong lower bound for news summarization (Nenkova, 2005) and creates a summary by selecting the ﬁrst few sentences or words in the document. We extracted the ﬁrst 3 sentences for CNN documents and the ﬁrst 4 sentences for DailyMail (Narayan et al., 2018b).
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2613904329	the document p=(p1,...,pm) where pi ∈ Rf is a column in position matrix P ∈ RN×f, and N is the maximum number of positions. Position embeddings have proved useful for convolutional sequence modeling (Gehring et al., 2017b), because, in contrast to RNNs, they do not observethetemporalpositionsofwords(Shi et al., 2016). Let tD ∈ Rf ′ be the topic distribution of document Dand t′ = (t′ 1,...,t ′ m)the topic distribution
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2613904329	es. Secondly, hierarchical features can be extracted over larger and larger contents, allowing to represent long-range dependencies efﬁ- ciently through shorter paths. Our model builds on the work of Gehring et al. (2017b) who develop an encoder-decoder architecture for machine translation with an attention mechanism (Sukhbaatar et al., 2015) based exclusively on deep convolutional networks. We adapt this model to ou
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	104184427	g. For CONVS2S3 and T-CONVS2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings. We trained our convolutional models with Nesterov’s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.99 and renormalized gradients if their norm exceeded 0.1 (Pascanu et al., 2013). We used a learning rate of 0.10 and once the validation perplexity stopped improving, we 2
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	1843891098	that often include a ﬁrstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al.,2016;See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2026191715	nd PTGEN (using aone-way ANOVA with posthoc Tukey HSD tests; p &lt; 0.01). All other differences are not statistically signiﬁcant. For our second experiment we used a questionanswering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018b) to assess the degree to which the models retain key information from the document. We used the same 50 documents as in our ﬁrst elicitation study. We wrote two fact-based ques
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2613904329	o stabilize learning. Our topic-enhanced model calibrates longrange dependencies with globally salient content. As a result, it provides a better alternative to vanilla convolutional sequence models (Gehring et al., 2017b) and RNN-based summarization models (See et al., 2017) for capturing cross-document inferences and paraphrasing. At the same time it retains the computational advantages of convolutional models. Eac
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2275625487	putational advantages of the convolutional architecture. The idea of capturing document-level semantic information has been previously explored for recurrent neural networks (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017), however, we are not aware of any existing convolutional models. Topic Sensitive Embeddings Let D denote a document consisting of a sequence of words (w1,...,wm); we embed Dinto
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2307381258	tion paradigms that have been identiﬁed over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2
2888482885	Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	2026191715	tions canbeanswered,thebetterthe corresponding system is at summarizing the document as a whole. Five participants answered questions for each summary. We followed the scoring mechanism introduced in Clarke and Lapata (2010). A correct answer was marked with a score of one, partially correct answers with a score of 0.5, and zero otherwise. The ﬁnal score for a system is the average of all its question scores. Answers aga
2888643222	Sarcasm Analysis Using Conversation Context	2565831772	and Bhattacharyya2015). Use of Rhetorical Questions. We also found that sarcastic utterances that use rhetorical questions (RQ), especially in the discussion forum (e.g., IAC v2) are hard to identify.Oraby et al. (2016) hypothesized that sarcastic utterances of RQtype are of the following structure; they contain questions in the middle of a post, that are followed by a statement. Since many discussion posts are long
2888643222	Sarcasm Analysis Using Conversation Context	2529281176	gh using the “post&quot; message as context seems to improve for the sarcastic class (Oraby et al.2017). Unlike the above approaches that model the utterance and context together,Wang et al.(2015) andJoshi et al. (2016a) use a sequence labeling approach and show that conversation helps in sarcasm detection. Inspired by this idea of modeling the current turn and context separately, in our prior work (Ghosh, Fabbri,
2888643222	Sarcasm Analysis Using Conversation Context	2512532697	nt Corpus (IAC) (Oraby et al.2016) and Reddit man, and Van den Bosch2013;Riloff et al.2013;Maynard and Greenwood2014;Joshi, Sharma, and Bhattacharyya2015;Ghosh, Guo, and Muresan2015;Joshi et al.2016b;Ghosh and Veale 2016). In many instances, however, even humans have difﬁculty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al.2014). Thus, to detect the speaker’s sarcastic intent
2888643222	Sarcasm Analysis Using Conversation Context	2565831772	ocial language categories such as agreement/disagreement (between a pair of online posts), nastiness, and sarcasm. There are different version of IACand we use a speciﬁc subset of IACin this research.Oraby et al. (2016) have introduced Sarcasm Corpus V2, a subset of the Internet Argument Corpus V2, which contain 9,400 posts labeled as sarcastic or non-sarcastic (balanced dataset). To obtain the gold labels,Oraby et
2888643222	Sarcasm Analysis Using Conversation Context	2512532697	vs. non-sarcastic utterances using various lexical and pragmatic features (González-Ibáñez, Muresan, and Wacholder2011;Liebrecht, Kunneman, and Van den Bosch2013;Muresan et al.2016;Joshi et al.2016b;Ghosh and Veale 2016), rules and text-patterns (Veale and Hao2010), speciﬁc hashtags (Maynard and Greenwood 2014) as well as semi-supervised approach (Davidov, Tsur, and Rappoport2010). Researchers have also examined diff
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	102708294	of such applications. First, knowledgeable articles and snippets can be used as a data source for knowledge base construction. Currently, majority of popular knowledge bases, like YAGO [16], DBpedia [1] and etc., extracted knowledge based on Wikipedia, WordNet, GeoNames and so on. Compared with the data scale of social media, knowledge in these structured or semi-structured resources summarized by h
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2022166150	e describe two of such applications. First, knowledgeable articles and snippets can be used as a data source for knowledge base construction. Currently, majority of popular knowledge bases, like YAGO [16], DBpedia [1] and etc., extracted knowledge based on Wikipedia, WordNet, GeoNames and so on. Compared with the data scale of social media, knowledge in these structured or semi-structured resources su
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2104246439	intra-class compactness by the special designed intrinsic graph and penalty graph. NLP via Neural Network. Knowledgeable document extraction is related to NLP classication. For such task, Tai et al. [17] proposed a model that predict the semantic relatedness of two sentences and sentiment classication based on LSTM model. Kim et al. [13] proposed a CNN model based on hyperparameter tuning and static
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2071238250	lassication. Kessler et al. [12] rst used the term \genre&quot; to represent any widely recognized class of texts dened by some common communicative purposes or other functional traits. Finn et al. [8] proposed that the genre classication is orthogonal to the topic classication. That is to say the documents had same topic can have dierent genre and documents in the same genre can also have dier
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2071238250	owledgeable and unknowledgeable documents, we put them into our feature set. These features include the number of words, the length of document, the number of sentences and average length of sentences[8, 18]. We also extend these features, such as the number of paragraphs, average sentences’ number of each paragraph, the number of distinct words in title and so on. The results demonstrate that these feat
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2068737686	rnessed from the so-far largest corpus consisting of 326 million knowledgeable sentences extracted from 1:68 billion web pages [21]. However, these sentences are extracted only by the Hearst patterns [10]. For extracting more knowledgeable snippets to construct more comprehensive knowledge base, semantic-based methods are needed to complement the previous pattern-based ones. 1 arXiv:1808.07228v1 [cs.C
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2153579005	sentence level and sentence-to-document level. At the word-to-sentence level, we transform words into sentence embeddings. Hence, we rst generate word embeddings as the input by applying the word2vec [15] and setting dimension to 200. The vocabulary we used contains 272;582 Chinese words, and the out-ofvocabulary words are replaced with a special token \UNK&quot;. Then, for a sentence of the given doc
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2120615054	standing images and natural language text in recent years [23, 24], and achieved great success in multiple applications, such as image recognition and captioning [14, 20, 22], text sentiment analysis [11, 6], non-photorealistic rendering [9], etc.. Specically, we propose SSNN, a joint CNN-based model, to understand the abstract concept of documents in dierent domains collaboratively and judge whether a
2888686418	Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents.	2138605095	t knowledge base, Probase, with 2:7 million concepts was automatically harnessed from the so-far largest corpus consisting of 326 million knowledgeable sentences extracted from 1:68 billion web pages [21]. However, these sentences are extracted only by the Hearst patterns [10]. For extracting more knowledgeable snippets to construct more comprehensive knowledge base, semantic-based methods are needed
2888693386	The Importance of Generation Order in Language Modeling	2130942839	., 2014) can be seen as augmenting that memory. Text generation via neural networks, as in language models and machine translation, proceeds almost universally left-to-right (Jozefowicz et al., 2016; Sutskever et al., 2014). This is in stark contrast to phrase-based machine translation systems (Charniak et al., 2003) which traditionally split token translation and “editing” (typically via reordering) into separate stage
2888693386	The Importance of Generation Order in Language Modeling	2108321481	ly intractable, and model quality can only be evaluated via external tasks. In addition to surface-form intermediate representation, syntax-based representations have a rich history in text modeling. Chelba and Jelinek (1998); Yamada and Knight (2001 );Graham and Genabith 2010 Shen et al. (2018) integrate parse structures, explicitly designed or automatically learned, into the decoding process. Similar to the second phase
2888693386	The Importance of Generation Order in Language Modeling	2057147815	moving beyond the left-to-right generation order by developing alternative multi-stage strategies such as syntax-aware neural language models (Bowman et al., 2016) and latent variable models of text (Wood et al., 2011). Before embarking on a long-term research program to ﬁnd better generation strategies that improve modern neural networks, one needs evidence that the generation strategy can make a large difference.
2888693386	The Importance of Generation Order in Language Modeling	2626778328	nd we can compare different vocabulary partitioning strategies both against each other and against a singlepass language model. Our implementation consists of two copies of the Transformer model from Vaswani et al. (2017). The ﬁrst copy just generates the template, so it has no encoder. The second copy is a sequence-to-sequence model that translates the template into the complete sentence. There are threeplaces inthis
2888693386	The Importance of Generation Order in Language Modeling	2130942839	strategies can be quite large. 4 Related Work For tasks conditioning on sequences and sets, it is well known that order signiﬁcantly affects model quality in applications such as machine translation (Sutskever et al., 2014), program synthesis (Vinyals et al., 2016), and text classiﬁcation (Yogatama et al., 2016). Experimentally, Khandelwal et al. (2018) show that recurrent neural networks have a memory that degrades wit
2888693386	The Importance of Generation Order in Language Modeling	2626778328	tremely successful statistical modelsoftextinlanguage modeling and machine translation. Despite differences in model architectures, state of the art neural nets generate sequences from left to right (Vaswani et al., 2017; Jozefowicz et al., 2016; Wu et al., 2016). Although in some sense humans produce and consume language from left to right as well, there are many other intuitively appealing ways to generate text. Fo
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	1821462560	and ˝is a temperature coefﬁcient that is normally set to 1. A higher ˝produces a softer probability distribution, and thus, provides more information about the relative similarity between classes. As Hinton et al. (2014) suggested, the above losses can be jointly optimized as follows: L( S) = L CE( S) + L KD( S) where is usually set as ˝2 since the magnitudes of gradients produced by L KD scale as 1=˝2. 3 Attent
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	1690739335	r distillation can better handle the biased distillation problem, which is more severe in the adversarial dataset. Finally, we replace the joint training process with a stage-wise fashion proposed by Romero et al. (2015). Concretely, we ﬁrst warm up the student by matching the attention distribution as a pre-training step, and then minimize the rest of losses to train the model. The result, however, shows that this s
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	1821462560	efﬁciency and robustness by transferring the knowledge from a cumbersome ensemble model to a single model. Knowledge Distillation. Knowledge distillation is ﬁrst explored by Bucila et al. (2006) and Hinton et al. (2014), which attempts to transfer knowledge deﬁned as soft output distributions from a teacher to a student. Later works have been proposed to distill not only the ﬁnal output but also intermediate represe
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	1690739335	er knowledge deﬁned as soft output distributions from a teacher to a student. Later works have been proposed to distill not only the ﬁnal output but also intermediate representation from the teacher (Romero et al., 2015;Zagoruyko and Komodakis,2017;Huang and Wang,2017). Papernot et al. (2016) show that knowledge distillation can be used to prevent the network from adversarial attacks in image recognition. Radosavovi
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	2174868984	l.,2014) has been proposed to train a student model with the supervision of a teacher model. Such idea is further explored to enhance the generalizability and robustness of image recognition systems (Papernot et al., 2016). Some subsequent works attempt to transfer teacher’s intermediate representation, such as feature map (Romero et al.,2015;Zagoruyko and Komodakis,2017), neuron selectivity (Huang and Wang,2017) and s
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	2516930406	ng comprehension datasets (Hermann et al.,2015;Hill et al.,2016; Rajpurkar et al.,2016;Joshi et al.,2017), end-toend neural networks have achieved promising results (Wang et al.,2018;Yu et al.,2018). Wang and Jiang (2017) combine the match-LSTM with pointer networks to predict the answer boundary. Wang et al. (2017) match the context aginst itself to reﬁne the passage representation. Later, a variety of attention mech
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	2427527485	QANet (E) - - 82.7 89.0 RMR7 78.9 86.3 79.5 86.6 RMR (E) 81.2 87.9 82.3 88.5 RMR + A2D 80.3 87.5 81.5 88.1 Table 1: Comparison of different approaches on the SQuAD test set, extracted on May 9, 2018: Rajpurkar et al. (2016)1, Huang et al. (2018)2, Peters et al. (2018)3, Wang et al. (2017)4, Wang et al. (2018)5, Yu et al. (2018)6 and Hu et al. (2018)7. BiSAE refers to BiDAF + Self Attention + ELMo. (E: ensemble model) We
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	2738015883	system, however, has two major drawbacks: the inference time is slow and a huge amount of resource is needed. Second, existing models are not robust since they are vulnerable to adversarial attacks. Jia and Liang (2017) show that the models are easily fooled by appending an adversarial sentence into the passage. Such fragility on adversarial examples severely diminishes the practicality of current MRC systems. One p
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	591148856	ttacks in image recognition. Radosavovic et al. (2017) introduce data distillation that annotates large-scale unlabelled data for omni-supervised learning. In the ﬁeld of natural language processing, Mou et al. (2016) distill task-speciﬁc knowledge from word embeddings. Kuncoro et al. (2016) propose to learn a single parser from an ensemble of parsers. Kim et al. (2016) investigate knowledge distillation for neura
2888706357	Attention-Guided Answer Distillation for Machine Reading Comprehension	2174868984	udent. Later works have been proposed to distill not only the ﬁnal output but also intermediate representation from the teacher (Romero et al., 2015;Zagoruyko and Komodakis,2017;Huang and Wang,2017). Papernot et al. (2016) show that knowledge distillation can be used to prevent the network from adversarial attacks in image recognition. Radosavovic et al. (2017) introduce data distillation that annotates large-scale unl
2888757417	Improving Abstraction in Text Summarization	2335367650	15) to train models for generating longer, multi-sentence summaries up to 100 words. The New York Times dataset (Sandhaus,2008) has also been used as a benchmark for the generation of long summaries (Durrett et al., 2016;Paulus et al.,2017). Trainingstrategiesforsequentialmodels. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, th
2888757417	Improving Abstraction in Text Summarization	2729842244	mismatch between the optimization objective and the evaluation metrics by directly optimizing evaluation metrics. This approach has led to consistent improvements in domains such as image captioning (Zhang et al., 2017) and abstractive text summarization (Paulus et al.,2017). A recent approach to training sequential models utilizes generative adversarial networks to improving the human perceived quality of generated
2888757417	Improving Abstraction in Text Summarization	2133564696	the source document and create concise summaries with phrases not in the source document. The state-of-the-art abstractive summarization models are based on sequence-tosequence models with attention (Bahdanau et al., 2015). Extensions to this model include a selfattention mechanism (Paulus et al.,2017) and an article coverage vector (See et al.,2017) to prevent repeated phrases in the output summary. Different training
2888824653	AtomicWikiEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse	1486649854	) or unsupervised heuristics (Guu et al.,2018); in contrast, we provide a large corpus of natural, human-produced edits. Also related is recent work in sentence representation learning from raw text (Kiros et al., 2015;Peters et al.,2018), bitext (McCann et al., 2017), and other supervised tasks including NLI (Conneau et al.,2017). Especially related is work on learning representations from weakly-labelled discours
2888824653	AtomicWikiEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse	2250539671	re among the top 10 phrases. We also compute Similarity@1 as the mean cosine similarity of each top-ranked phrase and respective gold phrase over the test set. We use the sum of the Glove embeddings (Pennington et al., 2014) of each word in the phrase as a simple approximation of the phrase vector. Table11shows the results. We see that, compared to the model trained on General Wikipedia, the model trained on WikiAtomicEd
2888910376	Bridging Knowledge Gaps in Neural Entailment via Symbolic Models	1840435438	ely (middle layer in Fig.2). Neural Entailment We use a simple neural entailment model, Decomposable Attention (Parikh et al.,2016), one of the state-of-the-art models on the SNLI entailment dataset (Bowman et al., 2015). However, our architecture can just as easily use any other neural entailment model. We initialize the model parameters by training it on the Science Entailment dataset. Given the sub-facts from the
2888910376	Bridging Knowledge Gaps in Neural Entailment via Symbolic Models	2608309533	ral and symbolic methods to correctly identify the entailment relation while Ensemble fails to do so. 4 Related Work Compared to neural only (Bowman et al.,2015; Parikh et al.,2016) or symbolic only (Khot et al., 2017;Khashabi et al.,2016) systems, our model Table 3: Few randomly selected examples in the test set between symbolic only, neural only, Ensemble and NSnet inference. The symbolic only model shows its th
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2606974598	2We use the OpenNMT-pytorch implementation https://github.com/OpenNMT/OpenNMT-py and a pre-trained model downloaded from http://opennmt. net/OpenNMT-py/Summarization.htmlwhich has higher result than See et al. (2017)’s summarizer. 3The pre-trained model generates multiple sentences summary, but we use only the ﬁrst sentence summary for evaluation in accordance with the AMR dataset. In Table 3, we can see that our
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2739046565	es (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-a
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2524520086	f the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al. ,2016 ;Konstas et al. 2017 Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, themainprevious workon AMR-based abstractive summarization (Liu et al., 2015) only generated bag-
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2606974598	gold standard AMR parses and parses obtained using the RIGA (Barzdins and Gosko, 2016) parser by 7.4 and 10.5 ROUGE-2 points respectively. Our model also outperforms a strong baseline seq2seq model (See et al., 2017) for summarization by 2 ROUGE-2 points. 2 Related Work Abstractive Summarization using AMR: In Liu et al. (2015) work, the source document’s sentences were parsed into AMR graphs, which were then comb
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2149837184	graphs using the hyper-parameters tuned in the previous paragraph. Liu et al. (2015) used parses from both the manual annotation of the Proxy dataset as well as those obtained using the JAMR parser (Flanigan et al., 2014). Instead of JAMR we use the RIGA parser (Barzdins and Gosko, 2016) which had the highest accuracy in the SemEval 2016 Task 8 (May, 2016). We compare our result against Liu et al. (2015)’s bag of word
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2252123671	ired to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure whichcanbeutilized in information aggregation during summarization. However, the use of AMR also has its ow
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	655477013	Konstas et al. 2017 Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, themainprevious workon AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. In this paper, we propose an approach to guide the NLG stage in AMR-based abstractive summarization using information from the source document.
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2606974598	nNMT BRNN)2,3 which summarizes directly from the source document to summary sentence without using AMR as an interlingua and is trained on CNN/DMcorpus (Hermann et al.,2015)using the same settings as See et al. (2017). AMR NLG Model F1 ROUGE parses R-1 R-2 R-L Gold Guided 40.4 20.3 31.4 Unguided 38.9 12.9 27.0 Liu et al. (2015) 39.6 6.2 22.1 RIGA Guided 42.3 21.2 33.6 Unguided 37.8 10.7 26.9 Liu et al. (2015) 40.9
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	655477013	ntion (Luong et al., 2015) which consists of an encoder and a decoder. The encoder computes the hidden representation of the input, {z1,z2,...,z k}, which is the linearized summary AMR graph, G′ from Liu et al. (2015), following Van Noord and Bos (2017)’s preprocessing steps. Following this, the decoder generates the target words, {y1,y2,...,y m}, using the conditional probability P s2s(y j|y &lt;j,z), which is ca
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	655477013	OUGE-2 points respectively. Our model also outperforms a strong baseline seq2seq model (See et al., 2017) for summarization by 2 ROUGE-2 points. 2 Related Work Abstractive Summarization using AMR: In Liu et al. (2015) work, the source document’s sentences were parsed into AMR graphs, which were then combined through merging, collapsing and graph expansion into a single AMR graph representing the source document. F
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	655477013	owever we use a simpler approach using a probabilistic language model in the scoring mechanism. 3 Guiding NLG for AMR-based summarization We ﬁrst brieﬂy describe the AMR-based summarization method of Liu et al. (2015) and then our guided NLG approach. 3.1 AMR-based summarization In Liu et al. (2015)’s work, each of the sentence of the source document was parsed into an AMR graph, and combined into a source graph,
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	1843891098	paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several w
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2606974598	source document through the process of paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	2524520086	t our proposed approach: (1) Is our baseline model comparable with the state-of-the-art AMR-to-text approaches? (2) Model BLEU Our model (unguided NLG) 21.1 NeuralAMR (Konstas et al., 2017) 22.0 TSP (Song et al., 2016) 22.4 TreeToStr (Flanigan et al., 2016) 23.0 Table 1: Results for AMR-to-text Does the guidance from the source document improve the result of AMR-to-Text in the context of summarization? (3) Does the
2888912028	Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation	655477013	uided NLG 29.6 68.6 39.6 61.3 Table 2: BLEU and ROUGE results forguidedandunguided models using test dataset. Guided NLG for full summarization In this experiment we combine our guided NLG model with Liu et al. (2015)’s work in order to generate ﬂuent texts from their summary AMR graphs using the hyper-parameters tuned in the previous paragraph. Liu et al. (2015) used parses from both the manual annotation of the
2888912391	How agents see things: On visual representations in an emergent language game	2564324149	re in game theory already showed that convergence towards successful communication is ensured under speciﬁc conditions (see Skyrms(2010) and references therein). However, the important contribution ofLazaridou et al. (2017) is to play a signaling game with real-life images instead of artiﬁcial symbols. This raises new empirical questions that are not answered by the general mathematical results, such as: When the agents
2888912391	How agents see things: On visual representations in an emergent language game	2160654481	other, and to the input ones. In order to compare the similarity structure of input, Sender and Receiver spaces, we borrow representational similarity analysis (RSA) from computational neuroscience (Kriegeskorte et al., 2008). Given two sets r 1 and r 2 of representations of the same item collection (e.g., r 1 is the collection of input images mapped in Sender embedding space and r 2 is the same collection represented by
2888914530	Decoupling Strategy and Generation in Negotiation Dialogues	2769917417	ork and Discussion Recent work has explored the space between goal-oriented dialogue and open-domain chit-chat through collaborative or competitive language games, such as collecting cards in a maze (Potts, 2012), ﬁnding a mutual friend (He et al.,2017), or splitting a set of items (DeVault et al.,2015;Lewis et al.,2017). Our CRAIGSLISTBARGAIN dialogue falls in this category, but exhibits richer and more dive
2888925928	Mapping Language to Code in Programmatic Context	2148190602	asets makes training deep neural models very effective, as we saw in the experimental evaluation. While massive amounts of Github code have been used before for creating datasets on source code only (Allamanis and Sutton, 2013, 2014; Allamanis et al., 2016), we instead extract from Github a dataset of NL and code with an emphasis on context, in order to learn to map NL to code within a class. 9 Conclusion In this paper, we
2888925928	Mapping Language to Code in Programmatic Context	2161002933	ctly equal to their references. 8 Related Work There is signiﬁcant existing research on mapping NL directly to executable programs in the form of logical forms (Zettlemoyer and Collins, 2005), λ-DCS (Liang et al., 2013), regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), database queries (Iyer et al., 2017; Zhong et al., 2017) and general purpose programs (Balog et al., 2016; Allamanis et al.,
2888925928	Mapping Language to Code in Programmatic Context	2304113845	re decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a two-step attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone &amp; MTG; Ling et al., 2016), or manua
2888925928	Mapping Language to Code in Programmatic Context	1522301498	e 3: Ablation of model features on the development set. the output of the decoder over c t. We train our model for 30epochs using mini-batch gradient descent with a batch size of 20, and we use Adam (Kingma and Ba, 2015) with an initial learning rateof0.001foroptimization. Wedecay our learning rate by 80%based on performance on the development set after every epoch. InferenceandMetrics Inference isdonebyﬁrst encoding
2888925928	Mapping Language to Code in Programmatic Context	1816313093	We evaluate a number of encoder-decoder models that generate source code derivations fromNLand the class environment. Our best model encodes all environment components broken down into subword units (Sennrich et al., 2016) separately, using Bi-LSTMs and decodes these contextual representations to produce a sequence of valid production rules that derive syntactically valid source code. The decoder also uses a two-step a
2888925928	Mapping Language to Code in Programmatic Context	1860267373,2010608861	iang et al., 2013), regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), database queries (Iyer et al., 2017; Zhong et al., 2017) and general purpose programs (Balog et al., 2016; Allamanis et al., 2015b). Ling et al. (2016) generate Java and Python source code from NL for card games, conditioned on categorical card attributes. Manshadi et al. (2013) generate code based on input/output examples for
2888925928	Mapping Language to Code in Programmatic Context	2304113845	Mechanism Since the class environment at test time can belong to previously unseen new domains, our model needs to learn to copy variables and methods into the output. We use the copying technique of Gu et al. (2016a) to compute a copy probability at every time step t using vector b of dimensionality H. copy(t)=σ(bTc t) Sinceweonly require named identiﬁers oruser deﬁned types to be copied, both of which are prod
2888925928	Mapping Language to Code in Programmatic Context	1902237438	NL representation, h i,using thecurrent decoder state, s t,tocompute a set of attention weights α t, which are used to combine h i into an NL context vector z t. We use a general attention mechanism (Luong et al., 2015), extended to perform multiple steps. α t,i = exp(sT tFh i) P i exp(sTtFh i) z t = X i α t,ih i The context vector z t is used to attend over every type (return type) and variable (method) name in the
2888925928	Mapping Language to Code in Programmatic Context	2101105183	ns or 500production rules. To evaluate the quality of the output, we use Exact match accuracy between the reference and generated code. As a measure of partial credit, we also compute the BLEU score (Papineni et al., 2002), following recent work on code generation (Ling et al., 2016; Yin and Neubig, 2017). BLEU is an n-gram precision-based metric that willbehigher whenmoresubparts ofthepredicted code match the provided
2888925928	Mapping Language to Code in Programmatic Context	2257123346	odels very effective, as we saw in the experimental evaluation. While massive amounts of Github code have been used before for creating datasets on source code only (Allamanis and Sutton, 2013, 2014; Allamanis et al., 2016), we instead extract from Github a dataset of NL and code with an emphasis on context, in order to learn to map NL to code within a class. 9 Conclusion In this paper, we introduce new data and methods
2888925928	Mapping Language to Code in Programmatic Context	1902237438	owed by the name, with a different separator between them. The encoder is an nlayer LSTM which initializes an LSTM-based decoder using its ﬁnal hidden states. The decoder uses an attention mechanism (Luong et al., 2015) over the encoder states to produce a conditional distribution over the next source code token (not production rule) given all the previous tokens. We replace UNK tokens in the output with source toke
2888925928	Mapping Language to Code in Programmatic Context	2304113845	Python source code from NL for card games, conditioned on categorical card attributes. Manshadi et al. (2013) generate code based on input/output examples for applications such as database querying. Gu et al. (2016b) use neural models to map NL queries to a sequence of API calls, and Neelakantan et al. (2015) augment neural models with a small set of basic arithmetic and logic operations to generate more meanin
2888925928	Mapping Language to Code in Programmatic Context	2101105183	test time (some e.g. class environments are LookupCommand, ColumnFileReader and ImageSequenceWriter). Our model achieves an exact match accuracy of 8.6% and a BLEU score (a metric for partial credit; Papineni et al., 2002) of 22.11, outperforming retrieval and recent neural methods. We also provide an extensive ablative analysis, quantifying the contributions that come from the context and the model, and suggesting tha
2888925928	Mapping Language to Code in Programmatic Context	1860267373,2010608861	ts, starting with arg0. We also replace all method names with the word function since it doesn’t affect the semantics of the resulting program. Generating informative method names has been studied by Allamanis et al. (2015a). We replace all string literals in the code with constants as they are often debug messages. Finally, we use an ANTLR java grammar3 that is post-processed by adding additional non-terminals and rul
2888925928	Mapping Language to Code in Programmatic Context	2304113845	ules that derive syntactically valid source code. The decoder also uses a two-step attention mechanism to match words in the NL with environment components, and then uses a supervised copy mechanism (Gu et al., 2016a) to incorporate environment elements in the resulting code. We describe this architecture below. 3.1 Encoder The encoder computes contextual representations of the NL and each component in the envir
2888999003	Neural Metaphor Detection in Context	2064675550	crucial for metaphor detection. given a sentence, detecting all of the metaphorical words (independent of their POS tags). We ﬁnd that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data. We improve the previous state-of-the-art by 7.5
2888999003	Neural Metaphor Detection in Context	2115340919	e ELMo (Embeddings from Language Models) vectors e i from Peters et al. (2018). These vectors have been shown to be useful for word sense disambiguation, a task closely related to metaphor detection (Birke and Sarkar, 2006). 3.1 Sequence Labeling Model Figure 1 shows the model architecture. We input the word representation [w i;e i] to a bidirectional LSTM, producing a contextualized representation h i for each token. T
2888999003	Neural Metaphor Detection in Context	1522301498	ied dropout on the input to LSTM and on the input to the feedforward layer. We ﬁne-tuned learning rate and dropout rate for each model on each dataset. We used SGD to optimize the CLS model and Adam (Kingma and Ba, 2013) for the SEQ model. We used spaCy (Honnibal and Montani, 2017) for lemmatization, tokenization, and part-of-speech tagging. 2The best performing model on the VUA Metaphor Detection Shared Task at the
2888999003	Neural Metaphor Detection in Context	2250539671	ing model (SEQ) and classiﬁcation model (CLS) for the verb classiﬁcation task, and the sequence labeling model (SEQ) for the sequence labeling task. Implementation Details We used 300d GloVe vectors (Pennington et al., 2014) and 1024d ELMo vectors. We used additional 50d index embedding for the classiﬁcation task. The LSTM module has a 300d hidden state. We applied dropout on the input to LSTM and on the input to the fee
2888999003	Neural Metaphor Detection in Context	2126530744	metaphors involving uncommon verbs. 6 Related Work There has been signiﬁcant work on studying different features for metaphor detection, including concretenesss and abstractness (Turney et al., 2011; Tsvetkov et al., 2014; Koper and im Walde,¨ 2017), imaginability (Broadwell et al., 2013; Strzalkowski et al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), ba
2888999003	Neural Metaphor Detection in Context	2115340919	ntences (example sentences in WordNet), compared to sentences in other datasets which come from resources such as 1For detailed information about each dataset, please refer to original papers: TroFi (Birke and Sarkar, 2006), MOH (Mohammad et al., 2016), VUA (Steen et al., 2010). MOH-X refers to a subset of MOH dataset used in previous work (Shutova et al., 2016) where verb and its argument are extracted from each senten
2888999003	Neural Metaphor Detection in Context	2250456113	t al., 2013), feature norms (Bulat et al., 2017), sensory features (Tekiroglu et al., 2015; Shutova et al., 2016), bag-of-words features (Koper and im Walde, 2016), and semantic¨ class using WordNet (Hovy et al., 2013; Tsvetkov et al., 2014). More recently, embedding-based approaches (K¨oper and im Walde, 2017; Rei et al., 2017) showed gains on various benchmarks. Many neural models with various features and archi
2889009749	Learning Neural Templates for Text Generation	2157812664	aining data (Ratnaparkhi, 2002; Wong and Mooney,2007; Belz,2008; LuandNg,2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Maires
2889009749	Learning Neural Templates for Text Generation	2739046565	interest in extending these methods to build general-purpose, data-driven natural language generation (NLG) systems (Mei et al.,2016;Dusek and Jurcıcekˇ ,2016;Lebret et al.,2016;Chisholm et al.,2017;Wiseman et al., 2017). These encoder-decoder models (Sutskever et al.,2014;Cho et al.,2014;Bahdanau et al., 2015) use a neural encoder model to represent a source knowledge base, and a decoder model to emit a textual desc
2889009749	Learning Neural Templates for Text Generation	2133564696	generation (NLG) systems (Mei et al.,2016;Dusek and Jurcıcekˇ ,2016;Lebret et al.,2016;Chisholm et al.,2017;Wiseman et al., 2017). These encoder-decoder models (Sutskever et al.,2014;Cho et al.,2014;Bahdanau et al., 2015) use a neural encoder model to represent a source knowledge base, and a decoder model to emit a textual description word-by-word, conditioned on the source encoding. This style of generation contrasts
2889009749	Learning Neural Templates for Text Generation	1521413921	icitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney,2007; Belz,2008; LuandNg,2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde
2889009749	Learning Neural Templates for Text Generation	1521413921	as and Lapata,2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classiﬁers or rerankers in order to actually accomplish the generation (Angeli et al., 2010;Konstas and Lapata,2013). In addition, these models explicitly model knowledge base ﬁeld selection, whereas the model we present is fundamentally an end-to-end model over generation segments. Recentl
2889009749	Learning Neural Templates for Text Generation	1902237438	odels for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversationsanddialogues(Shangetal.,2015;Wenetal., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tack
2889009749	Learning Neural Templates for Text Generation	2133564696	ss of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversationsanddialogues(Shangetal.,2015;Wenetal., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural
2889009749	Learning Neural Templates for Text Generation	2135363470	studied), or perhaps it is simply failing to ﬂuently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence). Traditional NLG systems (Kukich,1983;McKeown ,1992 ;Belz 2008 Gatt and Reiter 2009), in contrast, largely avoid these problems. Since they typically employ an explicit planning component, which decides which knowledge base records to focus on, and a surface rea
2889028433	Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis.	2409027918	ally targets the data efﬁciency problem. 2. PROPOSED APPROACH We use a baseline Tacotron architecture speciﬁed in [8], where we use a GMM attention [9], LSTM-based decoder with zoneout regularization [10] and phoneme inputs derived from normalized text. We use Grifﬁn-Lim [11] as the inversion algorithm to convert the predicted spectrograms to waveforms, as our main focus is to enable Tacotron training
2889028433	Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis.	1571950845,1578102511,1783473872	incorporating the framework. For evaluation, we perform both objective and subjective tests. There exists previous work studying the application of unsupervised and weakly supervised learning for TTS [4, 5, 6, 7]. Related to our work, for example, [7] uses pre-trained word vectors in a LSTM-based acoustic model in parametric TTS [7]. These studies consider learning methods within the traditional TTS paradigm,
2889037901	Mapping natural language commands to web elements	1486649854	n the pre-order traversal of the DOM hierarchy. 3.2 Embedding-based model A common method for matching two pieces of text is to embed them separately and then compute a score from the two embeddings (Kiros et al., 2015;Tai et al.,2015). For a command cand elements e 1;:::;e k, we deﬁne the following conditional distribution over the elements: p(e i jc) /exp[s(f(c);g(e i))] where sis a scoring function, f(c) is the
2889037901	Mapping natural language commands to web elements	2251079237	ronments. Examples of such actions include API calls (Young et al.,2013;Su et al.,2017;Bordes and Weston,2017), database queries (Zelle and Mooney,1996;Zettlemoyer and Collins,2007;Berant et al.,2013;Yih et al., 2015), navigation (Artzi and Zettlemoyer,2013; Janner et al.,2018), and object manipulation (Tellex et al.,2011;Andreas and Klein,2015;Guu et al.,2017;Fried et al.,2018). For web pages and graphical user i
2889048668	AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale.	2108598243	munity do not always scale well to industrial scenarios. In computer vision, there are many high quality free data sets which transform research efforts into industrial applications, such as ImageNet [1] and COCO [2]. In Mandarin ASR, although there are corpus like thchs30 [3] and hkust [4], a large-scale high-quality free corpus is still needed. In AISHELL-1 [5], we released 170 hours of Mandarin sp
2889048668	AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale.	2284628133	n, there are many high quality free data sets which transform research efforts into industrial applications, such as ImageNet [1] and COCO [2]. In Mandarin ASR, although there are corpus like thchs30 [3] and hkust [4], a large-scale high-quality free corpus is still needed. In AISHELL-1 [5], we released 170 hours of Mandarin speech with high quality human transcriptions. Various training and evaluati
2889048668	AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale.	2755682845	strial applications, such as ImageNet [1] and COCO [2]. In Mandarin ASR, although there are corpus like thchs30 [3] and hkust [4], a large-scale high-quality free corpus is still needed. In AISHELL-1 [5], we released 170 hours of Mandarin speech with high quality human transcriptions. Various training and evaluation recipes based on such corpus have been developed in Kaldi [6], which is a robust and
2889107415	Evaluating Theory of Mind in Question Answering	1525961042	fﬁce and not the bathroom). Recent research has focused on developing neural models that succeed in such scenarios (Sukhbaatar et al.,2015;Henaff et al.,2017). As a benchmark to evaluate these models,Weston et al. (2016) released a dataset – Facebook bAbi – that provides a set of toy tasks, each examining a speciﬁc type of reasoning. For example, the scenario in Table1evaluates the capacity to reason using a single s
2889107415	Evaluating Theory of Mind in Question Answering	2563734883	sed on their relevance to the question. The model is expected to learn to attend to Sally’s memory module if the question is about her belief about a state of the world. The Recurrent Entity Network. Henaff et al. (2017) propose a memory-augmented architecture, EntNet, with two interesting properties; ﬁrst, their model is a recurrent neural network and thus can capture the sequential nature of the events in a story.
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2739007515	ch is simply their concatenation. Text-to-text training instances are deﬁned as all the unique pairs of (C,S), where C is a complex sentence and S is its simpliﬁcation into multiple simple sentences (Narayan et al., 2017; Aharoni and Goldberg, 2018). For training, we delimit the simple sentences with a special symbol. Wedepartfromtheprior workbyonly usinga subset of theWebSplit training set: wetake aﬁxed sub-sample s
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2739007515	the dataset. This scheme produced superior performance in preliminary experiments. Asa quality measure, wereport multi-reference 3We use WebSplit v1.0 throughout, which is the scaled-up re-release by Narayan et al. (2017) at http://github.com/shashiongithub/Split-and-Rephrase, commit a9a288c. Preliminary experiments showed the same trends on the smaller v0.1 corpus, as resplit by Aharoni and Goldberg (2018). ↓train/ev
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2739007515	domly reserved 5000 examples each for tuning, validation and testing, producing 989,944 unique complex training sentences, compared to the 16,938 of WebSplit (cf. Table 3). 2.3 Comparison to WebSplit Narayan et al. (2017) derived the WebSplit corpus by matching up sentences in the WebNLG corpus (Gardent et al., 2017) according to partitions of their underlying meaning representations (RDF triples). The WebNLG corpus i
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2574872930	e standard word distribution with a distribution over the words in the input sentence. Training details are as described in the Appendix of Aharoni and Goldberg (2018) using the OpenNMT-py framework (Klein et al., 2017).6 3.1 Results We compare to the SOURCE baseline, which is the previously reported method of taking the unmodiﬁed input sentence as prediction, and we add SPLITHALF, the natural baseline of determinis
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2251199578	hang et al.,2017) and translation (Koehn and Knowles,2017). Andtheschema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in t
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2143230354	plit corpus, we identify edits that involve sentences being split. A list of sentences for each snapshot is obtained by stripping HTML tags and Wikipedia markup and running a sentence break detector (Gillick, 2009). Temporally adjacent snapshots of a Wikipedia pagearethencompared tocheckforsentences that have undergone a split like that shown in Figure 1. We search for splits in both temporal directions. Given
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2108373063	possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simpliﬁcation (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify edits that involve sentences being split. A list of sentences for each
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2108373063	samemeaning. Performingthissplit-and-rephrase task is one of the main operations in text simpliﬁcation, alongside paraphrasing and dropping less salient content (Siddharthan, 2006; Zhu et al., 2010; Woodsend and Lapata, 2011, i.a.). The area of automatic text simpliﬁcation has received a lot of attention (Siddharthan, 2014; Shardlow, 2014), yet still holds many open challenges (Xu et al., 2015). Splitting sentences in th
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2125527113	snapshots of entire documents at different timestamps, which makes it possible to reconstruct edit histories for documents. This has been exploited for many NLP tasks, including sentence compression (Yamangil and Nelken, 2008), text simpliﬁcation (Yatskar et al., 2010; Woodsend and Lapata, 2011; Tonelli et al., 2016) and modeling semantic edit intentions (Yang et al., 2017). To construct the WikiSplit corpus, we identify e
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2739007515	and translation (Koehn and Knowles,2017). Andtheschema-free nature of the task may allow for future supervision in the form of crowd-sourced rather than expensive expert annotation (He et al., 2015). Narayan et al. (2017) introduce the WebSplit corpus for the split-and-rephrase task and report results for several models on it. Aharoni and Goldberg (2018) improve WebSplit by reducing overlap in the data splits, and ∗Bo
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2511538013	uristic, and it provides multiple reference decompositions for each complex sentence, which tends to improve the correlation of automatic metrics with human judgment in related text generation tasks (Toutanova et al., 2016). 3 Experiments In order to understand how WikiSplit can inform the split-and-rephrase task, we vary the composition of the training set when training aﬁxedmodel architecture. We compare three trainin
2889111319	Learning To Split and Rephrase From Wikipedia Edit History	2739007515	y: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a modelwithqualitativelybetterpredictionsthat score 32 BLEU points above the prior best result on the WebSplit benchmar
2889191148	Neural Cross-lingual Named Entity Recognition with Minimal Resources	2148894869	rd order differences. To cope with the ﬁrst challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al.,2011;Kim et al., 2012;Wang and Manning,2014;Ni et al.,2017). Since parallel corpora may not be always available,Mayhew et al.(2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-
2889234142	One-Shot Relational Learning for Knowledge Graphs	2022166150	nce improvements over existing embedding models, and also eliminates the need of retraining the embedding models when dealing with newly added relations.1 1 Introduction Large-scale knowledge graphs (Suchanek et al., 2007;Vrandeciˇ c and Kr´ ¨otzsch ,2014;Bollacker et al.,2008;Auer et al.,2007;Carlson et al.,2010) represent every piece of information as binary relationships between entities, usually in the form of tri
2889234142	One-Shot Relational Learning for Knowledge Graphs	2250635077	unique entities and # R. denotes the number of all relations. # Tasks denotes the number of relations we use as one-shot tasks. Existing benchmarks for knowledge graph completion, such as FB15k-237 (Toutanova et al., 2015) and YAGO3-10 (Mahdisoltani et al.,2013) are all small subsets of real-world KGs. These datasets consider the same set of relations during training and testing and often include sufﬁcient training tri
2889242953	Identifying Well-formed Natural Language Questions	1531374185	proving Question Generation Automatic question generation is the task of generating questions that ask about the information or facts present in either a given sentence or paragraph (Vanderwende,2008;Heilman and Smith, 2010).Du et al.(2017) present a state-of-theart neural sequence-to-sequence model to generate questions from a given sentence/paragraph. The model used is an attention-based encoder-decoder network (Bahdan
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	203948990	, and in fact some earlier work on discourse in MT focused on pronouns speciﬁcally because they were supposed to be easier to evaluate than other aspects of discourse coherence such as lexical choice [2]. Problems arise in two ways. First, pronounusage in corpus data is often less clear-cut than one mightexpect,withsometimesvaguereferenceandoccasional violations of the rules of gender and number agre
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2626778328	dataset3, and it is likely that it avoided many potential errors simply by failing to produce any output at all. The YANDEX system, a recent NMT system that builds on the Transformer NMT architecture [21] and models the current sentence together with one previous sentence of context, performs much better on the test suite than all the other systemsinourcorpus. Thisindicatesthatanup-to-dateNMT can have
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	203948990,2151996595	ill considerable room for improvement in examples with cross-sentence dependencies. 1. Introduction Pronountranslationstillposesseriouschallengesformachine translation(MT)systemsdespiteyearsofresearch[1,2,3,4]. This can be ascribed to a combination of factors including an incomplete understanding of the problem, evaluation difﬁculties, and the fact that low system performance is often obscured by the prese
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2151996595	is an interesting follow-upproblem for future work. 8.2. MT Performance Gender agreement was long assumed to be the most importantproblemthatneededtobeaddressedtosolvetheissue of pronoun translation [1, 30]. This was eventually recognised to be insufﬁcient, and Guillou suggested that the function of pronounswasanotherimportantfactoraffectingtheirtranslation[3]. Ourevaluationresultsconﬁrmthatbothofthesef
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2419539795	ion. We extend our corpus with three NMT systems providedtousbyresearchersfromleadingNMTgroups. The ﬁrst (LIMSI) is the s-hier system described in paper [18]. It was trained on OpenSubtitles2016 data [19], and is designed to exploit context from previous source and target sentences when translating discourse phenomena (including pronouns). The second (NYU) is based on the NMT baseline described in [20
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2142239252	n example in the DiscoMT 2015 evaluation set (210 pronouns)usingthemanualannotationsinDiscoMT2015.test. We then re-computed the ofﬁcial accuracy metric, named Acc+OTHER in the DiscoMT 2015 evaluation [24], for the systems contained in the original DiscoMT data set, restricting the pronoun examples to those with categories matching the set used in the PROTEST evaluation (leaving 195 pronouns in total).
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2625092622	s generated retrospectively using GIZA++ [27]. We reject the option of using the output of the attention model for NMT systems as it is known that attention and word alignment may dramatically diverge[28]. Retrospectively computed alignments are clearly less accurate than those output by a decoder. As a result, the number of examples that could not be annotatedcorrectlybecause of incorrectwordalignmen
2889267346	Pronoun Translation in English-French Machine Translation: An Analysis of Error Types.	2626778328	the training data of the LIMSI and NYU systems. The third NMT system (YANDEX) is an English-French version of a system developedfor English–Russian[5]. It is based on the Transformer NMT architecture [21] and uses context from the preceding sentence to improve the translation of discourse phenomena. It is trained on a subset of the Europarl, News Commentary, and TED data from the DiscoMT 2015 shared t
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2493916176	adelta Optimizer (Zeiler, 2012) and ﬁnetune them by Stochastic Gradient Descent (SGD) with learning rate 0.001 (Kiefer et al., 1952). In Chinese crime case classiﬁcation dataset, we utilize fastText (Bojanowski et al., 2017) to pretrain the word vectors based on corpus collected by Internet crawlers. In relation classiﬁcation dataset, we choose a publicly available word vectors (Mikolov et al., 2018) 2. The embedding tab
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2311110368	connectionist have virtues and deﬁciencies, and we need integrated systems that can exploit the advantages of both. Recently, there is a movement towards a fruitful combination of these two streams. Hu et al. (2016) present a teacher-student framework encapsulating the logical structured knowledge into a neural network, which forces NN to emulate the predictions of a rule-regularized teacher. Lu et al. (2017) pr
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2230472587,2416885651	them implicitly with neural networks. Neural module networks Neural Module Networks (NMN), ﬁrst proposed in Visual Question Answering (VQA), is composed of collections of joint-trained neural models (Andreas et al., 2016a). Inspired by recurrent neural networks and recursive neural networks, which both involve repeated application of a single computational module, any VQA network can be composed of ﬁnite computationa
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2184218725	l., 2014) and other application systems which are based on connectionist methods partially or fully but applied to tasks which conceptually operates on a symbolic level such as visual analogy-making (Reed et al., 2015) and Go-playing (Silver et al., 2016). In addition, a line of research aims to encode symbolic rules into neural networks. Hu et al. (2016) propose a teacher-student framework to combine neural networ
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2493916176	lem from REs to modules and their parameters. We train a novel seq2seq model to predict RPNs from REs. 3.5 TRAINING METHOD The training is based on the pre-trained word vectors trained with fastText (Bojanowski et al., 2017) and all vectors are kept static during the training. We introduce methods to train modules and layouts separately. For modules, the training is divided into two phases: pretraining and ﬁnetune. In th
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2230472587,2416885651	mically composed of reusable modules based on linguistic structure. All modules in NMN are independent, composable and the computation for each problem is different due to its different architecture. Andreas et al. (2016a) use rule-based dependency parser (Zhu et al., 2013) to generate layouts to build NMN. Following Andreas et al. (2016a), some improved methods have been proposed. Andreas et al. (2016b) present a mo
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	1513168562	networks (NMN) which have symbolic structures. Speciﬁ- cally, the transformation involves 2 steps: •Parse a RE into an action tree composed of ﬁnite pre-deﬁned actions. This operation is inspired by Kaplan and Kay (1994) where each RE is considered as a ﬁnite-state machine (FSM). The types and orders of actions are determined by a neural parser or a symbolic parser. •Represent the RE actions as neural-symbolic module
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2311110368	perates on a symbolic level such as visual analogy-making (Reed et al., 2015) and Go-playing (Silver et al., 2016). In addition, a line of research aims to encode symbolic rules into neural networks. Hu et al. (2016) propose a teacher-student framework to combine neural networks with logic rules, transferring the structured information encoded in the logic rules into the network parameters. Lu et al. (2017) prese
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2126433015	structure. All modules in NMN are independent, composable and the computation for each problem is different due to its different architecture. Andreas et al. (2016a) use rule-based dependency parser (Zhu et al., 2013) to generate layouts to build NMN. Following Andreas et al. (2016a), some improved methods have been proposed. Andreas et al. (2016b) present a model for learning to select layouts from a set of autom
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2111457781	systems have been applied to various problems and successful applications include ontology learning, hardware/software speciﬁcation, fault diagnosis, robotics, training and assessment in simulators (Hitzler et al., 2005; de Penning et al., 2011; Garcez et al., 2015; Besold et al., 2017). Recently, there are other research efforts which are in the topical proximity of the core ﬁeld in neural-symbolic integration (Bes
2889277908	Generalize Symbolic Knowledge With Neural Rule Engine.	2780932362	utilize fastText (Bojanowski et al., 2017) to pretrain the word vectors based on corpus collected by Internet crawlers. In relation classiﬁcation dataset, we choose a publicly available word vectors (Mikolov et al., 2018) 2. The embedding table in every model is not trainable, in other words, all words vectors are kept static during training. More hyperparameters are shown in Table 2 4.5 RESULT Model Precision Recall
2889472770	Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Scientific Question Answering.	2427527485	the ‘search-and-answer’ strategy and achieved strong performance (Chen et al., 2017; Kwon et al., 2018; Wang et al., 2018) spanning multiple QA datasets such as TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), MS-Macro (Nguyen et al., 2016), among others. However, open-domain QA tasks become inherently more difﬁcult when (1) dealing with questions with little available evidence; (2) solving questions wher
2889472770	Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Scientific Question Answering.	2427527485	iv:1808.09492v4 [cs.CL] 2 Oct 2018 Table 1: Differences among popular QA datasets. Dataset Opendomain Multiple choice Passage retrieval No ranking supervision1 ARC (Clark et al., 2018) 3 3 3 3 SQuAD (Rajpurkar et al., 2016) 3 TriviaQA (Joshi et al., 2017) 3 MS-Macro (Nguyen et al., 2016) 3 Query1: Mercury , the planet nearest to the Sun , has extreme surface temperatures , ranging from 465 C in sunlight to -180 C in dar
2889482553	Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation	2143539737	that between experts (0.254). To the best of our knowledge, this is the ﬁrst time that IAA of professional translators and nonexperts has been compared for the human evaluation of MT. In related work,Callison-Burch (2009) compared the agreement level of two types of non-expert translators: MT developers (referred to in that paper as ‘experts’) and crowd workers. He showed that crowd workers can reach the agreement lev
2889487253	Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging	2338266296	com/bplank/bilstm-aux. The parameter l=40 was set on dev data across all languages. Besides using 10 epochs, word dropout rate (p=:25) and 40-dimensional lexicon embeddings, we use the parameters fromPlank et al. (2016). For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy. For the learning curve, we average over 5 random samples with 3 runs each. 4 Results Table1shows the tagging a
2889487253	Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging	2338266296	r faster convergence. 2 Method DSDS is illustrated in Figure1. The base model is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber,2005; Hochreiter and Schmidhuber,1997;Plank et al., 2016;Kiperwasser and Goldberg,2016). Let x 1:n arXiv:1808.09733v1 [cs.CL] 29 Aug 2018 be a given sequence of input vectors. In our base model, the input sequence consists of word embeddings w~ and the two
2889487253	Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging	2016630033	fromDas and Petrov(2011), and we even outperform theirs on four languages: Czech, French, Italian, and Spanish. 6 Related Work Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001), tag dictionaries (Li et al.,2012), annotation of seed training data (Garrette and Baldridge, 2013) or even more recently some combination of these, e.g., via multi-task learning (Fang and 20 40 # EM
2889487253	Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging	2168199177	lian, and Spanish. 6 Related Work Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001), tag dictionaries (Li et al.,2012), annotation of seed training data (Garrette and Baldridge, 2013) or even more recently some combination of these, e.g., via multi-task learning (Fang and 20 40 # EM iterations 54 56 accuracy (%) all languages 20 40 # EM iterations 78.0 78.5 79.0 accuracy (%) Li et
2889502429	Learning End-to-End Goal-Oriented Dialog with Multiple Answers	1793121960	ext utterances, we also modify the evaluation criteria so that the system is rewarded if it predicts any of the multiple correct next utterances. 6 Experiments and Results End-to-end memory networks (Sukhbaatar et al., 2015) are an extension of Memory Networks proDataset no match-type + match-type Per-turn Per-dialog Per-turn Per-dialog Original-bAbI dialog task 98.5 77.1 98.8 81.5 Permuted-bAbI dialog task* 96.4 58.2 96
2889502429	Learning End-to-End Goal-Oriented Dialog with Multiple Answers	1514535095	nau et al. (2014) use it for Machine Translation (MT), where the MT system can attend to different words in the input language sentence while producing different words in the output language sentence.Xu et al. (2015) use it for image caption generation where the system attends to different parts of the image while generating different words in the caption. Madotto and Attardi(2017) use it for Question Answering (
2889502429	Learning End-to-End Goal-Oriented Dialog with Multiple Answers	2403702038	tion models that learn from chatlogs of human-to-human interaction hold the promise of quickly bootstrapping dialog systems and keep evolving them based on new data. Recent work ((Vinyals and Le,2015;Bordes et al., 2016;Serban et al.,2016)) has shown that dialog models can be trained in an end-to-end manner with satisfactory results. However, human dialog has some unique properties that many other learning tasks do
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2120045257	”restaurant in the centre”), we have extended the simulated agenda-based user (Schatzmann and Young, 2009) with a probability r of the user addressing the relation instead of the value.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2160371091	To achieve this, a hierarchical policy model based on feudal reinforcement learning (Dayan and Hinton, 1993) has been implemented following the approach of Casanueva et al.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2251058040	For belief tracking, an extended version of the focus tracker (Henderson et al., 2014)—an effective rule-based tracker—was used for the conversational entities and the conversational world that also discounts probabilities if the respective value has been rejected by the user.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	889023230,2410983263	Model-free approaches like end-to-end generative networks (Serban et al., 2016; Li et al., 2016) have interesting properties (e.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2739549753,2118462278,2519349389,2728821832	Up until now, these systems have successfully been applied to single- or multi-domain taskoriented dialogues (Su et al., 2017; Casanueva et al., 2017; Lison, 2011; Wang et al., 2014; Papangelis and Stylianou, 2017; Gašić et al., 2017; Budzianowski et al., 2017; Peng et al., 2017) where each dialogue is modelled as multiple independent single-domain sub-dialogues.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	1550825643	Previous work on dialogue modelling already incorporated the idea of objects or entities to be the principal component of the dialogue state (Grosz, 1977; Bilange, 1991; Montoro et al., 2004; Xu and Seneff, 2010; Heinroth and Minker, 2013).
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2251130685	Ramachandran and Ratnaparkhi (2015) proposed a belief tracking approach using relational trees.
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2728821832,2047335008	rem (Williams and Young, 2006; Daubigney et al., 2012; Gašić and Young, 2014; Williams et al., 2017; Su et al., 2017; Casanueva et al., 2017; Papangelis and Stylianou, 2017).
2889540167	Addressing Objects and Their Relations: The Conversational Entity Dialogue Model	2047335008	For training and evaluation of the proposed framework, both the master policy and all sub-policies are modelled with the GP-SARSA algorithm (Gašić and Young, 2014).
2889557241	Towards automated customer support	2468484304	encoder-decoder architectures, was published recently [13]. In another related work, Boyanov et al. [3] explored the utility of neural models on data from SemEval-2016task3onCommunityQuestionAnswering[15].Theycompared seq2seqmodelswithretrieval-basedones,performingmodelselectionusingquestionansweringmeasures,andstudiedtheabilityofthechatbottoanswerfreeformquestions. Twitterdataisparticularlysuitablefo
2889557241	Towards automated customer support	2154652894	mersupportonTwitter,andwecomparetwotypesofchatbots:(i)based oninformationretrieval(IR),and(ii)onneuralquestionanswering.Wefurther explore semantic similarity measures since generic ones such as ROUGE [8], BLEU [16] and METEOR [2], which come from machine translation or text summarization,arenotwellsuitedforchatbots. The remainder of this paper is organized as follows: Section 2 presents related work.
2889557241	Towards automated customer support	2154652894	the problem is related to machine translation (MT) and text summarization (TS), which are nowadaysalsoaddressedusingseq2seqmodels,researchershavebeenusingMT andTSevaluationmeasuressuchasBLEU[16],ROUGE[8],andMETEOR[2], whichfocusprimarilyonwordoverlapandmeasurethesimilaritybetweenthe chatbot’sresponseandthegoldcustomersupportanswertotheuserquestion. However,ithasbeenargued[10,11]thatsuchword-overlappi
2889624842	Learning Gender-Neutral Word Embeddings	2250539671	the datasets (Zhao et al.,2017;Yao and Huang,2017) and word embeddings (Garg et al.,2017;Caliskan et al.,2017) but did not provide constructive solutions. 3 Methodology In this paper, we take GloVe (Pennington et al., 2014) as the base embedding model and gender as the protected attribute. It is worth noting that our approach is general and can be applied to other embedding models and attributes. Following GloVe (Pennin
2889699743	Textual Analogy Parsing: What’s Shared and What’s Compared among Analogous Facts	2145454741	n cognition (Tversky and Gati, 1978;Holyoak and Thagard,1996;Goldstone and Son,2005;Penn et al.,2008;Holyoak,2012). Our model of textual analogy is particularly inﬂuenced by Structure Mapping Theory (Falkenhainer et al., 1989;Gentner and Markman,1997), an inﬂuential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been much work focused on inferring lexical anal
2889787757	HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering	2427527485	es are 2 Table5. After retrieving these 10 paragraphs, we then use the model trained in the distractor setting to evaluate its performance on these ﬁnal candidate paragraphs. Following previous work (Rajpurkar et al., 2016), we use exact match (EM) and F 1 as two evaluation metrics. To assess the explainability of the models, we further introduce two sets of metrics involving the supporting facts. The ﬁrst set focuses o
2889839819	Multilingual Cross-domain Perspectives on Online Hate Speech.	2008652694	ce (precision &amp; recall). CTRS-008 12/24 For example, the LIBSVM algorithm (Chang &amp; Lin, 2011) yields 82% F 1 score (the mean of precision and recall) for jihadism detection, while Perceptron (Collins, 2002) yields 84% F 1 for German right-wing extremism and 92.5% for sexism. We achieved the best results with a deep learning CNNConvolutional Neural Network ( ; Kim, 2014) for sexism: about 95%. This shows
2889839819	Multilingual Cross-domain Perspectives on Online Hate Speech.	2150376021	hniques, e.g., to reduce each vector to its two most significant features, after which they can be clustered and represented as a 2-D visualization. We experimented with spherical k-means clustering (Hornik et al., 2012) and t-SNE visualization (Maaten &amp; Hinton, 2008). By visually inspecting such clusters, we observed that right-wing extremism mainly focuses on immigration, crime and politics, and that the incel
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2096956703	adds missing ones, and (2) disambiguates related words. Our unsupervised method relies on synsets induced automatically from synonymy dictionaries. In contrast to prior approaches, such as the one by Pennacchiotti and Pantel (2006), our method not only disambiguates the hypernyms but also extracts new relationships, substantially improving F-score over the original extraction in the input collection of hypernyms. We are the ﬁrs
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	1572567744	can be used to automatically construct taxonomies (Bordea et al., 2016; Faralli et al., 2017; Faralli et al., 2018), expand search engine queries (Gong et al., 2005), improve semantic role labeling (Shi and Mihalcea, 2005), perform generalizations of entities mentioned in questions (Zhou et al., 2013), and so forth. One of the important use cases of hypernyms is lexical expansion as in the following sentence: “This bar
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2137023796	ering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text. Snow et al. (2004) learned such patterns automatically, based on a set of hyponymhypernym pairs. Pantel and Pennacchiotti (2006) arXiv:1809.06223v1 [cs.CL] 17 Sep 2018 Figure 1: Outline of the proposed method for sense-aware hypernymy extraction using synsets. presented another approach for weakly supervised extraction of simi
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2096956703	ms, they do not take into account word sense representations: this is despite hypernymy being a semantic relation holding between senses. The only sense-aware approach we are aware of is presented by Pennacchiotti and Pantel (2006). 1https://commoncrawl.org Given a set of extracted binary semantic relationships, this approach disambiguates them with respect to the WordNet sense inventory (Fellbaum, 1998). In contrast to our wor
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2068737686	original extraction in the input collection of hypernyms. We are the ﬁrst to use sense representations to improve hypernymy extraction, as opposed to prior art. 2 Related Work In her pioneering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text. Snow et al. (2004) learned such patterns automatically, based on a set of hyponymhypernym pairs. Pantel and Pennacchiotti
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2068737686	rce of ground truth hypernyms. For Russian, we use a composition of three different hypernymy pair datasets summarized in Table 1: a dataset extracted from the lib.rus.ec electronic library using the Hearst (1992) patterns implemented for the Russian language in the PatternSim5 toolkit (Panchenko et al., 2012), a dataset extracted from the Russian Wiktionary, and a dataset extracted from the sense deﬁnitions i
2889842084	Unsupervised Sense-Aware Hypernymy Extraction.	2068737686	rs or their concatenation. Recent approaches to hypernym extraction went into three directions: (1) unsupervised methods based on such huge corpora as CommonCrawl1 to ensure extraction coverage using Hearst (1992) patterns (Seitner et al., 2016); (2) learning patterns in a supervised way based on a combination of syntactic patterns and distributional features in the HypeNet model (Shwartz et al., 2016); (3) tr
2889844776	Game-Based Video-Context Dialogue	2311783643	generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference (Lowe et al. ,2015 ;Liu et al. 2016b Li et al. 2016). We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation. bloodtrail bloodtrail bloodtrail blo
2889844776	Game-Based Video-Context Dialogue	2113850638	at history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al.,2003;Waibel et al., 2001;Schmidt and Bannon,1992). In the live video stream direction,Fu et al.(2017) andPing and Chen(2017) used real-time comments to predict the frame highlights in a video, andBarbieri et al.(2017) presen
2889844776	Game-Based Video-Context Dialogue	2144960104	ill bring the context [hv f ;h u f ] into the same space as the response hr f , and get a suitable similarity score. For optimizing our discriminative model, we use max-margin loss function similar toMao et al. (2016) andYu et al.(2017). Given a positive training triple (v;u;r), let the corresponding negative training triples be (v0;u;r), (v;u0;r), and (v;u;r0), i.e., one modality is wrong at a time in each of the
2889844776	Game-Based Video-Context Dialogue	2250539671	ore feeding it as input to the video context encoder. We use word-level RNN model with 100 dimension word embedding size and a vocabulary size of 27;000. We initialize the word embeddings with Glove (Pennington et al., 2014) vectors. We unroll the video context LSTM to a maximum of 60 time steps and the chat context to a maximum of 70 time steps. For the response, we unroll the RNN to a maximum of 10 time steps. All of o
2889844776	Game-Based Video-Context Dialogue	836999996	s 4.2.1 Triple Encoder For our simpler discriminative model, we use a ‘triple encoder’ to encode the video context, chat context, and response (see Fig.5), as an extension of the dual encoder model inLowe et al. (2015). The task here is to predict the given trainFigure 5: Overview of our ‘triple encoder’ discriminative model, with bidirectional-LSTM-RNN encoders for video, chat context, and response. ing triple (v;
2889844776	Game-Based Video-Context Dialogue	2575870198	ue systems. Also, generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference (Lowe et al. ,2015 ;Liu et al. 2016b Li et al. 2016). We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation. bloodtrail bloodtra
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2735642330	.82 Table 3: Comparison of the three models on the task of language modeling, using the PTB dataset [54]. Models Accuracy BLEU Shen et al. [40] 79.5 12.4 Shen et al. [40] on Texar 82.5 13.0 Hu et al. [17] on Texar 88.6 38.0 Table 4: Text style transfer on the Yelp data [40]. The ﬁrst row is the original opensource implementation by the author [40]. The subsequent two rows are Texar implementations of
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	1793121960	(a) The canonical encoder-decoder, sometimes with attentions A [42,2,30,45], or copy mechanisms [14,47,15]; (b) Variational encoder-decoder [5,51]; (c) Encoder-decoder augmented with external memory [41,4]; (d) Adversarial model using a binary discriminator C, with or without reinforcement learning [28,55,53]; (e) Multi-task learning with multiple encoders and/or decoders [29,11]; (f) Augmenting with c
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2626778328	3 Experiments We conduct case studies on technique sharing that is advantageously supported by Texar: (1) We deploy the state-of-the-art machine translation model, i.e., self-attentional Transformer [45], on other various tasks to study its generality, and obtain improved performance over previous methods; (2) We apply various model paradigms on the task of language modeling to compare the different
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2605045867	47,15]; (b) Variational encoder-decoder [5,51]; (c) Encoder-decoder augmented with external memory [41,4]; (d) Adversarial model using a binary discriminator C, with or without reinforcement learning [28,55,53]; (e) Multi-task learning with multiple encoders and/or decoders [29,11]; (f) Augmenting with cyclic loss [17,13]; (g) Learning to align with adversary, either on samples y or hidden states [25 ,26 40
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2130942839	cal and readable text, and 2) to realize in generated text any desired information inferred from inputs. To this end, a few key techniques are increasingly widely-used, such as neural encoderdecoders [42], attentions [2,30,45], memory networks [41], adversarial methods [12,25], reinforcement learning [36,3], structured supervision [17,19,52], as well as optimization techniques, data pre-processing and
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2097333193	ction Text generation spans a broad set of natural language processing tasks that aim at generating natural language from input data or machine representations. Such tasks include machine translation [2,8], dialog systems [49,39], text summarization [16,37], article writing [50, 27], text paraphrasing and manipulation [17,52,31], image captioning [48,22], and more. Recent years have seen rapid progress
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	179875071	eam-search decoding, etc. Different learning algorithms then call different schemes as a subroutine in the learning procedure—for example, maximum likelihood learning uses decoding with ground truths [32], a policy gradient algorithm can use stochastic decoding [36], and an adversarial learning can use either stochastic decoding for policy gradient-based updates [53] or Gumbel-softmax reparameterized
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	1793121960	generated text any desired information inferred from inputs. To this end, a few key techniques are increasingly widely-used, such as neural encoderdecoders [42], attentions [2,30,45], memory networks [41], adversarial methods [12,25], reinforcement learning [36,3], structured supervision [17,19,52], as well as optimization techniques, data pre-processing and result post-processing procedures, evaluati
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2099471712,2542835211	information inferred from inputs. To this end, a few key techniques are increasingly widely-used, such as neural encoderdecoders [42], attentions [2,30,45], memory networks [41], adversarial methods [12,25], reinforcement learning [36,3], structured supervision [17,19,52], as well as optimization techniques, data pre-processing and result post-processing procedures, evaluations, etc. 1 arXiv:1809.00794v
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2735642330	iques are increasingly widely-used, such as neural encoderdecoders [42], attentions [2,30,45], memory networks [41], adversarial methods [12,25], reinforcement learning [36,3], structured supervision [17,19,52], as well as optimization techniques, data pre-processing and result post-processing procedures, evaluations, etc. 1 arXiv:1809.00794v1 [cs.CL] 4 Sep 2018 $SSOLFDWLRQV 0RGHOWHPSODWHV&amp;RQÀJÀOHV (YD
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2130942839,2626778328	itectures in recent text generation literatures. E denotes encoder, D denotes decoder, C denotes classiﬁer (i.e., binary discriminator). (a) The canonical encoder-decoder, sometimes with attentions A [42,2,30,45], or copy mechanisms [14,47,15]; (b) Variational encoder-decoder [5,51]; (c) Encoder-decoder augmented with external memory [41,4]; (d) Adversarial model using a binary discriminator C, with or withou
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2735642330	already known. 3.3 Text Style Transfer To further demonstrate the versatility of Texar for composing complicated model architectures, we next choose the the newly emerging task of text style transfer [17,40]. The task aims to manipulate the text of an input sentence to change from one style to another (e.g., from positive sentiment to negative), given only non-parallel training data of each style. The cr
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2123086176	language processing tasks that aim at generating natural language from input data or machine representations. Such tasks include machine translation [2,8], dialog systems [49,39], text summarization [16,37], article writing [50, 27], text paraphrasing and manipulation [17,52,31], image captioning [48,22], and more. Recent years have seen rapid progress of this active area in both academia and industry,
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2735642330	m input data or machine representations. Such tasks include machine translation [2,8], dialog systems [49,39], text summarization [16,37], article writing [50, 27], text paraphrasing and manipulation [17,52,31], image captioning [48,22], and more. Recent years have seen rapid progress of this active area in both academia and industry, especially with the adoption of modern deep learning approaches in many o
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2099471712	nd (4) max denotes the optimization and learning procedure. Note that the above can have multiple losses imposed on different parts of components and parameters (e.g., generative adversarial networks [12]). Texar is designed to properly decouple the four elements, and allow free combinations of them through uniform interfaces. Such design has underlay the strong modularity of the toolkit. In particula
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	889023230	neration tasks beyond machine translation. We deploy the self-attention Transformer decoder on two tasks, namely, variational autoencoder (VAE) based language modeling [5] and conversation generation [39]. The ﬁrst task is to use the VAE model [23] for language modeling. LSTM RNN has been widely-used in VAE for decoding sentences. We follow the experimental setting in previous work [5,51], and test tw
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	1793121960	ning (almost) does not improve in terms of perplexity. This is partly because of the high variance of the policy gradient in seqGAN learning. We further evaluate a memory network-based language model [41] which has the same number of free parameters (11M) with the LSTM RNN model. The test set perplexity is signiﬁcantly higher than the LSTM RNNs, which is not unreasonable because LSTM RNN models are we
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	889023230	ntly improves over the VAE with conventional LSTM decoder. The second task is to generate a response given conversation history. We use the popular hierarchical recurrent encoder-decoder model (HRED) [39] as the base model, which treats a conversation as a transduction task. The conversation history is seen as the source sequence and is modeled with a hierarchical encoder. Each utterance in the dialog
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	1793121960	ques on a single task. This can be valuable for research community to standardize experimental conﬁgura9 Models Test PPL LSTM RNN with MLE [54] 74.23 LSTM RNN with seqGAN [53] 74.12 Memory Network LM [41] 94.82 Table 3: Comparison of the three models on the task of language modeling, using the PTB dataset [54]. Models Accuracy BLEU Shen et al. [40] 79.5 12.4 Shen et al. [40] on Texar 82.5 13.0 Hu et a
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2735642330,2751118800	sarial model using a binary discriminator C, with or without reinforcement learning [28,55,53]; (e) Multi-task learning with multiple encoders and/or decoders [29,11]; (f) Augmenting with cyclic loss [17,13]; (g) Learning to align with adversary, either on samples y or hidden states [25 ,26 40]. where (1) f  is the model that deﬁnes the model architecture and the intrinsic inference procedure; (2) D is
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	1895577753,1905882502	sentations. Such tasks include machine translation [2,8], dialog systems [49,39], text summarization [16,37], article writing [50, 27], text paraphrasing and manipulation [17,52,31], image captioning [48,22], and more. Recent years have seen rapid progress of this active area in both academia and industry, especially with the adoption of modern deep learning approaches in many of the tasks. On the other
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	889023230,2438667436	spans a broad set of natural language processing tasks that aim at generating natural language from input data or machine representations. Such tasks include machine translation [2,8], dialog systems [49,39], text summarization [16,37], article writing [50, 27], text paraphrasing and manipulation [17,52,31], image captioning [48,22], and more. Recent years have seen rapid progress of this active area in
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2626778328	text, and 2) to realize in generated text any desired information inferred from inputs. To this end, a few key techniques are increasingly widely-used, such as neural encoderdecoders [42], attentions [2,30,45], memory networks [41], adversarial methods [12,25], reinforcement learning [36,3], structured supervision [17,19,52], as well as optimization techniques, data pre-processing and result post-processin
2889862754	Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation	2626778328	tudy on the newly-emerging task of text style transfer, which involves composite neural architectures beyond the conventional encoder-decoder. 3.1 One Technique on Many Tasks: Transformer Transformer [45] is a recently developed model that achieves state-of-the-art performance on machine translation. Different from the widely-used attentional sequence-to-sequence models [2], Transformer introduces a n
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2250770256	“alicense”gforms a direct connection whose relation is “HasPrerequisite”. Similarly, f“driving”, “road”g In this work, concepts are words and phrases that can be extracted from natural language text (Speer and Havasi 2012). arXiv:1809.03568v2 [cs.CL] 5 Oct 2018 also forms a direct connection. Moreover, there are also indirect connections here such as f“a car”, “getting to a destination”g, which are connected by a pivot
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2250770256	“Causes”, “pollution”) means that “the effect of XX is YY”; the “CapableOf” relation (e.g. “car”, “CapableOf”, “go fast”) means that “XXcanYY”, etc. More relations and explanations could be found at Speer and Havasi (2012). Approach Overview In this section, we give an overview of our framework to show our basic idea of solving commonsense reasoning problem. Details of each component will be described in the following
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2127795553	; Radford et al. 2018; Yang et al. 2018) belong to the textbased direction. Previous works on knowledge-based pretraining are typically validated on knowledge base completion or link prediction task (Bordes et al. 2013; Socher et al. 2013; Chen et al. 2018a). Our work belongs to the second line. We pretrain models from commonsense knowledge base and apply the approach to the question answering task. We believe that
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2127426251	8; Yang et al. 2018) belong to the textbased direction. Previous works on knowledge-based pretraining are typically validated on knowledge base completion or link prediction task (Bordes et al. 2013; Socher et al. 2013; Chen et al. 2018a). Our work belongs to the second line. We pretrain models from commonsense knowledge base and apply the approach to the question answering task. We believe that combining both stru
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2116341502	BiLSTM(Emb(c)) (3) From the second aspect, we represent each concept based on the representations of its neighbors and the relations that connect them. We get inspirations from graph neural network (Scarselli et al. 2009). We regard a relation that connects two concepts as the compositional modiﬁer to modify the meaning of the neighboring concept. Matrix-vector multiplication is used as the composition function (Mitch
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2307268612	for” and “How many people went to the movie together”. We believe that deep question understanding, such as parsing a question based on a predeﬁned grammar and operators in a semantic parsing manner (Liang 2016), is required to handle these questions, which is a very promising direction, and we leave it to future work. Related Work Our work relates to the ﬁelds of question answering, the integration of knowl
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2250770256	ions based on their knowledge about the world, it is a great challenge for machines when there is limited training data. In this paper, we leverage external commonsense knowledge, such as ConceptNet (Speer and Havasi 2012), to improve the commonsense reasoning capability of a question answering (QA) system. We believe that a desirable way is to pre-train a generic model from external commonsense knowledge about the wor
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2250770256	ir of concepts. Here, we present how to calculate the commonsense based score of a question sentence and a candidate answer sentence. In our experiment, we retrieve commonsense facts from ConceptNet (Speer and Havasi 2012). As described above, each fact from ConceptNet can be represented as a triple, namely c = (subject;relation;object). For each sentence (or paragraph), we retrieve a set of facts from ConceptNet. Spec
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2116341502	ring. Commonsense-based Model In this section, we ﬁrst describe how to pre-train commonsense-based functions in order to capture the semantic relationships between two concepts. Graph neural network (Scarselli et al. 2009) is used to integrate context from the graph structure in an external commonsense knowledge base. Afterwards, we present how to use the pretrained functions to calculate the relevance score between tw
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2127795553	river RelatedTo Figure 6: An example from SemEval 2018 that requires sophistic reasoning based on commonsense knowledge. ceptNet. We implement three baselines as follows. The ﬁrst baseline is TransE (Bordes et al. 2013), which is a simple yet effective method for KB completion that learns vector embeddings for both entities and relations on a knowledge base. We re-implement and train TransE model on ConceptNet. The
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2409591106	uted directly based on the co-occurred frequency between concepts in a knowledge base, without learning a embedding vector for each concept. The third baseline is Key-Value Memory Network (KVMemNet) (Miller et al. 2016), which has been used in commonsense inference (Mihaylov and Frank 2018). It ﬁrst retrieves supporting evidences from external KB, and then regards the knowledge as a memory and uses them with a keyva
2889866549	Improving Question Answering by Commonsense-Based Pre-Training.	2250770256	ved by common sense. We only use the challenge set in our experiment. Commonsense Knowledge This section describes the commonsense knowledge base we investigate in our experiment. We use ConceptNet5 (Speer and Havasi 2012), one of the most widely used commonsense knowledge bases. Our approach is generic and could also be applied to other commonsense knowledge bases such as WebChild (Tandon, de Melo, and Weikum 2017), w
2889938809	Entity Tracking Improves Cloze-style Reading Comprehension	2097998348	et al.(2017). The model is regularized with dropout (Srivastava et al.,2014) and optimized with ADAM (Kingma and Ba,2014). For all experiments we performed a random search over hyperparameter values (Bergstra and Bengio, 2012), and report the results of the models that performed best on the validation set. Our implementation is available at https://github.com/ harvardnlp/readcomp. Results and Discussion Table2shows the ful
2889938809	Entity Tracking Improves Cloze-style Reading Comprehension	2539583692	ted way. We show an example from LAMBADA in Figure1. In CBT, as well as the similar CNN/Daily Mail dataset (Hermann et al.,2015), the answer yis always contained in x whereas in LAMBADA it may not be.Chu et al. (2017) showed, however, that training only on examples where yis in xleads to improved overall performance, and we adopt this approach as well. Related Work The ﬁrst popular neural network reading comprehen
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2549835527	each of their arguments, where the model reads the sentence, with one of the verbs randomly replaced with a hverbitoken. This is analogous to the agreement task explored in previous work on English (Linzen et al., 2016) and other languages (Gulordava et al., 2018), but in an arguably more challenging settings, as the Basque task requires the model: (a) to identify all the verb’s arguments; (b) to learn the ergative–
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	1810943226	this is sometimes desirable, in some instances we would like to evaluate the ability of the model to implicitly acquire hierarchical representations. Alternatively, one can train language model (LM) (Graves, 2013; Jo´zefowicz et al., 2016; Melis et al.,2017;Yogatama et al.,2018)tomodel the probability distribution of a language, and use common measures for quality such as perplexity as an indication of the mo
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2549835527	A different approach is testing the model on a grammatical task that does not require an extensive grammatical annotation, but is yet indicative of syntax comprehension. Speciﬁcally, previous works (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018) used the task of predicting agreement, which requires detecting hierarchal relations between sentence constituents. Labeled data for such a task re
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2549835527	hmark for the ability of models to capture regularities in human language. 2 Background and Previous Work To shed light on the question of hierarchical structure learning, a previous work on English (Linzen et al., 2016) has focused on subject-verb agreement: Theformofthird-person present-tense verbs in English is dependent upon the number of their subject (“They walk” vs. “She walks”). Agreement prediction is an int
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	1522301498	by a MLP of size 128, that receives as an input the hidden state of the BiLSTM over each word in the sentence. The whole model, including the embedding, is trained end-to-end with the Adam optimizer (Kingma and Ba, 2014). 6A unidirectional LSTM baseline achieved accuracy scores of 86.6%, 91.7% and 98.2% and recall values of 78.9%, 100% and 60.1% for ergative, absolutive and dative verb arguments prediction, respectiv
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2301095666	ructure requires a use of established benchmarks. A common approach is the use of an annotated corpus to learn an explicit syntax-oriented task, such as parsing or shallow parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) . While such an approach does evaluate the ability of the model to learn syntax, it has several drawbacks. First, the annotation process relies on human experts and is thus
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2549835527	tasks such as language modeling, parsing, and machine translation. RNNs were shown to be able to capture long-term dependencies and statistical regularities in input sequences (Karpathy et al., 2015; Linzen et al., 2016; Shi et al., 2016; Jurafsky et al., 2018; Gulordava et al., 2018). An adequate evaluation of the ability of RNNs to capture syntactic structure requires a use of established benchmarks. A common appr
2889947987	Can LSTM Learn to Capture Agreement? The Case of Basque.	2549835527	ucture of the input, as once the arguments of each present-tense verb in the sentence are found and their grammatical relation to the verb is established, predicting the verb form is straightforward. Linzen et al. (2016) tested different variants of the agreement prediction task: categorical prediction of the verb form based on the left context; grammatical assessment of the validity of the agreement present in a giv
2890031347	Events Beyond ACE: Curated Training for Events.	2109378394	annotation, a static corpus selected to be rich in the target event types is annotated. Active learning augments existing training data by having a human oracle annotate system queries (or features (Settles, 2011)). We explored a novel form of annotation, curated training (CT), in which teachers (annotators) actively seek out informative training examples. 2. Curated Training In CT the teacher created a priori
2890031347	Events Beyond ACE: Curated Training for Events.	2130714105	subsequent work has tried improve performance through the use of more complex inference (Li et al., 2013), by transductively drawing on outside sources of information, or both (Ji and Grishman, 2008; Hong et al., 2011). Such approaches have produced modest reductions in error over a pipeline of simple classiﬁers trained on ACE. In our efforts to improve on the KBP EA 2014 systems, we were stymied by a lack of data,
2890082223	Causal Explanation Analysis on Social Media	2250539671	tags and POS tags in our previous features) obtained the best performance for predicting Contingency class to which causality belongs. Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter 5 for each token of extracted discourse arguments from messages. For the distributional representation of discourse arguments, we run a Word-level LSTM on the words’ embeddings wit
2890082223	Causal Explanation Analysis on Social Media	1581597064	ty prediction and causal explanation identiﬁcation models. 2 Related Work Identifying causal explanations in documents can be viewed as discourse relation parsing. The Penn Discourse Treebank (PDTB) (Prasad et al., 2007) has a ‘Cause’ and ‘Pragmatic Cause’ discourse type under a general ‘Contingency’ class and Rhetorical Structure Theory (RST) (Mann and Thompson,1987) has a ‘Relations of Cause’. In most cases, the de
2890124903	TVQA: Localized, Compositional Video Question Answering	2427527485	ar et al.,2016;Hermann et al., 2015;Hill et al.,2015).Richardson et al.(2013) collected MCTest, a multiple choice QA dataset intended for open-domain reading comprehension. With the same goal in mind,Rajpurkar et al. (2016) introduced the SQuAD dataset, but their answers are speciﬁc spans from long passages. Weston et al.(2015) designed a set of tasks with automatically generated QAs to evaluate the textual reasoning ab
2890124903	TVQA: Localized, Compositional Video Question Answering	2136462581	datasets have been introduced, including DAQUAR (Malinowski and Fritz,2014), COCO-QA (Ren et al.,2015a), FM-IQA (Gao et al.,2015), Visual Madlibs (Yu et al.,2015), VQA (Antol et al.,2015), Visual7W (Zhu et al., 2016), etc. In addition, multiple video-based QA datasets have also been collected recently, e.g., MovieQA (Tapaswi et al.,2016), MovieFIB (Maharaj et al.,2017a), PororoQA (Kim et al.,2017), TGIF-QA (Jang
2890124903	TVQA: Localized, Compositional Video Question Answering	1544827683	os with associated dialogues. Text Question Answering: The related task of text-based question answering has been extensively explored (Richardson et al.,2013;Weston et al.,2015;Rajpurkar et al.,2016;Hermann et al., 2015;Hill et al.,2015).Richardson et al.(2013) collected MCTest, a multiple choice QA dataset intended for open-domain reading comprehension. With the same goal in mind,Rajpurkar et al. (2016) introduced
2890124903	TVQA: Localized, Compositional Video Question Answering	2194775991	question. Genome (Krishna et al.,2017) to detect object and attribute regions in each frame. Both regional features and predicted detection labels can be used as model inputs. We also use ResNet101 (He et al., 2016) trained on ImageNet (Deng et al.,2009) to extract whole image features. Regional Visual Features: On average, our videos contain 229 frames, with 16 detections per frame. It is not trivial to model s
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2115367993,2123595463	[74] for “principled” unsupervised learning; although similar ideas for learning by matching statistics have been explored earlier, e.g. for decipherment (see above), and also for machine translation [64, 70]. [50] extend ODM to sequences, and apply it to OCR with known character segmentations and pre-trained image features. In essence, ODM [74], or Empirical-ODM [50] minimises the KL-divergence cost betw
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2130942839,2157331557	, and lends generalisation ability to inputs of arbitrary length during inference. More recent methods treat the text-recognition problem as one of sequence prediction in an encoder-decoder framework [14, 73]. [72] adopted this framework first, using HOG features with Connectionist Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1527085884,2114781615	2) linguistic: assigning these clusters to the correct character identity. Indeed, earlier attempts at unsupervised text recognition proposed two-stage solutions corresponding to the two sub-problems [3, 28, 36, 39]. We address the first problem by exploiting the properties of standard fully-convolutional networks [51] — namely locality and translation invariance of the network’s filters. The second problem is e
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	70975097	ad to develop learning algorithms that can work with unaligned annotations. In this paradigm, images containing text can be extracted e.g. from scanned documents or by mining online image collections [33]. Independently, strings containing the same type of text (but not exactly the same text) can be readily harvested from machine readable text corpora (e.g. WMT datasets [1]). Both steps can be impleme
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2126086996	alphabet/language (plaintext). When the input is visual symbols, it becomes equivalent to text recognition. Some early works [13, 56] for optical character recognition (OCR), indeed model it as such. [27] cluster connected components in binarised document images and assign them to characters by maximising overlap with a fixed lexicon of words based on character frequencies and co-occurrence; [28, 36]
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2179581459	breaking a 1:1–substitution cipher [62]. The latter problem is NP-hard under a bi-gram language model [60]. While several solutions like aligning uni-gram (i.e. frequency matching) or n-gramstatistics[50,74]havebeenproposedtraditionallyforbreaking ciphers [16], we instead adopt an adversarial approach [20]. The result is a compact fully-convolutional sequence (i.e. multiple words/text-string) recognition
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2339754110,2552465644,2592480533	dings ye ∈Rn×d for both, the real and predicted strings are obtained: ye = y·W, whereW ∈RK×d are the character embeddings (d=256). We adopt the fully-convolutional PatchGAN discriminator architecture [31, 48, 49]. The embedded input ye is fed to a stack of five 1D-convolutional layers, each with 512 filters of size5. Each layer is followed by layer-normalisation [8] and leaky-ReLU (slope=0.2); zero padding is
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1491389626,1812645736	er set of methods process a whole word image, modelling it either as retrieval in a collection of word images from a fixed lexicon [4 ,18 22 65] or as learning multiple position dependent classifiers [32, 34, 63]. Our recognition model is similar to these character-sequence classifiers in that we train with a fixed number of output characters; but there is an important difference: we discard their fully-conne
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2116435618	eratedstrings a deterministic function of the input. We achieve this by removing the noise input from Φ which is normally used in generator networks. Furthermore, we do not use dropout regularization [71]. Training Objective. The discriminator D Yoperates in the domain of discrete symbols. While the real symbols are represented as one-hot vectors or vertices Vert(∆K)of the standard simplex, the genera
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2552465644	eters are initialised with Xavier initialization [17]. We use the RMSProp optimizer [76] with a constant learning rate of 0.001. The two-part discriminator loss objective is multiplied with 1 2 as in [31]. The models are implemented in TensorFlow [2]. 5 0 2000 4000 6000 8000 10000 iterations 0.0 0.2 0.4 0.6 0.8 1.0 character accuracy word length 3 5 7 9 11 13 Figure 7: Effect of text length on converg
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2147800946,2168868236	ic problem in pattern recognition and computer vision that has enjoyed continued interest over the years, owing to its many practical applications, such as recognising printed [69, 75] or handwritten [12, 43] documents, or more recently, text in natural images [35, 54, 58]. Consequently, many different and increasingly accurate methods have been developed. Yet, all such methods adopt thesamesupervisedlear
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1903029394	image and the predicted characters are sufficient to drive learning successfully. 4 IMPLEMENTATION Both, the recogniser (Φ) and the discriminator (D Y) are implemented as fully-convolutional networks [51]. The recogniser ingests an image of text and produces a sequence of character logits. The discriminator operates instead on character strings represented as sequence of character vectors, and produce
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2136346830	imes learnt first (runs2,3); see section5.3for the reason and further discussion. 17th century 408-pages manuscript, by also first clustering symbols but decipher using the noisy-channel framework of [38] through finite-state-machines. [46] learn mappings from hidden-states of an HMM with their transition probabilities initialised with conditional bi-gram distributions. [40] propose an iterative schem
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2605287558	line using an adversary [20]. Recent works [7, 42] have shown preliminary results for unsupervised machine translation using such adversarial losses, however they closely follow the CyleGAN framework [81], which learns a bidirectional mapping back-and-forth between the input and target domains to enforce bijection. This framework has also been applied recently in CipherGAN to break ciphers [19]. The C
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2179581459	local predictors may be able to match local text statistics such as n-grams, but would not be able to match global text statistics, such as forming proper words and sentences (see also section 6.1 of [74] for similar ideas). (2) ReducedStochasticity.Wealsomakethegeneratedstrings a deterministic function of the input. We achieve this by removing the noise input from Φ which is normally used in generato
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2402144811	n [17]. We use the RMSProp optimizer [76] with a constant learning rate of 0.001. The two-part discriminator loss objective is multiplied with 1 2 as in [31]. The models are implemented in TensorFlow [2]. 5 0 2000 4000 6000 8000 10000 iterations 0.0 0.2 0.4 0.6 0.8 1.0 character accuracy word length 3 5 7 9 11 13 Figure 7: Effect of text length on convergence.Training with longer words leads to faste
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2053007711	nal networks [51] — namely locality and translation invariance of the network’s filters. The second problem is equivalent to solving for the correct permutation, or breaking a 1:1–substitution cipher [62]. The latter problem is NP-hard under a bi-gram language model [60]. While several solutions like aligning uni-gram (i.e. frequency matching) or n-gramstatistics[50,74]havebeenproposedtraditionallyfor
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2335728318	ndividual training images. For example, for a text-image of cats , the corresponding annotation is the string {c,a,t,s}. A straightforward but tedious approach is to collect such annotations manually [37, 57, 77]; however, since datasets often comprise several million examples [32, 41], this scales poorly. Another, perhaps more pragmatic, approach is to engineer highly-sophisticated synthetic data generators
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2252189001	he network’s filters. The second problem is equivalent to solving for the correct permutation, or breaking a 1:1–substitution cipher [62]. The latter problem is NP-hard under a bi-gram language model [60]. While several solutions like aligning uni-gram (i.e. frequency matching) or n-gramstatistics[50,74]havebeenproposedtraditionallyforbreaking ciphers [16], we instead adopt an adversarial approach [20
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1980829015	noisy-channel framework of [38] through finite-state-machines. [46] learn mappings from hidden-states of an HMM with their transition probabilities initialised with conditional bi-gram distributions. [40] propose an iterative scheme for bootstrapping predictions for learning HMMs models, and recognise handwritten text. However, their approach is limited to (1) word images, (2) fixed lexicon (≈44K word
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1533861849	obtain the final scalar scoreD Y(y). Optimization details. Recogniser, discriminator and character embeddings are trained jointly end-to-end. The parameters are initialised with Xavier initialization [17]. We use the RMSProp optimizer [76] with a constant learning rate of 0.001. The two-part discriminator loss objective is multiplied with 1 2 as in [31]. The models are implemented in TensorFlow [2]. 5
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1924985727	oder-decoder framework [14, 73]. [72] adopted this framework first, using HOG features with Connectionist Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features with stronger CNN features, while [44, 67] have adopted the soft-attention [9] based recurrent decoders. Note, all these methods learn from labelled training examples. Unsupervi
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1491389626,2618530766	onding annotation is the string {c,a,t,s}. A straightforward but tedious approach is to collect such annotations manually [37, 57, 77]; however, since datasets often comprise several million examples [32, 41], this scales poorly. Another, perhaps more pragmatic, approach is to engineer highly-sophisticated synthetic data generators to mimic real images [24, 32, 79]. However, this requires developing new g
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1980829015,2112890726	rcatedbyspace)astokens,andisnormalizedbynumber of ground-truth words in ygt. 5.2 Effect of text length on convergence Although earlier works use low-order, namely uni/bi-gram statistics for alignment [40, 46], higher-ordern-grams could be more informative. In this experiment we examine the impact of the length of the training text-sequences on convergence. We train separate models on synthetic datasets co
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2179581459	re-defined lexicon of words. UnsupervisedLearningbyMatchingDistributions. Output Distribution Matching (ODM) which aligns the distributions of predictions with distributions of labels was proposed in [74] for “principled” unsupervised learning; although similar ideas for learning by matching statistics have been explored earlier, e.g. for decipherment (see above), and also for machine translation [64,
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2605287558	ring y = Φ(x)should correspond to the text represented in the input image x. A possible way to encourage grounding is to ensure that the image x can be recovered back from the string y. Both CycleGAN [81] and CipherGAN [19] achieve this by learning a second inverse mapping Ψ : Y→Xfrom the target domain back to the input and complete the cycle Ψ(Φ(x))≜x. However, learning a mapping from character strin
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1836465849	s four blocks, each consisting of two convolution layers, followed by a 2×2max-pooling layer. Each convolutional layer comprises of 32 filters of3×3 dimensions, and is followed by batch-normalisation [30] and leaky-ReLU activation (slope= 0.2) [52]. Since max-pooling in each block downsamples the input by a factor of two, final output dimensions are2×n ×D (where, D = 32 is the number of features). The
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1922126009,2061802763	s enjoyed continued interest over the years, owing to its many practical applications, such as recognising printed [69, 75] or handwritten [12, 43] documents, or more recently, text in natural images [35, 54, 58]. Consequently, many different and increasingly accurate methods have been developed. Yet, all such methods adopt thesamesupervisedlearning approachthatrequiresexampleimages of text annotated with the
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2153182373	s generalisation ability to inputs of arbitrary length during inference. More recent methods treat the text-recognition problem as one of sequence prediction in an encoder-decoder framework [14, 73]. [72] adopted this framework first, using HOG features with Connectionist Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features with s
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1607307044,2343052201	sets often comprise several million examples [32, 41], this scales poorly. Another, perhaps more pragmatic, approach is to engineer highly-sophisticated synthetic data generators to mimic real images [24, 32, 79]. However, this requires developing new generators for each new textual domain, and could be problematic for special cases such as text in ancient manuscripts. arXiv:1809.08675v1 [cs.CV] 23 Sep 2018 ;
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2294053032,2298368322	t, using HOG features with Connectionist Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features with stronger CNN features, while [44, 67] have adopted the soft-attention [9] based recurrent decoders. Note, all these methods learn from labelled training examples. Unsupervised Text Recognition. Unsupervised methods for text recognition c
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2133564696	Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features with stronger CNN features, while [44, 67] have adopted the soft-attention [9] based recurrent decoders. Note, all these methods learn from labelled training examples. Unsupervised Text Recognition. Unsupervised methods for text recognition can be classified into two categories
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2127141656	text-recognition problem as one of sequence prediction in an encoder-decoder framework [14, 73]. [72] adopted this framework first, using HOG features with Connectionist Temporal Classification (CTC) [23] to align the predicted characters with the image features. [26, 66] replaced HOG features with stronger CNN features, while [44, 67] have adopted the soft-attention [9] based recurrent decoders. Note
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1903029394	text recognition proposed two-stage solutions corresponding to the two sub-problems [3, 28, 36, 39]. We address the first problem by exploiting the properties of standard fully-convolutional networks [51] — namely locality and translation invariance of the network’s filters. The second problem is equivalent to solving for the correct permutation, or breaking a 1:1–substitution cipher [62]. The latter
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2605287558	cause text recognition requires translating between two very differentmodalities, viz. text and images, which is much harder than translating within the same modality, e.g. between images in CycleGAN [81] where only local textureismodified,orbetweencharacterstringsinCipherGAN [19], where the characters are permuted. Instead of enforcing cycle-consistency, we encourage grounding via the following two k
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2118696714	Text Recognition. Unsupervised methods for text recognition can be classified into two categories. First, category includes generative models for document images. A prime example is the Ocular system [10], which jointly models the text content, as well as the noisy rendering process for historical documents, and infers the parameters through the EM-algorithm [15], aided by an n-gram language model. Th
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2112890726	tion5.3for the reason and further discussion. 17th century 408-pages manuscript, by also first clustering symbols but decipher using the noisy-channel framework of [38] through finite-state-machines. [46] learn mappings from hidden-states of an HMM with their transition probabilities initialised with conditional bi-gram distributions. [40] propose an iterative scheme for bootstrapping predictions for
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	1527085884,2114781615	uch. [27] cluster connected components in binarised document images and assign them to characters by maximising overlap with a fixed lexicon of words based on character frequencies and co-occurrence; [28, 36] also follow the same general approach. [3] break the Borg cipher, a 2 e s i a r n t o l c d u g p m h b y f v k w z x j q 0.00 0.02 0.04 0.06 0.08 0.10 0.12 relative character frequency learning orde
2890140353	Learning to Read by Spelling: Towards Unsupervised Text Recognition.	2001642682	xt in images, is a classic problem in pattern recognition and computer vision that has enjoyed continued interest over the years, owing to its many practical applications, such as recognising printed [69, 75] or handwritten [12, 43] documents, or more recently, text in natural images [35, 54, 58]. Consequently, many different and increasingly accurate methods have been developed. Yet, all such methods ado
2890276793	Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints	1654173042	Some of the earliest work on data-driven chatbots (Ritter et al.,2011) explored the use of phrase-based Statistical Machine Translation (SMT) on large numbers of conversations gathered from Twitter (Ritter et al., 2010). Subsequent progress on the use of neural networks in machine translation inspired the use of Sequence-to-Sequence (Seq2Seq) models for data-driven response generation (Shang et al., 2015;Sordoni et
2890276793	Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints	2574872930	ikelihood of the target given the source, P(YjX), then re-ranks them using a separately trained source given tar6http://nlp.stanford.edu/data/OpenSubData.tar 7OpenNMT is used for training our models (Klein et al., 2017). get model, P(XjY). Combining both directions in this way has the effect of maximizing mutual information (Li et al.,2016a). TA-Seq2Seq: Another relevant baseline is the TASeq2Seq model of Xing et. a
2890276793	Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints	2125320996	as our training set and 10k as a validation set. Due to the noisy nature of the OpenSubtitles conversations we do not use them for evaluation. Instead, we leverage the Cornell Movie Dialogue Corpus (Danescu-Niculescu-Mizil and Lee, 2011) which is much smaller but contains accurate speaker annotations. We extracted all two turn conversations (source target pair) from this corpus and removed those with less than three and more than 25
2890347893	Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation	2102258316	ctions. These methods aim to automatically discover translation rules from a corpus of data, and often leverage the fact that navigation directions are composed of sequential commands. For instance, (Wong and Mooney, 2006;Matuszek et al.,2010;Chen and Mooney, 2011) used statistical machine translation to map instructions to a formal language deﬁned by a grammar. Likewise, (Kollar et al.,2010;Tellex et al.,2011) mapped
2890347893	Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation	2118781169	discover translation rules from a corpus of data, and often leverage the fact that navigation directions are composed of sequential commands. For instance, (Wong and Mooney, 2006;Matuszek et al.,2010;Chen and Mooney, 2011) used statistical machine translation to map instructions to a formal language deﬁned by a grammar. Likewise, (Kollar et al.,2010;Tellex et al.,2011) mapped commands to spatial description clauses bas
2890347893	Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation	55989445	ol”). In this plan, “R-1”,“C-1”, “C-0”, and “O3” are symbols for locations (nodes) in the graph. We compare the performance of translation approaches based on four metrics: - Exact Match (EM). As in (Shimizu and Haas, 2009), EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0. - F1 score (F1). The harmonic average of the precision and recall over all the test set (Chinchor and Sundheim,1993).
2890446733	Evaluating Semantic Rationality of a Sentence: A Sememe-Word-Matching Neural Network based on HowNet.	1522301498	ly use the word itself as the sememe. We use the same dimension of 128 for word embeddings and sememe embeddings, and they are randomly initialized and can be learned during training. Adam optimizer (Kingma and Ba, 2014) is used to minimize cross entropy loss function. We apply dropout regularization (Srivastava et al.,2014) to avoid overﬁtting and clip the gradients (Pascanu et al.,2013) to the maximum norm of 5.0.
2890482192	Towards Language Agnostic Universal Representations	2574872930	on the available training data. Table shows error of sentiment model. We also compare against encodings learned as a by-product of multi-encoder and decoder neural machine translation as a baseline (Klein et al., 2017). We see that UG representations are useful in situations when there is a lack of data in an speciﬁc language. The language agnostics properties of UG embeddings allows us to do successful zero-shot l
2890549803	Training and Prediction Data Discrepancies: Challenges of Text Classification with Noisy, Historical Data.	2133058591	9) survey the different types of text noise and techniques to handle noisy text. Similarly to Agarwal et al., they also focus on spelling errors, and on errors due to statistical machine translation. Lopresti (2009) studies the effects of OCR errors on NLP tasks, such as tokenization, POS tagging, and summarization. Similarly, Taghva et al. (2000) evaluate the effect of OCR errors on text categorization. They sh
2890549803	Training and Prediction Data Discrepancies: Challenges of Text Classification with Noisy, Historical Data.	2039013656	spelling errors, and on errors due to statistical machine translation. Lopresti (2009) studies the effects of OCR errors on NLP tasks, such as tokenization, POS tagging, and summarization. Similarly, Taghva et al. (2000) evaluate the effect of OCR errors on text categorization. They show that OCR errors have minimum effect on a Naive Bayes classiﬁer. All of the above studies focus on text level noise. In contrast, Fr
2890549803	Training and Prediction Data Discrepancies: Challenges of Text Classification with Noisy, Historical Data.	2099140003	that, to their surprise, even at 40% noise, there is little or no drop in accuracy. However, they do not report results on experiments in which the training data is dirty and the test data is clean. Roy and Subramaniam (2006) describe the generation of domain models for call centers from noisy transcriptions. They note that successful models can be built with noisy ASR transcriptions with high word error rates (40%). Venk
2890603981	Studying the History of the Arabic Language: Language Technology and a Large-Scale Historical Corpus.	2250283392	eme is looked up again in the dictionary. If found, the mapped lemma is returned; otherwise, the morpheme is returned. Part-of-speech tagging.The POS tagger uses the simplied PATB tagset proposed by [18]. It attempts to nd the optimal tag for each morpheme produced by the segmenter, as well as determining the gender (masculine or feminine) and number for nouns and adjectives (singular, dual, or plura
2890708025	Nightmare at test time: How punctuation prevents parsers from generalizing.	2250861254	-based parser proposed in (Kiperwasser and Goldberg, 2016)(KGRAPHS) , the arc-eager MALTPARSER (Nivre et al., 2007), the TURBOPARSER (Ferna´ndez-Gonza´lez and Martins, 2015), and the STANFORD parser (Chen and Manning, 2014). UUPARSER is a neural transition-based dependency parser, while KGRAPHS is a neural graph-based parser. MALTPARSER is a more traditional transition-based parser, and TURBOPARSER is a more traditional
2890708025	Nightmare at test time: How punctuation prevents parsers from generalizing.	2114296159	reative uses of punctuation. Such uses are abundant; see Table 1 for examples from Twitter. Such situations, where highly predictive features are absent or distorted at test time, were referred to in Globerson and Roweis (2006) as nightmare at test time. Human reading is very robust to variation in punctuation No punctuation (1) i have so many questions i dont know where to start Creative punctuation (2) What. The. Fuck. Ev
2890708025	Nightmare at test time: How punctuation prevents parsers from generalizing.	2301095666	sensitive to differences in punctuation. Our dependency parsers We use ﬁve parsers in our experiments: the Uppsala parser (UUPARSER) (de Lhoneux et al., 2017a,b), the graph-based parser proposed in (Kiperwasser and Goldberg, 2016)(KGRAPHS) , the arc-eager MALTPARSER (Nivre et al., 2007), the TURBOPARSER (Ferna´ndez-Gonza´lez and Martins, 2015), and the STANFORD parser (Chen and Manning, 2014). UUPARSER is a neural transition-b
2890709663	Analysis of Risk Factor Domains in Psychosis Patient Health Records.	1032927584,1917989004	Despite prior research indicating that similar classification tasks to ours are more effectively performed by RBF networks (Scheirer et al., 2014; Jain et al., 2014; Bendale and Boult, 2015), we find that a MLP network performs marginally better with significantly less preprocessing (i.
2890709663	Analysis of Risk Factor Domains in Psychosis Patient Health Records.	2095705004	To prevent overfitting to the training data, we utilize a dropout rate (Srivastava et al., 2014) of 0.
2890709663	Analysis of Risk Factor Domains in Psychosis Patient Health Records.	1665214252	Rectified Linear Units, f(x) = max(0, x) (Nair and Hinton, 2010) Adaptive Moment Estimation (Kingma and Ba, 2014) We use the TfidfVectorizer tool included in the scikit-learn machine learning toolkit (Pedregosa et al.
2890709663	Analysis of Risk Factor Domains in Psychosis Patient Health Records.	2101234009	Rectified Linear Units, f(x) = max(0, x) (Nair and Hinton, 2010) Adaptive Moment Estimation (Kingma and Ba, 2014) We use the TfidfVectorizer tool included in the scikit-learn machine learning toolkit (Pedregosa et al., 2011) to generate our TF-IDF vector space models, stemming tokens with the Porter Stemmer tool provided by the NLTK library (Bird et al.
2890835244	On Folding and Twisting (and whatknot): towards a topological view of syntax.	1590964158	’Miley, 1969; Kuroda, 1976; also Arc Pair Grammar; Johnson &amp; Postal, 1980 and its spiritual successor, Metagraph Grammar; Postal, 2010. More orthodox generative analyses are to be found, e.g., in Kracht, 2001; Beim Graben &amp; Gerth, 2012). In these works, operations apply to nodes, creating or deleting edges, in order to establish syntactic dependencies. Let us see a simple example. Assume that we have
2890835244	On Folding and Twisting (and whatknot): towards a topological view of syntax.	1510385841	omsky-normal grammars and the correct generative power that is required to provide adequate structural descriptions for natural language strings has been object of much study (see, e.g., Joshi, 1985; Watumull, 2012; Kornai, 1985 for a healthy variety of perspectives) there is no unanimous agreement with respect to the kind of computational problems to which knot theory belongs: is knot recognition P complete? (
2890861846	Iterative Document Representation Learning Towards Summarization with Polishing	2112077341	xtractive summarization either rely Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002;Radev et al.,2004;Woodsend and Lapata, 2010;Yan et al.,2011a,b,2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata,2016;Nallapati et al.,2016a). Although existing extractive summarization metho
2890867094	TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation	2605887895	del (Zhong et al.,2017;Xu et al.,2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016;Dong and Lapata,2018).Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-speciﬁc context free grammar provided to the model a
2890867094	TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation	2252123671	linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as - calculus (Zettlemoyer and Collins,2005) or the abstract meaning representations (Banarescu et al., 2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic comman
2890867094	TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation	2158396456	parsing, it is common for meaning representations to represent executable programs such as SQL queries (Zhong et al., 2017), robotic commands (Artzi and Zettlemoyer, 2013), smart phone instructions (Quirk et al., 2015), and even general-purpose programming languages like Python (Yin and Neubig,2017;Rabinovich et al.,2017) and Java (Ling et al.,2016). 1Available at https://github.com/pcyin/tranX. An earilier version
2890867094	TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation	2511108736	rammar of MRs in the structure of the model (Zhong et al.,2017;Xu et al.,2017). To alleviate this issue, there have been recent efforts in neural semantic parsing with general-purpose grammar models (Xiao et al., 2016;Dong and Lapata,2018).Yin and Neubig (2017) put forward a neural sequence-to-sequence model that generates tree-structured MRs using a series of tree-construction actions, guided by the task-speciﬁc
2890884631	Coherence-Aware Neural Topic Modeling	2250539671	om/jhlau/ topic_interpretability. that the two WETC formulations behave very similarly. In addition, both WETC PW and WETC C correlate to human judgement almost equally well as NPMI when using GloVe (Pennington et al., 2014) vectors2. With the above observations, we propose the following procedure to construct a WETC-based surrogate topic coherence regularization term: (1) let E2RjVj D be the pre-trained word embedding m
2890884631	Coherence-Aware Neural Topic Modeling	2250539671	Sutton,2017) and incorporate topic coherence awareness into topic modeling. Our approaches of constructing topic coherence training objective leverage pre-trained word embeddings (Mikolov et al.,2013;Pennington et al., 2014;Salle et al.,2016;Joulin et al.,2016). The main motivation is that word embeddings carry contextual similarity information that is highly related to the mutual information terms involved in the calcu
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2224454470	: natural language (NL) sentence; middle: meaning representation (MR); bottom: dependency-based hybrid tree representation. Kwiatkowski et al.,2010;Jones et al.,2012) and neural network based models (Dong and Lapata, 2016;Cheng et al.,2017). Following various previous research efforts (Wong and Mooney,2006;Lu et al.,2008;Jones et al.,2012), in this work, we adopt a popular class of semantic formalism – logical forms t
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2163274265	age sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving signiﬁcant attention from the NLP community. Various systems (Zelle and Mooney, 1996;Kate et al.,2005;Zettlemoyer and Collins, 2005;Liang et al.,2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wo
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2135754437	l.,2008;Kwiatkowski et al.,2010;Jones et al., 2012;Lu,2014;Zou and Lu,2018). Wong and Mooney(2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem.Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called hybridtreewhere both natural language words and semantics
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2147389891	luation methodology We conduct experiments on the publicly available variablefree version of the GeoQuery dataset, which has been widely used for semantic parsing (Wong and Mooney,2006;Lu et al.,2008;Jones et al., 2012). The dataset consists of 880 pairs of natural language sentences and the corresponding treestructured semantic representations. This dataset is annotated with eight languages. The original annotation
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2251939518	neural network will calculate a score for each possible semantic unit, including m. The two words are ﬁrst mapped to word embeddings e pand e c(both of dimension d). Next, we use a bilinear layer10 (Socher et al., 2013;Chen et al.,2016) to capture the interaction between the parent and the child in a dependency: r i= eT pU i c where r irepresents the score for the i-th semantic unit and U i2Rd d. The scores are the
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2135754437	odels on GeoQuery dataset. (yrepresents the system is using lambda-calculus expressions as meaning representations.) standard evaluation procedure used in various previous works (Wong and Mooney,2006;Lu et al., 2008;Jones et al.,2012;Lu,2015) to construct the Prolog query from the tree-structured semantic representation using a standard and publicly available script. The queries are then used to retrieve the ans
2890904301	Dependency-based Hybrid Trees for Semantic Parsing	2147389891	In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney,2006;Kate and Mooney,2006;Lu et al.,2008;Kwiatkowski et al.,2010;Jones et al., 2012;Lu,2014;Zou and Lu,2018). Wong and Mooney(2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem.Lu et al. (2008) proposed a generative process to
2890953671	Multi-view Models for Political Ideology Detection of News Articles	2251617662	bias (Gentzkow and Shapiro,2010;Groseclose and Milyo,2005;Iyyer et al.,2014). Even if one were to argue that such bias may not be reﬂective of a lack of objectivity, prior researchDardis et al.(2008);Card et al. (2015) note that framing of topics can signiﬁcantly inﬂuence policy. Since manual detection of political ideology is challenging at a large scale, there has been extensive work on developing computational m
2890953671	Multi-view Models for Political Ideology Detection of News Articles	2102498033	captures the degree to which a particular newspaper uses partisan terms or co-allocations.Gerrish and Blei(2011) predict the voting patterns of Congress members based on supervised topic models whileAhmed and Xing (2010);Lin et al.(2008) use similar models to predict bias in news articles, blogs, and political speeches (Iyyer et al.,2014). Differing from the above,Sim et al.(2013) propose a novel HMMbased model to in
2890961898	Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text	2604890851	0 80.3 / 72.1 46.7 / 38.6 23.2 / 13.0 GN 96.8 / 97.2 86.6 / 80.8 67.8 / 62.8 25.3 / 15.3 Table 4: Hits@1 / F1 scores compared to SOTA models using only KB or text: MINERVA (Das et al.,2017a), R2-AsV (Watanabe et al., 2017), Neural Symbolic Machines (NSM) (Liang et al.,2017), DrQA (Chen et al.,2017), RGCN (Schlichtkrull et al.,2017) and KV-MemNN (Miller et al.,2016). *DrQA is pretrained on SQuAD. #Re-implemented. hetero
2890961898	Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text	2171278097	Answering (QA) is the task of ﬁnding answers to questions posed in natural language. Historically, this required a specialized pipeline consisting of multiple machinelearned and hand-crafted modules (Ferrucci et al., 2010). Recently, the paradigm has shifted towards training end-to-end deep neural network models for the task (Chen et al.,2017;Liang et al.,2017; Raison et al.,2018;Talmor and Berant,2018; Iyyer et al.,20
2890961898	Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text	102708294	how to effectively combine both types of information. Surprisingly little prior work has looked at this problem. In this paper we focus on a scenario in which a large-scale KB (Bollacker et al.,2008;Auer et al., 2007) and a text corpus are available, but neither is sufﬁcient alone for answering all questions. A na¨ıve option, in such a setting, is to take stateof-the-art QA systems developed for each source, and a
2890964500	Constructing Financial Sentimental Factors in Chinese Market Using Natural Language Processing.	2081580037	WordNet WordNet is a large lexical database of English. In WordNet, synsets are interlinked by means of conceptual-semantic and lexical relations. The main relation between words in WordNet is synonym[3]. By using a network to show the relation between words, WordNet helps us ﬁnd synonym of words and also shows how two much words are similar with each other in the perspective of meanings. 4] Having t
2890970518	A case for deep learning in semantics	1889268436	computing power that together make it possible to optimize even very elaborate models eﬀectively. Socher et al. (2011) pioneered the use of tree-structured neural models for semantic tasks (see also Socher et al. 2012, 2013), building on early work on recursive connectionist architectures (Pollack 1990; Smolensky 1990; Plate 1994; Goller &amp; Ku¨chler 1996) and on compositional distributional semantics (Mitchell
2890970518	A case for deep learning in semantics	2200913422	es (Pollack 1990; Smolensky 1990; Plate 1994; Goller &amp; Ku¨chler 1996) and on compositional distributional semantics (Mitchell &amp; Lapata 2010; Baroni &amp; Zamparelli 2010; for an overview, see Baroni et al. 2014). These proposals are explicitly guided by the principle of compositionality and the usual assumptions and practices of formal semantics. Recurrent neural networks (RNNs; Elman 1990) are closely relat
2890970518	A case for deep learning in semantics	1549334072	On the other hand, DL theories have been much less successful to date in modeling the functional vocabulary that semanticists have mostly specialized in. This is not a representational challenge, as Clark et al. (2011) show with their compositional distributed models of meaning, but rather one arising from the demands of machine learning. For instance, the DL theory of section 2 is essentially monotyped: every mean
2890970518	A case for deep learning in semantics	2251939518	with narrow semantic characterizations employed only where they prove useful for meeting these broader goals. Certain insights and goals might be subverted in the process. To take one simple example, Socher et al. (2013) motivate a tree-structured recursive neural tensor network, which is essentially an elaboration of the model in (2) with a greater capacity to capture the relationships between the two child vectors.
2890970518	A case for deep learning in semantics	71795751	orth). The recent surge in interest in DL derives in large part from advances in machine learning and computing power that together make it possible to optimize even very elaborate models eﬀectively. Socher et al. (2011) pioneered the use of tree-structured neural models for semantic tasks (see also Socher et al. 2012, 2013), building on early work on recursive connectionist architectures (Pollack 1990; Smolensky 199
2890970518	A case for deep learning in semantics	2251939518	he rest of the compositional system, rather than from the perspective of what they actually mean (in the pre-theoretic sense) when speakers actually use them. Just as it might seem idiosyncratic that Socher et al. (2013) model only the argumentative structure of but, these accounts can also seem idiosyncratic in how they attend only to what intensional semantics can easily accommodate. There is a related, subtler diﬀ
2890984451	Picking Apart Story Salads	2342155211	arrative schema learning, a related task. We include variants of NYT and NYTHARD with story salads consisting of event tuple representations instead of natural language sentence representations, as inPichotta and Mooney (2016). We label these variants as NYT-EVENT and NYT-EVENT-HARD. Event tuples are in the form &lt;VERB, SUBJ, DOBJ, PREP, POBJ&gt;, where as many preposition and prepositional object pairs as necessary are
2890984451	Picking Apart Story Salads	2597655663	Bilenko et al.,2004), in particular LSTMs (Hochreiter and Schmidhuber ,1997;Dai and Le 2015), CNNs (Kim ,2014;Conneau et al. 2017), and attention mechanisms (Bahdanau et al.,2015;Hermann et al. ,2015;Lin et al. 2017). By exploring our ability to pick apart story salads with these stateof-the-art NLP modeling tools, we attempt to (i) show the value of the story salad task as a new NLP task that warrants extensive
2890984451	Picking Apart Story Salads	2149029960	clustering. Our baselines work directly on sentence embeddings and therefore ignore the task-speciﬁc supervision available in our labeled training data. Inspired by the work inBilenko et al.(2004) andFinley and Joachim (2005) on supervised clustering, we aim to exploit this supervision using a learned distance metric in our clustering.8 Figure3shows our model, which produces a distribution P(same js 1;s 2;d): the probabil
2890984451	Picking Apart Story Salads	2616922365	ing (Bilenko et al.,2004;Finley and Joachim,2005). We also take advantage of the recent success of deep learning in leveraging a continuous semantic space (Pennington et al., 2014 ;Kiros et al. ,2015 Mekala et al. 2017 Wieting and Gimpel ,2017;Wieting et al. ) for word/sentence/event encoding; neural components for enhanced supervised clustering (Bilenko et al.,2004), in particular LSTMs (Hochreiter and Schmidhuber
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2137871902	anguage Understanding The SLU task is one of more mature research areas in NLP. Many works have focused on exploring neural architectures for the SLU task. Plain RNNs and LSTMs were ﬁrst explored in (Mesnil et al. 2015; Yao et al. 2014). (Kurata et al. 2016b) proposed sequence-to-sequence (Seq2Seq) models. Hybrid models between RNNs and CRFs were explored in (Huang, Xu, and Yu 2015). Joint language understanding mo
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2099471712	ecurrent auto-encoders (VRAE) were ﬁrst proposed by (Fabius and van Amersfoort2014).Generativeadversarial networks (GAN) are another class of latent variable models with implicit latent distribution (Goodfellow et al. 2014). Advances have been made in applying the GAN modeltotextgeneration(Yuetal.2017;Fedus,Goodfellow, and Dai 2018). Recently, much attention has been drawn to the tasks of controllable generation and sty
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2399880602	exacerbate the issue (Torralbaand Efros 2011). Recentyears,therehavebeensigniﬁcantadvancesinvariationalautoencoders(VAE)(KingmaandWelling2013)and other latent variable models for textual generation (Serban et al. 2017; Yu et al. 2017; Hu et al. 2017; Li et al. 2017), prompting investigations into the possibility of improving model performances through generative data augmentation (Kaﬂe, Yousefhussien,and Kanan 201
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2735642330	fros 2011). Recentyears,therehavebeensigniﬁcantadvancesinvariationalautoencoders(VAE)(KingmaandWelling2013)and other latent variable models for textual generation (Serban et al. 2017; Yu et al. 2017; Hu et al. 2017; Li et al. 2017), prompting investigations into the possibility of improving model performances through generative data augmentation (Kaﬂe, Yousefhussien,and Kanan 2017; Kurata, Xiang, and Zhou 2016;
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	1836465849	ization For datahungry models, appropriate regularization is necessary to achieve high performance for many tasks. Model regularizers such as dropout (Srivastava et al. 2014) and batch normalization (Ioffe and Szegedy 2015) are widely accepted techniques to prevent model overﬁtting and promote noise robustness. Transfer learning is another regularization techniquetoenhancethegeneralizationpowerofmodelsthathas achieved s
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2165698076	model overﬁtting and promote noise robustness. Transfer learning is another regularization techniquetoenhancethegeneralizationpowerofmodelsthathas achieved success across numerous domains and tasks (Pan and Yang 2010). Data augmentation (DA) is a separate class of regularization methods that create artiﬁcial training data to obtain better resulting models. Most DA techniques proposed in the literature can be categ
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2031342017	n space for x. However, in real world cases, the actual distribution representedbytheDwcouldbedistortedduetobiasesintroduced during erroneous data collection process or due to undersampling variance (Torralba and Efros 2011). Let such distortion be a function ωb ∈ Ω : P → P. The distorted data distribution p⋆= ω b(p) divergesfromthetruedistribution , i.e.d(p⋆,p) &gt;0 wheredis somestatistical distancemeasure such as KL-d
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2581637843	ntyears,therehavebeensigniﬁcantadvancesinvariationalautoencoders(VAE)(KingmaandWelling2013)and other latent variable models for textual generation (Serban et al. 2017; Yu et al. 2017; Hu et al. 2017; Li et al. 2017), prompting investigations into the possibility of improving model performances through generative data augmentation (Kaﬂe, Yousefhussien,and Kanan 2017; Kurata, Xiang, and Zhou 2016;Hou et al. 2018).
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2612953412	he rest of the network. The word embeddings had been initialized with the GloVe vectors (Pennington,Socher, and Manning2014). For the generative model, the encoder is a single-layer BiLSTM-Max model (Conneau et al. 2017), which encodes the word embeddings of word tokens wi ∈ w in both directions and produces the ﬁnal hidden state by applying max-pooling-over-time on combined encoder hidden outputs h(e) 1 ,..., T (102
2891051852	Data Augmentation for Spoken Language Understanding via Joint Variational Generation	2735642330	xtgeneration(Yuetal.2017;Fedus,Goodfellow, and Dai 2018). Recently, much attention has been drawn to the tasks of controllable generation and style transfer, which have been successfully explored in (Hu et al. 2017; Shen et al. 2017)using variational models. Data Augmentation and Regularization For datahungry models, appropriate regularization is necessary to achieve high performance for many tasks. Model regul
2891308403	Transforming Question Answering Datasets Into Natural Language Inference Datasets.	2304113845	me encoder weights are used for both inputs). D is then generated using a three-layer LSTM decoder equipped with one attention head (Bahdanau et al.,2015) for each input, and a copy mechanism based onGu et al. (2016).6 Word embeddings are initialized with 5The random split between train (80%), dev (10%) and test (10%) sets was made based on Wikipedia article titles corresponding to the QA pairs. 6The copy mechani
2891308403	Transforming Question Answering Datasets Into Natural Language Inference Datasets.	2125436846	mplied by P, yielding a negative NLI pair. Incorrect answers are available in QA datasets featuring multiple choice answers, such as MovieQA (Tapaswi et al.,2016), RACE (Lai et al.,2017b) and MCTest (Richardson et al., 2013). Unanswerable questions are available in SQuADRUn (Rajpurkar et al.,2018) and we expect the number of such datasets to grow with the advancement of QA research. Inference labels. In existing NLI data
2891308403	Transforming Question Answering Datasets Into Natural Language Inference Datasets.	2143927888	nces: declarative sentences are transformed into other declarative sentences. This is the same type signature as for sentence simpliﬁcation (Chandrasekar et al.,1996), paraphrase (Lin and Pantel,2001;Bannard and Callison-Burch, 2005) and summarization (Jones,1993), highlighting the close connection between these tasks. Importantly, declarative sentences are closed under this set of operations, allowing them to be chained together
2891308403	Transforming Question Answering Datasets Into Natural Language Inference Datasets.	2557764419	t al.,2016)) or even a single sentence (e.g. QAMR (Michael et al.,2018)). This yields a short, simple premise in the resulting NLI example. However, some weakly supervised QA datasets such as NewsQA (Trischler et al., 2017), RACE and TriviaQA (Joshi et al.,2017) choose P be an entire document or even corpus of documents. In this case, the resulting NLI pair’s premise could be large, but is still valid. In Table1, we des
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	1947758080	aging recurrent structures [12, 13]. Previous work proposed an RNNLMbased NLG that can be trained on any corpus of dialogue actutterance pairs without hand-crafted features and any semantic alignment [14]. The following work based on sequenceto-sequence (seq2seq) further obtained better performance by employing encoder-decoder structure with linguistic knowledge such as syntax trees [15, 16, 17, 18].
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	1948566616	e next step given the current dialogue state. Finally the semantic frame of the system action is then fed into a natural language generation (NLG) module to construct a response utterance to the user [8, 9]. As a key component to a dialogue system, the goal of NLG is to generate natural language sentences given the semantics provided by the dialogue manager to feedback to users. As the endpoint of inter
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	648786980	ext time step, rather than the output generated by the network. The teacher forcing techniques can also be triggered only with a certain probability, which is known as the scheduled sampling approach [20]. We adopt scheduled sampling methods in our experiments. In the proposed framework, an input of a decoder contains not only the output from the last step but one from the last decoding layer. Therefo
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	2130942839	The framework of the proposed hierarchical NLG model is illustrated in Figure 1, where the model architecture is based on an encoder-decoder (seq2seq) structure with attentional hierarchical decoders [15, 16]. In the encoder-decoder architecture, a typical generation process includes encoding and decoding phases: First, a given semantic representation sequence x = fw tgT 1 is fed into a RNN-based encoder
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	2761412636,2762142222	nderstanding module (NLU) to 1The source code is available at https://github.com/MiuLab/ HNLG. classify the domain along with domain-speciﬁc intents and ﬁll in a set of slots to form a semantic frame [5, 6, 7]. A dialogue state tracking (DST) module predicts the current state of the dialogue by means of the semantic frames extracted from multi-turn conversations. Then the dialogue policy determines the sys
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	2016589492	prove the performance, several strategies including scheduled sampling, a repeat input mechanism, curriculum learning, and an attention mechanism are utilized. 2.2. Scheduled Sampling Teacher forcing [19] is a strategy for training RNN that uses model output from a prior time step as an input, and it works by using the expected output at the current time step ^y t as the input at the next time step, r
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	1947758080	r, and other related linguistic knowledge at the same time. Some prior work applied additional techniques such as reranker and beam-search to select a better result among multiple generated sequences [14, 17]. However, it is still an unsolved issue to the NLG community. Therefore, we propose a hierarchical decoder to address the above issue, where the core idea is to allow the decoding layers to focus on
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	2403702038,2513380446	stic patterns 1. INTRODUCTION Spoken dialogue systems that can help users to solve complex tasks have become an emerging research topic in artiﬁcial intelligence and natural language processing areas [1, 2, 3, 4]. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today, there are several virtual intelli
2891339902	Investigating Linguistic Pattern Ordering In Hierarchical Natural Language Generation	2130942839	tic alignment [14]. The following work based on sequenceto-sequence (seq2seq) further obtained better performance by employing encoder-decoder structure with linguistic knowledge such as syntax trees [15, 16, 17, 18]. However, due to grammar complexity and lack of diction knowledge, it is still challenging to generate long and complex sentences by a simple encoder-decoder structure. To address the issue, previous
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2062164080	ance in speech recognition: there are concrete applications ranging from mobile voice search [3], transcriptions of broadcast news, videos [4] or conversations [5] to recognition in noisy environments[6, 7, 8]. The availability of large training corpora for a given application domain allows to train a DNN with many layers and parameters in order to improve the classiﬁcation performance. On the contrary, in
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	1993882792	anguage. Index Terms— Transfer learning, Multi-task learning, non-nativespeech recognition,children’s speech 1. INTRODUCTION Nowadays the usage of deep neural networks hidden Markov models (DNN-HMMs) [1, 2] provides effective performance in speech recognition: there are concrete applications ranging from mobile voice search [3], transcriptions of broadcast news, videos [4] or conversations [5] to recogn
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2056914870	d project PF-Star (2002-2004). During the PFStar project, noticeable amountof speech data were collected fromEnglish, German, Italian and Swedish children[28]; for one of the Italian corpora see also [29]. For the purposes of this work, children’s speech pronouncedby English, German and Italian students in the three languages were considered, as shown in Table 1. As already mentioned, data from native
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2147768505	en Markov models (DNN-HMMs) [1, 2] provides effective performance in speech recognition: there are concrete applications ranging from mobile voice search [3], transcriptions of broadcast news, videos [4] or conversations [5] to recognition in noisy environments[6, 7, 8]. The availability of large training corpora for a given application domain allows to train a DNN with many layers and parameters in
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	1994606281	ning data is represented by multi-task learning. This approach has been demonstrated effective for multi-lingual speech recognition, especially if the size of training data for each language is small [11, 12, 13, 14, 15]. The reasonofthis is dueto the fact that the sharedhiddenlayers of the DNN used to estimate the emission probabilities of HMM states in a hybrid Automatic SpeechRecognition(ASR)system[1],arelanguagei
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2087006792	owadays the usage of deep neural networks hidden Markov models (DNN-HMMs) [1, 2] provides effective performance in speech recognition: there are concrete applications ranging from mobile voice search [3], transcriptions of broadcast news, videos [4] or conversations [5] to recognition in noisy environments[6, 7, 8]. The availability of large training corpora for a given application domain allows to t
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2169883442	of phones. In the past several approaches have been proposed to take into account the pronunciation errors of non-native speakers [17, 18], spanning from the usage of non-native pronunciation lexicon [19, 20, 21, 22, 23] to acoustic model adaptation using either native data and non native data [24, 25, 26, 27]. As previously mentioned, we use transfer learning to adapt the multi-lingual DNN trained on native data fro
2891363679	Non-Native Children Speech Recognition Through Transfer Learning	2399016933	tion errors of non-native speakers [17, 18], spanning from the usage of non-native pronunciation lexicon [19, 20, 21, 22, 23] to acoustic model adaptation using either native data and non native data [24, 25, 26, 27]. As previously mentioned, we use transfer learning to adapt the multi-lingual DNN trained on native data from Italian,GermanandEnglishchildren. Basically,onlytheweights of the output layer of the net
2891424260	Appendix - Recommended Statistical Significance Tests for NLP Tasks.	2123301721	abeled attachment score [7] t-test bootstrap/ permutation Dependency parsing 1, 2, 4 ROUGE [9] — bootstrap/ permutation Summarization 2 BLEU [11] — bootstrap/ permutation Machine translation 2 METEOR [2] — bootstrap/ permutation Machine translation 2 PINC [3] — bootstrap/ permutation Paraphrasing 2 2https://en.wikipedia.org/wiki/Confusion_matrix 3https://en.wikipedia.org/wiki/Accuracy_and_precision 4
2891424260	Appendix - Recommended Statistical Significance Tests for NLP Tasks.	2101105183	ermutation Dependency parsing 1, 2, 4 LAS (sentencelevel) Labeled attachment score [7] t-test bootstrap/ permutation Dependency parsing 1, 2, 4 ROUGE [9] — bootstrap/ permutation Summarization 2 BLEU [11] — bootstrap/ permutation Machine translation 2 METEOR [2] — bootstrap/ permutation Machine translation 2 PINC [3] — bootstrap/ permutation Paraphrasing 2 2https://en.wikipedia.org/wiki/Confusion_matr
2891424260	Appendix - Recommended Statistical Significance Tests for NLP Tasks.	2096335387,2098345921	iki/Spearman%27s_rank_correlation_coefficient 8https://en.wikipedia.org/wiki/Pearson_correlation_coefficient 2 CIDEr [12] — bootstrap/ permutation Image description generation 2 MUC, B3, CEAFe, BLANC [13, 1, 10, 8] — bootstrap/ permutation Coreference resolution 2, 7 Krippendorf’s alpha, Cohen’s kappa Statistical measures of agreement [6, 4] — bootstrap/ permutation Annotation reliability 2 MRR Mean reciprocal
2891424260	Appendix - Recommended Statistical Significance Tests for NLP Tasks.	2164290393	n Dependency parsing 1, 2, 4 ROUGE [9] — bootstrap/ permutation Summarization 2 BLEU [11] — bootstrap/ permutation Machine translation 2 METEOR [2] — bootstrap/ permutation Machine translation 2 PINC [3] — bootstrap/ permutation Paraphrasing 2 2https://en.wikipedia.org/wiki/Confusion_matrix 3https://en.wikipedia.org/wiki/Accuracy_and_precision 4https://en.wikipedia.org/wiki/Precision_and_recall 5http
2891424260	Appendix - Recommended Statistical Significance Tests for NLP Tasks.	1956340063	on_and_recall 6https://en.wikipedia.org/wiki/Perplexity 7https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient 8https://en.wikipedia.org/wiki/Pearson_correlation_coefficient 2 CIDEr [12] — bootstrap/ permutation Image description generation 2 MUC, B3, CEAFe, BLANC [13, 1, 10, 8] — bootstrap/ permutation Coreference resolution 2, 7 Krippendorf’s alpha, Cohen’s kappa Statistical measur
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2609776793	, 2017), and argumentation mining (Eger et al., 2017; Schulz et al., 2018).
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2612953412	, 2018), of dimensionality 600, and InferSent (Conneau et al., 2017), of dimensionality 4096.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2609776793	• (6): The AM dataset with original split in train, dev, and test (Eger et al., 2017), and with InferSent input embeddings.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2609776793	In both cases, dev and test follow the original train splits (Eger et al., 2017).
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2609414830	Classical sequence tagging tasks include POS tagging, chunking, NER, discourse parsing (Braud et al., 2017), and argumentation mining (Eger et al.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2250861254	cube is the function f(x) = x3, proposed in Chen and Manning (2014) for an MLP used in dependency parsing.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2117539524	However, as with previous works, they also only evaluated their newly discovered as well as their (rectifier) baseline activation functions on few different datasets, usually taken from the image classification community such as CIFAR (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015), and using few types of different networks, such as the deep convolutional networks abounding in the image classification community (Szegedy et al.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2156387975	In this respect, the so-called ReLU function (Glorot et al., 2011) has proven much more suitable.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2740215900	For our RNN implementations, we use the accompanying code of (the state-of-the-art model of) Reimers and Gurevych (2017), which is implemented in keras.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2343649478	SentEval the AM dataset for task diversity, and derive it by projecting token-level annotations in the dataset from Stab and Gurevych (2017) to the sentence level.
2891455536	Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks	2609776793	As shown in Goodfellow et al. (2013), maxout can approximate any convex function.
2891501508	Extending Neural Generative Conversational Model using External Knowledge Sources	2130942839	al.,1988) to compute weight updates using the derivative of a loss function with respect to the parameters over all previous time-steps. 3.2 Seq2Seq Dialogue Architecture Generative dialogue models (Sutskever et al., 2014;Serban et al.,2015) extends the language model learned by RNNs to generate natural language that are conditioned not only on the previous words generated in the response but also on a representation
2891501508	Extending Neural Generative Conversational Model using External Knowledge Sources	1512387364	ns respectively, and ec iand ec^ represent true and model(f) predicted external knowledge encoding. 4.2 External Context Vector We use Wikipedia summary (Scheepers,2017) and NELL knowledge base (KB) (Carlson et al., 2010) to compute the external knowledge encoding for every context in the context-response pairs. Algorithm1oultines the pseudocode for computing the external context vector (eci). For ith input context, t
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	630532510	-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 (Pierre and Jörg, 2016) and 2018 corpora and LDC2010T21, LDC2010T23 LDC corpora, and obtained a total of 64k parallel sentences. For Swahili, we were only able to gather 42k s
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	21006490	(x c;y c) is a contrastive term (i.e. negative sampling), controls the weight of the negative examples in the loss. For the distance measure, we use the L2 norm dist(x;y) = kx yk 2. A ranking loss (Weston et al., 2011) of the form L rank (x;y) = max(0; dist c) + )) + max(0; dist(x c;y) + dist(x;y)) that pushes the sentence embeddings of a translation pair to be closer than the ones of negative pairs leads to very p
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2612953412	2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence encoder for multiple languages using publicly available parallel corpora (John
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2607106700	37.5 24.6 24.1 Word translation P@1 73.7 73.9 65.9 61.1 61.9 60.6 55.0 51.9 35.8 25.4 48.6 48.2 - - Table 3: BLEU scores of our translation models (XX-En) P@1 for multilingual word embeddings. 2016; Schwenk et al., 2017; España-Bonet et al., 2017), the lack of a large-scale, sentence-level semantic evaluation has limited their adoption by the community. In particular, these methods do not cover the scale of language
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1522301498	500,000 frequent words in the dictionary, which generally covers more than 98% of the words found in XNLI corpora. We set the number of hidden units of the BiLSTMs to 512, and use the Adam optimizer (Kingma and Ba, 2014) with default parameters. As in (Conneau et al., 2017), the classiﬁer receives a vector [u;v;ju vj;u v], where uand vare the embeddings of the premise and hypothesis provided by the shared encoder, an
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2130942839	aligning sentence embedding spaces from multiple languages which is present below. We consider two ways of extracting feature vectors from the BiLSTM: either using the initial and ﬁnal hidden states (Sutskever et al., 2014), or using the element-wise max over all states (Collobert and Weston, 2008). The ﬁrst approach is commonly used as a strong baseline for monolingual sentence embeddings (Arora et al., 2017; Conneau a
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2612953412	ally covers more than 98% of the words found in XNLI corpora. We set the number of hidden units of the BiLSTMs to 512, and use the Adam optimizer (Kingma and Ba, 2014) with default parameters. As in (Conneau et al., 2017), the classiﬁer receives a vector [u;v;ju vj;u v], where uand vare the embeddings of the premise and hypothesis provided by the shared encoder, and corresponds to the element-wise multiplication (see
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2153579005	BOW). Although naïve, this method often provides a strong baseline. More sophisticated approaches—such as the unsupervised SkipThought model of Kiros et al. (2015) that extends the skip-gram model of Mikolov et al. (2013b) to the sentence level—have been proposed to capture syntactic and semantic dependencies inside sentence representations. While these ﬁxed-size sentence embedding methods have been outperformed by t
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	630532510	Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel se
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2612953412	we consider our multilingual sentence embeddings as being pretrained and only learn a classiﬁer on top of them to evaluate their quality, similar to socalled “transfer” tasks in (Kiros et al., 2015; Conneau et al., 2017) but in the multilingual setting. 4.2.3 Aligning Sentence Embeddings Training for similarity of source and target sentences in an embedding space is conceptually and computationally simpler than gener
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2294774419	eddings that appear in the parallel dictionary, O d(R) is the group of orthogonal matrices of dimension d, and Uand V are obtained from the singular value decomposition (SVD) of YXT: U VT = SVD(YXT). Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task. In this paper, we pretrain our embeddings using the common-crawl word embe
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2274912527	er that use case in this work. This translation approach carries with it the risk that the semantic relations between the two sentences in each pair might not be reliably preserved in translation, as Mohammad et al. (2016) observed for sentiment. We investigate this potential issue in our corpus and ﬁnd that, while it does occur, it only concerns a negligible number of sentences (see Section 3.2). 3.1 Data Collection T
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2131744502	hatsoever (Artetxe et al., 2017; Conneau et al., 2018b). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2103362845	ity in four languages. There have also been efforts to build multilingual RTE datasets, either through translating English data (Mehdad et al., 2011), or annotating sentences from a parallel corpora (Negri et al., 2011). More recently, Agi´c and Schluter (2018) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Am
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2171082019	r knowledge from one language to another. For instance, Zhang et al. (2016) show that cross-lingual embeddings can be used to extend an English part-of-speech tagger to the cross-lingual setting, and Xiao and Guo (2014) achieve similar results in dependency parsing. Cross-lingual embeddings also provide an efﬁcient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2142074148	between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1828724394	lement-wise max over all states (Collobert and Weston, 2008). The ﬁrst approach is commonly used as a strong baseline for monolingual sentence embeddings (Arora et al., 2017; Conneau and Kiela, 2018; Gouews et al., 2014). Concretely, we consider the English fastText word embedding space as being ﬁxed, and ﬁne-tune embeddings in other languages so that the average of the word vectors in a sentence is close to the aver
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1840435438	maximum over all hidden states (BiLSTMmax) compared to taking the last hidden state (BiLSTM-last). Unsuprisingly, BiLSTM results are better than the pretrained CBOW approach for all languages. As in Bowman et al. (2015), we also observe the superiority of BiLSTM encoders over CBOW, even when ﬁne-tuning the word embeddings of the latter on the MultiNLI training set, thereby again conﬁrming that the NLI task requires
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2144945507	Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al.,
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2153579005	n the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al.
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2612953412	network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018; Conneau et al., 2018a). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending the
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2607106700	ome effort on developing multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a composi
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2612953412	been proposed to capture syntactic and semantic dependencies inside sentence representations. While these ﬁxed-size sentence embedding methods have been outperformed by their supervised counterparts (Conneau et al., 2017; Subramanian et al., 2018), some recent developments have shown that pretrained language models can also transfer very well, either when the hidden states of the model are used as contextualized word
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2274912527	rks, XNLI is the ﬁrst large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their English translations, and conclude that most of them come from cultural differences. Similarly, Smith et al. (2016
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1486649854	SE library of Conneau et al. (2018b). 4.2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence encoder for multiple languages using
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1486649854	space. In that case, we consider our multilingual sentence embeddings as being pretrained and only learn a classiﬁer on top of them to evaluate their quality, similar to socalled “transfer” tasks in (Kiros et al., 2015; Conneau et al., 2017) but in the multilingual setting. 4.2.3 Aligning Sentence Embeddings Training for similarity of source and target sentences in an embedding space is conceptually and computation
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1840435438	tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018;
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	1486649854	tions, usually referred to as continuous bag-of-words (CBOW). Although naïve, this method often provides a strong baseline. More sophisticated approaches—such as the unsupervised SkipThought model of Kiros et al. (2015) that extends the skip-gram model of Mikolov et al. (2013b) to the sentence level—have been proposed to capture syntactic and semantic dependencies inside sentence representations. While these ﬁxed-si
2891555348	XNLI: Evaluating Cross-lingual Sentence Representations.	2175723921	txe et al., 2017; Conneau et al., 2018b). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-word
2891581340	AD3: Attentive Deep Document Dater	2151718942	, 2008), temporal scoping of events and facts (Allan et al., 1998; Talukdar et al., 2012b), document summarization (Wan, 2007) and analysis (de Jong et al.
2891581340	AD3: Attentive Deep Document Dater	2074276750	, 2012b), document summarization (Wan, 2007) and analysis (de Jong et al.
2891581340	AD3: Attentive Deep Document Dater	2108922662	(2014) propose a termburstiness (Lappas et al., 2009) based statistical method for the task.
2891581340	AD3: Attentive Deep Document Dater	2108922662	, 2014): This is a purely statistical method which uses lexical similarity and term burstiness (Lappas et al., 2009) for dating documents in arbitrary length time frame.
2891581340	AD3: Attentive Deep Document Dater	2133564696	, 2016a), and machine translation (Bahdanau et al., 2014) have motivated us to use attention during document dating.
2891581340	AD3: Attentive Deep Document Dater	2133564696,2626778328	, 2016a), machine translation (Bahdanau et al., 2014; Vaswani et al., 2017).
2891581340	AD3: Attentive Deep Document Dater	2171810632	, 2016b), question answering (Yang et al., 2016a), machine translation (Bahdanau et al.
2891581340	AD3: Attentive Deep Document Dater	2470673105	Attention Network: Attention networks have been well exploited for various tasks such as document classification (Yang et al., 2016b), question answering (Yang et al.
2891581340	AD3: Attentive Deep Document Dater	2070633821	made for document time-stamping task include statistical language models proposed by de Jong et al. (2005b) and Kanhabua and Nørvåg (2008).
2891581340	AD3: Attentive Deep Document Dater	1812967757	Most of the documents obtained from the Web either contain DCT that cannot be trusted or contain no DCT information at all (Kanhabua and Nørvåg, 2008).
2891581340	AD3: Attentive Deep Document Dater	2140244223,2157275230	Event Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010).
2891581340	AD3: Attentive Deep Document Dater	1812967757	A few generative approaches (de Jong et al., 2005b; Kanhabua and Nørvåg, 2008) as well as a discriminative model (Chambers, 2012) have been previously proposed for this task.
2891581340	AD3: Attentive Deep Document Dater	2019054776	A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a).
2891581340	AD3: Attentive Deep Document Dater	2133564696,2171810632	Motivated by the effectiveness of attention based models in different NLP tasks (Yang et al., 2016a; Bahdanau et al., 2014), we incorporate attention in our method in a principled fashion.
2891581340	AD3: Attentive Deep Document Dater	2470673105	Recent success of attention-based deep learning models for classification (Yang et al., 2016b), question answering (Yang et al.
2891581340	AD3: Attentive Deep Document Dater	2123442489	The syntactic dependency structure is extracted by Stanford CoreNLP’s dependency parser (Manning et al., 2014).
2891581340	AD3: Attentive Deep Document Dater	1993972354,2070633821	Tasks such as information retrieval (Li and Croft, 2003; Dakka et al., 2008), temporal scoping of events and facts (Allan et al.
2891620272	Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System.	1601924930	.60 for modiﬁed questions and 0.61 for original questions. 3 Experiments Attempting to understand the behavior of the system we take inspiration from works focusing on visualizing deep net internals (Li et al., 2015; Karpathy et al., 2015). We apply measures speciﬁc to the mechanisms present in our tested system: questionself-attentionandhiddenlayersoftheRNN. We run experiments on SQuAD development set. Question
2891620272	Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System.	2738015883	rsarial examples in NLP, combined with investigating deep net structure. 1 Introduction Recent advances in interpretability for NLP focus on the problem of adversarial examples (Ribeiro et al., 2018)(Jia and Liang, 2017) which lead systems to mistakenly change output. In case of QA systems, either questionsor contextsare modiﬁed, and it is shown that seemingly small changes in semantics ﬂip system decisions. In this
2891620272	Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System.	2427527485	tput in 9.5% of cases, with little inﬂuence on decision certainty. We then proceed to explain this phenomenon by observing the behavior of deep net architecture andthecharacteristicsoftheSQuADdataset(Rajpurkar et al., 2016) itself. AsabasisofourresearchweusetheQAsystem described in Chen et al. (2017). We pick this mo*Both authors contributed equally. del for its good performance and state-of-the-art approach. 2 Negating
2891620272	Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System.	1951216520	uestions and 0.61 for original questions. 3 Experiments Attempting to understand the behavior of the system we take inspiration from works focusing on visualizing deep net internals (Li et al., 2015; Karpathy et al., 2015). We apply measures speciﬁc to the mechanisms present in our tested system: questionself-attentionandhiddenlayersoftheRNN. We run experiments on SQuAD development set. Question self-attention As descr
2891732163	MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling	2251235149	famous example of this kind is the Let’s Go Bus Information System which offers live bus schedule information over the phone (Raux et al.,2005) leading to the ﬁrst Dialogue State Tracking Challenge (Williams et al., 2013). Taking the idea of the Let’s Go system forward, the second and third DSTCs (Henderson et al.,2014b,c) have produced bootstrapped human-machine datasets for a restaurant search domain in the Cambridg
2891732163	MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling	1654173042	mantic labels (Hemphill et al. ,1990 ;Williams et al. 2013 Asri et al. 2017 Wen et al. ,2017;Eric et al. Shah et al. 2018); and corpora without semantic labels but with an implicit user goal in mind (Ritter et al., 2010;Lowe et al.,2015). Despite these efforts, aforementioned datasets are usually constrained in one or more dimensions such as missing proper annotations, only available in a limited capacity, arXiv:181
2891732163	MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling	836999996	rge-scale dialogue corpora have been released in the past, such as the Twitter (Ritter et al.,2010) dataset, the Reddit conversations (Schrading et al.,2015), and the Ubuntu technical support corpus (Lowe et al., 2015). Although previous work (Vinyals and Le, 2015) has shown that a large learning system can learn to generate interesting responses from these corpora, the lack of grounding conversations onto an exist
2891732163	MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling	2251235149	sed on whether a structured annotation scheme is used to label the semantics, these corpora can be roughly divided into two categories: corpora with structured semantic labels (Hemphill et al. ,1990 ;Williams et al. 2013 Asri et al. 2017 Wen et al. ,2017;Eric et al. Shah et al. 2018); and corpora without semantic labels but with an implicit user goal in mind (Ritter et al., 2010;Lowe et al.,2015). Despite these effor
2892024358	Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs.	2164210961	cing stylistic variation. 2 Related Work The restaurant domain has always been the domain of choice for NLG tasks in dialogue systems (Stent et al., 2004; Gasˇic´ et al., 2008; Mairesse et al., 2010; Howcroft et al., 2013), as it offers a good combination of structured information availability, expression complexity, and ease of incorporation into conversation. Hence, even the more recent neural models for NLG continue
2892024358	Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs.	1570821890	efforts to collect training data for NLGwith emphasisonstylistic variation(Nayak et al.,2017; Novikova et al., 2017a; Oraby et al., 2017). While there is previous work on stylistic variation in NLG (Paiva and Evans, 2004; Mairesse and Walker, 2007), this work did not use crowd-sourced utterances for training. More recent work in neural NLG that explores stylistic control has not needed to control semantic correctness
2892024358	Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs.	2735574368	in neural NLG that explores stylistic control has not needed to control semantic correctness, or examined the interaction between semantic correctness and stylistic variation (Sennrich et al., 2016; Ficler and Goldberg, 2017). AlsorelatedistheworkofNiu and Carpuat (2017) that analyzes how dense word embeddings capture style variations, Kabbara and Cheung (2016) who explore the ability of neural NLG systems to transfer sty
2892024358	Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs.	2157331557	y needs to indicate slots to emphasize along with the generated MR whenever applicable. 6 Evaluation 6.1 Experimental Setup For our sequence-to-sequence NLG model we use the standard encoder-decoder (Cho et al., 2014) architecture equipped with an attention mechanism as deﬁned in Bahdanau et al. (2015). The samples aredelexicalized before being fedintothe model as input, so as to enhance the ability of the model t
2892073803	Unsupervised Controllable Text Formalization	2157163421	2018), formal-informal text classification using linguistic feature (Sheika and Inkpen 2012), politeness analysis (Danescu-Niculescu-Mizil et al. 2013), polite-conversation generation (Niu and Bansal 2018) using encoder-decoder models, but these do not perform controllable text-transformation. Similarly other relevant generation frameworks for formal-text generation by Sheikha and Inkpen (2011) and paraphrase generation (Wubben, Van Den Bosch, and Krahmer 2010; Prakash et al.
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2570431255	. [98] 2016 DAT text DANN ﬁrst work in domain adversarial training Chen et al. [12] 2017 DAT text DANN semi-supervised supported; Wasserstein distance used for smoothing training process Zhang et al. [95] 2017 DAT text DANN attention scoring network is added for document embedding Li et al. [99] 2017 DAT text DANN with attention mechanisms Zhao et al. [101] 2018 DAT text multisource DANN extended for
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2613984776	&amp; Cardie [103] 2018 DAT text multinomial ASPD multinomial discriminator for multi-domain Mohammed &amp; Busso [55] 2018 DAT speech DANN adapted for speech emotion recognition Chang &amp; Scherer [105] 2017 AT speech DCGAN spectrograms with ﬁxed width are randomly selected and chopped from a varied length of audio ﬁles Deng et al. [54] 2017 AT speech GAN use hidden-layer representations from discri
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2173520492	-GAN [70]. Network-type-based: in addition, several GAN variants have been named after the network topology used in the GAN conﬁguration, such as the DCGAN based on deep convolutional neural networks [19], the AEGAN based on autoencoders [71], the C-RNN-GAN based on continuous recurrent neural networks [72], the AttnGAN based on attention mechanisms [73], and the CapsuleGAN based on capsule networks [
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2412320034	-GAN) exploits the labels of real data to guide the learning procedure [65]. Other GAN variants in this 1https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv category include the BiGAN [66], the CycleGAN [67], the DiscoGAN [68], the InfoGAN [69], and the Triple-GAN [70]. Network-type-based: in addition, several GAN variants have been named after the network topology used in the GAN conﬁ
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2554314924	actor in the diversity of generated samples in batches [110]. Alternatively, the unroll-GAN allows the generator to ‘unroll’ the updates of the discriminator in a manner which is fully differentiable [111], and the AdaGAN combines an ensemble of GANs in a boosting framework to ensure diversity [112]. B. Other Ongoing Breakthroughs In most conditional GAN frameworks, the emotional entity is generated by
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2608338293	allows the model to Fig. 5: Framework of VoiceGAN. Source: [15] perform the facial expression transformations appropriately even for unseen facial expression characteristics. Another related work is [91], in which the authors focused on voice conversion in natural speech and proposed a variational autoencoding WGAN. Note that, data utilised in [91] are not frame aligned, but still are in pairs. Emoti
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2432004435	ches dedicated to solving the mode collapse problem are continually emerging. For example, the loss function of the generator can be modiﬁed to factor in the diversity of generated samples in batches [110]. Alternatively, the unroll-GAN allows the generator to ‘unroll’ the updates of the discriminator in a manner which is fully differentiable [111], and the AdaGAN combines an ensemble of GANs in a boos
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2173520492	d virtual faces reﬂect appropriate emotional reactions to a person’s behaviours. B. Other GAN-based Approaches in Image/Video In addition to the cGAN-based framework, other GAN variants such as DCGAN [19] and InfoGAN [69] have been investigated for emotional face synthesis. In [19], it is shown that, vector arithmetic operations in the input latent space can yield semantic changes to the image generat
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2748059655	data augmentation frameworks have been proposed in the literature, which aim to supplement the data manifold to approximate the true distribution [96]. For speech emotion recognition, researchers in [97] implemented an adversarial autoencoder model. In this work, highdimensional feature vectors of real data are encoded into 2- D dimensional representations, and a discriminator is learnt to distinguis
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2594698675	have been designed for a given task, thus serve their own speciﬁc interests. Examples, to name just a few, include the Sketch-GAN proposed for sketch retrieval [75], the ArtGAN for artwork synthesis [76], the SEGAN for speech enhancement [77], the WaveGAN for raw audio synthesis [78], and the VoiceGAN for voice impersonation [15]. IV. EMOTION SYNTHESIS As discussed in Section II-A, the most promising
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1945616565	e the examples that are created by making small, but intentionally, perturbations to the input to incur large and signiﬁcant perturbations in outputs (e.g., incorrect predictions with high conﬁdence) [104]. Adversarial training, however, addresses this vulnerability in recognition models by introducing mechanisms to correctly handle the adversarial examples. In this way, it improves not only robustness
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2748059655	ecognition performance of the rare class improve, the average performance over all classes also increased [96]. One ongoing research issue relating to GANs is how best to label the generated data. In [97], they adopted a Gaussian mixture model which is built on the original data, whereas the authors in [96] took a set of class-speciﬁc GANs to generate images, respectively, which requires no additional
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2613984776	emanded. Note that, the discriminator shares layers with another two classiﬁers to predict valence and activation simultaneously. This method has been shown to improve generalisability across corpora [105]. A similar approach was conducted in [54] to learn robust representations from emotional speech data for autism detection. More recently, a cGAN-based framework was proposed for continuous speech emo
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2165698076	es to aid the target emotion recognition task. This is a highly non-trivial task, the source and target domains are often highly mismatched with respect to the domains in which the data are collected [45], such as different recording environments or websites. For example, in sentiment analysis, the word ‘long’ for evaluating battery life has a positive connotation, whereas when assessing pain it tends
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2423557781	esis [78], and the VoiceGAN for voice impersonation [15]. IV. EMOTION SYNTHESIS As discussed in Section II-A, the most promising generative models, for synthesis, currently include PixelRNN/CNN [34], [79], VAE [35], and GANs [16]. Works undertaken with these models highlight their potential for creating realistic emotional samples. The PixelRNN/CNN approach, for example, can explicitly estimate the li
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2748059655	et al. [15] 2018 CVS speech voiceGAN no need of paired data Zhu et al. [96] 2018 DA image cycleGAN transfer data from A to B; require no further labelling process on the transferred data Sahu et al. [97] 2017 DA speech adversarial AE use GMM built on the original data to label generated data -&gt;noisy data; sensitive to mode collapse Ganin et al. [98] 2016 DAT text DANN ﬁrst work in domain adversari
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2434741482	etry from given images, reporting higher scores on evaluation metrics including BLEU, novelty, and relevance. Also, it has been claimed that InfoGAN converges faster than a conventional GAN framework [69]. However, it should be noted that, a fair experimental comparison of various generative adversarial training models associated with affective computing 10 TABLE II: A summary of adversarial training
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2434741482	eﬂect appropriate emotional reactions to a person’s behaviours. B. Other GAN-based Approaches in Image/Video In addition to the cGAN-based framework, other GAN variants such as DCGAN [19] and InfoGAN [69] have been investigated for emotional face synthesis. In [19], it is shown that, vector arithmetic operations in the input latent space can yield semantic changes to the image generations. For example
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2620623908	ges. These include the sparsity and unbalance problems of the training data [11], the instability of the emotion recognition models [12], [13], and the poor quality of the generated emotional samples [14], [15]. Despite promising research efforts and advances in leveraging techniques, such as semisupervised learning and transfer learning [11], ﬁnding robust solutions to these challenges is an open and
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2598581049	to guide the learning procedure [65]. Other GAN variants in this 1https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv category include the BiGAN [66], the CycleGAN [67], the DiscoGAN [68], the InfoGAN [69], and the Triple-GAN [70]. Network-type-based: in addition, several GAN variants have been named after the network topology used in the GAN conﬁguration, such as the DCGAN based on d
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2600556233	hus serve their own speciﬁc interests. Examples, to name just a few, include the Sketch-GAN proposed for sketch retrieval [75], the ArtGAN for artwork synthesis [76], the SEGAN for speech enhancement [77], the WaveGAN for raw audio synthesis [78], and the VoiceGAN for voice impersonation [15]. IV. EMOTION SYNTHESIS As discussed in Section II-A, the most promising generative models, for synthesis, curr
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1945616565	ing performance of an emotional AI system is also heavily dependent on its robustness to unseen data. A trivial disturbance on the sample (adversarial examples) might result in an opposite prediction [104], which naturally has to be avoided for a robust recognition model. 6RXUFH 7DUJHW [ (a) before DANN 6RXUFH 7DUJHW [ (b) after DANN Fig. 8: Illustration of data distributions from source and target bef
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2585630030	irection include the EnergyBased GAN (EBGAN) [59], the Least Squares GAN (LSGAN) [60], the Loss-Sensitive GAN (LS-GAN) [61], the Correlational GAN (CorrGAN) [62], and the Mode Regularized GAN (MDGAN) [63], to name but a few. Structure-based: these GAN variants have been proposed and developed to improve the structure of conventional GAN. For example, the conditional GAN (cGAN) adds auxiliary informati
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2570431255	issue in the ﬁelds of affective computing and sentiment analysis. In this regard, it has been shown that emotion recognition performance can be improved with various data augmentation paradigms [94], [95]. Data augmentation is a family of techniques which artiﬁcially generate more data to train a more efﬁcient (deep) learning model for a given task. Conventional data augmentation methods focus on gene
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1731081199	ling process on the transferred data Sahu et al. [97] 2017 DA speech adversarial AE use GMM built on the original data to label generated data -&gt;noisy data; sensitive to mode collapse Ganin et al. [98] 2016 DAT text DANN ﬁrst work in domain adversarial training Chen et al. [12] 2017 DAT text DANN semi-supervised supported; Wasserstein distance used for smoothing training process Zhang et al. [95] 2
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2474425887	lso a large number of GAN variants that have been designed for a given task, thus serve their own speciﬁc interests. Examples, to name just a few, include the Sketch-GAN proposed for sketch retrieval [75], the ArtGAN for artwork synthesis [76], the SEGAN for speech enhancement [77], the WaveGAN for raw audio synthesis [78], and the VoiceGAN for voice impersonation [15]. IV. EMOTION SYNTHESIS As discus
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2610961739	lysis [9]. Further, an end-to-end deep learning framework which automatically learns high-level representations from raw audio and video signals has been shown to be effective for emotion recognition [10]. However, when deployed in real-life applications, affective computing and sentiment analysis systems face many challenges. These include the sparsity and unbalance problems of the training data [11]
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1731081199	m the source domain can be applied directly OD\HU FODVVODEHO\ GRPDLQODEHOG / G  G ORVV/ \ ORVV/ / \  I IRUZDUGSURSEDFNSURS Fig. 7: Framework of Domain-Adversarial Neural Network (DANN). Source: [98] siﬁer G d(:; d). A typical Domain-Adversarial Neural Network (DANN) is illustrated in Figure 7. Particular to the DANN architecture, a gradient reversal layer is introduced between the domain classi
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2072712810	major issue in the ﬁelds of affective computing and sentiment analysis. In this regard, it has been shown that emotion recognition performance can be improved with various data augmentation paradigms [94], [95]. Data augmentation is a family of techniques which artiﬁcially generate more data to train a more efﬁcient (deep) learning model for a given task. Conventional data augmentation methods focus o
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2173520492	ng) was proposed by Goodfellow et al. [16], and has attracted widespread research interests across a range of machine learning domains [17], [18], including affective computing and sentiment analysis [19]–[21]. The initial adversarial training framework, Generative Adversarial Networks (GANs), consists of two neural networks – a generator and a discriminator, which contest with each other in a two-pla
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2732784815	ng speech and thus provides deductible contributions to optimise the generator. However, with the ongoing development of SER and the already discussed success of GANs in conventional speech synthesis [113] and image/video and text generation (cf. Section IV), we strongly believe that major breakthroughs will be made in this area sometime in the near future. Similarly, current state-of-the-art emotion c
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2434741482	ning procedure [65]. Other GAN variants in this 1https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv category include the BiGAN [66], the CycleGAN [67], the DiscoGAN [68], the InfoGAN [69], and the Triple-GAN [70]. Network-type-based: in addition, several GAN variants have been named after the network topology used in the GAN conﬁguration, such as the DCGAN based on deep convolutional
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2559110679	ogy used in the GAN conﬁguration, such as the DCGAN based on deep convolutional neural networks [19], the AEGAN based on autoencoders [71], the C-RNN-GAN based on continuous recurrent neural networks [72], the AttnGAN based on attention mechanisms [73], and the CapsuleGAN based on capsule networks [74]. Task-oriented: lastly, there are also a large number of GAN variants that have been designed for a
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2267126114	ple, the WaveNet network developed by Oord et al. [33] efﬁciently synthesises speech signals, and Pixel Recurrent Neural Networks (PixelRNN) and Variational AutoEncoder (VAE), proposed by Oord et al. [34] and Kingma et al. [35] respectively, have been shown to be effective for generating images. To date, the majority of these studies have not considered emotional information. A small handful of works
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2028975938	r-term opinions or attitudes and is more commonly associated with natural language processing. A plethora of applications can beneﬁt from the development of affective computing and sentiment analysis [3]–[8]; examples include natural and friendly human–machine interaction systems, intelligent business and customer service systems, and remote health care systems. Thus, affective computing and sentimen
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2620623908	raction scenario; smooth the video synthesis with context information Wang &amp; Ariteres [84] 2018 SYN motion cGAN to simulate a latent vector with seq2seq AE; controlled by` emotion Rajeswar et al. [14] 2017 SYN text (W)GAN-GP gradient penalty; generate sequences with ﬁxed length Liu et al. [85] 2018 SYN text (poetry) I2P-GAN multi-adversarial training; generate poetry from images Fedus et al. [21]
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2099471712	raining Instability: In the adversarial training process, ensuring that there is balance and synchronization between the two adversarial networks plays an important role in obtaining reliable results [16]. That is, the goal optimisation of adversarial training lies in ﬁnding a saddle point of, rather than a local minimum between, the two adversarial components. The inherent difﬁculty in controlling th
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2019759670	have been recorded in distinctive acoustic environments and by different speakers [11]. These mismatches have been shown to lead to a performance degradation of models analysed in real-life settings [27], [45], [46]. In addressing this challenge, Glorot et al. [46] presented a deep neural network based approach to learn the robust representations across different domains for sentiment analysis. Simil
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2165698076	been recorded in distinctive acoustic environments and by different speakers [11]. These mismatches have been shown to lead to a performance degradation of models analysed in real-life settings [27], [45], [46]. In addressing this challenge, Glorot et al. [46] presented a deep neural network based approach to learn the robust representations across different domains for sentiment analysis. Similar app
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2253891449	rn the robust representations across different domains for sentiment analysis. Similar approaches have also been proposed by Deng et al. [47] for emotion recognition from speech. Moreover, You et al. [48] successfully transferred the sentiment knowledge from text to predict the sentiment of images. However, it is still unclear if their learnt representations are truly domain-invariant or not. On the o
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2620623908	s also been investigated for both text generation [86], [87] and speech synthesis [78]. In particular, sentence generation conditioned on sentiment (either positive or negative) has been conducted in [14] and [21], but both only on ﬁxed-length sequences (11 words in [14] and 40 words in [21]). One example can be the three generated samples with a ﬁxed length (40 words) found in [21] (also shown in Tab
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2570431255	s can potentially be learnt. to the target domain [55]. Accordingly, the original DANN paradigm has been adapted to learn domain-invariant representations for sentiment classiﬁcation. For example, in [95], [99], attention mechanisms were introduced to give more attention to relevant text when extracting features. In [12], [100], the Wasserstein distance was estimated to guide the optimisation of the d
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2748059655	s of real data can yield suitable margins between different emotion categories. Additionally, when adding the generated data to the original data for training, performance can be marginally increased [97]. Similarly, a cycleGAN has been utilised for face-based emotion recognition [96]. To tackle the data inadequacy and unbalance problems, faces in different emotions have been generated from non-emotio
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2613984776	se perturbation against the current model p(yjx;), which can be calculated with r adv= arg max r;krk logp(yjx+r;): (4) In the context of affective computing and sentiment analysis, the authors in [105] utilised DCGAN and multi-task learning strategies to leverage a large number of unlabelled samples, where the unlabelled samples are considered as adversarial examples. More speciﬁcally, the model ex
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1731081199	sentations. By this training strategy, the representations learnt from different domains cannot be easily distinguished, as demonstrated in Figure 8. Further details on how to train DANN are given in [98]. Using a DANN, a common representation between data from the source and target domains can potentially be learnt. to the target domain [55]. Accordingly, the original DANN paradigm has been adapted t
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2101105183	sets of the generated faces. Similarly, to quantitatively evaluate models for emotion conversion, other evaluation measurements raised in the literature include BiLingual Evaluation Understudy (BLEU) [52] and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [53] for text, and a signal-to-noise ratio test for speech [15]. However, the quantitative performance evaluation for emotion perception
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2267126114	synthesis [78], and the VoiceGAN for voice impersonation [15]. IV. EMOTION SYNTHESIS As discussed in Section II-A, the most promising generative models, for synthesis, currently include PixelRNN/CNN [34], [79], VAE [35], and GANs [16]. Works undertaken with these models highlight their potential for creating realistic emotional samples. The PixelRNN/CNN approach, for example, can explicitly estimate
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2434741482	t persevered to become cast in a very good way I didn t realize that the book was made during the 70s The story was Manhattan the Allies were to which reﬂect the structured semantic data distribution [69]. For instance, it has been demonstrated that by varying one latent code, the emotions of the generated faces can change from stern to happy [69]. C. Approaches in Other Modalities As well as the gene
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2173520492	T: virtual adversarial training), modalities, and published years. GATH: generative adversarial talking head, ASPD: adversarial shared-private model. paper year task modality model note Radfod et al. [19] 2016 SYN image DCGAN vector arithmetic can be done in latent vector space, e.g., smiling woman - neutral woman + neutral man = smiling man Chen et al. [69] 2016 SYN image infoGAN latent code can be i
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	1731081199	he test phase, in order to improve the robustness of recognition models (cf. Section II-C). However, it is unclear if the learnt representations are truly domaingenerative or still domain-speciﬁc. In [98], Ganin et al. ﬁrst introduced domain adversarial training to tackle this problem. Typically, a feature extractor G f(:; f) projects data from two separate domains into highlevel representations, whi
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2581637843	timent labels (estimated from images) have been created via a multi-adversarial training approach [85]. Correspondingly, adversarial training has also been investigated for both text generation [86], [87] and speech synthesis [78]. In particular, sentence generation conditioned on sentiment (either positive or negative) has been conducted in [14] and [21], but both only on ﬁxed-length sequences (11 wo
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2605032625	tion, several GAN variants have been named after the network topology used in the GAN conﬁguration, such as the DCGAN based on deep convolutional neural networks [19], the AEGAN based on autoencoders [71], the C-RNN-GAN based on continuous recurrent neural networks [72], the AttnGAN based on attention mechanisms [73], and the CapsuleGAN based on capsule networks [74]. Task-oriented: lastly, there are
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2009059481	ubjective nature of 3 emotions which dictates the need for several annotators to label the same samples in order to diminish the effect of personal biases [41]. In tackling this challenge, Kim et al. [42] proposed an unsupervised learning approach to learn the representations across audiovisual modalities for emotion recognition without any labelled data. Similarly, Cummins et al. [43] utilised CNNs p
2892133643	Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives.	2434741482	year task modality model note Radfod et al. [19] 2016 SYN image DCGAN vector arithmetic can be done in latent vector space, e.g., smiling woman - neutral woman + neutral man = smiling man Chen et al. [69] 2016 SYN image infoGAN latent code can be interpreted; support gradual transformation Huang &amp; Khan [51] 2017 SYN image/video DyadGAN interaction scenario; identity + attribute from the interviewe
2892167359	Cascaded Mutual Modulation for Visual Reasoning	2194775991	or q i. (Figure2, middle). Each ResBlock contains a 1 1 convolution, a 3 3 convolution, a batch-normalization (Ioffe and Szegedy,2015) layer before FiLM modulation, followed by a residual connection (He et al., 2016). (Figure2, right. We keep the same ResBlock structure as (Perez et al.,2018)). To be consistent with (Johnson et al.,2017b;Perez et al., 2018), we concatenate the input visual features V i 1 of each
2892193869	Attentional Multi-Reading Sarcasm Detection	2064675550	able length sequences and have shown to be successful in various NLP tasks (Ghaeini et al., 2018a,c;Bahdanau et al.,2014;Ghaeini et al., 2016). Consequently, we utilize a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) for encoding the given comment and response. Here we simply read and encode the comment and response using a BiLSTM. Equations1 and2formally represent this component. u = BiLSTM(u) (1) v = BiLSTM(v
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2329847998	. As can be seen, ILP produces the best output. In contrast, NAMAS is the worst system for ﬂuency. In order to be able to judge the reliabilityofthehumanannotations, wefollowtheprocedure suggested by Pavlick and Tetreault (2016) and used by Toutanova et al. (2016), and compute thequadratic weightedκ(Cohen,1968)forthehuman ﬂuency scores of the system-generated compressions as 0.337. 4.2 LM Hyperparameters and Training We trai
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2470324779	atical error detection (Atwell, 1987; Wagner et al., 2007; Schmaltz et al., 2016; Liu and Liu, 2017) and grammatical error correction (Islam and Inkpen, 2011; Ng et al.,2013,2014; Bryant and Ng,2015; Yuan and Briscoe, 2016). However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic ﬂuency evaluation in NLP has been rare. Heilm
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2108325777	correct output will match any of a ﬁnite number of given references. This results in difﬁculties for current reference-based evaluation, especially of ﬂuency, causing word-overlap metrics like ROUGE (Lin and Och, 2004) to correlate only weakly with human judgments (Toutanova et al., 2016). As a result, ﬂuency evaluation of NLG is often done manually, which is costly and time-consuming. Evaluating sentences on their
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2108325777	hidden units each, and are trained for 2,000,000steps with a minibatch size of 128. For optimization, we employ ADAM (Kingma and Ba, 2014). 4.3 Baseline Metrics ROUGE-L. Our ﬁrst baseline is ROUGEL (Lin and Och, 2004), since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based on their longest common subsequence. Generated and reference compressions are
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2123891489	man et al. (2014) predicted the ﬂuency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. Stent et al. (2005) and Cahill (2009) found only lowcorrelation betweenautomaticmetricsandﬂuency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit ﬂuency evaluation of NL
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	108011198	paraphrases and the output of a German surface realiser, respectively. Explicit ﬂuency evaluation of NLG, including compression and the related taskofsummarization, hasmostly beenperformed manually. Vadlapudi and Katragadda (2010) used LMs for the evaluation of summarization ﬂuency, but their models were based on part-of-speech tags, which we do not require, and they were non-neural. Further, they evaluated longer texts, not s
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2608615602	quality. Similarly, Song et al. (2013) discussed BLEU being unreliable at the sentence or subsentence level (in contrast to the system-level), or for only one single reference. This was supported by Isabelle et al. (2017), who proposed a so-called challenge set approach as an alternative. Graham et al. (2016) performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a c
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2128856065	rd tomatch givenreferences. Theyemphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. Elliott and Keller (2014) reported the same for BLEU and image caption generation. Dusˇek et al.(2017)suggested anRNN to evaluate NLG at the utterance level, given only the input meaning representation. 7 Future Work The work
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	1489525520	rk is in line with previous criticism of evaluating NLG tasks with a single score produced by word-overlap metrics. The need for better evaluation for machine translation (MT) was expressed, e.g., by Callison-Burch et al. (2006), who doubted the meaningfulness of BLEU, and claimed that a higher BLEU score was neither a necessary precondition nor a proof of improved translation quality. Similarly, Song et al. (2013) discussed
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	2108325777	se an alternative evaluation. We aim at closing this gap. 6.2 Compression Evaluation Automatic compression evaluation has mostly had a strong focus on content. Hence, word-overlap metrics like ROUGE (Lin and Och, 2004) have been widely used for compression evaluation. However, they have certain shortcomings, e.g., they correlate best for extractive compression, whilewe,incontrast, areinterested inanapproach which g
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	1522301498	spectively, WPSLOR to correlate. Our best networks consist of two layers with 512 hidden units each, and are trained for 2,000,000steps with a minibatch size of 128. For optimization, we employ ADAM (Kingma and Ba, 2014). 4.3 Baseline Metrics ROUGE-L. Our ﬁrst baseline is ROUGEL (Lin and Och, 2004), since it is the most commonly used metric for compression tasks. ROUGE-L measures the similarity of two sentences based
2892228078	Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!	1664028424	tive systems. Alternatives include success rate (Jing, 2000), simple accuracy (Bangalore et al., 2000), which is based on the edit distance between the generation and the reference, or word accuracy (Hori and Furui, 2004), the equivalent for multiple references. 6.3 Criticism of Common Metrics for NLG In thesense that wepromote an explicit evaluation of ﬂuency, our work is in line with previous criticism of evaluating
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2100897593	, 2015, 2016), handwritten text recognition (Wang et al., 2013), and more recently, dialogue (Young et al.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2427527485	, 2015) and SQuAD (Rajpurkar et al., 2016) have made the training of end-to-end neural models
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2551396370	, 2016) have encouraged the development of many advanced, high performing attention-based neural models (Seo et al., 2017; Dhingra et al., 2017).
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2094728533	, from Freebase (Bollacker et al., 2008)) as opposed to semantics-based commonsense (e.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2250770256	We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet (Speer and Havasi, 2012).
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2551396370	BiDAF attention (Seo et al., 2017), emulating a single reasoning step within the multi-step reasoning process.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	1793121960	context can effectively emulate multi-step reasoning (Sukhbaatar et al., 2015).
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2133564696	We employ Bahdanau attention mechanism (Bahdanau et al., 2015) to attend over the context (c being the output of self-attention layer):
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	1544827683	er lexicons and simpler passage structures when compared to humangenerated text. There also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al.,2016) have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop reasoning capabiliti
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	1956340063	We also evaluate on CIDEr (Vedantam et al., 2015) which emphasizes annotator con-
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	1894439495	Knowledge path extraction has been shown to be effective at the task (Bordes et al., 2014; Bao et al., 2016).
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2094728533	Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs (Bollacker et al., 2008).
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	1525961042	Much progress has been made in reasoning-based MRCQA on the bAbI dataset (Weston et al., 2016),
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2250770256	Recently, largescale graphical commonsense databases such as ConceptNet (Speer and Havasi, 2012) use graphical structure to express intricate relations between concepts, but effective goal-oriented graph
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2250770256	We remedy this issue by introducing grounded commonsense (background) information using relations between concepts from ConceptNet (Speer and Havasi, 2012)1 that help inference by introducing useful connections between concepts in the context and question.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2250770256	for selecting useful, grounded multi-hop relational knowledge paths from ConceptNet (Speer and Havasi, 2012) via a pointwise mutual information (PMI) and term-frequency-based scoring function.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2551396370	has been shown that models designed for previous tasks (Seo et al., 2017; Kadlec et al., 2016) have limited success on these new datasets.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2551396370	Then, we use bidirectional attention (Seo et al., 2017) to emulate a hop of reasoning by focusing on relevant aspects of the context.
2892280852	Commonsense for Generative Multi-Hop Question Answering Tasks	2250770256	We first tried to naively add ConceptNet information by initializing the word embeddings with the ConceptNet-trained embeddings, NumberBatch (Speer and Havasi, 2012) (we also change embedding size from 256 to 300).
2892336869	Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation.	2140610559	an be applied to any embedding model and does not require any additional resource. Other authors have also proposed postprocessing methods for word embeddings with different motivations. For instance,Faruqui et al. (2015b) transform word embeddings into more interpretable sparse representations, obtaining improvements in several benchmark tasks.Rothe et al.(2016) propose an orthogonal transformation to concentrate th
2892336869	Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation.	2518186251	manghelich et al.,2016) and document classiﬁcation (Taddy,2015). While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg,2014c;Arora et al., 2016;Gittens et al.,2017), the fundamental idea behind all of them is to assign a similar vector representation to similar words. For that purpose, most embedding models build upon co-occurrence statistic
2892336869	Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation.	2136930489	t kind of relationships an embedding model should capture in practice. For instance, some authors distinguish between genuine similarity1 (as in car - automobile) and relatedness2 (as in car - road) (Budanitsky and Hirst, 2006;Hill et al.,2015). From another perspective, word similarity could focus on semantics (as in sing - chant) or syntax (as in sing - singing) (Mikolov et al.,2013). We refer to these two aspects as the
2892336869	Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation.	2387546565	these traditional count-based models as we in fact do in this paper. Finally, there are others authors that have also pointed limitations in the intrinsic evaluation of word embeddings. For instance,Faruqui et al. (2016) andBatchkarov et al.(2016) argue that word similarity has many problems like the subjectivity and difﬁculty of the task, the lack of statistical signiﬁcance and the inability to account for polysemy,
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2131774270	.5 dropout rate, one rectified linear unit (ReLU) layer, and the softmax output layer. E. Recurrent Neural Network (RNN) We employ Long Short-Term Memory (LSTM) [17] and Bi-directional LSTM (Bi-LSTM) [18]. The target numeral in a tweet is encoded with character- and word-based schemes as Fig. 1(b). It is regarded as a sequence of vectors. Each vector contains three parts, including representation of t
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	1832693441	arget numerals along with their annotated (sub)categories are used to train SVM. D. Convolutional Neural Network (CNN) CNN is one of the popular neural network (NN) models for sentence classification [16]. We encode a target numeral in a tweet with character- and word-based schemes with the matrix composed of three parts as shown in Fig. 1(a). Fig. 1. Representation of a target numeral in a tweet for
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2064675550	en dimensions, one dropout layer with 0.5 dropout rate, one rectified linear unit (ReLU) layer, and the softmax output layer. E. Recurrent Neural Network (RNN) We employ Long Short-Term Memory (LSTM) [17] and Bi-directional LSTM (Bi-LSTM) [18]. The target numeral in a tweet is encoded with character- and word-based schemes as Fig. 1(b). It is regarded as a sequence of vectors. Each vector contains thr
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2123167824	II, we evaluate the trading strategies based on crowd opinion. Section VIIII concludes this work. II. RELATED WORK Temporal, a category of numerals, is one of the foci in previous work. Ling and Weld [3] propose an extractor for temporal information with probabilistic inference. Tourille et al. [4] attempt to extract numeral information from clinical documents. Davidov and Rappoport [5] extract numer
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2124104135	Ling and Weld [3] propose an extractor for temporal information with probabilistic inference. Tourille et al. [4] attempt to extract numeral information from clinical documents. Davidov and Rappoport [5] extract numerical information like size and depth from the web and experiment on the question answering task. Madaan et al. [6] deal with numeral relation extraction, and propose the state-of-the-art
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2008056655	nd 5,838 in (T10) will be converted to D,DDD. In these two instances, the pattern D/DD is more likely to be annotated as Temporal than the pattern D,DDD. C. Support Vector Mechine (SVM) The SVM model [15] is considered as our baseline classifier. A target numeral in a tweet is represented as two parts. The first part describes the tweet itself, and the second part describes the contextual clues near t
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	2171468534	nts. That shows the importance of numerical information in finance. Sentiment analysis, a widely-studied topic in the NLP community, is one of the applications of numeral understanding. Bollen et al. [8] show that the public mood on Twitter is correlated to Dow Jones Industrial Average value. Li et al. [9] introduce sentiment of news articles into their model, and indicate that sentiment information
2892359842	Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting	1965235124	topic in the NLP community, is one of the applications of numeral understanding. Bollen et al. [8] show that the public mood on Twitter is correlated to Dow Jones Industrial Average value. Li et al. [9] introduce sentiment of news articles into their model, and indicate that sentiment information do help the accuracy of predicting stock price. Khedr et al. [10] use news sentiment analysis results to
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2028175314	, 2017) 7 7 3 7 LC-QuAD (Trivedi et al., 2017) 3 7 3 7 ComplexQuestions (Bao et al., 2016) 7 3 3 7 GraphQuestions (Su et al., 2016) 3 7 3 3 ComplexWebQuestions (Talmor and Berant, 2018) 3 7 3 7 TREC (Voorhees and Tice, 2000) 7 3 3 7 Table 1: Comparison of ComQA with existing QA datasets over various dimensions. search engine logs to collect their questions (Berant et al., 2013), which creates a bias towards simpler quest
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2606964149	). ComQA is orders of magnitude larger than TREC QA. Reading comprehension is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al., 2017; Yang et al., 2015). This setting is different from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	1981419611	. QAoverknowledgebases. Recent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Berant et al., 2013; Fader et al., 2013; Reddy et al., 2014; Mohammed et al., 2018; Xu et al., 2016; Yang et al., 2014; Yao and Durme, 2014; Yih et al., 2015). Over the past ﬁve years, many datasets
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2252136820	(Talmor and Berant, 2018) 3 7 3 7 TREC (Voorhees and Tice, 2000) 7 3 3 7 Table 1: Comparison of ComQA with existing QA datasets over various dimensions. search engine logs to collect their questions (Berant et al., 2013), which creates a bias towards simpler questions that search engines can already answer reasonably well. In contrast, ComQA questions come from WikiAnswers, a community QA website where users pose que
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2252136820	09.09528v1 [cs.CL] 25 Sep 2018 Dataset Large scale (&gt;5K) Real Information Needs Complex Questions Question Paraphrases ComQA(This paper) 3 3 3 3 Free917 (Cai and Yates, 2013) 7 7 7 7 WebQuestions (Berant et al., 2013) 3 3 7 7 SimpleQuestions (Bordes et al., 2015) 3 7 7 7 QALD (Usbeck et al., 2017) 7 7 3 7 LC-QuAD (Trivedi et al., 2017) 3 7 3 7 ComplexQuestions (Bao et al., 2016) 7 3 3 7 GraphQuestions (Su et al.,
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2134489415	2000; Dietz and Gamari, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prager et al., 2004; Ravichandran and Hovy, 2002; Saquete et al., 2004, 2009; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answers, combined with lea
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	1980095184	2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prager et al., 2004; Ravichandran and Hovy, 2002; Saquete et al., 2004, 2009; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answers, combined with learned models for question types. The TREC QA evaluation series provide hundre
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2251079237	antic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Berant et al., 2013; Fader et al., 2013; Reddy et al., 2014; Mohammed et al., 2018; Xu et al., 2016; Yang et al., 2014; Yao and Durme, 2014; Yih et al., 2015). Over the past ﬁve years, many datasets were introduced for this setting. However, as Table 1 shows, they are either small in size (QALD, Free917, and ComplexQuestions), composed of synthetically gen
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2295690548	atasets over various phenomena. We manually annotated 100 random questions from each dataset. Avg. Prec Avg. Rec Avg. F1 Abujabal et al. (2017) 21:2 38:4 22:4 Bast and Haussmann (2015) 20:7 37:6 21:6 Berant and Liang (2015) 10:7 15:4 10:6 Berant et al. (2013) 13:7 20:1 12:0 Fader et al. (2013) 7:22 6:59 6:73 Table 4: Results of baselines on ComQA test set. WebQuestions Free917 ComQA F1 Accuracy F1 Abujabal et al. (2017)
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2131726681	ation stored with entities in Freebase. We observe that the Wikipedia answer entities have no counterpart in Freebase for only 7% of the ComQA questions. This suggests an oracle F1 score of 93:0. For Fader et al. (2013), which is over web extractions, we mapped Wikipedia URLs to their titles. 6.3 Results Table 4 shows the performance of the baselines on the ComQA test set. Overall, the systems achieved poor performa
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2028175314	the best answering performance (Savenkov and Agichtein, 2016; Sun et al., 2018; Xu et al., 2016). QA over textual corpora. QA has a long tradition in IR and NLP, including benchmarking tasks in TREC (Voorhees and Tice, 2000; Dietz and Gamari, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prag
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2252136820	cent efforts have focused on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Berant et al., 2013; Fader et al., 2013; Reddy et al., 2014; Mohammed et al., 2018; Xu et al., 2016; Yang et al., 2014; Yao and Durme, 2014; Yih et al., 2015). Over the past ﬁve years, many datasets were introduced for
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2135514656	ctoid QA task, with the distinction tied to the underlying resources used for answering and the nature of these answers. Traditionally, the problem of QA has been explored over large textual corpora (Cui et al., 2005; Dietz and Gamari, 2017; Ferrucci, 2012; Harabagiu et al., 2001, 2003; Ravichandran and Hovy, 2002; Saquete et al., 2009; Voorhees and Tice, 2000) with answers being textual phrases. More recently th
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2131726681	from each dataset. Avg. Prec Avg. Rec Avg. F1 Abujabal et al. (2017) 21:2 38:4 22:4 Bast and Haussmann (2015) 20:7 37:6 21:6 Berant and Liang (2015) 10:7 15:4 10:6 Berant et al. (2013) 13:7 20:1 12:0 Fader et al. (2013) 7:22 6:59 6:73 Table 4: Results of baselines on ComQA test set. WebQuestions Free917 ComQA F1 Accuracy F1 Abujabal et al. (2017) 51:0 78:6 22:4 Bast and Haussmann (2015) 49:4 76:4 21:6 Berant and Lia
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	1981419611	e 5: Results of baselines on different datasets. ing systems using their publicly available code: (i) Abujabal et al. (2017), which automatically generates templates using question-answer pairs; (ii) Bast and Haussmann (2015), which instantiates query templates followed by query ranking; (iii) Berant and Liang (2015), which relies on agendabased parsing and imitation learning; (iv) Berant et al. (2013), which uses rules t
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2295690548	e: (i) Abujabal et al. (2017), which automatically generates templates using question-answer pairs; (ii) Bast and Haussmann (2015), which instantiates query templates followed by query ranking; (iii) Berant and Liang (2015), which relies on agendabased parsing and imitation learning; (iv) Berant et al. (2013), which uses rules to build queries from questions; and (v) Fader et al. (2013), which maps questions to queries
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2028175314	erics and dates, ComQA adopts the SI and TIMEX3 standards, respectively. 3 Overview In this work, a factoid question is a question whose answer is one or a small number of entities or literal values (Voorhees and Tice, 2000). For example, “Who were the secretaries of state under Barack Obama?” and “When was Germany’s ﬁrst post-war chancellor born?”. 3.1 Questions in ComQA Questions: A question in our dataset can exhibit
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	1981419611	g KBs. Both examples above demonstrate the beneﬁts of combining text and structured resources. 6.4 Error Analysis For the two best performing systems on ComQA, QUINT (Abujabal et al., 2017) and AQQU (Bast and Haussmann, 2015), we manually inspected ComQA questions on which they failed, 100 questions per system. We classiﬁed failure sources into four categories: compositionality, temporal, comparison or NER. Table 6 shows
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2252136820	and Hovy, 2002; Saquete et al., 2009; Voorhees and Tice, 2000) with answers being textual phrases. More recently the problem has been explored over large structured resources such as knowledge bases (Berant et al., 2013; Unger et al., 2012; Yahya et al., 2013), with answers being semantically grounded entities. Very recent work demonstrated that the two variants are complementary, and a combination of the two result
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2167435923	i, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prager et al., 2004; Ravichandran and Hovy, 2002; Saquete et al., 2004, 2009; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answers, combined with learned models for question type
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2090243146	inspected a random subset of the 2:1K WikiAnswers clusters and found that questions in the same cluster are semantically related but not equivalent, which is line with observations in previous work (Fader et al., 2014). Dong et al. (2017) reported that 45% of question pairs were related rather than genuine paraphrases. For example, Figure 2 shows 10 questions in the same WikiAnswers cluster. Obtaining accurate para
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	1981419611	le 3: Comparison of ComQA with existing datasets over various phenomena. We manually annotated 100 random questions from each dataset. Avg. Prec Avg. Rec Avg. F1 Abujabal et al. (2017) 21:2 38:4 22:4 Bast and Haussmann (2015) 20:7 37:6 21:6 Berant and Liang (2015) 10:7 15:4 10:6 Berant et al. (2013) 13:7 20:1 12:0 Fader et al. (2013) 7:22 6:59 6:73 Table 4: Results of baselines on ComQA test set. WebQuestions Free917 ComQ
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2131726681	lowed by query ranking; (iii) Berant and Liang (2015), which relies on agendabased parsing and imitation learning; (iv) Berant et al. (2013), which uses rules to build queries from questions; and (v) Fader et al. (2013), which maps questions to queries over open vocabulary facts extracted from a large text corpus of Web documents. Note that our intention is not to assess the quality of current systems, but to show t
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2427527485	of magnitude larger than TREC QA. Reading comprehension is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al., 2017; Yang et al., 2015). This setting is different from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2557764419	n TREC QA. Reading comprehension is a recently introduced task, where the goal is to answer a question from a given textual paragraph (Kocisk´y et al., 2017; Lai et al., 2017; Rajpurkar et al., 2016; Trischler et al., 2017; Yang et al., 2015). This setting is different from factoid QA, where the goal is to answer questions from a large repository of data (be it textual or structured), and not a single paragraph. QAover
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2131726681	used on natural language questions as an interface for KBs, where questions are translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Berant et al., 2013; Fader et al., 2013; Reddy et al., 2014; Mohammed et al., 2018; Xu et al., 2016; Yang et al., 2014; Yao and Durme, 2014; Yih et al., 2015). Over the past ﬁve years, many datasets were introduced for this setting. Howeve
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2167435923	and the nature of these answers. Traditionally, the problem of QA has been explored over large textual corpora (Cui et al., 2005; Dietz and Gamari, 2017; Ferrucci, 2012; Harabagiu et al., 2001, 2003; Ravichandran and Hovy, 2002; Saquete et al., 2009; Voorhees and Tice, 2000) with answers being textual phrases. More recently the problem has been explored over large structured resources such as knowledge bases (Berant et al.,
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2131726681	o group questions into paraphrase clusters and pair them with answers. Past work has demonstrated the beneﬁts of paraphrasing for QA (Abujabal et al., 2018; Berant and Liang, 2014; Dong et al., 2017; Fader et al., 2013). Motivated by this, we judiciously use crowdsourcing to obtain clean paraphrase clusters from WikiAnswer’s noisy ones, resulting in ones like those shown in Figure 1, with both lexical and syntactic
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2028175314	problem of QA has been explored over large textual corpora (Cui et al., 2005; Dietz and Gamari, 2017; Ferrucci, 2012; Harabagiu et al., 2001, 2003; Ravichandran and Hovy, 2002; Saquete et al., 2009; Voorhees and Tice, 2000) with answers being textual phrases. More recently the problem has been explored over large structured resources such as knowledge bases (Berant et al., 2013; Unger et al., 2012; Yahya et al., 2013),
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2090243146	questions. In this work, we exploit the annotations where users mark questions as duplicates as a basis for paraphrase clusters, and clean those. Concretely, we started with the WikiAnswers crawl by Fader et al. (2014). We obtained ComQA from this crawl primarily through a large scale crowdsourcing effort to ensure it is of high quality. We describe this effort in what follows. The original resource curated by Fade
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2016852756	REC (Voorhees and Tice, 2000; Dietz and Gamari, 2017) and CLEF (Magnini et al., 2004; Herrera et al., 2004). This has predominantly focused on retrieving answers from textual sources (Ferrucci, 2012; Harabagiu et al., 2006; Prager et al., 2004; Ravichandran and Hovy, 2002; Saquete et al., 2004, 2009; Yin et al., 2015). In IBM Watson (Ferrucci, 2012), structured data played a role, but text was the main source for answe
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2072692647	rhees and Tice, 2000) with answers being textual phrases. More recently the problem has been explored over large structured resources such as knowledge bases (Berant et al., 2013; Unger et al., 2012; Yahya et al., 2013), with answers being semantically grounded entities. Very recent work demonstrated that the two variants are complementary, and a combination of the two results in the best answering performance (Save
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2096765155	ses: (1) temporal, (2) comparison, (3) single entity, and (4) multi-entity questions. We used SUTime (Chang and Manning, 2012) to identify temporal questions and the Stanford named entity recognizer (Finkel et al., 2005) to detect named entities. We used part-of-speech patterns to identify comparatives, superlatives, and ordinals. Clusters which did not have questions belonging to any of the above classes were discar
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2028175314	tems on ComQA, demonstrating that our dataset can be a driver of future research on QA. 1 Introduction Factoid QA is the task of answering questions whose answer is one or a small number of entities (Voorhees and Tice, 2000). To advance research in QA in a manner consistent with the needs of end users, it is important to have access to benchmarks that reﬂect real user information needs by covering various question phenom
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2251289180	translated to structured queries via semantic parsing (Bao et al., 2016; Bast and Haussmann, 2015; Berant et al., 2013; Fader et al., 2013; Reddy et al., 2014; Mohammed et al., 2018; Xu et al., 2016; Yang et al., 2014; Yao and Durme, 2014; Yih et al., 2015). Over the past ﬁve years, many datasets were introduced for this setting. However, as Table 1 shows, they are either small in size (QALD, Free917, and ComplexQ
2892452601	ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters	2252136820	underlying answering resource: either KBs or textual extractions. We ran the followDataset Size Compositional Temporal Comparison Telegraphic Empty Answer ComQA 11;214 32% 24% 30% 8% 4% WebQuestions (Berant et al., 2013) 5;810 2% 7% 2% 0% 0% ComplexQuestions (Bao et al., 2016) 2;100 39% 34% 9% 0% 0% Table 3: Comparison of ComQA with existing datasets over various phenomena. We manually annotated 100 random questions
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2167662839	’s intention and extract units of information in the request. Design of such large scale NLU systems requires a fast and scalable training with capabilities such as asynchronous and parallel training [3, 4]. One of the NLU design approaches is a system modularized into domain-speciﬁc components (namely, domain classiﬁer, intent classiﬁer and named entity recognizers), where each domain represents a core
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2137143056	ased on a re-ranker approach inlude corpus weight estimation [13], minimum-risk training on translation forests [14], batch tuning for statistical machine translation [15] and minimium risk annealing [16]. In particular, the Yahoo! learning to rank challenge [17] led to several advances in hypothesis ranking. Despite providing promising results on multiple tasks, the methods propose training a single
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2159755860	hes that minimize error metrics based on a re-ranker approach inlude corpus weight estimation [13], minimum-risk training on translation forests [14], batch tuning for statistical machine translation [15] and minimium risk annealing [16]. In particular, the Yahoo! learning to rank challenge [17] led to several advances in hypothesis ranking. Despite providing promising results on multiple tasks, the m
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2784672094	n [13], minimum-risk training on translation forests [14], batch tuning for statistical machine translation [15] and minimium risk annealing [16]. In particular, the Yahoo! learning to rank challenge [17] led to several advances in hypothesis ranking. Despite providing promising results on multiple tasks, the methods propose training a single model as a combination strategy, which will not maintain mo
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2103149536	nk [11] and Mcrank [12]. A few other noteworthy approaches that minimize error metrics based on a re-ranker approach inlude corpus weight estimation [13], minimum-risk training on translation forests [14], batch tuning for statistical machine translation [15] and minimium risk annealing [16]. In particular, the Yahoo! learning to rank challenge [17] led to several advances in hypothesis ranking. Despi
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2115410424	re-ranking algorithms such as LambdaMART [6], Adarank [11] and Mcrank [12]. A few other noteworthy approaches that minimize error metrics based on a re-ranker approach inlude corpus weight estimation [13], minimum-risk training on translation forests [14], batch tuning for statistical machine translation [15] and minimium risk annealing [16]. In particular, the Yahoo! learning to rank challenge [17] l
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	179314280	ss all the domains. Crook et al. [7] extend a similar model to a multi-lingual setting. Re-ranking of hypothesis coming from a system has been approached in other problems such as machine translation [8], obtaining correct NLU hypotheses given multiple speech recognition hypotheses [9], as well as re-ranking speech recognition hypotheses themselves [10]. On the other hand, researchers have also focus
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2115410424	t the DC, IC and NER scores are very well linearly correlated with SemER and IE values. A linear combination model has also been tested in several related re-ranking tasks such as machine translation [13] and speech recognition [23]. We experiment with four different re-ranker optimization models as described below. 3.4.1. Baseline: Uniform re-ranker The ﬁrst re-ranker model used in our experiments is
2892453983	A Re-Ranker Scheme For Integrating Large Scale NLU Models	2142537246	ypotheses [9], as well as re-ranking speech recognition hypotheses themselves [10]. On the other hand, researchers have also focused on designing re-ranking algorithms such as LambdaMART [6], Adarank [11] and Mcrank [12]. A few other noteworthy approaches that minimize error metrics based on a re-ranker approach inlude corpus weight estimation [13], minimum-risk training on translation forests [14], b
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	25648700	; Porteous and Cavazza 2009; Gerv´as et al. 2005; Riedl and Young 2010). However, these approaches have also required extensive domain knowledge engineering. On the other hand, open story generation (Li et al. 2013; Martin, Harrison, and Riedl 2016) is the problem of creating a story for any domain without new knowledge engineering nor retraining. Open story generation approaches include casebased reasoning on
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	1622242599	ator will be provided with a goal, outcome state, or other guides to ensure the resulting story is coherent. Thus, many previous approaches to story generation have relied on planning (Lebowitz 1987; Porteous and Cavazza 2009; Gerv´as et al. 2005; Riedl and Young 2010). However, these approaches have also required extensive domain knowledge engineering. On the other hand, open story generation (Li et al. 2013; Martin, Har
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	2121863487	ently by giving it “clues”. In this work, we show how one can analyze a corpus of stories to construct a dense, approximate reward signal. Reinforcement Learning Preliminaries Reinforcement learning (Sutton and Barto 1998) is a technique that is used to solve a Markov decision process (MDP). A MDP is a tuple M = hS;A;T;R; iwhere S is the set of possible world states, Ais the set of possible actions, Tis a transition fu
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	1757796397	the following functions. The function wn()gives the WordNet (Miller 1995) Synset of the argument two levels up in the hypernym tree (i.e. the grandparent Synset). The function vn() gives the VerbNet (Mnih et al. 2013) class of the argument. We use the same event representation in this work. Some sentences can contain more than one event. We split sentences into multiple events so that there is a potential one-to-m
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	25648700	n systems attempt to learn or acquire domain knowledge. Early open story generation techniques include textual case-based reasoning (Swanson and Gordon 2012a) and learning from crowdsourced examples (Li et al. 2013). Recurrent neural networks (RNNs) are promising for open story generation because models can be trained on a large, diverse corpus of stories and then used to predict the probability of the next lett
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	2236262502	neration approaches include casebased reasoning on blogs (Swanson and Gordon 2012b), domain learning from crowdsourced corpora (Li et al. 2013), and neural language models trained on diverse corpora (Roemmele and Gordon 2015; Martin et al. 2018; Fan, Lewis, and Dauphin 2018). To date, most existing open story generation systems lack the ability to receive guidance from the user to achieve a Denotes equal contribution. sp
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	1622242599	nt ordering and plot coherence over the baseline story generator. Related Work Early story generation systems relied on symbolic planning (Meehan 1977; Lebowitz 1987; Cavazza, Charles, and Mead 2002; Porteous and Cavazza 2009; Riedl and Young 2010; arXiv:1809.10736v1 [cs.CL] 27 Sep 2018 Ware and Young 2011) or case-based reasoning (Perez y´ Perez and Sharples 2001; Gerv´ as et al. 2005). These tech-´ niques could only gen
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	2236262502	sing for open story generation because models can be trained on a large, diverse corpus of stories and then used to predict the probability of the next letter/character, word, or sentence in a story. Roemmele and Gordon (2015) and Khalifa et al. (2017) used Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber 1997) to generate stories. Gehring et al. (2018) used a hierarchical convolutional RNN to expand shor
2892472836	Controllable Neural Story Generation via Reinforcement Learning.	2081580037	y to denote there is no object of the verb or any additional sentence information. As with Martin et al., we stem all words and then apply the following functions. The function wn()gives the WordNet (Miller 1995) Synset of the argument two levels up in the hypernym tree (i.e. the grandparent Synset). The function vn() gives the VerbNet (Mnih et al. 2013) class of the argument. We use the same event representa
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2025768430	Autoencoder Models trained on autoencoding are the only ones that do not consistently improve with the amount of training data, which is unsurprising as unregularized autoencoders are prone to learning identity mappings (Vincent et al., 2008).
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2773956126	Although Belinkov et al. (2017a) find that translating into morphologically poorer languages leads to a slight improvement in encoder representations, we expect that our study of English-German translation will provide a reasonable overall picture of the representations that can be learned in data-rich translation.
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2574872930	our data. Finally, we limit both the English and German vocabularies to the 50k most frequent tokens in the training set. 3.2 MODEL ARCHITECTURE AND TRAINING We train all our models using OpenNMT-py (Klein et al., 2017) and use the default options for model sizes, hyperparameters, and training procedure—except we increase the size of the LSTMs, make the encoders bidirectional, and use validation-based learning rate
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2101105183	We report model performance in terms of perplexity and BLEU (Papineni et al., 2002) in Table 1.
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2612953412	unlabeled data is skip-thought (Kiros et al., 2015), the technique of training a sequence-to-sequence model to predict the sentence preceding and following each sentence in a running text. InferSent (Conneau et al., 2017)—the technique of pretraining encoders on natural language inference data— yields strikingly better performance when such labeled data is available. Work in transfer learning of representations has re
2893141505	Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis	2773956126	Upper Layers Belinkov et al. (2017a) find that, for translation models, the first layer consistently outperforms the second on POS tagging.
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2098297786	:5, since it places twice as much emphasis 3.2. Evaluation metrics 36 on precision than recall, while F 1 weighs precision and recall equally. This metric has also been used in CoNLL-2014 shared task [46]. Our F 0:5 is deﬁned as follows: F 0:5 = (1+0:5 2): precision recall 0:52 precision+recall (3.37) To illustrate, consider correction system B in example3.2. Based on our deﬁned metrics, the performan
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2101105183	d in [50]. 2. 6.3.3 BLEU and GLEU BLEU was one of the ﬁrst automatic metrics used in measuring translation accuracy and has became one of most common metrics in machine translation systems evaluation [51]. In addition to this metric, we also use GLEU which is a simple variant of BLEU showing a better correlation with human judgments in the evaluation task [52]. The following table shows the evaluation
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2252205254	Dialectal errors: speciﬁc words are detected as incorrect if not present in the Almaany reference dictionary 2. 4.2.1 QALB corpus structure Using the annotation style of the CoNLL-2013 shared task in [48], the QALB corpus is consists of blocks of annotated phrases. In each block, the ﬁrst line starting by ’S’ is a document token that may encompass a single sentence or a paragraph of different sentence
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2130942839	e decoder and different representations in the encoder. Kalchbrenner and Blunsom [22] used an RNN for the decoder and a convolutional neural network for encoding the source sentence. Sutskever et al. [23] and Luong et al. [24], on the other hand, have used multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both encoder and decoder. Cho et al. [25], Bahdanau et al. [26] and
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2006969979	e desired output has the highest probability giving speciﬁc parameters of the model . Therefore, we have: Tb = argmax T P(TjS;) (2.4) which is called the Fundamental Equation of Machine Translation [17]. Various approaches are used to model the probability. n-gram-based language models, as mentioned in section2.1.2, often also function as a probabilistic technique for error correction. Another commo
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2101105183	FN, FPN - a - FP FP - - a FN FN Table 3.1: Extended Writer-Annotator-System evaluation system where w&gt;1. 3.2.4 BLEU and GLEU One of the most widely used automatic evaluation metrics is BLEU score [51]. It is computed as the geometric mean of the modiﬁed n-gram precision, multiplied by a brevity penalty, ˆ, to control for recall by penalizing short translations: BLEU= ˆ( YN i=1 precision i) 1 N (3.
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2064675550	g long-term dependencies. Long Short-Term Memory (LSTM) network is an architecture for the RNN, capable of learning long-term dependencies. They were initially introduced by Hochreiter and Schmidhube [33] and were popularized by many other works in the following years (particularly in [34,35,36,37,38]). It is among the most widely used models in Deep Learning for NLP today. An LSTM network is composed
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	1526096287	hese neural machine translation (NMT) systems have been used in recent related works with different architectures of the decoder and different representations in the encoder. Kalchbrenner and Blunsom [22] used an RNN for the decoder and a convolutional neural network for encoding the source sentence. Sutskever et al. [23] and Luong et al. [24], on the other hand, have used multiple layers of an RNN wi
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	1967703013	ing similarity key technique. 2.2.3 Rule-based techniques By analyzing the most common spelling errors, some researchers have attempted to create a knowledge base of errors for the task of correction [13,14,15] using rule-based 2.2. Error correction techniques 19 models that encode grammatical knowledge. These rules are generally based on the morphological characteristics of the language. 2.2.4 Probabilisti
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	99445809	he models, an output is the most probable element in the softmax probability distribution (demonstrated in algorithm1). Although, this is not the only approach to generate an output. Dahlmeier and Ng [59] developed a beam-search decoder to iteratively generate sentencelevel candidates and ranking them. In the case of encoder-decoder and attention-based encoder-decoder, since the output size can be var
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2098297786,2252205254	n the gold-standard corrections. Although M2 Scorer is currently a standard metric in evaluating error correction systems, having been used to rank error correction systems in the 2013 and 2014 CoNLL [48,46] and EMNLP 2014 [49] shared tasks, it also has some weak points. In experimenting with the baseline system, since there is theoretically no correction to be done, the results of M2 Scorer appear to be
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2131774270	nd the other backwards (right-to-left propagation), both with two different hidden units but connected to the same output(ﬁgure3.6). This model is called Bidirectional Recurrent Neural Network (BRNN) [40]. ::: ~s t1 ~s ~s t+1 ~s t+2 :::Forwardstates s t+2 s t+1 s t s Backwardstates::: t1 ::: ~x t1 ~x t ~x t+1 ~x t+2 ~o ~o t+1 ~o t+2 t ~o t1 ... ... Figure 3.6: A bidirectional recurrent neural network
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2125308790	ory. Future researches may reveal a clearer distinction between attention and memory mechanisms. In the recent studies, reinforcement learning techniques have been also used for error correction task [62,63]. APPENDIX A Appendix A.1 Adam Adaptive Moment Estimation (Adam) [64] is a stochastic optimization method that computes learning rates for each parameter. In addition to storing an exponentially decay
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	1951216520	re not a practicable choice for modeling the long-distance dependencies. On the other hand, the recurrent neural networks (RNN) have been demonstrated to be more capable of modeling such dependencies [21]. Neural networks are also based on the probability distribution of language and they have shown recent success in different tasks related to natural language processing. This study focuses on attenti
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	139960808,1810943226	the RNN, capable of learning long-term dependencies. They were initially introduced by Hochreiter and Schmidhube [33] and were popularized by many other works in the following years (particularly in [34,35,36,37,38]). It is among the most widely used models in Deep Learning for NLP today. An LSTM network is composed of memory cells and gate units to calculate the internal states. Three gates control the behavior
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2130942839	tput would be used for calculating the error loss as well. In some of the ﬁrst studies that used encoder-decoder RNN models, a ﬁxed-length vector is used to represent the context of a source sentence [25,23]. The fact that a ﬁxedlength, regardless of the size of the input, is used, means a limitation on the model to represent variable-size input strings. More recently, Bahdanau et al. [26] explored the a
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2066792529	and transposition, in order to transform an incorrect input into the most probable word, i.e., the one with least edit distance. Hamming [4], Jaro–Winkler [5], Wagner–Fischer [6], Damerau-Levenshtein [7] and Levenshtein [8] are among the most famous edit distance algorithms. We will Levenshtein distance later in one of the evaluation metrics in section3.2.2. 2.2.2 Similarity key technique Similarity
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2005708641,2288502450	y for the parameters. BRNNs have previously given improved results in various domains in speech processing [41,42]. More recently, deep BRRN have been used where each lower layer feeds the next layer [43,44]. 3.1. Neural Networks 31 3.1.4 Sequence-to-sequence models Apart from long dependency, another challenge in language models using neural networks is the variable-length output spaces, e.g., words and
2893707415	Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction.	2142187732	ymbol. More highly complex probabilistic network representations have been also introduced [16,19]. Hidden Markov Models (HMM), for example, have demonstrated a strong ability to model human language [20], ut since HMMs assume conditional independence of the previous words except the last one, they are not a practicable choice for modeling the long-distance dependencies. On the other hand, the recurre
2894607199	Weakly Supervised Object Detection in Artworks	1536680647	In [1], it is proposed to learn to detect new specic classes by taking advantage of the knowledge of wider classes. In [5] a weakly supervised deep detection network is proposed based on Fast R-CNN [25]. Those works have been improved in [50] by adding a multistage classier renement. In [9] a multi-fold split of the training data is proposed to escape local optima. In [34], a two step strategy is
2894607199	Weakly Supervised Object Detection in Artworks	2101611867	(bike, bird, dog, cat, car, person) that are included in the PASCAL VOC, in order to study cross-domain transfer learning. On this database, we compare our approach to the methods from [29] and from [5], to the baseline MAX discussed above, as well as to the classical MIL approach MI-SVM [2] (using a maximum of 50 iterations and no restarts). In [29], a style transfer transformation (Cycle-GAN [56])
2894607199	Weakly Supervised Object Detection in Artworks	2108745803	boxes correspond to this category. For this, we propose a new multiple-instance learning method, that will be detailed in Section 3.1. In contrast with classical approaches to the MIL problem such as [2] the proposed heuristic is very fast. This, combined with the fact that we do not need ne-tuning, permits a exible on-they learning of new category in a few minutes. Weakly Supervised Detection in Art
2894607199	Weakly Supervised Object Detection in Artworks	2518995963	cluding some CNNs, for the detection of people in cubist artworks. In [41], it is shown that the YOLO network trained on natural images can, to some extend, be used for people detection in cubism. In [52], it is proposed to perform people detection in a wide variety of artworks (through a newly introduced database) by ne-tuning a network in a supervised way. People can be detected with high accuracy e
2894607199	Weakly Supervised Object Detection in Artworks	2108745803	contains the category, whereas if y i = 1 no box does. The goal 6 N. Gonthier et al. is then to decide which boxes correspond to the category. Instead of the classical SVM generalisation proposed in [2] and based on an iterative procedure, we look for an hyperplan minimising the functional dened below. We look for w2RM, b2R achieving min (w;b)L(w;b) (1) with ˚(w;b) = XN i=1 y i n y i Tanh ˆ max k2f
2894607199	Weakly Supervised Object Detection in Artworks	2155541015	d on painting databases by using convolutional neural networks (CNNs) designed for the classication of photographs [11,55]. These results occur in a general context were methods of transfer learning [15] (changing the task a model was trained for) and domain adaptation (changing the nature of the data a model was trained on) are increasingly applied. Classifying and analysing paintings is of course o
2894607199	Weakly Supervised Object Detection in Artworks	1536680647	depiction techniques. The method introduced in [52] yields excellent detection performances on this database, but necessitates instance-level annotations for training. The authors rely on Fast R-CNN [25], of which they only keep the three rst 9 https://github.com/naoto0804/cross-domain-detection 10 The performance come from the original paper [29]. 11 Standard deviation computed on 100 runs of the al
2894607199	Weakly Supervised Object Detection in Artworks	2101611867	is detector is used to predict localisation of objects on watercolor images annotated at the image level. The detector is then ne-tuned on those images in a fully supervised manner. Bilen and Vedaldi [5] proposed a Weakly Supervised Deep Detection Network (WSDDN), which consists in transforming a pre-trained network by replacing its classication part by a two streams network (a region proposal strea
2894607199	Weakly Supervised Object Detection in Artworks	2108745803	hat minimising L(w;b) amounts to seek a hyperplan separating the most positive element of each positive image from the least negative element of the negative image, sharing similar ideas as in MI-SVM [2] or LatentSVM [19]. The Tanhis here to mimic the SVM formulation in which only the worst margins count. We divide by n y i to account for unbalanced data. Indeed most example images are negative ones
2894607199	Weakly Supervised Object Detection in Artworks	639708223	hen applied in order to avoid redundant detections. The resulting multiple instance learning method is called MI-max. 3.2 Implementation details Faster R-CNN We use the detection network Faster R-CNN [42]. We only keep its region proposal part (RPN) and the features corresponding to each proposed region. In order to yield and ecient and exible learning of new classes, we choose to avoid retraining or
2894607199	Weakly Supervised Object Detection in Artworks	2101611867	IL pooling strategy. Table 2. Watercolor2k (test set) Average precision (%). Comparison of the proposed MI-max method to alternative approaches. Net Method bike bird car cat dog person mean VGG WSDDN [5] 10 1.5 26.0 14.6 0.4 0.5 33.3 12.7 SSD DT+PL [29] 10 76.5 54.9 46.0 37.4 38.5 72.3 54.3 RES-152-COCO MAX [12] 74.0 34.5 26.8 17.8 21.5 21.0 32.6 MI-SVM [2] 66.8 23.5 6.7 13.0 8.4 14.1 22.1 MI-max [Ou
2894607199	Weakly Supervised Object Detection in Artworks	2101611867	initialisation phase which is crucial due to the non-convexity of the problem. In [1], it is proposed to learn to detect new specic classes by taking advantage of the knowledge of wider classes. In [5] a weakly supervised deep detection network is proposed based on Fast R-CNN [25]. Those works have been improved in [50] by adding a multistage classier renement. In [9] a multi-fold split of the tr
2894607199	Weakly Supervised Object Detection in Artworks	2253323104	l discriminative patches [3,10]. In order to enhance the generalisation capacity of these approaches, it was proposed in [54] to model object through graphs of labels. More generally, it was shown in [26] that structured models are more prone to succeed in cross-domain recognition than appearance-based models. Next, several works have tried to transfer the tremendous classication capacity of convolut
2894607199	Weakly Supervised Object Detection in Artworks	639708223	lem of object detection in paintings, that is, being able to both localise and recognise objects, has been less studied. In [12], it is shown that applying a pre-trained object detector (Faster R-CNN [42]) and then selecting the localisation with highest condence can yield correct detections of PASCAL VOC classes. Other works attacked this dicult problem by restricting it to a single class. In [23],
2894607199	Weakly Supervised Object Detection in Artworks	639708223	r method on paintings, we start with a sanity check experiment on PASCAL VOC2007 [18]. We compare our weakly supervised approach, MI-max, to the plain application of the fully supervised Faster R-CNN [42] and to the weakly supervised MAX procedure recalled above. We perform the comparison using two dierent architectures (for the three methods), RES-101-VOC07 and RES-512-COCO, as explained in the prev
2894607199	Weakly Supervised Object Detection in Artworks	2518995963	n works, the object detection task (classifying and localising an object) has been less studied in the case of paintings, although exciting results have been obtained, again using transfer techniques [12,52,29]. Methods that detect objects in photographs have been developed thanks to massive image databases on which several classes (such as cats, people, cars) arXiv:1810.02569v1 [cs.CV] 5 Oct 2018 2 N. Gont
2894607199	Weakly Supervised Object Detection in Artworks	2518995963	of the network using manual location annotations on their database. In Table 3, one can see that our approach MI-max yields detection results that are very close to the fully supervised results from [52], despite a much lighter training procedure. In particular, as already explained, our procedure can be trained directly on large, globally annotated database, for which manually entering instance-leve
2894607199	Weakly Supervised Object Detection in Artworks	2110807182	ographic elements. 2 Related Work Object recognition and detection in artworks Early works on cross-domain (or cross-depiction) image comparisons were mostly concerned with sketch retrieval, see e.g. [13]. Various local descriptors were then used for comparing and classifying images, such as part-based models [46] or mid-level discriminative patches [3,10]. In order to enhance the generalisation capac
2894607199	Weakly Supervised Object Detection in Artworks	2108745803	onding statistical problem is referred to as multiple instance learning (MIL) [14]. A well-known solution to this problem through a generalisation of Support Vector Machine (SVM) has been proposed in [2]. Several approximations of the involved non-convex problem have been proposed, see e.g. [22] or the recent survey [7]. Recently, this problem has been attacked using classication and detection neura
2894607199	Weakly Supervised Object Detection in Artworks	2518995963	performances. 4.2 Detection evaluation on Watercolor2k and People-Art databases We compare our approach with two recent methods performing object detection in artworks, one in a fully supervised way [52] for detecting people, the other using a (partly) weakly supervised method to detect several VOC classes on watercolor images [29]. For the learning stage, the rst approach uses instance-level annotat
2894607199	Weakly Supervised Object Detection in Artworks	2194775991	rained in a weakly supervised manner) for two networks RES-101-VOC07 and RES152-COCO. Net Method aero bicy bird boa bot bus car cat cha cow dtab dog hors mbik pers plnt she sofa trai tv mean RES- FSD [27] 73.6 82.3 75.4 64.0 57.4 80.2 86.5 86.2 52.7 85.2 66.9 87.0 87.1 82.9 81.2 45.7 76.8 71.2 82.6 75.5 75.0 101- MAX 20.8 47.0 26.1 20.2 8.3 41.1 44.9 60.1 31.7 54.8 46.4 42.9 62.2 58.7 20.9 21.6 37.6 1
2894607199	Weakly Supervised Object Detection in Artworks	2518995963	s (54.3 % versus 48.9 %) with respect to the method [29], which is retrained using style transfer and instance-level annotations on photographs. Experiment 2 : People-Art This database, introduced in [52], is made of artistic images and bounding boxes for the single class person. This database is particularly challenging because of its high variability in styles and depiction techniques. The method in
2894607199	Weakly Supervised Object Detection in Artworks	639708223	tion by transfer learning In this section, we propose our approach to the weakly supervised detection of visual category in paintings. In order to perform transfer learning, we rst apply Faster R-CNN [42] (a detection network trained on photographs) which is used as a feature extractor, in the same way as in [12]. This results in a set of candidate bounding boxes. For a given visual category, the goal
2894607199	Weakly Supervised Object Detection in Artworks	2108745803	tive and negative sets of detections (bounding boxes) for the angel category. 3.1 Multiple Instance Learning The usual way to perform MIL is through the resolution of a non-convex energy minimisation [2], although ecient convex relaxations have been proposed [30]. One disadvantage of these approaches is their heavy computational cost. In what follows, we propose a simple and fast heuristic to this p
2894607199	Weakly Supervised Object Detection in Artworks	2161753062	tuations, such as Child Jesus, the crucixion of Jesus, Saint Sebastian, etc. Although there has been a recent eort to increase open-access databases of artworks by academia and/or museums workforce [38,11,32,37,48,44,17,39], they usually don’t include systematic and reliable keywords. One exception is the database from the Rijkmuseum, with labels based on the IconClass classication system [28], but this database is mos
2894607199	Weakly Supervised Object Detection in Artworks	2101104841	ximations of the involved non-convex problem have been proposed, see e.g. [22] or the recent survey [7]. Recently, this problem has been attacked using classication and detection neural networks. In [47], it is proposed to learn a smooth version of an SVM on the features from R-CNN [24] and to focus on the initialisation phase which is crucial due to the non-convexity of the problem. In [1], it is pr
2894711731	Efficient and Accurate Abnormality Mining from Radiology Reports with Customized False Positive Reduction.	2130140624	(RADA) extracts medical concepts through a specialized glossary of domain concepts, attributes, and predeﬁned grammar rules (Johnson et al., 1997). Breast Imaging Reporting and Data System (BI-RADS) (Nassif et al., 2009) uses a specialized lexicon, syntax analyzer, concept ﬁnder, and negation detector to extract information from Mammography reports. Medical Language Extraction and Encoding System (MEDLEE) extracted c
2894711731	Efficient and Accurate Abnormality Mining from Radiology Reports with Customized False Positive Reduction.	1950048153	eresting approaches have been proposed to tackle different aspects of this complex problem. Negation detection has become an important area of focus leading to the development of tools such as NegEx (Chapman et al., 2001b; 2013; 2001a) that ﬁnds ‘negation triggers’ in clinical text and identiﬁes the concepts or entities within the scope of the trigger. MetaMap, cTakes, and other lexicon-based tools now integrate NegE
2894711731	Efficient and Accurate Abnormality Mining from Radiology Reports with Customized False Positive Reduction.	2004111828	es and scoring systems to rank and select candidate mappingsAronson &amp; Lang (2010a), the dominant approach to solving this problem is context-sensitive entity extraction (Word-Sense Disambiguation)Finkel et al. (2004) through the use of neural embeddings and sequence models leveraging the power of deep learning or probilistic modelsChen et al. (2018); Esuli et al. (2013); Gehrmann et al. (2018); Wu et al. (2017).
2894711731	Efficient and Accurate Abnormality Mining from Radiology Reports with Customized False Positive Reduction.	2115570566	tity extraction (Word-Sense Disambiguation)Finkel et al. (2004) through the use of neural embeddings and sequence models leveraging the power of deep learning or probilistic modelsChen et al. (2018); Esuli et al. (2013); Gehrmann et al. (2018); Wu et al. (2017). These methods, when used individually or in combination with traditional approachesXia et al. (2013) led to cleaner concept extraction with less noisy and f
2894711731	Efficient and Accurate Abnormality Mining from Radiology Reports with Customized False Positive Reduction.	2049521800	usable for downstream clinical analyis tasks. The Radiology Analysis tool (RADA) extracts medical concepts through a specialized glossary of domain concepts, attributes, and predeﬁned grammar rules (Johnson et al., 1997). Breast Imaging Reporting and Data System (BI-RADS) (Nassif et al., 2009) uses a specialized lexicon, syntax analyzer, concept ﬁnder, and negation detector to extract information from Mammography rep
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2096140469	-range role of the model in performing alignment and language modeling along with acoustic to character label mapping at each iteration. In this paper, we explore the multilingual training approaches [9, 10, 11] used in hybrid DNN/RNN-HMMs to incorporate them into the seq2seq models. In a context of applications of multilingual approaches towards seq2seq model, CTC is mainly used instead of the attention mod
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2102113734	deling 1. INTRODUCTION The sequence-to-sequence (seq2seq) model proposed in [1, 2, 3] is a neural architecture for performing sequence classiﬁcation and later adopted to perform speech recognition in [4, 5, 6]. The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classiﬁ
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2327501763	e main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classiﬁcation (CTC) [6, 5] and attention [4, 7] based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models re
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2094147890	gual CTC is proposed in [12], which uses a universal phoneset, FST decoder and language model. The z All three authors share equal contribution authors also use linear hidden unit contribution (LHUC) [13] technique to rescale the hidden unit outputs for each language as a way to adapt to a particular language. Another work [14] on multilingual CTC shows the importance of language adaptive vectors as a
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2131774270	ing window of size 25 ms with 10ms stride. KALDI toolkit [25] is used to perform the feature processing. The fbank features are then fed to a seq2seq model with the following conﬁguration: The Bi-RNN [26] models mentioned above uses a LSTM [27] cell followed by a projection layer (BLSTMP). In our experiments below, we use only a character-level seq2seq model trained by CTC and attention decoder. Thus
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	2516608830	onal LSTM (BLSTM) or deep CNN followed by BLSTMs. Convolutional neural networks (CNN) has achieved great success in image recognition [21]. Previous studies applying CNN in seq2seq speech recognition [22] also showed that incorporating a deep CNNs in the encoder could further boost the performance. In this work, we investigate the effect of convolutional layers in joint CTC-attention framework for mul
2894835365	Multilingual Sequence-to-Sequence Speech Recognition: Architecture, Transfer Learning, and Language Modeling	1524333225	in this work for training and evaluation. 80 dimensional Mel-ﬁlterbank (fbank) features are then extracted from the speech samples using a sliding window of size 25 ms with 10ms stride. KALDI toolkit [25] is used to perform the feature processing. The fbank features are then fed to a seq2seq model with the following conﬁguration: The Bi-RNN [26] models mentioned above uses a LSTM [27] cell followed by
2895467332	Unsupervised Machine Learning of Open Source Russian Twitter Data Reveals Global Scope and Operational Characteristics.	2168400688	e run-up to the election. DISCUSSION Twitter and other social media analysis is an extensively studied area due to the relative simplicity of extracting large volumes of data. By way of example, [3], [13], [14], [15], [16], [17], 1Mean correction removes the signal DC component allowing for enhanced analysis. 2Manifold learning is (essentially) non-linear data embedding and clustering on a high-dimens
2895467332	Unsupervised Machine Learning of Open Source Russian Twitter Data Reveals Global Scope and Operational Characteristics.	2105745072	ection. DISCUSSION Twitter and other social media analysis is an extensively studied area due to the relative simplicity of extracting large volumes of data. By way of example, [3], [13], [14], [15], [16], [17], 1Mean correction removes the signal DC component allowing for enhanced analysis. 2Manifold learning is (essentially) non-linear data embedding and clustering on a high-dimensional manifold, ra
2895467332	Unsupervised Machine Learning of Open Source Russian Twitter Data Reveals Global Scope and Operational Characteristics.	2112896229	the election. DISCUSSION Twitter and other social media analysis is an extensively studied area due to the relative simplicity of extracting large volumes of data. By way of example, [3], [13], [14], [15], [16], [17], 1Mean correction removes the signal DC component allowing for enhanced analysis. 2Manifold learning is (essentially) non-linear data embedding and clustering on a high-dimensional manifo
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2556691798	To acquire the choice-aware passage embedding cpi , we utilize dot product between non-linear mappings of word embeddings to compute the attention scores for the passage [9].
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2551396370	Many architectures on MRC follow the process of representation, attention, fusion, and aggregation [16,24,27,5,20,25].
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2551396370	BiDAF [16] fuses the passage-aware question, the question-aware passage, and the original passage in context layer by concatenation, and then uses a BiLSTM for aggregation.
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	1525961042,2557764419,2606964149	Several datasets have been constructed for testing the comprehension ability of a system, such as MCTest [15], SQuAD [14], BAbI [22], TriviaQA [6], RACE [8], and NewsQA [17].
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2470673105	Following [26], we summarize the global perspective representation {gi} 1 to a fixed length vector r.
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2250539671	Glove word embedding We use the 300-dimensional Glove word embeddings trained from 840B Web crawl data [13].
2895696039	Multi-Perspective Fusion Network for Commonsense Reading Comprehension	2064675550	Thirdly, the original representations and interaction representations are fused together and then aggregated by a Bidirectional Long Short-Term Memory Network (BiLSTM) [4] to get high-order semantic information.
2896457183	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	2108598243	ghtto-left LSTM to generate features for downstream tasks. Among three, only BERT representations are jointly conditioned on both left and right context in all layers. models pre-trained on ImageNet (Deng et al., 2009;Yosinski et al.,2014). 3 BERT We introduce BERT and its detailed implementation in this section. We ﬁrst cover the model architecture and the input representation for BERT. We then introduce the pre-
2896457183	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	2626778328	n BERT and OpenAI GPT are discussed in Section3.6. 3.1 Model Architecture BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described inVaswani et al. (2017) and released in the tensor2tensor library.2 Because the use of Transformers has become ubiquitous recently and our implementation is effectively identical to the original, we will omit an exhaustive
2896457183	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	2131744502	scratch (Turian et al.,2010). These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al.,2015;Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). As with traditional word embeddings, these learned representations are also typically used as features in a downstream model. ELMo (Peters et al.,2017) generalizes traditional word embedding researc
2896457183	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	2610748790	solute improvement), outperforming human performance by 2.0. 1 Introduction Language model pre-training has shown to be effective for improving many natural language processing tasks (Dai and Le,2015;Peters et al., 2017,2018;Radford et al.,2018;Howard and Ruder,2018). These tasks include sentence-level tasks such as natural language inference (Bowman et al.,2015;Williams et al.,2018) and paraphrasing (Dolan and Broc
2896457183	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	1566289585	training Procedure The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the concatenation of BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shufﬂed senten
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	2161431802	2018). In addition, recent work has studied the patterns which evoke certain polarities (Reed et al.,2017), the desires which make events affective (Ding et al., 2017), the emotions caused by events (Vu et al., 2014), or, conversely, identifying events or reasoning behind particular emotions (Gui et al., 2017). Compared to this prior literature, our work uniquely learns to model intents and reactions over a diver
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	2530157984	e ads for guns which might increase social distress (Goel and Isaac,2016). Also, pragmatic inference is a necessary step toward automatic narrative understanding and generation (Tomai and Forbus,2010;Ding and Riloff, 2016;Ding et al.,2017). However, this type of social commonsense reasoning goes far beyond the widely studied entailment tasks (Bowman et al., 2015;Dagan et al.,2006) and thus falls outside the scope of e
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	2748618075	et al.,2017), the desires which make events affective (Ding et al., 2017), the emotions caused by events (Vu et al., 2014), or, conversely, identifying events or reasoning behind particular emotions (Gui et al., 2017). Compared to this prior literature, our work uniquely learns to model intents and reactions over a diverse set of events, includes inference over event participants not explicitly mentioned in text,
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	2210838531	event descriptions. To further evaluate the geometry of the embedding space, we analyze interpolations between pairs of event phrases (from outside the train set), similar to the homotopic analysis ofBowman et al. (2016). For a handful of event pairs, we decode intents, reactions for PersonX, and reactions for other people from points sampled at equal intervals on the interpolated line between two event phrases. We s
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	1840435438	sk as predicting the textual descriptions of the implied commonsense instead of classifying various event attributes. Previous work in natural language inference has focused on linguistic entailment (Bowman et al., 2015;Bos and Markert,2005) while ours focuses on commonsense-based inference. There also has been inference or entailment work that is more generation focused: generating, e.g., entailed statements (Zhang
2896555051	Event2Mind: Commonsense Inference on Events, Intents, and Reactions	1840435438	ve understanding and generation (Tomai and Forbus,2010;Ding and Riloff, 2016;Ding et al.,2017). However, this type of social commonsense reasoning goes far beyond the widely studied entailment tasks (Bowman et al., 2015;Dagan et al.,2006) and thus falls outside the scope of existing benchmarks. In this paper, we introduce a new task, corpus, arXiv:1805.06939v1 [cs.CL] 17 May 2018 PersonX’s Intent Event Phrase Person
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2251103205	al. [3] who used an extended application, which they call Dynamic Convolutional Neural Network (DCNN) to deal with various input lengths and short- and long-term linguistic dependencies. Wang et al. [32] perform clustering in an embedding space to derive semantic features which they then feed to a CNN with a convolutional layer, followed by k-max pooling and a softmax layer for classiﬁcation. Charact
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2118020653	of the bAbI dialog dataset. 1 Introduction Recent advances in natural language processing make heavy use of neural network models. Solutions for tasks such as semantic tagging [8], text classiﬁcation [26] and sentiment analysis [9] rely on either Recurrent Neural Network (RNN) or Convolutional Neural Network (CNN) variants. In the latter case, the vast majority of the proposed models are based on char
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2194775991,2302255633	es of many lines (depending on ﬁlter size) of text. As far as the vanishing/exploding gradient is concerned, for large CNN architectures, we can easily take advantage of recent architectural advances [15, 16, 7], which speciﬁcally aim to improve its effects. In terms of linguistics, our approach is based on the distributional hypothesis [14], where our model produces compositional hierarchies of document sem
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2120615054	feature maps produced are then fed to a softmax layer for classiﬁcation. Despite its simplicity, this architecture exhibited good performance. Sentence modeling was further explored by Blunsom et al. [3] who used an extended application, which they call Dynamic Convolutional Neural Network (DCNN) to deal with various input lengths and short- and long-term linguistic dependencies. Wang et al. [32] per
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2061873838	generated text (not included in the training set) the model used for testing. For these examples, the table shows model predictions after the model was trained on the Amazon Review Polarity data set [24] containing reviews of products in various product categories. The dataset is used for binary (positive/negative) sentiment classiﬁcation of text and the metric (positivity score) is the class probabi
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	1810499140	ns directly on high-dimensional text data represented by one-hot vectors. An architectural variation was also proposed for adapting a bag-of-words model in the convolutional layers. Johnson and Zhang [20] used CNNs for sentiment and topic classiﬁcation in a semi-supervised framework, where they retained the representations derived by a CNN over text regions, and which they then integrated into the sup
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2626778328	Oct 2018 works devising yet more ways to improve performance in recurrent models [27, 30, 33]. Moreover, many state of the art recurrent models rely on the attention mechanism to improve performance [1, 23, 31], which places an additional computational burden on the overall method. To tackle the above problems, we use CNNs to process the entire text at once as an image. In other words, we convert our textua
2896639782	Image-based Natural Language Understanding Using 2D Convolutional Neural Networks.	2302255633	task they called large-scale authorship attribution. Bjerva et al. [2] introduced a semantic tagging method, which combines (a) stacked neural network models, consisting of a vanilla CNN or a ResNet [16] in the lower level for character-/word-level feature extraction and a bidirectional Gated Recurrent Unit (GRU) in the higher level, with (b) a residual bypass function which preserves the saliency of
2896967004	Analysis of Railway Accidents' Narratives Using Deep Learning	2104555393	an approach on crash report data between 2004 and 2005 in Queensland Australia. They used the Leximancer text mining tool to produce cluster maps and most frequent terms and clusters. Other research [9] introduced concept of chain queries that utilize text retrieval methods in combination with linkanalysis techniques. Recent work by Brown [10] provided a text analysis of narratives in accident repor
2896967004	Analysis of Railway Accidents' Narratives Using Deep Learning	2250539671	ec method provides a very powerful relationship discovery approach. 3) Global Vectors for Word Representation (GloVe): Another powerful word embedding technique is Global Vectors (GloVe) presented in [22]. The approach is very similar to the word2vec method where each word is represented by a high dimension vector, and trained based on the surrounding words over a huge corpus. The pre-trained embeddin
2896967004	Analysis of Railway Accidents' Narratives Using Deep Learning	2120615054	et al., who developed character-level CNN for text classiﬁcation [13]. Other work has provided additional extensions to include use of dynamic k-max pooling for the architecture in modeling sentences [14]. In RNN, the output from a layer of nodes can reenter as input to that layer. This architecture makes these deep learning models particularly suited for applications with sequential data including, t
2896967004	Analysis of Railway Accidents' Narratives Using Deep Learning	1523493493	ey simply return the minimum, average or maximum of the input values. Pooling reduces computation complexity, and memory use. Additionally, it can improve performance on translated and rotated inputs [26]. Pooling can be repeated multiple times depending on the size of input and the complexity of the model. The ﬁnal layer is traditional fully connected layers taking a ﬂattened output from the last poo
2896967004	Analysis of Railway Accidents' Narratives Using Deep Learning	2144012961	a layer of nodes can reenter as input to that layer. This architecture makes these deep learning models particularly suited for applications with sequential data including, text mining. Irsoy et al. [15] showed an implementation of deep RNN structure for sentiment analysis of sentences. The authors of this paper compared their approach to the state-of-theart conditional random ﬁelds baselines and sho
2897139265	Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting	2064675550	sentence in the documents (details in supplementary). To further incorporate global context of the document and capture the long-range semantic dependency between sentences, a bidirectional LSTM-RNN (Hochreiter and Schmidhuber, 1997;Schuster et al.,1997) is applied on the convolutional output. This enables learning a strong representation, denoted as h jfor the j-th sentence in the document, that takes into account the context o
2897269555	A DEEP ENSEMBLE MODEL WITH SLOT ALIGNMENT FOR SEQUENCE-TO-SEQUENCE NATURAL LANGUAGE GENERATION	2060833990	training set. 6 Evaluation Researchers in NLG have generally used both automatic and human evaluation. Our results report the standard automatic evaluation metrics: BLEU (Papineni et al.,2002), NIST (Przybocki et al., 2009), METEOR (Lavie and Agarwal,2007), and ROUGE-L (Lin,2004). For the E2E dataset experiments, we additionally report the results of the human evaluation carried out on the CrowdFlower platform as a part
2897289333	Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity.	1948566616	e of a restaurant or a database entity is often expected to appear verbatim in the generated text. Word-based models, in contrast, have to make use of delexicalization during pre- and postprocessing (Wen et al., 2015b;Duˇsek and Jurc ´ıcek,2016) or have to apply dedicated copy mechanisms (Gu et al.,2016;See et al.,2017;Wiseman et al.,2017) to handle open vocabularies. The other side of the coin is that sequences
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2113814270	able I. All these datasets have contributed greatly to the progress of lipreading. In this part, we will give a brief review of these well-known datasets shown in the table. AVICAR [12] and AVLetters [15] were proposed in 2004 and 2002 respectively and were widely used in an early period. The words in these two datasets are 10 digits and 26 letters from 100 speakers and 10 speakers respectively. These
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2136155248	Classes # of Speakers Resolution Pose Envir. Color/Gray Best Perf. Year AVICAR [12] 10 100 - Controlled In-car Gray 37.9% 2004 AVLetters [15] 26 10 Fixed 80 60 Controlled Lab Gray 43.5% 2002 OuluVS1 [22] 10 20 Fixed 80 60 Controlled Lab Color 91.4% 2009 OuluVS2 [1] 10 53 Fixed (6 different sizes) Controlled Lab Color 93.2% 2015 LRW [5] 500 &gt; 1000 Fixed 256 256 Natural TV Color 83.0% 2016 LRW-1000
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2136155248	these two datasets are 10 digits and 26 letters from 100 speakers and 10 speakers respectively. These two datasets provided an initial impetus for the early progress in automatic lipreading. OuluVS1 [22], released in 2009, consists of 10 phrases spoken by 20 subjects with 817 sequences in total. This dataset provides cropped mouth region sequences, which brings much convenience to related researchers
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2136155248	es to obtain good representations. Some well-known features include the Discrete Cosine Transform (DCT) [17], active appearance model (AAM), motion history image (MHI) [9], Local Binary Pattern (LBP) [22] and vertical optical ﬂow [19], to name a few. With the rapid development of deep learning technologies, more and more work began to perform endto-end recognition with the help of deep neural networks
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	1994002998	images. With this inspiration, some early lipreading work [13], [16], [8] try to obtain a discriminative representation of each frame individually with some pre-trained 2D CNN models, such as VGGNet [3] and residual networks [11]. One representative work is the multi-tower structure proposed by Chung and Zisserman in [8], where each tower takes a single frame or a T-channel image as input with each
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2106284211	ision task for decades. Most early methods focus on designing appropriate hand-engineered features to obtain good representations. Some well-known features include the Discrete Cosine Transform (DCT) [17], active appearance model (AAM), motion history image (MHI) [9], Local Binary Pattern (LBP) [22] and vertical optical ﬂow [19], to name a few. With the rapid development of deep learning technologies,
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2113814270	ISTING WELL-KNOWN WORD-LEVEL LIPREADING DATASETS Datasets # of Classes # of Speakers Resolution Pose Envir. Color/Gray Best Perf. Year AVICAR [12] 10 100 - Controlled In-car Gray 37.9% 2004 AVLetters [15] 26 10 Fixed 80 60 Controlled Lab Gray 43.5% 2002 OuluVS1 [22] 10 20 Fixed 80 60 Controlled Lab Color 91.4% 2009 OuluVS2 [1] 10 53 Fixed (6 different sizes) Controlled Lab Color 93.2% 2015 LRW [5] 500
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2136155248	r length and the best performance on this dataset in terms of classiﬁcation accuracy has achieved as high as 83% in merely two years. Some other popular word-level lipreading datasets include OuluVS1 [22] and OuluVS2 [1], which are proposed in 2009 and 2015 respectively. There are 10 classes in both datasets and the state-of-the-art performance has achieved an accuracy of more than 90%. These exciting
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2168824259	propriate hand-engineered features to obtain good representations. Some well-known features include the Discrete Cosine Transform (DCT) [17], active appearance model (AAM), motion history image (MHI) [9], Local Binary Pattern (LBP) [22] and vertical optical ﬂow [19], to name a few. With the rapid development of deep learning technologies, more and more work began to perform endto-end recognition with
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2121486117	rating visual information in audio-based speech recognition systems can bring obvious performance improvements, especially in cases where multiple speakers are present or the acoustic signal is noisy [10], [14]. Lipreading also plays an important role in several other scenarios, such as aids for hearing-impaired persons, analysis of silent movies, liveness veriﬁcation in video authentication systems,
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2194775991	tion, some early lipreading work [13], [16], [8] try to obtain a discriminative representation of each frame individually with some pre-trained 2D CNN models, such as VGGNet [3] and residual networks [11]. One representative work is the multi-tower structure proposed by Chung and Zisserman in [8], where each tower takes a single frame or a T-channel image as input with each channel corresponding to a
2897492880	LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.	2551572271	ty every 15 frames with the CNN-based face detector in SeetaFaceEngine2. D. Audio-to-Video Synchronization After the above process, we check for the synchronization issues and ﬁnd that similar to [7] [4], the audio and video streams in the collected videos can be out of sync, with the largest offset being less than one second. To tackle this problem, we introduce the SyncNet model in [6], which extra
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2631715525	&amp; Schmidhuber, 1997). This choice has the added advantage that initial entity representations share information through context, unlike in previous models (Henaff et al., 2017; Das et al., 2017; Bansal et al., 2017). Entities in the dataset can be multi-word expressions (e.g., electric oven). To obtain a single representation, we concatenate the contextualized hidden vectors corresponding to the start and end sp
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2563734883	with a bi-directional LSTM (Hochreiter &amp; Schmidhuber, 1997). This choice has the added advantage that initial entity representations share information through context, unlike in previous models (Henaff et al., 2017; Das et al., 2017; Bansal et al., 2017). Entities in the dataset can be multi-word expressions (e.g., electric oven). To obtain a single representation, we concatenate the contextualized hidden vecto
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2896391192	l LSTM (Hochreiter &amp; Schmidhuber, 1997). This choice has the added advantage that initial entity representations share information through context, unlike in previous models (Henaff et al., 2017; Das et al., 2017; Bansal et al., 2017). Entities in the dataset can be multi-word expressions (e.g., electric oven). To obtain a single representation, we concatenate the contextualized hidden vectors corresponding t
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2631715525	location-describing span of text in the recipe paragraph. 4 MODEL KG-MRC tracks the temporal state change of entities in procedural text. Naturally, the model is entity-centric (Henaff et al., 2017; Bansal et al., 2017): it associates each participant entity of the procedural text with a unique node and embedding in its internal graph. KG-MRC is also equipped with a neural machine reading comprehension model which i
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2563734883	model aims to ﬁnd the location-describing span of text in the recipe paragraph. 4 MODEL KG-MRC tracks the temporal state change of entities in procedural text. Naturally, the model is entity-centric (Henaff et al., 2017; Bansal et al., 2017): it associates each participant entity of the procedural text with a unique node and embedding in its internal graph. KG-MRC is also equipped with a neural machine reading compr
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2563734883	nguage is generated synthetically over a small lexicon, and hence models trained on bAbI often do not generalize well when tested on real-world data. For example, state-of-the-art models like ENTNET (Henaff et al., 2017) and Query Reduction Networks (Seo et al., 2017b) fail to perform well on PROPARA. PROREAD (Berant et al., 2014) introduced the PROCESSBANK dataset, which contains paragraphs of procedural text as in
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2252016937	well when tested on real-world data. For example, state-of-the-art models like ENTNET (Henaff et al., 2017) and Query Reduction Networks (Seo et al., 2017b) fail to perform well on PROPARA. PROREAD (Berant et al., 2014) introduced the PROCESSBANK dataset, which contains paragraphs of procedural text as in PROPARA. However, this earlier task involves mining arguments and rela2 tions from events, not tracking the dyna
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2563734883	which time-step an entity moves. Similarly on the latter task, KG-MRC obtains a 5.7% relative improvement over PROSTRUCT and 41% relative improvement over other entity-centric models such as ENTNET (Henaff et al., 2017). On the RECIPES dataset, the same model obtains competitive performance. 2 RELATED WORK There are few datasets that address the challenging problem of tracking entity state changes. The bAbI dataset
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2563734883	wo PROPARA tasks proposed by Dalvi et al. (2018) and Tandon et al. (2018), respectively, and ﬁnd that our single model outperforms each of the above models on their respective tasks of focus. ENTNET (Henaff et al., 2017) and query reduction networks (QRN) (Seo et al., 2017b) are two state-of-the-art entity-centric models for the bAbI dataset. ENTNET maintains a dynamic memory of hidden states with a gated update to t
2897513992	Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension	2194775991	y and location nodes with their history summary, hl i;t 1 , using an LSTM unit. Next, the updated node information is attached to the entity and location representations through two residual updates (He et al., 2016). These propagate information between the entity and location representations; i.e., if two entities are at the same location, then the corresponding entity representations will receive a similar upda
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2493916176	beddingaverage. Sentence representation is obtained by taking each dimension as whether an individual word appears in the sentence; (ii) fasttext wordembedding average. Fasttext pretrained embedding (Bojanowski et al. 2017) is used in this setting. Both methods use cosine value over two sentence representations to measure similarity. Machine Translation Results We ﬁrstly describe our results on machine translation in th
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2149933564	we have different number of layers of sentence encoder trainable. We unfreeze starting from the last layer, last 2 layers and ﬁnally to all 6 layers since last layer contains least general knowledge (Yosinski et al. 2014). The result shows that the performance ﬂuctuates and decrease a lot if all layers are trainable. Unfreezing the last several layers can improve the performance for some settings but it does not have
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2626778328	echanism by a bidirectional model as an example. We follows the state-of-the-art NMT training scheme. The major differences between our shared encoder model training scheme and the Transformer model (Vaswani et al. 2017) lie on the usage of bilingual training data, subword technique (Sennrich, Haddow, and Birch 2015), source/ target side vocabularies, and training loss. Suppose we need to train a shared encoder model
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2286300105	ed to use the content of one sentence to guide the representation of the other, in which an attention feature matrix is learned to inﬂuence the convolution ﬁlters. Different to the previous methods, (Pang et al. 2016) takes into account the rich interaction structures in the text matching process since the interaction structures are compositional hierarchies in which higher level signals are obtained by composing
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2211192759	has been a growing interest in developing solutions for the task from both academia and industry. In particular, deep learning techniques have been used extensively in STS under supervised settings (Yin et al. 2016; Pang et al. 2016). The common approach is to take advantage of pretrained word embeddings such as Word2Vec (Mikolov et al. 2013) , therefrom a deep neural network is used to extract the sentence rep
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2211192759	ical semantics over sentences. Although outperforming many traditional methods, these prior works rarely consider the impact of the other sentence when deriving the sentence representation. Until in (Yin et al. 2016), an attention-based model is proposed to use the content of one sentence to guide the representation of the other, in which an attention feature matrix is learned to inﬂuence the convolution ﬁlters.
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2626778328	ine translation model. In this section, we describe how we design and train the shared encoder translation model. SharedEncoderModelArchitecture We adopt the stateof-the-art transformer architecture (Vaswani et al. 2017) for our translation model. To simplify our explanation, Figure 1 shows a special example of shared encoder translation model: a bidirectional translation model. Inspired by (Johnson et al. 2017), the
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2626778328	or both models. Top 50,000 and 30,000 tokens are kept for source and target vocabularies. Parameter Setting and Evaluation Metrics Transformer sentence encoder uses the transformer-base setting from (Vaswani et al. 2017): 512 hidden size, 512 embedding size, 2048 ﬁlter size, 8 heads for multihead attention, 6-layer encoder and 6-layer decoder. We train all the models with a batch size of 4096 tokens, and 0.0003 learn
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	630532510	nd its translations MachineTranslationDataSet We used Paracrawl data 1, which contains about 16 million parallel sentence pairs for English $Spanish model training. OpenSubtitle 2018 portion of OPUS (Tiedemann 2012) is used for English $ Arabic model training, which contains about 31.9 million parallel sentence pairs. Both the test data consist of 1,000 randomly sampled bilingual sentence pairs from the correspo
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2286300105	ng interest in developing solutions for the task from both academia and industry. In particular, deep learning techniques have been used extensively in STS under supervised settings (Yin et al. 2016; Pang et al. 2016). The common approach is to take advantage of pretrained word embeddings such as Word2Vec (Mikolov et al. 2013) , therefrom a deep neural network is used to extract the sentence representations as wel
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2153579005	ng techniques have been used extensively in STS under supervised settings (Yin et al. 2016; Pang et al. 2016). The common approach is to take advantage of pretrained word embeddings such as Word2Vec (Mikolov et al. 2013) , therefrom a deep neural network is used to extract the sentence representations as well as the interactions between them. Subsequently, a ﬁnal Multi-Layer Perceptron (MLP) is trained from the repre
2897527206	Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages.	2626778328	sentence encoder is also a popular research topic in recent years. Most works are usually based on a multi-task learning framework. In (Cer et al. 2018), two variants of encoding models, Transformer (Vaswani et al. 2017) and Deep Averaging Network (Iyyer et al. 2015), allow for the trade-offs between accuracy and efﬁciency of diverse tasks, such as sentiment analysis and natural language inference. (Subramanian et al
2897686202	Universal Language Model Fine-Tuning with Subword Tokenization for Polish.	172230311	on counting statistics. This were recently replaced with deep neural network for popular languages like English. However most of the literature devoted to the Polish language considers n-gram models [12, 18, 19, 21]. Brocki et al. [4] showed that a simple neural network (5 context words with 50 dimensional embeddings and one hidden layer) greatly outperforms a 4-gram solution on a Polish corpus. Regardless of pe
2897686202	Universal Language Model Fine-Tuning with Subword Tokenization for Polish.	2544860310	k was later extended to transfer learning and classiﬁcation by [6]. Transfer learning in language modeling was shown to beneﬁt from slanted triangular learning rates and other techniques described by [17], originally used to quickly train computer vision models with minimal resources. LSTM based language models can be improved with use of adaptive methods during inference (neural cache [5] and dynamic
2897686202	Universal Language Model Fine-Tuning with Subword Tokenization for Polish.	1816313093	r full words incapable of learning useful features. The most successful attempt was FastText, which uses pieces of words. Another approach to address inﬂections in Polish is to use byte pair encoding [16], character level language models [11] or unigram subword tokenization [8]. We used the unigram algorithm as its representation of Polish words most closely ﬁtted the training pipeline of ULMFiT, and
2897793276	Current Trends and Future Research Directions for Interactive Music.	2094220291	, Garavel argues that models based on process calculi have not found widespread use because there are many calculi and many variants for each calculus, making difficult to choose the most appropriate [32]. In addition, he argues that it is difficult to express an explicit notion of time and real-time requirements in process calculi. Finally, Garavel argues that existing tools for process calculi are n
2897793276	Current Trends and Future Research Directions for Interactive Music.	1601268560	5]12. Ntcc is not only useful for music semantic interaction, ntcc has also been used in other fields such as modeling molecular biology [76], analyzing biological systems [36] and security protocols [47]. Therefore, advances on the simulation of ntcc models will be useful not only for music interaction, but also for other fields. Automatic verification. A disadvantage of ntcc is the lack of automatic
2897793276	Current Trends and Future Research Directions for Interactive Music.	2256764323	that allow us to extend the interactive scores semantics with conditional branching and loops in a very precise and declarative way. Conditional-branching timed interactive scores were introduced in [102, 101]. Such an extension has operational semantics based on ntcc, but it misses an abstract semantics to understand the conflicts among the temporal objects that take place when modeling conditions and cho
2897793276	Current Trends and Future Research Directions for Interactive Music.	2127908756	behavior of a model and also to verify properties of the model. As an example, ntcc was used to verify properties of a musicological problem of westernafrican music [77]. The reader may also look at [78] and [80] for other examples of verification of music interaction systems. 4.3 Structural Definition of the Score Interactive scores are composed by temporal objects and temporal relations. We conside
2897793276	Current Trends and Future Research Directions for Interactive Music.	2627058984	capable of real-time interaction and being able to control music objects such as sound, video and lights. 22 There are some interpreters for ntcc, but they are not suitable for real-time interaction [54, 75]. We chose a real-time capable interpreter for ntcc, Ntccrt [100], to execute our models. Ntccrt is based on Gecode [90]: state-of-the-art in constraint propagation. Ntccrt programs can be compiled in
2897793276	Current Trends and Future Research Directions for Interactive Music.	2249115354	the CCP paradigm. Process calculi has been applied to the modeling of interactive music systems [104, 111, 99, 110, 10, 106, 100, 59, 97, 93, 95, 98, 11, 103, 94, 101, 102, 92] and ecological systems [107, 64, 109, 65, 108]. Although there are programming languages based on CCP, as Garavel argued, the explicit notion of time is missing in most process calculi and, unfortunately, it is also the case of CCP. In CCP it is
2897793276	Current Trends and Future Research Directions for Interactive Music.	1541907158,2208930366,2248653485,2256764323	concepts of CCP. As an example Mozart/Oz [74, 113] is a multiparadigm programming language inspired in the CCP paradigm. Process calculi has been applied to the modeling of interactive music systems [104, 111, 99, 110, 10, 106, 100, 59, 97, 93, 95, 98, 11, 103, 94, 101, 102, 92] and ecological systems [107, 64, 109, 65, 108]. Although there are programming languages based on CCP, as Garavel argued, the explicit notion of time is missing in most process calculi and, unfortuna
2897793276	Current Trends and Future Research Directions for Interactive Music.	2043859376	cs for interactive scores whose temporal object duration can be any interval of integers. Allombert et al. proposed temporal relations with flexible intervals with only {0}, [0,∞) and (0,∞) intervals [5, 4]. In fact, arbitrary integer intervals are not allowed in neither Virage nor i-score, only flexible-time intervals. To handle temporal relations with arbitrary intervals, Allombert proposed in [2] to
2897793276	Current Trends and Future Research Directions for Interactive Music.	2161484642	d a qualitative constraint is, for instance, “a point occurs strictly before another”. There are some well-known classes of qualitative constraints: interval-interval (also known as Allen’s relations [1], shown in Figure 13), point-to-point and point-interval. Interval-interval temporal relations were conceived to model dense (continuos) time, but they can also be used for discrete time. According to
2897793276	Current Trends and Future Research Directions for Interactive Music.	2287649557	duration of the pause. Haury’s work inspired Allombert et al.’s models of interactive scores. 5.4 Score following Another kind of systems capable of real-time interaction are score following systems [23]. To use such systems, we must first write a score for the musician and for the computer. During execution, such systems track the performance of a real instrument and they may play music associated t
2897793276	Current Trends and Future Research Directions for Interactive Music.	2248653485	e different media, are discrete time, and they are also perceived as discrete by the listeners. Problem with synchronization. There is another problem derived from the time scales, as we discussed in [103]. The description of a music scenario requires a consistent relationship between the representation of the scenario in the composition environment and the execution. Artistic creation requires a compo
2897793276	Current Trends and Future Research Directions for Interactive Music.	2256764323	e model. As an example, it is possible to model conditions and also preserve temporal properties over all the branches, for instance, that . In our first models of conditional branching, published in [101, 102], we allowed branches starting in the same point have different durations. We left aside such an approach because it makes many scores incoherent and unplayable. An advantage of our extension of inter
2897793276	Current Trends and Future Research Directions for Interactive Music.	65934101	e store. Rtcc is also capable of delays within a single time unit. Olarte et al. also extended Rueda’s ntcc model. They extended the model to change the hierarchy of temporal objects during execution [58]. The spirit of such a model is different: they focus on changing the structure of the score during execution to allow the user to “improvise” on a written piece, whereas we are interested on a simple
2897793276	Current Trends and Future Research Directions for Interactive Music.	2259671449	earning methods. An interactive machine improvisation system capable of real-time must perform two activities concurrently: stylistic learning and stylistic simulation. As an example, the Omax system [12, 48] and the Continuator [61] construct models to represent the sequences played by the musician and create their own sequences based on the musician’s style. 16 http://scrime.labri.fr/ 17 http://www.avid
2897793276	Current Trends and Future Research Directions for Interactive Music.	2287649557	ent mathematical models and software that will be presented in this article. Sequencers Pro Tools, Qlab, Ableton Live Computerassisted improvisation [12, 48, 61] Meta-instruments [39] Score following [23] Asynchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Temporal constraints [1, 51, 20] Interactive sco
2897793276	Current Trends and Future Research Directions for Interactive Music.	2119170644	be very expressive to model synchronous languages such as Lustre and Esterel [91]. There is also an interpreter to execute tcc models [89]. 36 The non-deterministic timed concurrent constraint (ntcc) [56] adds non-determinism and asynchrony to tcc. Ntcc has been extendedly used for musical applications. We chose ntcc to express operational semantics of interactive scores because it allows for verifica
2897793276	Current Trends and Future Research Directions for Interactive Music.	2143082793	ficult to perceive a notion of discrete time, useful to model reactive systems communicating with an external environment (e.g., motion sensors and speakers). The temporal concurrent constraint (tcc) [84] calculus circumvents this limitation by introducing the notion of discrete time as a sequence of time units. At each time unit, a CCP computation takes place, starting with an empty store (or one tha
2897793276	Current Trends and Future Research Directions for Interactive Music.	2103121043	fied in applications based upon informal specifications, as it is the case for most existing software for music scenarios with interactive controls. The following properties were already presented in [105]. • Properties of the traces of execution. • There exist a trace σ that contains a word w; for instance, the sequence of notes C-D-E is part of n traces of execution. • There exists n traces σ that co
2897793276	Current Trends and Future Research Directions for Interactive Music.	2043859376	interactive scores. It was originally developed in Lisp, and then it was 31 ported to C++ during the ANR Virage14 project in 2008. Allombert et al. introduced Iscore as a new tool that replaces Boxes [4]. The comparison with Boxes is given in detail in [8]. Iscore uses Petri nets as its underlying model because Allombert argued that solving constraint satisfaction problems during execution may be inc
2897793276	Current Trends and Future Research Directions for Interactive Music.	65934101,2103121043,2127908756,2627058984	ity, probabilistic behavior, hybrid systems, discrete time and real-time. We also argue, in favor of CCP, that there has been a growing interest for CCP models of music interaction in the last decade [77, 80, 78, 79, 75, 5, 100, 58, 59, 105]. CCP processes can be analyzed from both a behavioral and declarative point of view, making them suitable for simulation and for verification of properties. Some programming languages have also been
2897793276	Current Trends and Future Research Directions for Interactive Music.	2125415493	le. Sequencers Pro Tools, Qlab, Ableton Live Computerassisted improvisation [12, 48, 61] Meta-instruments [39] Score following [23] Asynchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Temporal constraints [1, 51, 20] Interactive scores [4, 105, 104, 59, 106] Table 2: Literature mapping of mathematical models and softwar
2897793276	Current Trends and Future Research Directions for Interactive Music.	65934101,2103121043,2127908756,2627058984	on Live Computerassisted improvisation [12, 48, 61] Meta-instruments [39] Score following [23] Asynchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Temporal constraints [1, 51, 20] Interactive scores [4, 105, 104, 59, 106] Table 2: Literature mapping of mathematical models and software for music interaction In what follows we briefly explain Exp
2897793276	Current Trends and Future Research Directions for Interactive Music.	2043859376	nchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Temporal constraints [1, 51, 20] Interactive scores [4, 105, 104, 59, 106] Table 2: Literature mapping of mathematical models and software for music interaction In what follows we briefly explain Experimental music, non-linear music, Electroacoustic music and interactive mu
2897793276	Current Trends and Future Research Directions for Interactive Music.	2161484642	nts p,q∈P. We use the notation for temporal constraints of duration. Temporal positions of p and q are said to be constrained by ν(q)=ν(p)+Δ. The set of all temporal relations is R. Allen’s relations [1] without disjunction, over discrete time, can be easily expressed as point-to-point relations [51]. Furthermore, with point-to-point relations we can express relations that cannot be expressed in Alle
2897793276	Current Trends and Future Research Directions for Interactive Music.	2127908756	onal semantics of interactive scores because it allows for verification of temporal properties; for instance, it has been used to model music improvisation systems and a western-african music problem [77, 78]. In addition, there is a real-time capable interpreter for ntcc [100], and verifications tools and techniques are being developed in the recently started Colciencia’s REACT+ project23. Finally, anoth
2897793276	Current Trends and Future Research Directions for Interactive Music.	2406812518	ped using interactive scores. One alternative is to use interactive scores for rhythmic exercises for music students, easily modeled by constraints. Anders et al. have already discussed this approach [63], but we believe that it could be improved by allowing user interactions and temporal relations, which is possible in interactive scores. Another possibility is using user gestures to generate Electro
2897793276	Current Trends and Future Research Directions for Interactive Music.	2126962488	a polynomial time and space complexity. In fact, Floyd-Warshall has a time complexity of , where n is the number of points of the score. There are faster algorithms for this problem in the literature [66, 118]; however, they are efficient to calculate if a STP has a solution, but do not guarantee that the constraint problem remains satisfiable when dispatching the events during the execution of a score. Fo
2897793276	Current Trends and Future Research Directions for Interactive Music.	2259671449	it is presented a literature mapping of the different mathematical models and software that will be presented in this article. Sequencers Pro Tools, Qlab, Ableton Live Computerassisted improvisation [12, 48, 61] Meta-instruments [39] Score following [23] Asynchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Tempo
2897793276	Current Trends and Future Research Directions for Interactive Music.	2103121043	We also recall the notation for temporal constraints: t+Δ={t&apos;|t&apos;=t+δ,δ∈Δ}. [Sorry. Ignored \begin{proof} ... \end{proof}] The proof above is presented for hierarchical interactive scores in [105]. 4.5 Some Properties of the Scenarios We insist that a motivation of defining an abstract semantics in event structures is to prove properties of the system execution; in particular, properties about
2897793276	Current Trends and Future Research Directions for Interactive Music.	2161484642	s [39] Score following [23] Asynchronous dataflow languages [88] Synchronous dataflow languages [37, 38, 33, 17, 41] Process calculi [60, 59, 80, 78, 79, 75, 5, 100, 58, 59, 105] Temporal constraints [1, 51, 20] Interactive scores [4, 105, 104, 59, 106] Table 2: Literature mapping of mathematical models and software for music interaction In what follows we briefly explain Experimental music, non-linear music
2897793276	Current Trends and Future Research Directions for Interactive Music.	2142423074	server. A well-known process calculus is the pi-calculus. Unfortunately, the pi-calculus is not well suited to model reactive systems with partial information. Concurrent constraint programming (CCP) [83] is a process calculus to model systems with partial information. In CCP, a system is modeled as a collection of concurrent processes whose interaction behavior is based on the information (represente
2897793276	Current Trends and Future Research Directions for Interactive Music.	2125415493	sing is to overcome the existing problems of the asynchronous dataflow languages mentioned. 5.6 Synchronous dataflow languages There are three well-known french synchronous languages: Esterel, Lustre [37, 38] and Signal [33]. Benveniste et al. discussed the advantages and limitations of such languages 12 years after they were conceived [17]. They argue that synchronous languages were designed to implement
2897793276	Current Trends and Future Research Directions for Interactive Music.	2043859376	with temporal relations; however, user interaction was not provided. A recent model of interactive scores [2], 20 that significantly improves user interaction, has inspired two applications: i-score [4] to compose and perform Electroacoustic music and Virage [6] to control live performances and interactive exhibitions. We give a further discussion on the history of interactive scores. Scenarios in i
2897793276	Current Trends and Future Research Directions for Interactive Music.	2062151215	tion community needed a software for composition capable of describing a hierarchy of temporal objects and capable of real-time interaction! In 2005, they introduced a new model of interactive scores [7], extending the previous model developed by Desainte-Catherine and Brousse, and following the concepts of Haury’s meta-instrument [39]. This model admits modification of the starting and ending times
2897793276	Current Trends and Future Research Directions for Interactive Music.	2129819331	we must travel 2500 years back in time. Desainte-Catherine et al. argued that this problem was already discussed by Parmenides of Elea and Heraclitus of Ephesus long before the invention of computers [28] . Problems with the time models. According to Desainte-Catherine et al., what we call today Tape music, that began by editing and mixing sounds in magnetic tapes, is composed in a writing-oriented ma
2897793276	Current Trends and Future Research Directions for Interactive Music.	65934101,2103121043,2127908756,2627058984	uch tasks. Ntcc belongs to a bigger family of process calculi called concurrent constraint programming (CCP). In the last decade, there has been a growing interest for CCP models of music interaction [77, 80, 78, 79, 75, 5, 100, 58, 59, 105]12. Ntcc is not only useful for music semantic interaction, ntcc has also been used in other fields such as modeling molecular biology [76], analyzing biological systems [36] and security protocols [4
2897793276	Current Trends and Future Research Directions for Interactive Music.	2151273784	well-known to be NP-complete. One alternative to cope with this problem is to do a static analysis; for instance, a space efficient backtrack-free representation for constraint satisfaction problems [16]; however, to achieve such as representation, the order on which the temporal objects are going to be executed must be foreknown. Nonetheless, there are some scores in which this is possible, but for
2897793276	Current Trends and Future Research Directions for Interactive Music.	1551938402	It is worth noticing that it may be also possible to model such changes in the structure during execution using a special kind of Petri nets in which tokens are also nets, introduced by Köhler et al. [43]. Finally, in 2009, Allombert explained in his Ph.D. the results published previously in his models [2]. He also introduced some ideas on how to deal with durations of arbitrary intervals, he introduc
2898152919	Parsing Coordination For Spoken Language Understanding	2170909726	alender today Ground Truth O B-C I-C CC B-C O O O O No adv loss O B-C B-C CC B-C O O O O Adv loss O B-C I-C CC B-C O O O O Table 5. Examples utterances where adversarial training helps based learning [24, 25], and support vector machines [26]. Use of sequence tagging techniques such as HMMs [27] and CRFs [28] has also been popular for similar tasks. Since popularization of DNN usage in NLP, RNN-based mode
2898152919	Parsing Coordination For Spoken Language Understanding	2251830157	ge of NLP tasks including parsing [33]. These embeddings are good at learning morphology of words, replacing POS-based features in many tasks. They also makes the models more robust to unknown words. [8] introduced character-based embeddings that is used in our setup and [10] is perhaps the most similar sequence labeling architecture to ones used for this work. More recent breakthroughs in dynamic wo
2898152919	Parsing Coordination For Spoken Language Understanding	2005708641	is generally solved as sequence tagging task [5, 6]. We experiment with various sequence tagging DNN architectures with different conﬁgurations, all of which are based on bi-directional LSTM models. [7]. There are three major ways that the models we experimented with were different from each other: 1.A character-level encoder: This component extracts a feature vector for each word from its character
2898152919	Parsing Coordination For Spoken Language Understanding	2251830157,2295030615	h as HMMs [27] and CRFs [28] has also been popular for similar tasks. Since popularization of DNN usage in NLP, RNN-based models have replaced CRF as the de-facto standard for sequence tagging models [8, 29, 10]. Many models employ RNN with CRF loss [29, 10]. More recently, non-recurrent models such as CNN [2, 30, 31] and transformer network [32] have become popular for sequence labeling tasks due to their e
2898152919	Parsing Coordination For Spoken Language Understanding	2133280805	ing a off-the-shelf constituency-based syntactic parser to parse coordination. Just as our DNN model, we test the parser whether it can reliably ﬁnd spans of conjuncts. We use popular Stanford parser [16] to parse our data. Using our training data, we can extract tree patterns that capture coordinating conjunctions and conjuncts. For example, it is obvious that “CC” is important category to look for i
2898152919	Parsing Coordination For Spoken Language Understanding	2099008367	loss O B-C I-C CC B-C O O O O Table 5. Examples utterances where adversarial training helps based learning [24, 25], and support vector machines [26]. Use of sequence tagging techniques such as HMMs [27] and CRFs [28] has also been popular for similar tasks. Since popularization of DNN usage in NLP, RNN-based models have replaced CRF as the de-facto standard for sequence tagging models [8, 29, 10]. M
2898152919	Parsing Coordination For Spoken Language Understanding	2626778328	RF as the de-facto standard for sequence tagging models [8, 29, 10]. Many models employ RNN with CRF loss [29, 10]. More recently, non-recurrent models such as CNN [2, 30, 31] and transformer network [32] have become popular for sequence labeling tasks due to their efﬁciency. Character-based embeddings have been found effective for wide-range of NLP tasks including parsing [33]. These embeddings are g
2898152919	Parsing Coordination For Spoken Language Understanding	2251830157	tuition that they will inform the model about morphology of words. To make our character-level features ﬁxed length, we concatenate the last state of forward LSTM with the ﬁrst state of backward LSTM [8]. This gives us a character-based representation of the word wchar i . We chose to have character-level encoder for some experiments and we left it out for other experiments. 2.A word-level encoder: T
2898209642	Named Person Coreference in English News.	2098345921	ared tasks on coreference (at CoNLL2011 and 2012 (Pradhanet al. 2014) ) use the average of three scores as their ofﬁ- cial evaluation: MUC (Vilain et al. 1995), B3 (Bagga and Baldwin 1998) and CEAFE (Luo 2005). Prior work (Moosavi and Strube 2016) discussed the shortcoming of these metrics and introduced the link entity aware (LEA) score to tackle them. Below we describe these and point out how they are de
2898209642	Named Person Coreference in English News.	2162638401	we later expand on, is similar to the vastly popular entity linking task, in which named mentions in text are linked to an abstract entity, such as one deﬁned in Wikipedia (Mihalcea and Csomai 2007; Han and Sun 2011; Durrett and Klein 2014; Pan et al. 2015; Radhakrishnan, Talukdar, and Varma 2018). Solution 1 Solution 2 Solution 3 R P F1 R P F1 R P F1 MUC 0.55 1 0.71 0.66 1 0.8 0.44 1 0.61 B-cub 0.5 1 0.66 0.56
2898209642	Named Person Coreference in English News.	2293004735	n, is similar to the vastly popular entity linking task, in which named mentions in text are linked to an abstract entity, such as one deﬁned in Wikipedia (Mihalcea and Csomai 2007; Han and Sun 2011; Durrett and Klein 2014; Pan et al. 2015; Radhakrishnan, Talukdar, and Varma 2018). Solution 1 Solution 2 Solution 3 R P F1 R P F1 R P F1 MUC 0.55 1 0.71 0.66 1 0.8 0.44 1 0.61 B-cub 0.5 1 0.66 0.56 1 0.72 0.34 1 0.51 CEAFm
2898209642	Named Person Coreference in English News.	2119465010	NPC can help ﬁnd articles in which the person of interest is the focus of discussion, mentioned by pronounsin addition to their full name.1 Peoplearealsooftentargetsforinformationextraction systems (Ji and Grishman 2011) and for knowledge base completion tasks (West et al. 2014). Yet about half of the references to a person in text are notbyname(cf.theﬁrst columnofTable1).Systems need to extract information about the
2898209642	Named Person Coreference in English News.	2112468585	om the news that contain information about a given person. More relevant sentences can be extracted if we know which pronouns and nominals refertothisperson.Similarly,creationofpropernoun ontologies (Mann 2002) can use patterns other than (proper noun - common noun) if other references to the entity are known. Coreference and Named Entities References to people are distributed quite differently from other t
2898209642	Named Person Coreference in English News.	2004763266	tations, which can also lead to clusters not having an associated name. People-mention coreference In a ﬁrst pass, NER-DE ﬁnds all spans of text that are PERSON named entities, using the Cogcomp NER (Ratinov and Roth 2009). We later use a dependency parser (Honnibaland Johnson 2015) to ﬁnd the noun phrase of which that named entity serves as the syntactic head. This is the smallest span that contains all descendants of
2898209642	Named Person Coreference in English News.	2131357087	and track entities, which we later expand on, is similar to the vastly popular entity linking task, in which named mentions in text are linked to an abstract entity, such as one deﬁned in Wikipedia (Mihalcea and Csomai 2007; Han and Sun 2011; Durrett and Klein 2014; Pan et al. 2015; Radhakrishnan, Talukdar, and Varma 2018). Solution 1 Solution 2 Solution 3 R P F1 R P F1 R P F1 MUC 0.55 1 0.71 0.66 1 0.8 0.44 1 0.61 B-cu
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	2122702954	amplesee[31,27]foradirectapproachand[13,26]viasemanticparsing. The elements of an entity relation graph are argument-relation-argumenttriples, sometimes referred to by relation paths [31]. Similar to [16] we base ourselves in a binary c ijk entityrelation entity + (a,u,v) = c′ ijk relation where c′ ijk:=c +1 Fig.2. Updates of entries in an entity relation cube Context Update for Lambdas and Vectors 15
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	1608322251	or contraction. The jury is still very much out on what are the best operations for composing vectors. [18] consider pointwise addition and multiplication of vectors, matrix multiplication is used in [1]. Such operations are available to our theory. The table for these will have a different H(c)column and will be the same in all the other columns. The H(c)columns for these models are given in Table 4
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	1604644367,2148932298	g or a Gellish network. We work with entities and relations extracted from text. Discovering such graphs from corpora of text in an automatic way has been subject of much recent research,forexamplesee[31,27]foradirectapproachand[13,26]viasemanticparsing. The elements of an entity relation graph are argument-relation-argumenttriples, sometimes referred to by relation paths [31]. Similar to [16] we base ou
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	2144373529	hat draw inspirations from type theory but whose major development is developing concretewaysofconstructinglinearandmultilinearalgebraiccounterpartsforthesyntactic types, e.g. matrices and tensors of [8,2] and relational clusters of [16]. 2 M. Sadrzadeh and R. Muskens What these approaches, [4,15,17] more than [2,16], miss is acknowledging the inherent gap between the contextual and truth conditional s
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	2122702954	heory but whose major development is developing concretewaysofconstructinglinearandmultilinearalgebraiccounterpartsforthesyntactic types, e.g. matrices and tensors of [8,2] and relational clusters of [16]. 2 M. Sadrzadeh and R. Muskens What these approaches, [4,15,17] more than [2,16], miss is acknowledging the inherent gap between the contextual and truth conditional semantics; they closely follow th
2898329759	Static and Dynamic Vector Semantics for Lambda Calculus Models of Natural Language.	2150406842	ork with entities and relations extracted from text. Discovering such graphs from corpora of text in an automatic way has been subject of much recent research,forexamplesee[31,27]foradirectapproachand[13,26]viasemanticparsing. The elements of an entity relation graph are argument-relation-argumenttriples, sometimes referred to by relation paths [31]. Similar to [16] we base ourselves in a binary c ijk en
2898695519	CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge.	1599016936	;Nguyen et al.,2016; Joshi et al.,2017), where an answer is extracted from a textual context using relatively little external knowledge. Other small benchmarks, such as the Winograd Scheme Challenge (Levesque, 2011) and COPA (Roemmele et al.,2011), targeted common sense more directly, but have been difﬁcult to collect at scale. Recently, larger datasets, such as SWAG, tackled commonsense knowledge about situatio
2898846200	An Empirical Exploration of Curriculum Learning for Neural Machine Translation.	2101105183	earning rate, only the 9https://github.com/kevinduh/ sockeye-recipes/tree/master/egs/ curriculum 10BLEU is the standard evaluation for machine translation based on n-gram precision; higher is better (Papineni et al., 2002). 117 is the lowest number of checkpoints required to discriminate between the different schedules. reverse schedule outperforms the baseline. Similar trends are observed with other difﬁculty criteria
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2399456070	conversational AI systems that are used to parse utterances in personal assistants is the identiﬁcation of what the user intends to do (the intent) as well as the arguments of the intent (the slots) [23, 19]. For example, for a request such as Set an alarm for tomorrow at 7am a ﬁrst step in fulﬁlling such a request is to identify that the user’s intent is to set an alarm and that the required time argume
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	1902237438	ding layer. Each direction in each hidden layer has 512 dimensions which results in a total encoder output dimension of 1024.3 For the machine translation models, we further use dot-product attention [20] and to improve efﬁciency, we limit the output space of the softmax to 30 translation candidates as determined by word alignments as well as the 2,000 most frequent words [15]. Data For the Spanish mo
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2604986524	experiments or models. This presumably makes sense for the English-Thai transfer learning case since these two languages use different alphabets; given the results by Lin et al. [16] and Yang et al. [30], we would expect additional improvements by using character embeddings. Second, one could try to include a speciﬁc learning objective to embed translations into a similar vector space as used by Yu e
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2597655663	joint intent-slot prediction model. It ﬁrst embeds the utterance using an embedding matrix and then passes the word vectors to a biLSTM layer. For intent classiﬁcation, we use a self-attention layer [17] over the hidden states of the biLSTM input to a softmax projection layer; for slot detection, we pass for each word the concatenation of the forward and backward hidden states through a softmax layer
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2152691628	tasks, and in particular for part-of-speech tagging and dependency parsing. Early work trained part-of-speech taggers for individual languages and then trained delexicalized dependency parsers (e.g., [33, 22]). Further, a lot of syntactic and semantic parsing models recently successfully incorporated parameter sharing for training parsers in closely related languges. [8, 1, 26, 25, 6]. 7 10 50 100 200 zer
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2344508595	zed dependency parsers (e.g., [33, 22]). Further, a lot of syntactic and semantic parsing models recently successfully incorporated parameter sharing for training parsers in closely related languges. [8, 1, 26, 25, 6]. 7 10 50 100 200 zero embeddings CoVe XLU embeddings bidir. MT zero embeddings CoVe XLU embeddings zero embeddings CoVe XLU embeddings bidir. MT zero embeddings CoVe XLU embeddings bidir. MT 0 20 40
2898856000	Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog.	2604986524	ﬁcient cross-lingual transfer learning. 7 Related work Cross-lingual sequence labeling The task of cross-lingual and multilingual sequence labeling has gained a lot of attention recently. Yang et al. [30] used shared character embeddings for cross-lingual transfer, and Lin et al. [16] used shared character and sentence embeddings that were trained in a multitask setting for part-of-speech tagging and
2898860014	Transductive Learning with String Kernels for Cross-Domain Text Classification.	2102313011	-domain setting, due to the distribution gap between diﬀerent domains. However, researchers proposed several domain adaptation techniques by using the unlabeled test data to obtain better performance [5,14,16,25,37]. Interestingly, some recent works [13,18] indicate that string kernels can yield robust results in the cross-domain setting without any domain adaptation. In fact, methods based on string kernels hav
2898860014	Transductive Learning with String Kernels for Cross-Domain Text Classification.	2154121272	ased on shared users and keywords is proposed in [31]. 2.2 String Kernels In recent years, methods based on string kernels have demonstrated remarkable performance in various text classiﬁcation tasks [7,10,13,18,23,27,34]. String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams. Lodhi et al. [27] used 4 Radu Tudor Ionescu and Andrei M.
2898860014	Transductive Learning with String Kernels for Cross-Domain Text Classification.	1510073064	et al. [20,23]. For the transductive kernel classiﬁer, we select r = 1000 unlabeled test samples to be included in the training set for the second round of training. We choose Kernel Ridge Regression [38] as classiﬁer and set its regularization parameter to 10−5 in all our experiments. Although Gim´enez-P´erez et al. [13] used a diﬀerent classiﬁer, namely Kernel Discriminant Analysis, we observed that
2898860014	Transductive Learning with String Kernels for Cross-Domain Text Classification.	1510073064	niﬁcantly increase the performance of string kernels in cross-domain text classiﬁcation, particularly in English polarity classiﬁcation. 3 Transductive String Kernels String kernels. Kernel functions [38] capture the intuitive notion of similarity between objects in a speciﬁc domain. For example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply
2898904728	Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification.	2295434193	etworks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al.,2015), recurrent (Ji et al., 2016), character-based (Qin et al.,2016a), adversarial (Qin et al.,2017) neural networks, and pairaware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneﬁcia
2898904728	Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification.	2131861279	o relations, these are considered as two training instances. To allow for full comparability to earlier work, we here report results for three different settings. The ﬁrst one is denoted as PDTB-Lin (Lin et al., 2009); it uses sections 2-21 for training, 22 as dev and section 23 as test set. The second one is labeled PDTB-Ji (Ji and Eisenstein,2015), and uses sections 2-20 for training, 0-1 as dev and evaluates on
2898904728	Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification.	2153579005	s in PDTB to lowercase and normalize strings, which removes special characters. The word embeddings used for initializing the word representations are trained with the CBOW architecture in Word2Vec2 (Mikolov et al., 2013). All the weights in the model are initialized with uniform random. To better locate the connective positions in the target side, we use two position indicators (hconni, h=conni) which specify the sta
2898904728	Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification.	2166957049	section 5. 2 Related Work 2.1 Implicit Discourse Relation Classiﬁcation Implicit discourse relation recognition is one of the most important components in discourse parsing. With the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning soluti
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	2437005631	&amp; 39 Lampouras, &amp; Androutsopoulos Manning, 2002). The hypergraph’s weights are estimated using the inside-outside algorithm (Li &amp; Eisner, 2009) on the training corpus. Following Huang and Chiang (2007), the hypergraph nodes are then integrated with an n-gram language model trained on the sentences of the corpus. Given a new set of database entries, the most probable derivation is found in the hyper
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	1963569619	. We made a similar observation in Section 5.2.4. 6. Related work Simple ontology verbalizers (Cregan et al., 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener et al., 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Liang et al., 2011) typically produce texts describing individuals and classes without requiring manually authored domain-dependent linguistic
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	2126728163	on 5.2.4. 6. Related work Simple ontology verbalizers (Cregan et al., 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener et al., 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Liang et al., 2011) typically produce texts describing individuals and classes without requiring manually authored domain-dependent linguistic resources. They usually tokenize the o
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	2201007611	author&quot; is a noun. To express the Sor Oof a triple, sparql2nl tokenizes the label (or identier) of the corresponding individual or class, pluralizing the resulting name if it refers to a class. Ratnaparkhi (2000) aims to express a set of attribute-value pairs as a natural language phrase; e.g., fcity-from = Athens;city-to = New York;depart-day = Wednesdaygbecomes \ ights from Athens to New York on Wednesday&q
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	2126728163	d (Cregan, Schwitter, &amp; Meyer, 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener, Golbeck, Parsia, Kolovski, &amp; Hendler, 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Stevens, Malone, Williams, Power, &amp; Third, 2011; Liang et al., 2011). Although verbalizers can be viewed as performing a kind of light natural language generation (nlg), they us
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	2008652694	ewrite rules in the derivation, as well as lexical (e.g., word n-grams) and structural features (e.g., n-grams of record elds). The weights of the features are estimated with a structured perceptron (Collins, 2002) on the training corpus. Apart from simple verbalizers, all the other related methods discussed above require a parallel training corpus of texts (or sentences, or phrases) and their semantic represen
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	1557976501	g popularity of Linked Data (data published using Semantic Web technologies) have renewed interest in concept-to-text generation (Reiter &amp; Dale, 2000), especially text generation from ontologies (Bontcheva, 2005; Mellish &amp; Sun, 2006; Galanis &amp; Androutsopoulos, 2007; Mellish &amp; Pan, 2008; Schwitter, Kaljurand, Cregan, Dolbear, &amp; Hart, 2008; Schwitter, 2010; Liang, Stevens, Scott, &amp; Rector,
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	25469692	hwitter, &amp; Meyer, 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener, Golbeck, Parsia, Kolovski, &amp; Hendler, 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Stevens, Malone, Williams, Power, &amp; Third, 2011; Liang et al., 2011). Although verbalizers can be viewed as performing a kind of light natural language generation (nlg), they usually translate t
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	25469692	ially text generation from ontologies (Bontcheva, 2005; Mellish &amp; Sun, 2006; Galanis &amp; Androutsopoulos, 2007; Mellish &amp; Pan, 2008; Schwitter, Kaljurand, Cregan, Dolbear, &amp; Hart, 2008; Schwitter, 2010; Liang, Stevens, Scott, &amp; Rector, 2011; Williams, Third, &amp; Power, 2011; Androutsopoulos, Lampouras, &amp; Galanis, 2013). An ontology provides a conceptualization of a knowledge domain (e.g.,
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	1557976501	nts) of the ontology one by one to controlled, often not entirely uent English statements, typically without considering the coherence of the resulting texts. By contrast, more elaborate nlg systems (Bontcheva, 2005; Androutsopoulos, Oberlander, &amp; Karkaletsis, 2007; Androutsopoulos et al., 2013) can produce more uent and coherent multi-sentence texts, but they need domain-specic linguistic resources. For ex
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	1963569619	ontology verbalizers have been developed (Cregan, Schwitter, &amp; Meyer, 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener, Golbeck, Parsia, Kolovski, &amp; Hendler, 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Stevens, Malone, Williams, Power, &amp; Third, 2011; Liang et al., 2011). Although verbalizers can be viewed as performing a kind of light natu
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	1557976501	rces are typical of nlg systems (Reiter &amp; Dale, 2000; Mellish et al., 2006). Hence, we believe that our work is also applicable, at least in principle, to other nlg systems. For example, ontosum (Bontcheva, 2005), which generates natural language descriptions of individuals, but apparently not classes, from rdf schema and owl ontologies, uses similar processing stages, and linguistic resources corresponding t
2899105111	Extracting Linguistic Resources from the Web for Concept-to-Text Generation.	25469692	Related work Simple ontology verbalizers (Cregan et al., 2007; Kaljurand &amp; Fuchs, 2007; Schwitter et al., 2008; Halaschek-Wiener et al., 2008; Schutte, 2009; Power &amp; Third, 2010; Power, 2010; Schwitter, 2010; Liang et al., 2011) typically produce texts describing individuals and classes without requiring manually authored domain-dependent linguistic resources. They usually tokenize the owl identiers or
2899110663	Analyzing and learning the language for different types of harassment.	2160685721	deos [32], to reliably detect harassment between participants. 4 aper Goal Data Conclusions [6] language from . sampling ords. for language [10] aggressors media months, annotating tweets. profiles e [7] language . comments in opics. and xical, features. [12] on media are nodes. while because ord. data. e approach [21] the xt ˘ of (FBM): MySpace. and language. using language [22] victims incidents tw
2899110663	Analyzing and learning the language for different types of harassment.	1971222444	f-the-art. To verify the effectiveness of the type-oriented classifiers, we ran them on the harassing tweets of Golbeck corpus which is a publicly available state-of-the-art harassment-related corpus [8]. The corpus contains 20,428 annotated tweets of which only 5,277 are labeled as harassing. It does not distinguishes the nature of harassment. In [35], we annotated the harassing tweets of Golbeck co
2899110663	Analyzing and learning the language for different types of harassment.	1965667542	graph2vec and (iv) LIWC vector. We feed our classifiers with each of these individual vectors or a combination of them. The Term Frequency and Inverse Document Frequency (TFIDF). We use this approach [39] to transform each given tweet into a weight vector T. Distributional semantics (i.e., word2vec and paragraph2vec). Distributional semantics (so-called embedding models) [40] play a vital role in many
2899110663	Analyzing and learning the language for different types of harassment.	2090987251	even groups and collected 5 million tweets. To find useful features for emotion identification, they applied LIBLINEAR [26] and Multinomial Naive Bayes [27] algorithms. They extracted N-gram features [28] to analyze the emotion, and they applied Linguistic Inquiry and Word Count (LIWC) to expand the feature set with the related emotional words. Interestingly, the authors of [11] target cyber-aggressio
2899110663	Analyzing and learning the language for different types of harassment.	2595653137	i) analyze comments/review threads to better identify offensive content in non-text media such as YouTube videos [32], to reliably detect harassment between participants. 4 aper Goal Data Conclusions [6] language from . sampling ords. for language [10] aggressors media months, annotating tweets. profiles e [7] language . comments in opics. and xical, features. [12] on media are nodes. while because o
2899110663	Analyzing and learning the language for different types of harassment.	1971222444	ms and classify activities, such as flaming, harassment, racism, and terrorism on social media. Fuzzy rules were used to classify data, and a genetic algorithm was used for optimizing the parameters. [8] explores the correlation of behaviors and actions of people and their emotions. The authors developed a large emotion-labeled dataset of harassing tweets. They applied 131 emotion hashtag keywords ca
2899110663	Analyzing and learning the language for different types of harassment.	1971222444	oach [21] the xt ˘ of (FBM): MySpace. and language. using language [22] victims incidents tweet 2015. tweets. features. [23] that ers ullying and ˘ tweets. ords victims. [24] . schools Internet date. [8] of detection ˘ tweets harassment-related tweets. [33] orks ˘ weeks clustering, used media. [25] vities ork from MySpace. detecting logic users ullying [29] out and acebook users States 9.19) posts ar
2899110663	Analyzing and learning the language for different types of harassment.	2340954483	ring, and analysis of hurtful language to protect online users with the help of tools. The prior state-of-the-art is limited to detecting hurtful language such as hateful speech [1], abusive language [2], and profanity [3], collectively termed Negative Affective Language (NAL). In the following, we present the definitions and terms for variants of harassing language: 1Disclaimer: This paper is concer
2899110663	Analyzing and learning the language for different types of harassment.	2188648436	t is, it is better than the word2vec and paragraph2vec representations. This can be due to the small size of our training corpus and its inability to capture the semantics of data (tweets) adequately [44]. (ii) Adding LIWC vectors to the three basic vectorization approaches (i.e., T, W, P) improves the accuracy, sometimes as large as ˇ10%. (iii) The combination of the three vectors T, L and W resulted
2899110663	Analyzing and learning the language for different types of harassment.	2131744502	The vector representation of a tweet is computed as the concatenation, summation or average vector of all vectors associated with words in the tweet. The second embedding model, called paragraph2vec [41], learns an individual vector for any tweet. W denotes the low dimensional vector obtained by word2vec approach, and P denotes the low dimensional vector obtained by paragraph2vec approach. Training e
2899386490	Abstractive Summarization of Reddit Posts with Multi-level Memory Networks.	2154652894	ils can be found in the appendix. 5 Experiments 5.1 Experimental Setting Evaluation Metrics. We evaluate the summarization performance with two language metrics: perplexity and standard ROUGE scores (Lin, 2004). We remind that lower perplexity and higher ROUGE scores indicate better performance. Datasets. In addition to Reddit TIFU dataset, we also evaluate our model on abstractive subset of Newsroom datase
2899386490	Abstractive Summarization of Reddit Posts with Multi-level Memory Networks.	2493916176	to information ﬂow, from source text to summary generation. 4.1 Text Embedding Online posts include lots of morphologically similar words, which should be closely embedded. Thus, we use the fastText (Bojanowski et al., 2016) trained on the Common Crawl corpus, to initialize the word embedding matrix W emb. We use the same embedding matrix W emb for both source text and output sentences. That is, we represent a source tex
2899386490	Abstractive Summarization of Reddit Posts with Multi-level Memory Networks.	2607579284	opose one of early memory networks for language question answering (QA); since then, many memory networks have been proposed for QA tasks (Sukhbaatar et al.,2015;Kumar et al.,2016;Miller et al.,2016).Park et al. (2017) propose a convolutional read memory network for personalized image captioning. One of the closest works to ours may beSingh et al. (2017), which use a memory network for text summarization. However,
2899486018	Unsupervised Hyper-alignment for Multilingual Word Embeddings	2099471712	, 2017) have used a Generative Adversarial Network framework (Goodfellow et al., 2014).
2899486018	Unsupervised Hyper-alignment for Multilingual Word Embeddings	2147152072	The distributional information used to learn these word vectors derives from statistical properties of word co-occurrence found in large corpora (Deerwester et al., 1990).
2899486018	Unsupervised Hyper-alignment for Multilingual Word Embeddings	2158899491	Such representations are typically used in downstream tasks to improve generalization when the amount of data is scarce (Collobert et al., 2011).
2899503556	Dialogue Natural Language Inference.	2250539671	4 We use cosine similarity between the mean of TF-IDF weighted GloVe [18] word vectors and set τ = 0.
2899503556	Dialogue Natural Language Inference.	2612953412	5% [4], respectively), while the results on the Dialogue NLI gold test set (92.
2899503556	Dialogue Natural Language Inference.	1840435438	It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding [2, 24].
2899503556	Dialogue Natural Language Inference.	1840435438	Only) [7, 21], a model trained on the existing SNLI dataset [2] but evaluated on Dialogue NLI (InferSent SNLI), and a model which returns the most common class from the Dialogue NLI training set (Most Common Class).
2899503556	Dialogue Natural Language Inference.	1840435438,2250790822	The resulting sentence pairs are thus drawn from a natural dialogue domain that differs from existing NLI datasets, which are either drawn from different domains such as image captions or created using synthetic templates [2, 6, 9, 15, 20, 23, 24].
2899503556	Dialogue Natural Language Inference.	2612953412	For the sentence encoding method, we use InferSent [4], which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states.
2899503556	Dialogue Natural Language Inference.	1840435438	Separately, the framework of Natural Language Inference (NLI) [2, 5, 14] involves learning a mapping between a sentence pair and an entailment category.
2899503556	Dialogue Natural Language Inference.	2612953412	Thus, the NLI task has been used for learning general sentence representations [4] and for evaluating NLP models [19, 23], with the expectation that such models will be useful in downstream tasks.
2899513582	Engaging Image Chat: Modeling Personality in Grounded Dialogue.	1591706642	dialogue is not grounded in perception, e.g. much recent work explores sequence-to-sequence models or retrieval models for goal-directed (Henderson et al.,2014;Bordes et al.,2017) or chit-chat tasks (Vinyals and Le, 2015;Sordoni et al.,2015;Zhang et al.,2018). While these tasks are text-based only, many of the techniques developed can likely be transferred arXiv:1811.00945v1 [cs.CL] 2 Nov 2018 for use in multimodal s
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2465611764	[31] WCL Dataset [71] SEMEVAL 2015 - Task 17 [70] AP=7%, AR=12%, AF=8% [45] Wikipedia (2015 dump) SEMEVAL 2015 - Task 17 [70] AP=20%, AR=31%, AF=24% [46] Wikipedia (2015 dump) SEMEVAL 2016 - Task 13 [72] AP=14%, AR=30%, AF=19% Regarding the results obtained in each work, usually they are not comparable since the resources used in each approach are different. For instance, Hearst [2] achieved 57.5% of
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	141602984	. [49] measures. It differs from the one proposed by Weeds et al. because it reduces the weight of included features if they have lower weight within the vector of the broader term. Lenci and Benotto [55] expand the idea of Geffet and Dagan [51] and explore the possibility of identifying hypernyms in Distributional Similarity Models (DSMs) using directional (or asymmetric) similarity measures. They pr
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1480596212	(DR) for term disambiguation. Vossen [27] extracts the most likely chunking by looking for the most salient head and decomposing the remaining words into modiﬁers. As pointed out by Buitelaar et al. [28], in many languages the morphological system is very rich and enables the construction of semantically complex compound words, e.g., the German word “Kreuzbandverletzung” corresponds to three English
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2135964261	(March 2008) ResearchCyc3 Cv=1.6%, Nv=99.2%, EC=28.2% WordNet v.3.0 [60] Cv=8.7%, Nv=99.3%, EC=211.6% 3,500 annotated category pairs P=90.3%, R=78.5%, F B=84% [55] TypeDM [66] 14,547 tuples of BLESS [67] AP=40% ACC 2=86.98% [68] LonelyPlanet A gold standards associated to each corpus P=53%, R=89%, F=67% SmartWeb Football P=77%, R=83%, F=80% Biology news P=49%, R=56%, F=52% Java [69] P=76%, R=72%, F=7
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2128870637	% P=32% R=92% [13] Wikipedia (March 2008) ResearchCyc3 Cv=1.6%, Nv=99.2%, EC=28.2% WordNet v.3.0 [60] Cv=8.7%, Nv=99.3%, EC=211.6% 3,500 annotated category pairs P=90.3%, R=78.5%, F B=84% [55] TypeDM [66] 14,547 tuples of BLESS [67] AP=40% ACC 2=86.98% [68] LonelyPlanet A gold standards associated to each corpus P=53%, R=89%, F=67% SmartWeb Football P=77%, R=83%, F=80% Biology news P=49%, R=56%, F=52%
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2097606805	) tags or relations between words in a syntactic level (chunking). In this work we use syntactic parsers to extract noun-phrases. The English corpora were parsed using the Stanford Lexicalized Parser [83] (version 3.3.1), a well known parser and widely used in relation extraction [84–86]. For the Portuguese corpora we applied the PALAVRAS [87] parser, which has been used in many work [82, 88–90]. 4.2.
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2068737686	2.1 Methods Based on Lexico-Syntactic Patterns The idea of learning taxonomic relations from texts by using lexico-syntactic patterns in the form of regular expressions has been introduced by Hearst [2, 3]. The main idea underlying using patterns is that even if one has never encountered a term, he can infer its semantic relation. For example, consider the following phrase “The bow lute, such as the Ba
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2164985371	7 tuples of BLESS [67] AP=40% ACC 2=86.98% [68] LonelyPlanet A gold standards associated to each corpus P=53%, R=89%, F=67% SmartWeb Football P=77%, R=83%, F=80% Biology news P=49%, R=56%, F=52% Java [69] P=76%, R=72%, F=74% [57] ukWaC BLESS [67] data set P=87% WaCkypedia [15] Financial Times Manually created gold standard P=76%, R=67%, Reports form dredging company Manually created gold standard P=69
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2123961948	72%, F=74% [57] ukWaC BLESS [67] data set P=87% WaCkypedia [15] Financial Times Manually created gold standard P=76%, R=67%, Reports form dredging company Manually created gold standard P=69%, R=58%, [16] Web corpus SEMEVAL 2015 - Task 17 [70] AP=36%, AR=63%, AF=39% [31] WCL Dataset [71] SEMEVAL 2015 - Task 17 [70] AP=7%, AR=12%, AF=8% [45] Wikipedia (2015 dump) SEMEVAL 2015 - Task 17 [70] AP=20%, AR=
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1966907789	aced by its Positive Pointwise Mutual Information (PPMI) [91] value, where all negative values are set to zero. For computing the directional similarity we tested the measure proposed by Weeds et al. [49] and the measure proposed by Clarke [54] (hereafter ClarkeDE). We decided to use these two directional similarity measures among all the existent because the taxonomic relation can be inferred by its
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2013109830	act hyponym relations between terms, sometimes combining them with other techniques such as head-modiﬁer [15, 16], clustering [17, 18] or LSA [14], sometimes using them on the Web to extract contexts [19] or class instances [20, 21]. 2.2 Methods Based on Head-Modiﬁer Detection According to Radford [22], the head of a phrase is the grammatically most important word in the phrase, since it determines th
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1570274086	airs from Reuters corpus P=70% [62] LonelyPlanet Semantic Cotopy and Taxonomy overlap F t=40.52% All in all using Tourism ontology [63] P t=29.33% BNC corpus R t=65.49% Reuters 19872 Finance ontology [64] F f=33.11% P f=29.93% R f=37.05% [56] Reuters RCV1 corpus 3,772 annotated pairs [65] AP=47% P=32% R=92% [13] Wikipedia (March 2008) ResearchCyc3 Cv=1.6%, Nv=99.2%, EC=28.2% WordNet v.3.0 [60] Cv=8.7%
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1593045043	of association between terms and contexts is determined by a weight function. Thus, the value of the frequency of a term with a context is replaced by its Positive Pointwise Mutual Information (PPMI) [91] value, where all negative values are set to zero. For computing the directional similarity we tested the measure proposed by Weeds et al. [49] and the measure proposed by Clarke [54] (hereafter Clark
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2020082880	ch work keep using the patterns proposed by Hearst to extract hyponym relations between terms, sometimes combining them with other techniques such as head-modiﬁer [15, 16], clustering [17, 18] or LSA [14], sometimes using them on the Web to extract contexts [19] or class instances [20, 21]. 2.2 Methods Based on Head-Modiﬁer Detection According to Radford [22], the head of a phrase is the grammatically
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1480596212	cture. The decision is based on a score calculated for each potential parent, taking into account the distance between the target term and the list of ancestors parents. According to Buitelaar et al. [28] methods that rely on raw data or frequency counting may lead to data sparseness. In order to overcome this problem, approaches based on dimension-reduction techniques like Latent Semantic Analysis (L
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2166776180	Dagan [53] proposed a balanced version of the Weeds et al. [49] work, combining the precision of achieved by Weeds et al.with the Lin’s measure by taking their geometric average. Thus, Lin’s measure [52] works on penalizing vectors containing few features. Clarke [54] formalized the idea of distributional generality and computes the entailment between two words using a variation of Weeds et al. [49]
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1906820873	the degree of semantic similarity between two words is related to the degree of overlapping among their contexts. A better understanding about semantic similarity is presented by Lemaire and Denhiére [33] who point out that it could be viewed as an association of two terms, that is, the mental activation of one term when another term is presented. 4 GRANADA ET AL. Usually DSMs vary according to the as
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2166084599	e candidate bracketing. Velardi et al. [24] use Mutual Information [25] and Dice factor [26] to the terminology extraction and then calculate the Domain Relevance (DR) for term disambiguation. Vossen [27] extracts the most likely chunking by looking for the most salient head and decomposing the remaining words into modiﬁers. As pointed out by Buitelaar et al. [28], in many languages the morphological
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2029537714	e Tree (BRT). This approach explores a Bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node. Similarly to Caraballo [17], De Knijff et al. [41] build a taxonomy using hierarchical clustering methods. In that work De Knijff et al. claim that this type of method may generate hierarchies with terms containing multiple potential parents. Hence,
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2137023796	elying on lexico-syntactic patterns have a reasonable precision, their recall is very low [10, 11]. An approach to minimize the drawbacks of low coverage has been proposed by Pantel and Pennacchiotti [12] that apply the patterns proposed by Hearst [2, 3] using the Web as a big corpus. In the same direction, Ponzetto and Strube [13] attached the patterns presented by Hearst [2, 3] in a method for build
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2065275674	eness. In order to overcome this problem, approaches based on dimension-reduction techniques like Latent Semantic Analysis (LSA) [42] should be applied on term by context matrices. Gamallo and Bordag [43] explain that LSA can identify relations between words that do not co-occur directly in the corpus, i.e., words that do not share the same contexts. These co-occurrences can be obtained by means of ap
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2171717581	eshold was set to 0.8. Later on, Njike-Fotzo and Gallinari [37] extended this idea replacing the conditional probability as basic co-occurrence evidence by the Expectation-Maximization (EM) algorithm [38]. Such approach is based on log-likelihood indices and it is less accurate, however, it is faster to compute, thus, being applicable to larger text sources. Associating similar words, Caraballo [17] c
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2029344051	experts deciding whether or not a term belongs to the domain is more or less feasible. Furthermore, deciding the quality of a taxonomic relation is a more complex task. As mentioned by Velardi et al. [59], when annotators where asked to blindly produce a taxonomy from a given set of terms, they struggled with the domain terminology and produced a quite messy organization. Manual evaluation also has th
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2102381086	on for the extracted relations. Reference Resources used Evaluation Results [2] Grolier’s American Academic 106 relations of the “NP such as LNP” pattern P=57.5% Encyclopedia [61] using WordNet v.1.1 [48] [49] British National Corpus (BNC) 20,415 pairs of BNC using WordNet v.1.6 P=71% [51] Reuters RCV1 corpus 400 annotated pairs from Reuters corpus P=70% [62] LonelyPlanet Semantic Cotopy and Taxonomy
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1966907789	r the extracted relations. Reference Resources used Evaluation Results [2] Grolier’s American Academic 106 relations of the “NP such as LNP” pattern P=57.5% Encyclopedia [61] using WordNet v.1.1 [48] [49] British National Corpus (BNC) 20,415 pairs of BNC using WordNet v.1.6 P=71% [51] Reuters RCV1 corpus 400 annotated pairs from Reuters corpus P=70% [62] LonelyPlanet Semantic Cotopy and Taxonomy overl
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2166776180	g the relative relevance of features. This measure applies the IR evaluation method of Average Precision in order to identify the feature inclusion, while uses the symmetric similarity measure of Lin [52] to penalize low frequency words. 6 GRANADA ET AL. The idea is that the score increases with a larger number of features shared by uand v, while giving higher weight to highly ranked features of the n
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2165545635	gainst the state of the art systems, verifying their performance in some task. For instance, Rios-Alvarado et al. [68] reproduced the approaches presented by Cimiano et al. [62] and by Jiang and Tang [73] in order to compare with their proposal. The problem in emulating the approaches proposed by someone else is that not always the resources to reproduce the system are available (e.g., the corpus to t
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2155063366	to help them to design ontologies, improve ontology quality, anticipate and reduce future maintenance requirements, as well as help ontology users to choose the ontologies that best meet their needs [94]. A compilation of metrics for ontology evaluation is performed by Freitas and Vieira [95, 96]. Although these metrics are applied to ontologies, they also can be applied to other structures such as t
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1966907789	an hyponym of “tree” because it inherits the properties of the latter but is distinguished from the other trees by the hardness of its wood, shape of its leaves etc.. On the other hand, Weeds et al. [49] veriﬁed that distributional generality is correlated with semantic generality, i.e., hypernyms tend to occur in a larger variety of contexts than hyponyms. In this sense, the asymmetry is captured by
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2068737686	ic patterns is very simple: to deﬁne regular expressions that capture expressions and to map the results of the matching expression to a taxonomic structure between terms. Patterns proposed by Hearst [2, 3] were initially developed for English, but they have been widely spread to other languages such as Japanese [4], Dutch [5], Turkish [6], French [7] and Portuguese [8]. Taxonomic relations can be extra
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2166776180	imilarity scores is often biased by inaccurate feature weights. Thus, they propose a recalculation on weighted vectors taking into account the set of most similar words generated by the Lin’s measure [52]. Using the new weighted vectors, Geffet and Dagan propose the Distributional Inclusion Hypothesis. The hypothesis says that if the meaning of a word uentails another word v, then it is expected that
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2100071287	ince the words “Brazil”, “France” and “USA” are coordinate terms, i.e., terms that share the same hypernym (e.g., “country”). Figure 3: Representation of coordinate terms by a binary tree. Liu et al. [39] apply a multi-branched tree algorithm during the hierarchical clustering process, avoiding the problem of generating a binary tree. In order to build the multi-branched tree, they adopted a determini
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2068737686	is-a or not-is-a using methods based on connectivity in the network and lexico-syntactic matching. In order to improve the precision of terms extracted after applying the patterns proposed by Hearst [2, 3], Cederberg and Widdows [14] use Latent Semantic Analysis (LSA) to ﬁlter out terms that are not semantically related. When applying LSA, they intend to reduce the rate of error of the initial pattern-
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2029344051	n those gold standards. Lexical databases such as WordNet [60] or hand-crafted lists containing terms and their semantic relation are commonly used as gold standards. As pointed out by Velardi et al. [59] it is not clear how to evaluate the concepts and relations not found in the gold standard. As these terms can be either wrong or correct, the evaluation is in any case incomplete. Automatic evaluatio
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2068737686	nable precision, their recall is very low [10, 11]. An approach to minimize the drawbacks of low coverage has been proposed by Pantel and Pennacchiotti [12] that apply the patterns proposed by Hearst [2, 3] using the Web as a big corpus. In the same direction, Ponzetto and Strube [13] attached the patterns presented by Hearst [2, 3] in a method for building a taxonomy based on the content of Wikipedia s
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1573981182	NC using WordNet v.1.6 P=71% [51] Reuters RCV1 corpus 400 annotated pairs from Reuters corpus P=70% [62] LonelyPlanet Semantic Cotopy and Taxonomy overlap F t=40.52% All in all using Tourism ontology [63] P t=29.33% BNC corpus R t=65.49% Reuters 19872 Finance ontology [64] F f=33.11% P f=29.93% R f=37.05% [56] Reuters RCV1 corpus 3,772 annotated pairs [65] AP=47% P=32% R=92% [13] Wikipedia (March 2008
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2102381086	nd where the hyponym inherits all the features of the more generic concept and adds at least one feature that distinguishes it from its superordinate and from any other hyponyms of that superordinate [48]. For example, “mapple” is an hyponym of “tree” because it inherits the properties of the latter but is distinguished from the other trees by the hardness of its wood, shape of its leaves etc.. On the
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2020082880	ork, usually they are not comparable since the resources used in each approach are different. For instance, Hearst [2] achieved 57.5% of precision using encyclopedia texts. When Cederberg and Widdows [14] applied the same method on the British National Corpus they achieved a precision of 40%. Thus, it would be impossible to compare these values of precision and recall with the work presented by Cimian
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2151386575	rawbacks of low coverage has been proposed by Pantel and Pennacchiotti [12] that apply the patterns proposed by Hearst [2, 3] using the Web as a big corpus. In the same direction, Ponzetto and Strube [13] attached the patterns presented by Hearst [2, 3] in a method for building a taxonomy based on the content of Wikipedia structure. In their approach, the semantic relations between categories are labe
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2143742281	rce from other available resources, Oliveira [77, 78] takes advantage of available NLP tools. Onto.PT integrates the lexical-semantic network PAPEL [79], the electronic dictionaries Dicionário Aberto [80] and Wiktionary.PT9, and three public synset-based thesauri namely: TeP 2.0 [81], OpenWordNet-PT10 and OpenThesaurus.PT11. In this work we use the version 0.6 of Onto.PT in a WordNet RDF/OWL Basic12 m
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	141602984	s [65] AP=47% P=32% R=92% [13] Wikipedia (March 2008) ResearchCyc3 Cv=1.6%, Nv=99.2%, EC=28.2% WordNet v.3.0 [60] Cv=8.7%, Nv=99.3%, EC=211.6% 3,500 annotated category pairs P=90.3%, R=78.5%, F B=84% [55] TypeDM [66] 14,547 tuples of BLESS [67] AP=40% ACC 2=86.98% [68] LonelyPlanet A gold standards associated to each corpus P=53%, R=89%, F=67% SmartWeb Football P=77%, R=83%, F=80% Biology news P=49%,
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2020082880	s based on connectivity in the network and lexico-syntactic matching. In order to improve the precision of terms extracted after applying the patterns proposed by Hearst [2, 3], Cederberg and Widdows [14] use Latent Semantic Analysis (LSA) to ﬁlter out terms that are not semantically related. When applying LSA, they intend to reduce the rate of error of the initial pattern-based hyponymy extraction. E
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2029537714	s and the method that uses document subsumption to identify taxonomic relations. It also seems a good option to select the best hypernym for each term using the algorithm proposed by De Knijff et al. [41] since it reduces substantially the taxonomy and improves the precision. On the other hand, using this algorithm the recall and f-measure decrease signiﬁcantly for most methods. Taxonomies generated b
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2151386575	sing Tourism ontology [63] P t=29.33% BNC corpus R t=65.49% Reuters 19872 Finance ontology [64] F f=33.11% P f=29.93% R f=37.05% [56] Reuters RCV1 corpus 3,772 annotated pairs [65] AP=47% P=32% R=92% [13] Wikipedia (March 2008) ResearchCyc3 Cv=1.6%, Nv=99.2%, EC=28.2% WordNet v.3.0 [60] Cv=8.7%, Nv=99.3%, EC=211.6% 3,500 annotated category pairs P=90.3%, R=78.5%, F B=84% [55] TypeDM [66] 14,547 tuples
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1516501661	t the same in almost all corpora, we decided to use in the evaluation process only the values generated by ClarkeDE measure. The code for implementation these measures are freely available by Weeds13 [92]. SLQS: The model based on entropy was developed by Santus et al. [57] and relies on the idea that superordinare terms are less informative than their hyponyms. This model also uses the list of terms
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2029537714	to a term, maintaining the taxonomy with a tree structure. The decision for the correct hypernym of a term is based on a score calculated for each potential hypernym as described by De Knijff et al. [41]. The score is deﬁned in Equation 5 and takes into account the distance between the target term and the list of ancestors parents. score(p;x) = P(pjx)+ X a2A p w(a;x)P(ajx) (5) where pis the potential
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	1966907789	term v, then a signiﬁcant number of salient distributional syntactic features of uis also included in the feature vector of v. Szpektor and Dagan [53] proposed a balanced version of the Weeds et al. [49] work, combining the precision of achieved by Weeds et al.with the Lin’s measure by taking their geometric average. Thus, Lin’s measure [52] works on penalizing vectors containing few features. Clarke
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2145328028,2148540243	tween terms, sometimes combining them with other techniques such as head-modiﬁer [15, 16], clustering [17, 18] or LSA [14], sometimes using them on the Web to extract contexts [19] or class instances [20, 21]. 2.2 Methods Based on Head-Modiﬁer Detection According to Radford [22], the head of a phrase is the grammatically most important word in the phrase, since it determines the nature of the overall phra
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2250539671	vector offset is obtained as the average offset between 200 pairs of hyponym-hypernym in the same vector space. In order to create the vector space representations of words, Pocostales use the GloVe [47] log-bilinear model trained on the Wikipedia corpus. Since the vector generated by join the offset vector with the hyponym vector will rarely match the exact vector of the hypernym, the Cosine similar
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2059031628	have been widely spread to other languages such as Japanese [4], Dutch [5], Turkish [6], French [7] and Portuguese [8]. Taxonomic relations can be extracted fairly accurately using syntactic patterns [9]. In contrast, these patterns are usually brittle and may not occur very often in a corpus. Although approaches relying on lexico-syntactic patterns have a reasonable precision, their recall is very l
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2012180588	other words, if a term uis semantically narrower than term v, then a signiﬁcant number of salient distributional syntactic features of uis also included in the feature vector of v. Szpektor and Dagan [53] proposed a balanced version of the Weeds et al. [49] work, combining the precision of achieved by Weeds et al.with the Lin’s measure by taking their geometric average. Thus, Lin’s measure [52] works
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2068737686	with other work that also use directional similarity measures). 7 GRANADA ET AL. Table 1: Work that uses automatic evaluation for the extracted relations. Reference Resources used Evaluation Results [2] Grolier’s American Academic 106 relations of the “NP such as LNP” pattern P=57.5% Encyclopedia [61] using WordNet v.1.1 [48] [49] British National Corpus (BNC) 20,415 pairs of BNC using WordNet v.1.6
2900416551	Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages.	2123961948	y extraction. Even with drawbacks, much work keep using the patterns proposed by Hearst to extract hyponym relations between terms, sometimes combining them with other techniques such as head-modiﬁer [15, 16], clustering [17, 18] or LSA [14], sometimes using them on the Web to extract contexts [19] or class instances [20, 21]. 2.2 Methods Based on Head-Modiﬁer Detection According to Radford [22], the head
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2155247542	(Miwa and Bansal 2016) - - - 82.9 83.9 83.4 57.2 54.0 55.6 - - - - (Li and Ji 2014) - - - 85.2 76.9 80.8 68.9 41.9 52.1 - - - - (Durrett and Klein 2014) - - - - - - - - - 81.03* 74.89* 72.56* 76.16* (Bansal and Klein 2012) - - - - - - - - - 70.2* 72.5* - - (A) Full Model 87.52 87.21 87.36 85.68 85.69 85.69 68.53 54.48 61.30 73.89 61.34 59.11 64.78 (A-GM) Full Model - GM 87.12 87.09 87.10 87.15 87.33 87.24 70.40 56.40 6
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2229639163	16). Regarding the combination of entity mention detection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features like dependency trees (Miwa and Bansal 2016) or handengineered heuristics (Li and Ji 2014). Conclusion We proposed a hierarchically supervised multi-task learning model focused on a set of semantic task. This model achieved state-of-the-art res
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2155247542	28 ACE05 Train Dev Test Documents 351 80 80 Sentences 7,273 1,765 1,535 Mentions 26,470 6,421 1,535 Relations 4,779 1,179 1,147 For CR, we use different splits to be able to compare to previous work (Bansal and Klein 2012; Durrett and Klein 2014). These splits (introduced in (Rahman and Ng 2009)) use the whole ACE05 dataset leaving 117 documents for test while having 482 documents for training (as in (Bansal and Klein
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2134033474	re annotated. ACE05 also introduces 6 relation types (including OrganizationAfﬁliation (ORG-AFF), GEN-Afﬁliation (GEN-AFF), and Part-Whole (PART-WHOLE)). We use the same data splits as previous work (Li and Ji 2014; Miwa and Bansal 2016; Katiyar and Cardie 2017) for both RE and EMD and report F 1-scores, Precision, and Recall. We consider an entity mention correct if the model correctly predicted both the menti
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2134033474	asks so that low level tasks are supervised at lower levels of the architecture while keeping more comarXiv:1811.06031v2 [cs.CL] 26 Nov 2018 plex interactions at deeper layers. Unlike previous works (Li and Ji 2014; Miwa and Bansal 2016), our whole model can be trained end-to-end without any external linguistic tools or hand-engineered features while giving stronger results on both Relation Extraction and Entit
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2134033474	Avg.F 1 (Strubell et al. 2017) - - 86.99 - - - - - - - - - - (Katiyar and Cardie 2017) - - - 84.0 81.3 82.6 57.9 54.0 55.9 - - - - (Miwa and Bansal 2016) - - - 82.9 83.9 83.4 57.2 54.0 55.6 - - - - (Li and Ji 2014) - - - 85.2 76.9 80.8 68.9 41.9 52.1 - - - - (Durrett and Klein 2014) - - - - - - - - - 81.03* 74.89* 72.56* 76.16* (Bansal and Klein 2012) - - - - - - - - - 70.2* 72.5* - - (A) Full Model 87.52 87.21
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2516255829	cation to the input of the model while other tasks (so-called “higher level” tasks) require a deeper processing of the inputs and likely a more complex architecture. Following (Hashimoto et al. 2017; Søgaard and Goldberg 2016), we therefore introduce a hierarchy between the tasks so that low level tasks are supervised at lower levels of the architecture while keeping more comarXiv:1811.06031v2 [cs.CL] 26 Nov 2018 plex inte
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2251035762	CEAFe (CEAF˚ 4) as well as the average F 1 of the three metrics as computed by the ofﬁcial CoNLL-2012 scorer. Note that Durrett and Klein make use of external NLP tools including an automatic parser (Durrett and Klein 2013). We compare our model to several previous systems that have driven substantial improvements over the past few years both using graphical models or neural-net-based models. These are the strongest bas
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2740462959	CoNLL-2003) and coreference (CoNLL-2012). A-CoNLL: train A-RS-GM using CoNLL-2003 for NER; A-CoNLL2012: train A using CoNLL-2012 for coreference. Model NER (F 1) CR (F 1) Lample et al. (2016) 90.94 - Strubell et al. (2017) 90.54 - Peters et al. (2018) 92.22 - (A-CoNLL-2003) 91.63 70.14 Durrett and Klein (2014) - 61.71 Lee et al. (2017) (single) - 67.2 Lee et al. (2017) (ensemble) - 68.8 (A-CoNLL-2012) 86.90 62.48 Table
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2179519966	dden states of a bidirectional language model. ELMo embeddings have been shown to give state-of-the-art results in multiple NLP tasks (Peters et al. 2018). Character-level word embeddings: Following (Chiu and Nichols 2015; Lample et al. 2016), we use character-level word embeddings to extract character-level features. Specifically, we use a convolutional neural network (CNN) (followed by a max pooling layer) for the e
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2556468274	decisions are made independently for each token, in contrast to our work. One central question in multi-task learning is the training procedure. Several schemes have been proposed in the literature. Hashimoto et al. (2017) train their hierarchical model following the model’s architecture from bottom to top: the trainer successively goes through the whole dataset for each task before moving to the task of the following
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2612953412	del. The sentence embedding of an input sequence of length Lis computed from the Lhidden states of an encoder by taking the maximum value over each dimension of the last layer activations as done in (Conneau et al. 2017). Sentence embeddings are obtained from word and character-level embeddings by max-pooling over a sentence’s words. Averaging word embeddings is known to be a strong baseline for sentence embeddings (
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2293004735	Documents 351 80 80 Sentences 7,273 1,765 1,535 Mentions 26,470 6,421 1,535 Relations 4,779 1,179 1,147 For CR, we use different splits to be able to compare to previous work (Bansal and Klein 2012; Durrett and Klein 2014). These splits (introduced in (Rahman and Ng 2009)) use the whole ACE05 dataset leaving 117 documents for test while having 482 documents for training (as in (Bansal and Klein 2012), we randomly split
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2738152205	e two clusters: (My mom, She) and (the cake, it). CR is thus a task which requires a form of semantic representation to cluster the mentions pointing to the same entity. We use the model proposed in (Lee et al. 2017). This model considers all the spans in a document as potential mentions and learns to distinguish the candidate coreferent mentions from the other spans using a mention scorer to prune the number of
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2229639163	E05 also introduces 6 relation types (including OrganizationAfﬁliation (ORG-AFF), GEN-Afﬁliation (GEN-AFF), and Part-Whole (PART-WHOLE)). We use the same data splits as previous work (Li and Ji 2014; Miwa and Bansal 2016; Katiyar and Cardie 2017) for both RE and EMD and report F 1-scores, Precision, and Recall. We consider an entity mention correct if the model correctly predicted both the mention’s head and its type
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2134033474	ection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features like dependency trees (Miwa and Bansal 2016) or handengineered heuristics (Li and Ji 2014). Conclusion We proposed a hierarchically supervised multi-task learning model focused on a set of semantic task. This model achieved state-of-the-art results on the tasks of Named Entity Recognition,
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2740462959	as for EMD and RE (351/80/80). For coreference, ﬁgures that are comparable with (Durrett and Klein 2014) are tagged with an *. NER EMD RE CR Setup Model P R F 1 P R F 1 P R F 1 MUC B3 Ceafe Avg.F 1 (Strubell et al. 2017) - - 86.99 - - - - - - - - - - (Katiyar and Cardie 2017) - - - 84.0 81.3 82.6 57.9 54.0 55.9 - - - - (Miwa and Bansal 2016) - - - 82.9 83.9 83.4 57.2 54.0 55.6 - - - - (Li and Ji 2014) - - - 85.2 76.9
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2293004735	ER; A-CoNLL2012: train A using CoNLL-2012 for coreference. Model NER (F 1) CR (F 1) Lample et al. (2016) 90.94 - Strubell et al. (2017) 90.54 - Peters et al. (2018) 92.22 - (A-CoNLL-2003) 91.63 70.14 Durrett and Klein (2014) - 61.71 Lee et al. (2017) (single) - 67.2 Lee et al. (2017) (ensemble) - 68.8 (A-CoNLL-2012) 86.90 62.48 Table 5: Ablation study on the embeddings. We remove one by one the embeddings on the ﬁrst lay
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2293004735	et al. 2009) is most effective. In contrast, we propose a novel proportional sampling strategy, which we ﬁnd to be more effective. Regarding the selection of the set of tasks, our work is closest to (Durrett and Klein 2014; Singh et al. 2013). Durrett and Klein (2014) combine coreference resolution, entity linking (sometimes referred to as Wikiﬁcation) and mention detection. Singh et al. (2013) combine entity tagging,
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2516255829	g such as (Subramanian et al. 2018), each task has its own contextualized encoder (multi-layer BiLSTM) rather than a shared one, which we found to improve the performance. Our work is also related to Søgaard and Goldberg (2016) who propose to cast a cascade architecture into a multi-task learning framework. However, this work was focused on syntactic tasks and concluded that adding a semantic task like NER to a set of synta
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2612953412	ge Processing (NLP) models heavily rely on rich distributed representations (typically word or sentence embeddings) to achieve good performance. One example are so-called “universal representations” (Conneau et al. 2017) which are expected to encode a varied set of linguistic features, transferable to many NLP tasks. This kind of rich word or sentence embeddings can be learned by leveraging the training signal from d
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2134033474	int Resolution Model proposed by Bekoulis et al. (2018) in which the selection of the mentions and classiﬁcation of the relation between these mentions are performed jointly. Following previous work (Li and Ji 2014; Katiyar and Cardie 2017; Bekoulis et al. 2018), we only consider relations between the last token of the head mentions involved in the relation. Redundant relations are therefore not classiﬁed. The
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2229639163	level tasks are supervised at lower levels of the architecture while keeping more comarXiv:1811.06031v2 [cs.CL] 26 Nov 2018 plex interactions at deeper layers. Unlike previous works (Li and Ji 2014; Miwa and Bansal 2016), our whole model can be trained end-to-end without any external linguistic tools or hand-engineered features while giving stronger results on both Relation Extraction and Entity Mention Detection. Ou
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2556468274	limited amount of modiﬁcation to the input of the model while other tasks (so-called “higher level” tasks) require a deeper processing of the inputs and likely a more complex architecture. Following (Hashimoto et al. 2017; Søgaard and Goldberg 2016), we therefore introduce a hierarchy between the tasks so that low level tasks are supervised at lower levels of the architecture while keeping more comarXiv:1811.06031v2 [
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2103076621	to link mentions. Experiment setting Datasets and evaluation metrics We use labeled data from different sources to train and evaluate our model. For NER, we use the English portion of OntoNotes 5.0 (Pradhan et al. 2013). Following Strubell et al. (2017), we use the same data split as used for coreference resolution in the CoNLL-2012 shared task (Pradhan et al. 2012). We report the performance on NER using span level
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2740663516	ly used in combination with other tasks. The main work we are aware of is (Dhingra et al. 2018), which uses coreference clusters to improve reading comprehension and the works on language modeling by Ji et al. (2017) and Yang et al. (2016). Regarding the combination of entity mention detection and relation, we refer to our baselines detailed above. Here again, our predictors do not require additional features lik
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2179519966	r model are supervised by Named Entity Recognition labels. NER aims to identify mentions of named entities in a sequence and classify them into predeﬁned categories. In accordance with previous work (Chiu and Nichols 2015; Lample et al. 2016) the tagging module contains an RNN-based encoding layer followed by a sequence tagging module based on a conditional random ﬁeld (Lafferty, McCallum, and Pereira 2001). We use mu
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2229639163	n *. NER EMD RE CR Setup Model P R F 1 P R F 1 P R F 1 MUC B3 Ceafe Avg.F 1 (Strubell et al. 2017) - - 86.99 - - - - - - - - - - (Katiyar and Cardie 2017) - - - 84.0 81.3 82.6 57.9 54.0 55.9 - - - - (Miwa and Bansal 2016) - - - 82.9 83.9 83.4 57.2 54.0 55.6 - - - - (Li and Ji 2014) - - - 85.2 76.9 80.8 68.9 41.9 52.1 - - - - (Durrett and Klein 2014) - - - - - - - - - 81.03* 74.89* 72.56* 76.16* (Bansal and Klein 2012)
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2155069789	NER, we use the English portion of OntoNotes 5.0 (Pradhan et al. 2013). Following Strubell et al. (2017), we use the same data split as used for coreference resolution in the CoNLL-2012 shared task (Pradhan et al. 2012). We report the performance on NER using span level F 1 score on the test set. The dataset covers a large set of document types (including telephone conversations, web text, broadcast news and transla
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2738152205	NLL-2012 for coreference. Model NER (F 1) CR (F 1) Lample et al. (2016) 90.94 - Strubell et al. (2017) 90.54 - Peters et al. (2018) 92.22 - (A-CoNLL-2003) 91.63 70.14 Durrett and Klein (2014) - 61.71 Lee et al. (2017) (single) - 67.2 Lee et al. (2017) (ensemble) - 68.8 (A-CoNLL-2012) 86.90 62.48 Table 5: Ablation study on the embeddings. We remove one by one the embeddings on the ﬁrst layer of the best performing
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2556468274	nowledge and language understanding) at the bottom layers of the model architecture and supervising higher-level tasks at higher layers. The architecture of the model is shown in Figure 1. Following (Hashimoto et al. 2017), we use shortcut connections so that top layers can have access to bottom layer representations. Words embeddings Our model encodes words w t of an input sentence s = (w 1;w 2;:::;w n) as a combinati
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	1614862348	nt tasks in a multi-task setting. It is known that a model trained in a multi-task framework can take advantage of inductive transfer between the tasks, achieving a better generalization performance (Caruana 1993). Recent works in sentence embeddings (Subramanian et al. 2018; Jernite, Bowman, and Sontag 2017) indicate that complementary aspects of the sentence (e.g. syntax, sentence length, word order) should
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2293004735	o works are based on graphical models with hand-engineered factors. We are using a neural-net-based approach fully trainable in an end-to-end fashion, with no need for external NLP tools (such as in (Durrett and Klein 2014)) or hand-engineered features. Coreference resolution is rarely used in combination with other tasks. The main work we are aware of is (Dhingra et al. 2018), which uses coreference clusters to improve
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2124700572	ons 26,470 6,421 1,535 Relations 4,779 1,179 1,147 For CR, we use different splits to be able to compare to previous work (Bansal and Klein 2012; Durrett and Klein 2014). These splits (introduced in (Rahman and Ng 2009)) use the whole ACE05 dataset leaving 117 documents for test while having 482 documents for training (as in (Bansal and Klein 2012), we randomly split the training into a 70/30 ratio to form a validat
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2124700572	recall and F 1 Table 3: Results: Baselines and ablation study on the tasks. GM means that the same coreference module uses gold mentions at evaluation time and that we used the splits introduced in (Rahman and Ng 2009). Otherwise, we use for coreference the same splits as for EMD and RE (351/80/80). For coreference, ﬁgures that are comparable with (Durrett and Klein 2014) are tagged with an *. NER EMD RE CR Setup M
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2124700572	we report results using different settings and splits. More precisely, GM indicates that gold mentions were used for evaluation and that coreference was trained using the ACE05 splits introduced in (Rahman and Ng 2009). Using gold mentions is impossible in real settings so we also relax this condition leading to a more challenging task in which we make no use of external tools or metadata (such as speaker ID used b
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2516255829	reviously learned task as a new task is learned. This phenomenon is especially present when multiple tasks are trained sequentially. We selected the simple yet effective training method described in (Søgaard and Goldberg 2016; Ruder et al. 2017): after each parameter update, a task is randomly selected and a batch of the dataset attached to this task is also sampled at random to train the model. This process is repeated u
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2229639163	s input the concatenation of the lower layer representations [g e;g ner] and outputs sequence embeddings denoted by g emd. To be able to compare our results with previous works (Bekoulis et al. 2018; Miwa and Bansal 2016; Katiyar and Cardie 2017) on EMD, we identify the head of the entity mention rather than the whole mention. Coreference Resolution (CR) Ascending one layer higher in our model, CR is the task of iden
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2556468274	the RE task. This supports the intuition that knowledge gathered from one task is beneﬁcial to the other tasks in the hierarchical architecture of our model. Related work Our work is most related to Hashimoto et al. (2017) who develop a joint hierarchical model trained on syntactic and semantic tasks. The top layers of this model are supervised by semantic relatedness and textual entailment between two input sentences,
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2293004735	time and that we used the splits introduced in (Rahman and Ng 2009). Otherwise, we use for coreference the same splits as for EMD and RE (351/80/80). For coreference, ﬁgures that are comparable with (Durrett and Klein 2014) are tagged with an *. NER EMD RE CR Setup Model P R F 1 P R F 1 P R F 1 MUC B3 Ceafe Avg.F 1 (Strubell et al. 2017) - - 86.99 - - - - - - - - - - (Katiyar and Cardie 2017) - - - 84.0 81.3 82.6 57.9 5
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2252031764	tions is impossible in real settings so we also relax this condition leading to a more challenging task in which we make no use of external tools or metadata (such as speaker ID used by some systems (Clark and Manning 2015)). Comparing setups A and A-GM shows how the supervision from one module (e.g. CR) can ﬂow through the entire architecture and impact other tasks’ performance: RE’s F 1 score drops by ˘1 point on A. N
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2740462959	tting Datasets and evaluation metrics We use labeled data from different sources to train and evaluate our model. For NER, we use the English portion of OntoNotes 5.0 (Pradhan et al. 2013). Following Strubell et al. (2017), we use the same data split as used for coreference resolution in the CoNLL-2012 shared task (Pradhan et al. 2012). We report the performance on NER using span level F 1 score on the test set. The da
2900533563	A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks	2556468274	xamined in this work are more focused on learning semantic representations, thereby reducing the need to learn surface and syntactic information, as evidenced by the linguistic probing tasks. Unlike (Hashimoto et al. 2017) and other previous work (Katiyar and Cardie 2017; Bekoulis et al. 2018; Augenstein, Ruder, and Søgaard 2018), we do not learn label embeddings, meaning that the (supervised) output/prediction of a la
2900546573	End-to-End Learning for Answering Structured Queries Directly over Text.	2131494463	eed to be answered by the learned models. Our approach relies on the construction of models that are able to extract potential candidate answers from text. Following from [Dirk Weissenborn,2018] and [Kumar et al., 2016], we cast the problem in terms of a question answering task, where the input is a question (e.g. entity type + relation) and a document and the output is answer span within the document that binds th
2900546573	End-to-End Learning for Answering Structured Queries Directly over Text.	1535992660	n the database engine itself. Finally, there is a long history of mixing information retrieval and database style queries together. For example, for the purposes of querying over semistructured data [Abiteboul, 1997]. [Raghavan and Garcia-Molina,2001] provides an accessible introduction to that history. While our system is designed to answer database queries one can imagine easily extending to the semistructured
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	2737504179	and were able to achieve comparable results on standard evaluation sets. 2.2 Motivation Many top-performing sentence encoders (such as RSE) use max-pooling as the ﬁnal layer to encode the sentences (Nangia et al. 2017), and except DIIN, most top-performing coattention models calculate cross-alignment on the RNN hidden state of each token. These design trends are counterintuitive because max-pooling and attention me
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	2738015883	are in the bottem on the table. that of the original models, indicating that compositionality understanding is required to obtain a good result on CS0.7. 6 Related Work and Discussion Over-Stability: Jia and Liang (2017) used adversarial evaluation to show that models trained on the Stanford QA Dataset (Rajpurkar et al. 2016) were reliant on syntactic similarity for answering, revealing the over-stability of QA model
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	2737504179	e linguistic diagnostic datasets have been published to test NLI models’ ability to process certain linguistic phenomena such as coreference, double negation, etc. (Williams, Nangia, and Bowman 2018; Nangia et al. 2017; Poliak et al. 2018a; Wang et al. 2018). These datasets are helpful in that they explore the potential usefulness of existing models by demonstrating their abilities in speciﬁc scenarios. However, th
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	2250861254	itions behind our adversarial generation algorithms were correct, we conducted human evaluation for a sample of 100 examples for each eval3To create the adversarial data, we used the Stanford Parser (Chen and Manning 2014) from CoreNLP 3.8.0 to get the dependency parse of the sentences, on which we apply our strategires. Model SNLI MNLI Matched MNLI MisMatched Original BoW WS Original BoW WS Original BoW WS RSE 86.47 8
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	2427527485	ng is required to obtain a good result on CS0.7. 6 Related Work and Discussion Over-Stability: Jia and Liang (2017) used adversarial evaluation to show that models trained on the Stanford QA Dataset (Rajpurkar et al. 2016) were reliant on syntactic similarity for answering, revealing the over-stability of QA models. With similar motivation, we study the task of NLI by showing that models are overly focused on lexical f
2900556903	Analyzing Compositionality-Sensitivity of NLI Models	1840435438	ream, higher-level NLP tasks that require complex natural language understanding such as questionanswering and summarization. Large annotated datasets such as the Stanford Natural Language Inference (Bowman et al. 2015) (SNLI) and the Multi-Genre Natural Language Inference (Williams, Nangia, and Bowman 2018) (MNLI) have promoted the development of many different neural NLI models, including encoding and co-attention
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	1753482797	chniques in machine learning, and examine methods for evaluating its performance. 1 Introduction We introduce our main topic via a concrete example of neural machine translation. Traditional methods (Kalchbrenner and Blunsom 2013; Cho et al. 2014) are formulated by an encoder-decoder architecture, both of which are recurrent neural networks. An input sequence of source tokens is ﬁrst fed into the encoder, of which the last hi
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	2153579005	g to build up deep query-speciﬁc token representation. 4.3 Attention for Pre-training Pre-trained word embeddings are crucial to many NLP tasks. Traditional methods such as Skipgram, Cbow, and Glove (Mikolov et al. 2013a; Pennington, Socher, and Manning 2014; Mikolov et al. 2013b) take use of large text corpora to train an upsupervised prediction model based on contexts and learn a high dimensional distributed repre
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	2194775991	j] i v [j] i (34) They demonstrate that certain embeddings are preferred over others depending on characteristics of the word, such as concreteness and frequency. For example, the ImageNet embedding (He et al. 2016) receives larger weights than FastText embeddings (Bojanowski et al. 2016) for concrete words. 4.2 Attention for Gating Another application of attention is to integrate this mechanisms with memory upd
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	2626778328	rd bank has different meanings under different contexts, and we want our model to learn contextual token embeddings that can capture semantic information from their surrounding contexts. Transformer (Vaswani et al. 2017) is an exemplar novel attention-based architecture for machine translation. It is a hybrid neural network with sequential blocks of feed forward layers and self-attention layers. Similar to the previo
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	2626778328	surrounding contexts. and then the model is ﬁne-tuned to adapt to a downstream supervised task. BERT (Devlin et al. 2018) is a bi-directional pre-training model backboned by the Transformer Encoder (Vaswani et al. 2017), a deep hybrid neural network with feed forward layers and self-attention layers which we have brieﬂy discussed in section 3.3. During pre-training, one task is to learn a bidirectional masked langua
2900788776	An Introductory Survey on Attention Mechanisms in NLP Problems	2626778328	tention layer is parallel (therefore outweighs RNN) and parameter-efﬁcient (compared with CNN). Various techniques have been proposed to further enhance its representation power. Positional encoding (Vaswani et al. 2017) is introduced to provide the model with additional positional information of each token, an example of which can be constructed as follows: PE (pos;2i) = sinpos=100002i=d) (22) PE(pos;2i+1) = cos(pos
2901157526	Fading of collective attention shapes the evolution of linguistic variants.	2072606289	the competition of news in the web (13). The integration of these two basic behaviors allows then to understand diverse competing objects, with lifetimes that range from some hours or days for memes (14) and news (13) to decades for verbs. This suggests a general mechanism underlying cultural evolution, and provides us with a general framework to study cultural phenomena. Methods summary We extracted
2901157526	Fading of collective attention shapes the evolution of linguistic variants.	2127492100	the most popular verbs to regularization (23). Imitation and preference to novelty have been also suggested as necessary components for modelling data obtained from the competition of news in the web (13). The integration of these two basic behaviors allows then to understand diverse competing objects, with lifetimes that range from some hours or days for memes (14) and news (13) to decades for verbs.
2901157526	Fading of collective attention shapes the evolution of linguistic variants.	2127492100	wards the preferred form are modulated by collective behaviors such as the ones analyzed in the present work. Here we capitalized on the access to massive digital corpora (16, 22) and previous models (9, 13, 15) to advance a simple dynamical system that fits experimental traces of imperfect subjunctive variants collected from written texts. The model integrates two basic forms of collective behavior: an imit
2901157526	Fading of collective attention shapes the evolution of linguistic variants.	2072606289,2127492100	y fades among large populations is one of importance to understand social behavior, especially in the digital era. Quantitative investigations on news websites and meme propagation in social networks (13, 14) show that two competing effects are present: the growth in the number of people that attend to a given content and the habituation or competition from other content that makes the original less attra
2901224641	Large Scale Scene Text Verification with Guided Attention	2012689760	anscribing straight text. [21,15,22] tried to read curved scene text by using an attention mechanism, and spatial attention [21,22] has achieved the state-of-the-art performance. Scene text retrieval [11,9], which aims at retrieving images based on text content, is closely related to scene text verication. The verication task could be seen as a subtask for scene text retrieval, as it only cares about
2901224641	Large Scale Scene Text Verification with Guided Attention	2529436507	ced in VQA tasks [30,11], [4] has shown improved performance by adopting it. Despite the development of sophisticated models [24,13,31], many problems are still faced by researchers in the eld of VQA [15]. For example, many datasets Guided Attention for Large Scale Scene Text Verication 5 have bias. Models trained on language alone usually perform better than models trained on images alone [15,6]. Ma
2901224641	Large Scale Scene Text Verification with Guided Attention	2183341477	coordinate encoding, (2) a guided attention decoder which selectively pools features from the encoded feature map and generates the nal result. CNN Encoder with Coordinate Map We trimmed InceptionV3 [23] to construct our image encoder, which builds a deep CNN by stacking some carefully designed sub-network structure with dierent scales. 6 Guided Attention for Large Scale Scene Text Verication Fig.2
2901224641	Large Scale Scene Text Verification with Guided Attention	2520000195	e extensive experiments to study dierent properties of the framework. Our major experiments are on SVBM dataset, and we also evaluate the trained model in two public datasets: UberText [33] and FSNS [22]. These experiments conrm that our Guided Attention model is better suited at solving this task than a more traditional scene text reading based solution and by combing the proposed method and tradit
2901224641	Large Scale Scene Text Verification with Guided Attention	1895577753	e followings. 4.3 SVBM Quantitative Evaluation Baseline Models We compare several baseline models with our approach: (1) Google Cloud OCR(GCOCR) . (2) Attention OCR(OCR) [28]. (3) Show and Tell Model [27] with binary classication output. See supplementary material https://cloud.google.com/vision/docs/ocr 12 Guided Attention for Large Scale Scene Text Verication Before Hard Negative MiningAfter Hard
2901224641	Large Scale Scene Text Verification with Guided Attention	2061802763,2131673214,2339589954,2605076167	to extract useful text information out of images. This task is usually referred to as scene text reading. Many researchers divide the scene text reading problem into two sub-problems: text detection [2,3,17,34,8,19] and text recognition [9,12,21]. In order to build The two authors contribute equally The work is done while Dafang is in Internship at Google. arXiv:1804.08588v1 [cs.CV] 23 Apr 2018 2 Guided Attentio
2901224641	Large Scale Scene Text Verification with Guided Attention	2560730294	f VQA [15]. For example, many datasets Guided Attention for Large Scale Scene Text Verication 5 have bias. Models trained on language alone usually perform better than models trained on images alone [15,6]. Many categories of questions will favor certain answers, and it is hard to know how much information comes from the image or from the language prior. In addition to bias, evaluation metrics with res
2901224641	Large Scale Scene Text Verification with Guided Attention	1933349210	our framework is thus much easier to obtain since we only need images with corresponding lists of candidate strings and their labels. Our work also has a loose connection to visual question answering[1] or image text matching[23]. However, our framework also has several unique properties: (1) input text is character sequence instead of words. (2) Simple binary classication is used as evaluation metr
2901224641	Large Scale Scene Text Verification with Guided Attention	2520000195	can generalize well to other datasets. We choose the words that are of type: \business names&quot; in the dataset and only evaluate the recall of these positive text in the verication problem. FSNS [22] contains french street signs. The images are much easier than that in SVBM because the text are focused and clear. We randomly sample 49 text as negative text for each image for evaluation purpose. 4
2901224641	Large Scale Scene Text Verification with Guided Attention	1869752048	with Guided Attention We simply force it to transcribe the whole image into the positive business name it represents. Text based matching is performed afterwards. Model (3) is a modication based on [19] (by changing the output as a binary classication). It could also be regarded as removing the attention mechanism in our framework. So we call it \no attention&quot;. Comparison w.r.t baselines We rs
2901224641	Large Scale Scene Text Verification with Guided Attention	1949478088	h easier to obtain since we only need images with corresponding lists of candidate strings and their labels. Our work also has a loose connection to visual question answering[1] or image text matching[23]. However, our framework also has several unique properties: (1) input text is character sequence instead of words. (2) Simple binary classication is used as evaluation metric. (3) Order of input word
2901224641	Large Scale Scene Text Verification with Guided Attention	2522258376,2606982687	n or element-wise multiplication [14,35,5]. Attention has also been introduced in VQA tasks [30,11], [4] has shown improved performance by adopting it. Despite the development of sophisticated models [24,13,31], many problems are still faced by researchers in the eld of VQA [15]. For example, many datasets Guided Attention for Large Scale Scene Text Verication 5 have bias. Models trained on language alone
2901224641	Large Scale Scene Text Verification with Guided Attention	2520000195	n an image, then \Image Street View&quot; should also be considered as positive. In an experiment, we evaluate our framework and model on the SVBM dataset and two public datasets: UbetText [33], FSNS [22]. SVBM dataset contains image-text pairs without bounding box annotations and it makes training a scene text detector impossible. We did experiments on masking out the input image, and then testing th
2901224641	Large Scale Scene Text Verification with Guided Attention	1895577753	why we need hard negative mining to train a better model. B Appendix 2: Baseline In major experiment in SVBM dataset, we use \no attention&quot; to refer to a baseline method. The method is based on [27]. In Fig. 12, we show the architecture of the baseline model. The dierence is that we only give a binary prediction. Fig.12: The architecture of the baseline model \no attention&quot;. 18 Guided Atte
2901224641	Large Scale Scene Text Verification with Guided Attention	2144554289	ng system is dicult, especially considering the possible dierences in the domains of images in real usage and in training. A detector trained on currently available public datasets (e.g. ICDAR 2015 [16]) may have high possibility of failure on the new domain of images the system is deployed on. (2) Scene text is normally highly artistic. Heuristic based text matching is usually applied afterwards fo
2901224641	Large Scale Scene Text Verification with Guided Attention	2339589954	ps are performed to obtain each individual text instance. This category includes methods that use fully convolutional neural networks (FCNs) to do semantic segmentation on text to extract text blocks [34,8,36]. The second category uses modied region proposal networks (RPNs) to detect text [25,19]. Both approaches achieve state-of-the-art performance on public datasets. The challenges of scene text recogni
2901224641	Large Scale Scene Text Verification with Guided Attention	2064675550	s S e= Se 1 ;:::;S N , which is learned end-to-end during training. Our goal is to compute p valid = P(y= 1jS 1;:::;S N;I); (1) where y2f0;1gis the indicator of the existence of the text. We use LSTM [7] as our recurrent function to encode sequential features. Let us denote h t as the hidden state of the LSTM in time step t, then the update function of hidden state could be expressed as Eq. 2: h t =
2901224641	Large Scale Scene Text Verification with Guided Attention	2171810632	step [18]. Previous work use simple mechanisms to combine the visual and text features, such as concatenation or element-wise multiplication [14,35,5]. Attention has also been introduced in VQA tasks [30,11], [4] has shown improved performance by adopting it. Despite the development of sophisticated models [24,13,31], many problems are still faced by researchers in the eld of VQA [15]. For example, many
2901224641	Large Scale Scene Text Verification with Guided Attention	1869752048	tention map. It is computed based on Eq. 4. e t (i;j) represents how relevant is the feature f~ (i;j) to the current character embedding Se t . In this work, we choose to use attention function as in [19] in Eq. 5: t (i;j) = exp(et (i;j) ) PW u=1 H v=1 exp(et (u;v) ) ; et (i;j) = f attn(h t 1;f~ (i;j)); (4) f attn(h t 1;f~ (i;j)) = v T tanh(Wh t 1 + Uf~ ); (5) where W, V are weight matrix that could b
2901224641	Large Scale Scene Text Verification with Guided Attention	2519818067,2605076167	at use fully convolutional neural networks (FCNs) to do semantic segmentation on text to extract text blocks [34,8,36]. The second category uses modied region proposal networks (RPNs) to detect text [25,19]. Both approaches achieve state-of-the-art performance on public datasets. The challenges of scene text recognition are mainly due to the fact that text could be distorted in an image. Most regular (s
2901224641	Large Scale Scene Text Verification with Guided Attention	2189070436	xt, into an RNN, which produces the result in the nal time step [18]. Previous work use simple mechanisms to combine the visual and text features, such as concatenation or element-wise multiplication [14,35,5]. Attention has also been introduced in VQA tasks [30,11], [4] has shown improved performance by adopting it. Despite the development of sophisticated models [24,13,31], many problems are still faced
2901533372	Neural Multi-Task Learning for Citation Function and Provenance.	2609130030	, 2016), comparison of task relationships (Bingel and Søgaard, 2017), sequence labeling tasks (Rei, 2017), and so on.
2901533372	Neural Multi-Task Learning for Citation Function and Provenance.	2141619255	(Abu-Jbara et al., 2013) adopted and re-categorized the 12 classes defined by Teufel et al.
2901533372	Neural Multi-Task Learning for Citation Function and Provenance.	2338266296	Combined with artificial neural nets, multi-task learning has been successfully applied to a number of natural language processing tasks, such as part-of-speech tagging (Plank et al., 2016), comparison of task relationships (Bingel and Søgaard, 2017), sequence labeling tasks (Rei, 2017), and so on.
2901533372	Neural Multi-Task Learning for Citation Function and Provenance.	1491247056	At the turn of the millennium, Garzone and Mercer (Garzone and Mercer, 2000) constructed the first automatic classifier for citation functions.
2901533372	Neural Multi-Task Learning for Citation Function and Provenance.	2127589108	We use the double CNN (hereafter, “dCNN”) architecture for this task (similar to (Bromley et al., 1994)), where two CNNs accept and process the two inputs separately, but combine at the fully-connected layer
2901612318	On the Winograd Schema: Situating Language Understanding in the Data-Information-Knowledge Continuum	2003453784	e possibility of some systems to pass the test, not because any thinking is going on, but by trickery and deception. As Levesque points out, systems that have participated in the Loebner competition (Shieber 1994) usually use deception and trickery by throwing in “elaborate wordplay, puns, jokes, quotations, clever asides, emotional outbursts,” while avoiding clear answers to questions that a 5-year old would
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	2608207374	data [2]. However, when the available target speaker data is above 2 hours (˘2k utterances), SpeakerDependent (SD) models were better [3]. The change of paradigm introduced by auto-regressive models [4, 5, 6, 7, 8], has produced synthetic speech of unprecedented quality. These new models require much more data than traditional TTS but they are also more efﬁcient at integrating diverse data [9, 10, 11]. Several
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	2191779130	ing network consists of a two bidirectional Long Short-Term Memory (LSTMs) with a hidden size of 128. The mel-spectrograms for conditioning consisted of 80 coefﬁcients extracted using Librosa library [17] for frequencies from 50 Hz to 12 kHz. The model was trained on data from 74 speakers on 17 different languages with between 1k to 2.5k utterances per speaker. Around two thirds of the speakers were f
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	2619368999	odels [4, 5, 6, 7, 8], has produced synthetic speech of unprecedented quality. These new models require much more data than traditional TTS but they are also more efﬁcient at integrating diverse data [9, 10, 11]. Several studies have reported that it is easy to train multi-speaker models [9, 12] and that adding more speakers improves the loss function over the validation set [4]. Most approaches for multi-sp
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	2619368999	ply it. Whereas some use an external model, e.g. speaker classiﬁcation, to provide the embeddings [13, 12] others train the speaker embedding together with the model out of a onehot speaker ID vector [4, 9, 5]. Some approaches use the emPaper submitted to IEEE ICASSP 2019 bedding at the input only as a global conditioning [4], whereas others apply it at different levels within the model [9, 5]. Despite all
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	1580849279	reasonable quality with as little as 3 minutes of target speaker data [2]. However, when the available target speaker data is above 2 hours (˘2k utterances), SpeakerDependent (SD) models were better [3]. The change of paradigm introduced by auto-regressive models [4, 5, 6, 7, 8], has produced synthetic speech of unprecedented quality. These new models require much more data than traditional TTS but
2901796276	Effect of data reduction on sequence-to-sequence neural TTS.	2153914468	techniques based on mixing data from multiple speakers into an Average Voice Model (AVM) were developed. These techniques produce reasonable quality with as little as 3 minutes of target speaker data [2]. However, when the available target speaker data is above 2 hours (˘2k utterances), SpeakerDependent (SD) models were better [3]. The change of paradigm introduced by auto-regressive models [4, 5, 6,
2902095634	Automatic Parallel Corpus Creation for Hindi-English News Translation Task	2496235729	Brown’s method [3] worked by finding out sentence length with the number of words.
2902095634	Automatic Parallel Corpus Creation for Hindi-English News Translation Task	54558479	Parallel as well as comparable corpora both are used as a resource for translation process as well as for comparative studies (McEnery and Xiao, 2007)[4], when used in association with parallel corpora, comparable corpora act as a useful resource for comparative study.
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2008172256	ative adjunct), and UNDERSTOOD-OBJECT no-reﬂexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classiﬁed in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence,
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	1522301498	embedding vectors is deﬁned by the type of embeddings we use. During the ﬁnal classiﬁcation, we use a threshold of 0:7 to map the model’s predictions to binary outputs. For training, we use the Adam (Kingma and Ba, 2015) optimizer. All ANNs are trained for 15 epochs, but we apply the best performing model on the test set. Further, we use 4-fold crossvalidation: the set of verbs is split into 4 equally sized parts out
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2064675550	hs without improvement in Matthews correlation coefﬁcient on the development set. The architecture of the real/fake encoder is shown in Figure 1. A bidirectional long-short term memory network (LSTM, Hochreiter and Schmidhuber, 1997) reads the words of a sentence. A ﬁxed-length sentence embedding is then produced by a max-pooling operation over the concatenations of the forward and backward hidden states at each time-step. This e
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2549835527	ividual alternation for our models to learn. 7 Related Work This investigation is part of a growing body of work which seeks to investigate the linguistic competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence em
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2129842875	Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb.
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2250539671	lowing. Word Embeddings For our word-level experiments, we use two different embeddings which differ in the way of their creation. First, we use 300-dimensional GloVe embeddings trained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequently in natural language processing (NLP), so evaluating them for knowledge of verb frames will be relevant for their application to and future research on tasks req
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2143017621	objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 100k most frequent words in the BNC are replaced with &lt;unk&gt;. Our peripheral interest in how humans learn lexical frame-selectional properties motivates us to investigate t
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2740900542	ormation are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine translation. Finally, Ostling and Tiedemann (2017)¨ learned language embeddings via multilingual language
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2154721769	tains verbs which were classiﬁed according to their semantic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive,
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	2141599568	or test set. 4 Pre-Trained Representations Embeddings, i.e., vector representations of linguistic objects like characters, words, or sentences, encode helpful information for downstream applications (Mikolov et al., 2013). In particular, they can be used to leverage knowledge from one task for another and have been shown to improve performance on a diverse set of tasks. Embeddings are usually low-dimensional; common s
2902158770	Verb Argument Structure Alternations in Word and Sentence Embeddings	1522301498	tish National Corpus4 (BNC), optimizing a language modeling objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 100k most frequent words in the BNC are replaced with &lt;unk&gt;. Our peripheral interest in how humans learn lexic
2902423035	Advanced Rich Transcription System for Estonian Speech.	2157331557	The approach is inspired from [19], but our model is unidirectional as our experiments did not show improvements with a bidirectional model, and uses gated recurrent units [20] instead of LSTM.
2902423035	Advanced Rich Transcription System for Estonian Speech.	1894807994	Compound words are later reconstructed from the output of the speech recognition system using a hidden-event n-gram model [8].
2902423035	Advanced Rich Transcription System for Estonian Speech.	1894807994	To determine the inflection of a written number, we employ a semi-supervised machine learning approach similar to [8]: a support vector machine classifier is first built using the numbers that are already written as words in training texts, and the classifier is then used to determine the inflection for the rest of the numbers.
2902423035	Advanced Rich Transcription System for Estonian Speech.	2516386219	We manually constructed an FST that represents Estonian letter-tophoneme rules, using the Pynini toolkit [17].
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2170872814	[19] presents a brief overview of complex questions, such as list, descriptive, opinion, casual, and procedural.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1600749373,2134995885	, Saquete segments the composite question using a few pre-defined temporal signals and rules [26] or PowerAqua [20, 21] employs GATE [7] for tokenization, part of speech tagging and verb detection.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2107170653	Also, [24] defines complex questions as composite questions requiring federating information from various heterogeneous sources.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2107170653,2134995885,2258363977	In general, researchers treat complex questions in two ways: (i) decomposition approaches [15, 16] , (ii) segmenting approaches [2, 21, 24–26, 38].
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2258363977	HAWK [38] extracts sub-questions using dependency parser, thus, it depends on the accuracy of dependency parser.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2026810221,2123142779	It is important to mention that the required preprocessings for a given composite question is: (i) dependency parsing, (ii) constituency parsing [28], (iii) part of speech taging [34], (iv) Recognition (NER) [8,12], (v) Named Entity Linking [8, 12], and (vi) stop words removal.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1529731474,2009591769,2129842875	Later approaches, such as Open Information Extraction (OIE), however, has eliminated some limitations [10, 27, 40, 41].
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	2146842426	A QA system federating knowledge from various heterogeneous sources is called a hybrid QA system [3].
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1889440031,2076228604	SINA [29, 30] relies on a hidden Markov model to segment the given query with respect to the background knowledge.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1529731474	Our strategy for relation extraction relies on a comprehensive linguistic background regarding the grammar of English language, similar to the approaches presented in ClausIE [10] and LS3RyIE [39].
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1468923932	To support the importance of hybrid QA system, we further point out QALD [36] challenge, held annually from 2011.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1529731474	We used a textual corpus16, which was also employed by the prior art ClausIE [10] and LS3RYIE [39] to evaluate our relation extraction module.
2902468268	HCqa: Hybrid and Complex Question Answering on Textual Corpus and Knowledge Graph.	1529731474	We use a textual corpus13, which was also employed by the prior art ClausIE [10] and LS3RYIE [39].
2902619431	One for All: Neural Joint Modeling of Entities and Events	2102563561	on feature engineering to extract a diversity of features (Grishman et al. 2005; Ahn 2006; Ji and Grishman 2008; Gupta and Ji 2009; Patwardhan and Riloff 2009; LiaoandGrishman2010;2011;Hongetal.2011;McClosky et al. 2011; Huang and Riloff 2012; Miwa et al. 2014; Li et al. 2015). Some recent work has developed joint inference models for ED and ARP to address the error propagation issue in the pipelined approach. Those
2902619431	One for All: Neural Joint Modeling of Entities and Events	2251160286	shman et al. 2005; Ahn 2006; Ji and Grishman 2008; Gupta and Ji 2009; Patwardhan and Riloff 2009; LiaoandGrishman2010;2011;Hongetal.2011;McClosky et al. 2011; Huang and Riloff 2012; Miwa et al. 2014; Li et al. 2015). Some recent work has developed joint inference models for ED and ARP to address the error propagation issue in the pipelined approach. Those work exploits different structured prediction methods, in
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2081580037	, WordNet (Miller, 1995) for word-level and the Paraphrase Database (PPDB) (Ganitkevitch, Van Durme, and Callison-Burch, 2013) for phrase-level.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2115081467	We apply the same pipeline to the 10 word FrenchEnglish parallel corpus (Giga) (Callison-Burch et al., 2009), which has different domain coverage than CzEng.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2251882135	Many efforts have aimed to automatically expand gold resources, for example: Snow, Jurafsky, and Ng (2006) augmented WordNet by combining existing semantic taxonomies in WordNet with hypernym predictions and coordinate term classifications; and Pavlick et al. (2015b) tripled the lexical coverage of FrameNet by substituting words through PPDB, with verification of quality via crowdsourcing.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	1965605789	Some examples include: DIRT (Lin and Pantel, 2001), which extracts paraphrastic expressions over paths in dependency trees, based on an extension of the distributional hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954); Weisman et al.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2081580037	Lexical Resources WordNet (Miller, 1995) includes manually-curated sub-sentential paraphrases.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2608747952	Lexically constrained decoding (Hokamp and Liu, 2017) is a modification to beam search for neural machine translation that allows the user to specify tokens and token sequences that must (or must not) appear in the decoder output.
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	1958706068	Positional Constraints It has been observed that RNN decoders in dialogue systems can be nudged toward producing more diverse outputs by modifying decoding for only the first few tokens (Li et al., 2016).
2903763948	ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-­constrained Neural Machine Translation	2251386628	Recently, Post and Vilar (2018) proposed a variant of lexically constrained decoding that reduced complexity from linear to constant-time (in the number of constraints).
2904532340	FANDA: A Novel Approach to Perform Follow-up Query Analysis	1623072288	Setlur et al. (2016) introduce the scenarios of single queries in their realistic system, inspired by which, we summarize typical scenarios of follow-up queries in Table 1.
2904631866	SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering.	2250539671	We use 300-dim GloVe (Pennington et al., 2014) embedding and contextualized embedding for each word in context and question.
2904639187	Neural Word Search in Historical Manuscript Collections.	2142636459	- - Encoder-Decoder Net [50] PHOC - - - - - - - - - 85.4 - 85.6 Alternate Evaluation Protocol Ctrl-F-Net* DCToW 83.1 84.7 97.1 94.5 - - - - - - - - SW [39] PHOC 67.7 - - - - - - - 42.1 - - - BG index [38] PHOC - 73.3 - - - - - - - 48.6 - - SVM Fisher Vectors [48] PHOC 77.2 69.9 - - - - - - 38.7 44.7 - - there is a noticeable gap between the two sources of region proposals. Post ﬁltering, the gap widen
2904639187	Neural Word Search in Historical Manuscript Collections.	2604619303	ail: ftomas.wilkinson, anders.brung@it.uu.se  Jonas Lindstrom¨ is with the Department of History, Uppsala University, Sweden. E-mail: jonas.lindstrom@hist.uu.se  A preliminary version of this paper [1] appeared in ICCV 2017 Fig. 1. A search for the word ”witnet” (eng. ”the witness”) on one page of the Snevringe court records dataset, displaying the top 25 results. The greener the bounding box, the
2904639187	Neural Word Search in Historical Manuscript Collections.	1987937363	andwritten Text Recognition (HTR), where many of the assumptions of OCR software have either been relaxed, or removed altogether. Much work has been done on HTR on handwritten manuscripts [14], [15], [16], [17], though they come with their own set of assumptions and restrictions. First, compared to word spotting you typically need a relatively large amount of training data to learn a model good enough
2904639187	Neural Word Search in Historical Manuscript Collections.	2302255633	aspect ratio intact. Then it is fed through several layers of a CNN, until it has been spatially downsampled by a factor of 8. As the input is a full manuscript page, a 34-layer pre-activation ResNet [62] is used as the CNN architecture due to its small memory footprint while still achieving high performance. The feature maps are then fed through a localization module, that consists of: i) an RPN that
2904639187	Neural Word Search in Historical Manuscript Collections.	2604619303	based on the proposals left after the ﬁnal NMS stage. GW 15-5 IAM MAP 50% MAP 25% Recall MAP 50% MAP 25% Recall Model Variant Embedding QbE QbS QbE QbS 50% 25% QbE QbS QbE QbS 50% 25% Baselines from [1] Ctrl-F-Net DCToW 90.5 91.0 97.0 95.2 99.4 99.9 72.0 80.3 74.1 82.5 98.1 98.9 Ctrl-F-Net PHOC 90.9 90.1 96.7 93.9 - - 71.5 78.8 73.7 80.8 - - Baselines Ctrl-F-Net DCToW 91.9 92.9 96.8 95.7 93.8 99.5 7
2904639187	Neural Word Search in Historical Manuscript Collections.	1993155451	ced the idea of viewing the images as sequences of column features and applying sequence matching methods, in particular Dynamic Time Warping (DTW) [21], [22], [23], Hidden Markov Models (HMMs) [24], [25], [26], and to a lesser extent, Recurrent Neural Networks (RNNs) [27]. However, with the size of data ever increasing, the inefﬁciencies of sequence-based methods were becoming prohibitive. As a resul
2904639187	Neural Word Search in Historical Manuscript Collections.	2604619303	of court records used for contemporary historical research. The result is an increase in processing speed and data size by orders of magnitude, compared to manual work. This paper is an extension of [1], where an initial version of this work was presented. Compared to the previous incarnation we have: introduced a simpliﬁed model (Section 3.2); carried out an ablation study (Section 5.3); Improved u
2904639187	Neural Word Search in Historical Manuscript Collections.	2087914438	e idea of viewing the images as sequences of column features and applying sequence matching methods, in particular Dynamic Time Warping (DTW) [21], [22], [23], Hidden Markov Models (HMMs) [24], [25], [26], and to a lesser extent, Recurrent Neural Networks (RNNs) [27]. However, with the size of data ever increasing, the inefﬁciencies of sequence-based methods were becoming prohibitive. As a result, the
2904639187	Neural Word Search in Historical Manuscript Collections.	2098411764	e Latent Semantic Analysis [34] to perform multi-modal fusion, mapping their visual BoVW and textual representations to 3 the same space. In a similar vein, [32] use an attribute representation [35], [36] called Pyramidal Histogram of Characters (PHOC) as textual descriptor. They use Canonical Correlation Analysis (CCA) to learn a common subspace for the textual PHOC descriptor and visual Fisher Vecto
2904639187	Neural Word Search in Historical Manuscript Collections.	2604619303	e where they perform text recognition. 3 THE MODELS In this section we introduce the two models used in this paper. The ﬁrst is the Ctrl-F-Net that we introduced in the previous version of this paper [1]. The second model is a simpliﬁed version of Ctrl-F-Net we call Ctrl-F-Mini. In section 5.3, we provide results from a series of ablation studies investigating the performance of the two models. 3.1 C
2904639187	Neural Word Search in Historical Manuscript Collections.	1932188282	ED WORK The work in this paper is based on, and related to, a few different ﬁelds that we will brieﬂy review in this section. 2.1 Word spotting Since its introduction over 20 years ago, word spotting [7] in manuscript images has come a long way. The initial approach uses template matching, with the image itself as a feature descriptor. Subsequent work introduced the idea of viewing the images as sequ
2904639187	Neural Word Search in Historical Manuscript Collections.	2735253541	embedding. 2.3 Scene Text Recognition The model proposed in this paper is similar to work in endto-end scene text detection and recognition, which has been receiving increasing attention [55], [56], [57], [58], [59]. In [55], an end-to-end system for text localization, recognition and retrieval based on region proposals and CNNs is proposed. Another approach is made in [56], where characters are dete
2904639187	Neural Word Search in Historical Manuscript Collections.	2142636459	entation has been widely adopted by the word spotting community, and has even been put to good use in lexicon-based text recognition [37]. Since then, many methods extending it have been proposed. In [38], [39], the approach in [32] is extended to the segmentation-free setting. The method proposed in [40] replaces the Fisher Vectors by a convolutional neural network (CNN) and the PHOC representation i
2904639187	Neural Word Search in Historical Manuscript Collections.	1990042562	evel or on top of densely extracted features. This method is common in the QbE setting as the size of the query image in pixels known, allowing for constraints on the sizes of generated regions [26], [28], [30], [39]. For QbS, the width and height of the region needs to be estimated, given the query [46]. The main drawback with these methods is the large amounts of regions generated, resulting in fals
2904639187	Neural Word Search in Historical Manuscript Collections.	2156301828	a feature descriptor. Subsequent work introduced the idea of viewing the images as sequences of column features and applying sequence matching methods, in particular Dynamic Time Warping (DTW) [21], [22], [23], Hidden Markov Models (HMMs) [24], [25], [26], and to a lesser extent, Recurrent Neural Networks (RNNs) [27]. However, with the size of data ever increasing, the inefﬁciencies of sequence-based
2904639187	Neural Word Search in Historical Manuscript Collections.	1990042562	inefﬁciencies of sequence-based methods were becoming prohibitive. As a result, there was renewed interest in compact, ﬁxed-length representations that allow for fast Euclidean distance calculations. [28] build Bag-of-VisualWords (BoVW) [29] features on top of HOG descriptors, whereas [30], [31] use SIFT descriptors. In [32], Fisher Vectors [33] were built on top of SIFT descriptors. Moreover, [32] an
2904639187	Neural Word Search in Historical Manuscript Collections.	1536680647	ing network with 2 hidden layers, that does the ﬁnal embedding. For the mid-network scores and output wordness scores, we use a binary logistic loss. The bounding boxes are parameterized according to [64], both for the anchor box regression 5 region proposals: N1 x 4 CNN CNN 128 x H/8 x W/8 1 x H x W N1+N2 x 512 embed N M S DTP Bilinear Interpolation Fig. 3. Ctrl-F-Mini at test time. Given an input im
2904639187	Neural Word Search in Historical Manuscript Collections.	2254252455	investigating the performance of the two models. 3.1 Ctrl-F-Net The model we propose is a deep convolutional neural network inspired by previous work on object detection [18], dense image captioning [61] and segmentation-based word spotting [20]. We call it Ctrl-F-Net, named after the well known shortcut for word search in many word processors. It is an end-to-end trainable model that simultaneously
2904639187	Neural Word Search in Historical Manuscript Collections.	2147152072	iptors. Moreover, [32] and [31] both allow for QbS word spotting using similar approaches. In [31], the authors create a textual descriptor based on character n-grams and use Latent Semantic Analysis [34] to perform multi-modal fusion, mapping their visual BoVW and textual representations to 3 the same space. In a similar vein, [32] use an attribute representation [35], [36] called Pyramidal Histogram
2904639187	Neural Word Search in Historical Manuscript Collections.	2087914438	ixel level or on top of densely extracted features. This method is common in the QbE setting as the size of the query image in pixels known, allowing for constraints on the sizes of generated regions [26], [28], [30], [39]. For QbS, the width and height of the region needs to be estimated, given the query [46]. The main drawback with these methods is the large amounts of regions generated, resulting i
2904639187	Neural Word Search in Historical Manuscript Collections.	603908379	and negatives, are sampled and are used to calculate the mid-network wordness and regression losses. The boxes of varying size sampled from the RPN are then resized using Bilinear Interpolation [61], [63] to a ﬁxed output size of 8 20 pixels. They are then fed through the rest of the CNN and used as input to three parallel branches. The ﬁrst branch is fully connected (FC) layer with 4 outputs that reﬁ
2904639187	Neural Word Search in Historical Manuscript Collections.	2160741934	newed interest in compact, ﬁxed-length representations that allow for fast Euclidean distance calculations. [28] build Bag-of-VisualWords (BoVW) [29] features on top of HOG descriptors, whereas [30], [31] use SIFT descriptors. In [32], Fisher Vectors [33] were built on top of SIFT descriptors. Moreover, [32] and [31] both allow for QbS word spotting using similar approaches. In [31], the authors creat
2904639187	Neural Word Search in Historical Manuscript Collections.	1990042562	ns out that for many manuscripts, especially historical ones, this is not a valid assumption. To remedy this, an increasing number of segmentation-free1 word spotting methods have been proposed [26], [28], [30], [39], [44], [45], [46] Segmentation-free word spotting approaches can typically be placed into two broad categories based on how they generate region from within an image. The ﬁrst are methods
2904639187	Neural Word Search in Historical Manuscript Collections.	1980083143	ology to separate characters. A popular approach is to binarize the image, extract connected components, and group them in a bottom-up fashion using heuristics and ﬁnally extract bounding boxes [38], [45]. A similar approach is used in [49] for matching entire documents using distributions of word images. A combination of the two approaches is used in [47]. Here, extremal regions are extracted on top
2904639187	Neural Word Search in Historical Manuscript Collections.	1990550880	onsist of binary attributes, but a low-frequency, real-valued representation of a text string. It shares a greater similarity with the Spatial Pyramid of Characters introduced for text recognition in [51], which is similar to PHOC except that character occurrences are counted, not a binary presence/absence. The attribute and label embedding representations allow seamless retrieval of words not present
2904639187	Neural Word Search in Historical Manuscript Collections.	2326498110	ork (CNN) and the PHOC representation is learned using SVMs. Two methods were simultaneously proposed where the two step Fisher Vector and CCA approach is replaced with endto-end trainable CNNs [20], [41]. This strand of work has been consolidated and improved upon in [42], [43]. A majority of the proposed handwritten word spotting methods assume that the words or text lines have been segmented, or th
2904639187	Neural Word Search in Historical Manuscript Collections.	2735253541	orks (RPNs) to generate candidates for text regions and then transcribe them. The difference lies in that [58] uses the Connectionist Temporal Classiﬁcation (CTC) loss [60] to decode a region whereas [57] employs a Recurrent Neural Network (RNN). Similarly, [59] makes us of an RPN and a CTC loss, but with a novel ROIRotate operation that maps arbitrarily oriented region proposals to 4 region proposals
2904639187	Neural Word Search in Historical Manuscript Collections.	2604619303	oss requires a binary embedding, it is not applicable to the DCToW. We use the GW 15-5 and IAM datasets to evaluating the different model choices. The top section of Table 1 contains the results from [1], which are a bit different from the new baselines. Since [1] was published, we discovered a few mostly small bugs in our code, the most notable one is that the margin for the Cosine Embedding loss wa
2904639187	Neural Word Search in Historical Manuscript Collections.	1932188282	pages. It is a matter of looking for a black cat in a coal cellar. A need for a scalable solution exists as large part of the writings produced throughout history are yet to be studied. Word spotting [7], [8] is a way to address the problem of ﬁnding where to look. The task consists of locating and retrieving images of words given a user supplied query, much like a regular text word search found in c
2904639187	Neural Word Search in Historical Manuscript Collections.	2142636459	re Qis the number of queries. For the QbE evaluation, all the ground truth segmented word images in the test set is used. For QbS, all unique ground truth labels are used. Some methods, in particular [38], [39], [48], use a slightly different protocol for the GW dataset. Here all word instances in the dataset are used as queries for QbE, and all unique labels for QbS. The search is performed in all 20
2904639187	Neural Word Search in Historical Manuscript Collections.	639708223	rectly. 1.1 Contributions The contributions of this paper include: 1)Two models for segmentation-free query-by-string word spotting are introduced: An end-to-end trainable model based on Faster R-CNN [18] and previous work [19], [20]; and a simpliﬁed version that performs equally well or better in certain situations. 2)Two novel data augmentation strategies for full manuscript pages, crucial for preve
2904639187	Neural Word Search in Historical Manuscript Collections.	1980083143	eight of the region needs to be estimated, given the query [46]. The main drawback with these methods is the large amounts of regions generated, resulting in false positives and long processing times [45]. Additionally, due to a lack of attention, sliding window techniques are not robust against small shifts of the input, as this would lead to a misalignment av regions compared to ones extracted from
2904639187	Neural Word Search in Historical Manuscript Collections.	639708223	a series of ablation studies investigating the performance of the two models. 3.1 Ctrl-F-Net The model we propose is a deep convolutional neural network inspired by previous work on object detection [18], dense image captioning [61] and segmentation-based word spotting [20]. We call it Ctrl-F-Net, named after the well known shortcut for word search in many word processors. It is an end-to-end trainab
2904639187	Neural Word Search in Historical Manuscript Collections.	2142636459	set on the other hand, 9 TABLE 3 MAP comparison in % with state-of-the-art segmentation-free methods on the GW dataset. The Ctrl-F-Net results marked with an asterisk use the evaluation protocol from [38], [39], [48] (only relevant for GW 15-5). GW 15-5 GW 5-15 IAM MAP 50% MAP 25% MAP 50% MAP 25% MAP 50% MAP 25% Method Embedding QbE QbS QbE QbS QbE QbS QbE QbS QbE QbS QbE QbS Ctrl-F-Net DCToW 90.9 91.
2904639187	Neural Word Search in Historical Manuscript Collections.	2123024445	me technique is used for multi-label image classiﬁcation [52], and text recognition [51]. Moreover, word spotting shares similarities with approaches for multi-modal embeddings for zero-shot learning [53], [54], with the difference that the text modality has a ﬁxed embedding. 2.3 Scene Text Recognition The model proposed in this paper is similar to work in endto-end scene text detection and recognitio
2904639187	Neural Word Search in Historical Manuscript Collections.	2254252455	tives and negatives, are sampled and are used to calculate the mid-network wordness and regression losses. The boxes of varying size sampled from the RPN are then resized using Bilinear Interpolation [61], [63] to a ﬁxed output size of 8 20 pixels. They are then fed through the rest of the CNN and used as input to three parallel branches. The ﬁrst branch is fully connected (FC) layer with 4 outputs th
2904639187	Neural Word Search in Historical Manuscript Collections.	1974924846	ture descriptor. Subsequent work introduced the idea of viewing the images as sequences of column features and applying sequence matching methods, in particular Dynamic Time Warping (DTW) [21], [22], [23], Hidden Markov Models (HMMs) [24], [25], [26], and to a lesser extent, Recurrent Neural Networks (RNNs) [27]. However, with the size of data ever increasing, the inefﬁciencies of sequence-based metho
2904639187	Neural Word Search in Historical Manuscript Collections.	2087914438	It turns out that for many manuscripts, especially historical ones, this is not a valid assumption. To remedy this, an increasing number of segmentation-free1 word spotting methods have been proposed [26], [28], [30], [39], [44], [45], [46] Segmentation-free word spotting approaches can typically be placed into two broad categories based on how they generate region from within an image. The ﬁrst are m
2904639187	Neural Word Search in Historical Manuscript Collections.	1980083143	uscripts, especially historical ones, this is not a valid assumption. To remedy this, an increasing number of segmentation-free1 word spotting methods have been proposed [26], [28], [30], [39], [44], [45], [46] Segmentation-free word spotting approaches can typically be placed into two broad categories based on how they generate region from within an image. The ﬁrst are methods based on a sliding wind
2904639187	Neural Word Search in Historical Manuscript Collections.	2142636459	ust against small shifts of the input, as this would lead to a misalignment av regions compared to ones extracted from the un-shifted input. The second category are methods using connected components [38], [44], [45], [47], [48]. In [44], connected components in the shape of vertical strokes are extracted by performing mathematical morphology to separate characters. A popular approach is to binarize t
2904639187	Neural Word Search in Historical Manuscript Collections.	2098411764	part of a word. In the case of word spotting, the attributes are then used to retrieve words with similar attributes. Attribute representations have been successfully used in zero-shot learning [35], [36] The recently introduced embedding for word spotting DCToW [20] is similar to PHOC in that it is handengineered, but it does not consist of binary attributes, but a low-frequency, real-valued represen
2904639187	Neural Word Search in Historical Manuscript Collections.	1922126009	y has a ﬁxed embedding. 2.3 Scene Text Recognition The model proposed in this paper is similar to work in endto-end scene text detection and recognition, which has been receiving increasing attention [55], [56], [57], [58], [59]. In [55], an end-to-end system for text localization, recognition and retrieval based on region proposals and CNNs is proposed. Another approach is made in [56], where charact
2905749056	Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.	2251765408	acted a large attention in recent times. Most of this research focuses on crosslingual word embeddings (Ruder et al.,2017), which are commonly learned jointly from parallel corpora (Gouws et al.,2015;Luong et al., 2015). An alternative approach that is becoming increasingly popular is to train word embeddings independently for each language over monolingual corpora, and then map them to a shared space based on a bil
2906255241	Improving Context-Aware Semantic Relationships in Sparse Mobile Datasets.	2131744502	lgorithm can help improve semantic understanding in various settings. 1. Introduction Determining the semantic similarity between texts is an important task in practical NLP. New methods like Doc2Vec [4] and Contextual Salience [10] achieve better results by incorporating context in computing semantic similarity. However, these methods still rely on solely textual features. When a dataset is sparse,
2906255241	Improving Context-Aware Semantic Relationships in Sparse Mobile Datasets.	2131744502	uilding better contextual understanding. Doc2Vec is a method that learns continuous distributed vector representations for inputted text, allowing it to better incorporate text ordering and semantics [4]. This allows it to take the context of a document into account when computing similarity. CoSal computes the importance of a word given its context. This is then used to produce weighted bag-of-words
2906275210	Sentiment Classification of Customer’s Reviews About Automobiles in Roman Urdu	2169768310	For the computation part, Multinomial Naïve Bayes has better efficiency in learning and classification than the Decision Tree classifier [19] and consequently the classifiers which use decision tree at the backend.
2906275210	Sentiment Classification of Customer’s Reviews About Automobiles in Roman Urdu	2097726431	Growing availability of opinion-rich resources like online blogs, social media, review sites have raised new opportunities and challenges [2].
2906275210	Sentiment Classification of Customer’s Reviews About Automobiles in Roman Urdu	2029115643	As they hold very little or no information about the sentiment of the review, so they are removed from the data [13], [14].
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2616122292	, 2016; Eric et al., 2017). For example, the KB in the Table 1 will be denoted as {(Tom’s house, distance, 3 miles), ..., (Starbucks, address, 792 Bedoin St)}. On the other hand, the dialogue context X is stored in the dialogue memory module, where the speaker and temporal encoding are included as in Bordes & Weston (2017) like a triplet format.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2427764808	, 2016), and natural language generation (Sharma et al., 2016) are used.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2565031282	, 2017), query reduction networks modify query between layers (Seo et al., 2017), and memory networks (Bordes & Weston, 2017; Liu & Perez, 2017; Wu et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	1975244201,2473965551	, 2017), a set of modules for natural language understanding (Young et al., 2013; Chen et al., 2016), dialogue state tracking (Lee & Stent, 2016; Zhong et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2616122292	(Models with * are reported from Eric et al. (2017), where the problem is simplified to the canonicalized forms.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	1793121960	In addition, the MN is well-known for its multiple hop reasoning ability (Sukhbaatar et al., 2015), which is appealing to strengthen copy mechanism.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2565031282	Baselines are reported from Query Reduction Network (Seo et al., 2017), End-to-end Memory Network (Bordes & Weston, 2017), Gated Memory Network (Liu & Perez, 2017), Point to Unknown Word (Gulcehre et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2745673470	Such copy mechanisms have also been used in other natural language processing tasks, such as question answering (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2626778328	However, different from other models that use a single matrix representation for reading and writing, GLMP leverages end-to-end memory networks to perform multiple hop attention, which is similar to the stacking self-attention strategy in the Transformer (Vaswani et al., 2017).
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2616122292	In the human-human dialogue dataset (Eric et al., 2017), GLMP is able to surpass the previous state of the art on both automatic and human evaluation, which further confirms the effectiveness of our double pointers usage.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2616122292,2698810342	For human-human dialogue scenario, we follow previous dialogue works (Eric et al., 2017; Zhao et al., 2017; Madotto et al., 2018) to evaluate our system on two automatic evaluation metrics, BLEU and entity F1 score 2.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2409591106,2616122292	In the KB memory module, each element bi ∈ B is represented in the triplet format as (Subject, Relation, Object) structure, which is a common format used to represent KB nodes (Miller et al., 2016; Eric et al., 2017).
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2340944142	For the modularized systems (Williams & Young, 2007; Wen et al., 2017), a set of modules for natural language understanding (Young et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2698810342	In order to reduce human effort and scale up between domains, end-to-end dialogue systems, which input plain text and directly output system responses, have shown promising results based on recurrent neural networks (Zhao et al., 2017; Lei et al., 2018) and memory networks (Sukhbaatar et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2415583245	Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016) and meta-learning applications (Kaiser et al.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	1975244201,2340944142	Traditional pipeline solutions are composed of natural language understanding, dialogue management and natural language generation (Young et al., 2013; Wen et al., 2017), where each module is designed separately and expensively.
2906574053	Global-to-local Memory Pointer Networks for Task-Oriented Dialogue.	2616122292	We use two public multi-turn task-oriented dialogue datasets to evaluate our model: the bAbI dialogue (Bordes & Weston, 2017) and Stanford multi-domain dialogue (SMD) (Eric et al., 2017).
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2126631960	, all “classes”) that appear in the testing set also appear in the training corpus; this is known as “closed-set attribution” and is common practice in author recognition [35, 44].
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2054151502	ark, so that entry refers to the appearance of either of those two marks.) An alternative is to consider the frequency of punctuation marks relative to the number of characters or words in a document [16]. To compute f2;k and f3;k, we consider a categorical Markov chain on the ordered set of Pull out all the stops 9 punctuation marks and associate each punctuation mark with a state of the Markov chain
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2054151502	hat explore other features, such as the number of words between elements in ordered pairs of punctuation marks (even when they are not successive) and dierent ways of measuring punctuation frequency [16] and sentence length [47]), and try to quantify how large a sample of a document is necessary to correctly identify its features of punctuation style. If it is suciently small, it may even be possibl
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	1607829561	nce [25], an information-theoretic measure related to Shannon entropy and ideas from maximum-likelihood theory. KL divergence and variants of it have been used in prior research on author recognition [2,35,50]. One can also consider other similarity measures, such as chi-squared distance [35] and Jensen{Shannon divergence [1,15,31]. Consider a random variable Xwith discrete nite support x2X, and let p 2[0;
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2297500149	nctuation-based approach to stylometry also provide an opportunity to apply other methods for analyzing categorical time series (e.g., an extension of rough-path signatures to categorical time series [8,32]). On a more general front, relevant stylometric applications include analysis of stylistic dierences in punctuation between politicians from dierent political parties [5] and in comparisons between
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2048328704	ncy and the order of punctuation marks in a corpus of 651authors and 14947documents (see section 3). We then explore genre recognition [9,23,40,41] from a punctuation perspective and stylochronometry [6,12,21,22,37,45,48] in section 4 and section 5, respectively. There are not many studies of stylochronometry, and existing ones tend to be rather specic in nature (e.g., focused on particular authors, such as Shakespea
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2126631960	The latter is often regarded as the foundation of computerassisted stylometry (in contrast with methods based on human expertise) [35, 44].
2907323567	Pull out all the stops: Textual analysis via punctuation sequences.	2054151502	ythm of speech2 and how these features contribute to meaning [18]. To our knowledge, very few researchers have explored author recognition using only stylometric features that are punctuation-focused [7,16]. Additionally, the few existing works that include a punctuation-focused analysis used a very small author corpus (40 authors in [16] and 5 authors in [7]) and focused on the frequency with which di
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2119821739	., 1998), and one using a combination of LSTM and CNN architectures; 4 shallow versions, based on Naive Bayes (NB) (Rish, 2001), Random Forest (RF) (Liaw et al., 2002), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and XGBoost (Chen and Guestrin, 2016), and they use unigrams, bigrams, and trigrams as features. 12 individual adversarial evaluators are trained, all based on SVM. All 19 discriminative evaluators
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2154652894	, 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are commonly used to evaluate NLP tasks – machine translation and text summarization in particular.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2171928131	, 2011b), (Mikolov et al., 2011a) have inspired the use of neural network based architectures for the task of natural language generation (NLG).
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2130942839	, 2017), machine translation (Sutskever et al., 2014), (Bahdanau et al.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1956340063	: BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 7 2005), which are borrowed from machine translation tasks, ROUGE (Lin, 2004) that is borrowed from text summarization tasks, and CIDEr (Vedantam et al., 2015) that is borrowed from image description evaluation. An important aspect of these metrics is that they rely on matching n-grams in the target text (i.e., generated reviews) to the ground truth text, o
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2115221470	015), (Dai et al., 2017), machine translation (Sutskever et al., 2014), (Bahdanau et al., 2014), to text summarization (Rush et al., 2015), dialogue systems (Wen et al., 2015), and poetry generation (Zhang and Lapata, 2014), deep neural networks have shown promising results for many natural language processing applications, and they have quickly replaced traditionally handcrafted rule-based or template-based approaches
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2154652894	2005), which are borrowed from machine translation tasks, ROUGE (Lin, 2004) that is borrowed from text summarization tasks, and CIDEr (Vedantam et al.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2065553296	al text lies in how well it serves the user to fulﬁll a speciﬁc function. (Young, 1999) generate instructional texts and determine how informative these are when users carry the directions outlined. (Mani et al., 1999) perform extrinsic summary evaluation by comparing the functional value of a summary vs. the entire document. (Carenini and Moore, 2006) evaluate persuasive texts by assessing how users rank items in
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2161283199	his is an automated approximation of the Turing test, where machine judges are used to replace human judges. Discriminative machine judges can be trained either using a data set with explicit labels (Ott et al., 2011), or using a mixture of text written by real humans and those generated by the model being evaluated. The latter is usually referred to as adversarial evaluation. Bowman et al. proposes one of the ear
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1905882502	and Belz, 2009), (Mikolov et al., 2011b), (Mikolov et al., 2011a) have inspired the use of neural network based architectures for the task of natural language generation (NLG). From image captioning (Karpathy and Fei-Fei, 2015), (Dai et al., 2017), machine translation (Sutskever et al., 2014), (Bahdanau et al., 2014), to text summarization (Rush et al., 2015), dialogue systems (Wen et al., 2015), and poetry generation (Zhan
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1582774210	We find the best hyper-parameters (Bergstra and Bengio, 2012) using random search, and prevent the models from overfitting by using early stopping (Prechelt, 1998).
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2152921782	e humans as judges, either through some type of Turing test (Machinery, 1950) to distinguish generated text from human input texts, or to directly compare the texts generated by different generators (Mellish and Dale, 1998). Such approaches are hard to scale, and they have to be redone whenever a new generator is included. Practically, it is important to ﬁnd automated metrics to evaluate the quality of a generator indep
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1978078764	e how informative these are when users carry the directions outlined. (Mani et al., 1999) perform extrinsic summary evaluation by comparing the functional value of a summary vs. the entire document. (Carenini and Moore, 2006) evaluate persuasive texts by assessing how users rank items in a list, while (Di Eugenio et al., 2002) measure the learning gain in intelligent tutoring systems with an NLG component. Nevertheless, t
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2123301721	e utility of such an approach in developing generative models that interact with humans is an open question. Text Overlap Metrics, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are commonly used to evaluate NLP tasks – machine translation and text summarization in particular. They are borrowed to evaluate language generation by comparing the similarity between the generate
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2064675550	We employ a total of 7 metaadversarial evaluators: 3 deep versions, among which one using LSTM (Hochreiter and Schmidhuber, 1997), one using Convolutional Neural Network (CNN) (Kim, 2014), (LeCun et al.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1489525520	ermutations and substitutions) in a generated sentence, not equally grammatical or semantically plausible, are not distinguishable by the BLEU score. The authors posit in favour of human evaluations (Callison-Burch et al., 2006). GAN-based text generation models are compared against a maximum likelihood estimation (MLE) baseline in (Caccia et al., 2018). The authors ﬁnd that models trained with MLE yield superior quality-div
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2295598076	f LSTM and CNN architectures; 4 shallow versions, based on Naive Bayes (NB) (Rish, 2001), Random Forest (RF) (Liaw et al., 2002), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and XGBoost (Chen and Guestrin, 2016), and they use unigrams, bigrams, and trigrams as features. 12 individual adversarial evaluators are trained, all based on SVM. All 19 discriminative evaluators are trained with balanced training sets
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2028602070	ful, as long as the inter-rater disagreements are resolved. 5.2 Role of Diversity We also assess the role diversity plays in the rankings of the generators. To this end, we measure lexical diversity (Bache et al., 2013) of the samples produced by each generator as the ratio of unique tokens to the total number of tokens. We compute in turn lexical diversity for unigrams, bi12 Generative Text Model LSTM CNN CNN &amp;
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1999965501	h text overlap metrics. We also ﬁnd that diversity is an intriguing metric that is indicative of the assessments of different evaluators. 1 Introduction Recent developments in neural language models (Mikolov and Zweig, 2012), (Reiter and Belz, 2009), (Mikolov et al., 2011b), (Mikolov et al., 2011a) have inspired the use of neural network based architectures for the task of natural language generation (NLG). From image ca
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2099471712	However, it is shown that SS is an inconsistent training strategy, and instead adversarial training strategies are more suitable for generative models (Huszár, 2015). Generative Adversarial Networks (Goodfellow et al., 2014), or GANs, train generative models through an adversarial process. A GAN works through the interplay of two feedforward neural networks that are trained simultaneously by competing against each other:
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2616969219	how likely the discriminator would be fooled by a complete sequence of tokens, which is passed back to the intermediate state-action steps using Monte Carlo search. Following this direction, RankGAN (Lin et al., 2017) proposes a newer framework which evaluates the quality of a set of generated sequences collectively. The discriminator in RankGAN, R ˚, is trained to rank the model-generated sentences lower than hum
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1590378318	n a given context). Indeed, “there is no agreed objective criterion for comparing the goodness of texts” (Dale and Mellish, 1998), and there lacks a clear model of text quality in the NLG literature (Hardcastle and Scott, 2008). Conventionally, most NLG systems have been evaluated in a rather informal manner. (Reiter and Belz, 2009) divide existing evaluation methods commonly employed in text generation into three categorie
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	648786980	n leads to errors that accumulate quickly over the generated sequence and compromise the predictive power of the model (Lamb et al., 2016). To solve this problem, Scheduled Sampling (SS) is proposed (Bengio et al., 2015) to train RNNs, which mixes the ground-truth outputs and the model-generated outputs as the training inputs. However, it is shown that SS is an inconsistent training strategy, and instead adversarial
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2101105183	native evaluator makes more mistakes on reviews it generated, w.r.t. the ground truth. 3.3.3 Word overlap evaluators We include a set of 4 word overlap metrics that are used for NLG evaluation: BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 7 2005), which are borrowed from machine translation tasks, ROUGE (Lin, 2004) that is borrowed from text summarization tasks, and CIDEr (Vedantam et al., 2015) that is
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1982897610	nd Mellish, 1998), and there lacks a clear model of text quality in the NLG literature (Hardcastle and Scott, 2008). Conventionally, most NLG systems have been evaluated in a rather informal manner. (Reiter and Belz, 2009) divide existing evaluation methods commonly employed in text generation into three categories: i) evaluations based on task performance, assessing the impact of generated texts on end users, ii) huma
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2101105183	o patterns which are not apparent to humans, however the utility of such an approach in developing generative models that interact with humans is an open question. Text Overlap Metrics, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are commonly used to evaluate NLP tasks – machine translation and text summarization in particular. They are borrowed to evaluate language g
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2101105183	perior quality-diversity trade-off according to negative log-likelihood and Self-BLEU metrics, where Self-BLEU (Zhu et al., 2018) measures the diversity of the generated text by calculating the BLEU (Papineni et al., 2002) score for each generated sentence using all other generated sentences in the corpus as references. Shi et al. compare frameworks for text generation including MLE, SeqGAN, LeakGAN and Inverse Reinfor
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2016589492	The most popular strategy for training RNNs is teacher forcing (Williams and Zipser, 1989).
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	648786980	ramework Word LSTM temp 1.0 (Hochreiter and Schmidhuber, 1997) No Word LSTM temp 0.7 (Hochreiter and Schmidhuber, 1997) No Word LSTM temp 0.5 (Hochreiter and Schmidhuber, 1997) No Scheduled Sampling (Bengio et al., 2015) No Google LM (Jozefowicz et al., 2016) No Attention Attribute to Sequence (Dong et al., 2017) No Contexts to Sequences (Tang et al., 2016) No Gated Contexts to Sequences (Tang et al., 2016) No MLE Se
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2474824677	Recent developments in neural language models (Mikolov and Zweig, 2012), (Reiter and Belz, 2009), (Mikolov et al., 2011b), (Mikolov et al.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2064675550	Recurrent Neural Networks (RNNs), Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) models in particular, are widely used for generating sequential data including text.
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1982897610	rs to make rapid changes to their systems and automatically tune parameters without human intervention. Despite the beneﬁts, however, the use of automated metrics in the ﬁeld of NLG is controversial (Reiter and Belz, 2009), and their results are often criticized as not meaningful for a number of reasons. First, these automatic evaluations rely on a high-quality reference corpus as references, which is not often availab
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	590442793	screte nature of sequence data. SeqGAN (Yu et al., 2017) is one of the earliest GAN-based model for sequence generation, which treats the generation procedure as a sequential decision making process (Bachman and Precup, 2015). The model addresses the problem of non-differentiability of discrete outputs by treating the generator as a stochastic parameterized policy trained via policy gradient (Sutton et al., 2000) and opti
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2616969219	Sequence (Dong et al., 2017) No Contexts to Sequences (Tang et al., 2016) No Gated Contexts to Sequences (Tang et al., 2016) No MLE SeqGAN (Yu et al., 2017) Yes SeqGAN (Yu et al., 2017) Yes RankGAN (Lin et al., 2017) Yes LeakGAN (Guo et al., 2017) Yes Table 1: Overview of generative models employed for generating online product reviews. Every generator (other than the Google LM) is trained and validated on G-trai
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2616969219	t; the generator G is trained to confuse the ranker R ˚in such a way that its generated sentences can be ranked higher than the human written ones. Many GAN-based text generators (Yu et al., 2017), (Lin et al., 2017), (Rajeswar et al., 2017), (Che et al., 2017), (Li et al., 2017), (Zhang et al., 2017) are only capable of generating short texts, say 20 tokens long. LeakGAN (Guo et al., 2017) is proposed for genera
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2542835211	token by the model itself. This discrepancy between training and inference often leads to errors that accumulate quickly over the generated sequence and compromise the predictive power of the model (Lamb et al., 2016). To solve this problem, Scheduled Sampling (SS) is proposed (Bengio et al., 2015) to train RNNs, which mixes the ground-truth outputs and the model-generated outputs as the training inputs. However,
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2097998348	tors are trained, all based on SVM. All 19 discriminative evaluators are trained with balanced training sets; for the shallow models we use 10-fold cross validation. We ﬁnd the best hyper-parameters (Bergstra and Bengio, 2012) using random search, and prevent the models from overﬁtting by using early stopping (Prechelt, 1998). For every review in D-test (either annotated or not), a discriminative evaluator makes a judgment
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	2620623908	is trained to confuse the ranker R ˚in such a way that its generated sentences can be ranked higher than the human written ones. Many GAN-based text generators (Yu et al., 2017), (Lin et al., 2017), (Rajeswar et al., 2017), (Che et al., 2017), (Li et al., 2017), (Zhang et al., 2017) are only capable of generating short texts, say 20 tokens long. LeakGAN (Guo et al., 2017) is proposed for generating longer texts. In Lea
2907514250	Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation.	1982897610	also ﬁnd that diversity is an intriguing metric that is indicative of the assessments of different evaluators. 1 Introduction Recent developments in neural language models (Mikolov and Zweig, 2012), (Reiter and Belz, 2009), (Mikolov et al., 2011b), (Mikolov et al., 2011a) have inspired the use of neural network based architectures for the task of natural language generation (NLG). From image captioning (Karpathy and Fe
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2251939518	, 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan & Brockett, 2005), the Quora Question Pairs2 (QQP), and STS-Benchmark (STS; Cer et al.
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	1599016936	For the explicitly adversarial WNLI dataset (based on the Winograd Schema Challenge; Levesque et al., 2011), only one of our models reached even the most frequent class performance of 56.
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2612953412	There has been a great deal of work on sentence-to-vector encoding, a setting in which the pretrained encoder produces a fixed-size vector representation for each input sentence (Dai & Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Yang et al., 2018).
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2153579005	This has prompted interest in pretraining for sentence encoding: There is good reason to believe it should be possible to exploit outside data and training signals to effectively pretrain these encoders, both because they are intended to primarily capture sentence meaning rather than any task-specific skill, and because we have seen dramatic successes with pretraining in the related domains of word embeddings (Mikolov et al., 2013) and image encoders (Zamir et al.
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2551396370	For the remaining sentence-pair tasks (MNLI, QNLI, QQP, STS), we use an attention mechanism between all pairs of words, followed by a 512D×2 BiLSTM with max-pooling over time, following the basic mechanism used in BiDAF (Seo et al., 2017).
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2257408573	Translation We train MT models on two datasets: WMT14 English-German (Bojar et al., 2014) and WMT17 English-Russian (Bojar et al.
2907630459	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling.	2250861254	Work toward learning reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011), but has seen a recent surge in progress with the successes of CoVe (McCann et al.
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2107878631	al network is mostly used in sequence modelling problems in the domain of natural language processing (NLP) [29, 30, 31, 32, 33]. The significant issue of RNN is that vanishing and exploding gradient [28]. Long short-term memory (LSTM) is one of significant methods of deep learning that focusing on learning long-range temporal dependencies in large sequences of arbitrary length [2, 3, 4]. LSTM have es
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2160003195	al uses Identical Vectors or i-Vectors to extract features like accents and dialects in speech processing for effective results[14].
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	1657486103	Arabic Dialect Identification for NLP is a new branch and significant amount of work has been done in this area for both speech[9-11] as well as textual forms[13].
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2064675550	ce than the latter [27]. Long short-term memory (LSTM) is one of significant methods of deep learning that focusing on learning long-range temporal dependencies in large sequences of arbitrary length [2, 3, 4]. LSTM have established as a promising approach for sequence data modeling by solving the tasks such as machine translation [5] and many others [6]. Hence in this work, we use a LSTM model the subtask
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2064675550	exploding gradient [28]. Long short-term memory (LSTM) is one of significant methods of deep learning that focusing on learning long-range temporal dependencies in large sequences of arbitrary length [2, 3, 4]. LSTM have established as a promising approach for sequence data modeling by solving the tasks such as machine translation [5] and many others [6]. Where , , , term denotes the input gate output gate
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	1598796236	LSTM have established as a promising approach for sequence data modeling by solving the tasks such as machine translation [5] and many others [6].
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	1533946607	n was another subtask for the competition. Works on German Dialect Identification are lesser in number when compared to Arabic. One of the initial works use n-gram method at a character level for this[18].Another very different approach included recognising the high frequency words in the data[19].The dataset that we use here consists of speech transcripts provided by the VarDial 2017 subtask taken fr
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2130942839	range temporal dependencies in large sequences of arbitrary length [2, 3, 4]. LSTM have established as a promising approach for sequence data modeling by solving the tasks such as machine translation [5] and many others [6]. Where , , , term denotes the input gate output gate, forget gate and a memory cell respectively. Conventional RNN only facilitate to learn the past dependencies. Capturing future
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2130942839	range temporal dependencies in large sequences of arbitrary length [2, 3, 4]. LSTM have established as a promising approach for sequence data modeling by solving the tasks such as machine translation [5] and many others [6]. Hence in this work, we use a LSTM model the subtask of Discriminating Similar Languages (DSL) while we use Bidirectional LSTM for Arabic Dialect Identification(ADI) and German Di
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2014077556	two similar languages is a hurdle for Language Identification systems especially when the languages are nearly related.This area has been explored recently like the works by on South-Slavic languages[24], Mandarin varieties in Singapore,Taiwan and China [25], Malay and Indonesian languages [26]. Inspired by such works, a lot of shared task competitions give importance to such tasks. Arabic Dialect Id
2907758695	A Deep Learning Approach for Similar Languages, Varieties and Dialects.	2160802179	tions give importance to such tasks. Arabic Dialect Identification for NLP is a new branch and significant amount of work has been done in this area for both speech[9, 10, 11] as well as textual forms[13]. A.Shoufan and S.Al-Ameri discusses in detail the different approaches done for Arabic Dialect identification[12] till 2015. It can be used as a source to get information about the relevant works on
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	1793121960,2131494463	, 2015), multi-hop memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Kumar et al., 2016), as well as cross-sequence attention models for span-extraction QA.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2551396370	, 2017; 2018), bidirectional attention (Seo et al., 2017), and query-context attention (Yu et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	1843891098	, 2017), summarization (Rush et al., 2015), and semantic parsing (Dong & Lapata, 2018).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2738152205	, 2018; Deunsol Yoon, 2018), coreference resolution (Lee et al., 2017), dialogue state-tracking (Zhong et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2626778328	, 2018), machine translation (Vaswani et al., 2017), and semantic parsing (Kitaev & Klein, 2018).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	1544827683,2557764419	, 2018), news articles (Hermann et al., 2015; Trischler et al., 2017), books (Richardson et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2133564696	Bahdanau et al. (2015) demonstrate the use of attention over the encoder to capture soft alignments for machine translation.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2157331557	We begin by encoding each sequence using a bidirectional Gated Recurrent Units (GRUs) (Cho et al., 2014).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2463565445	Coattention as a means to encode codependent representations between two inputs has also been successfully applied to visual question answering (Lu et al., 2016) in addition to textual question answering.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2551396370	Coattention and similar techniques are crucial to single-document question answering models (Xiong et al., 2017; Wang & Jiang, 2017; Seo et al., 2017).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	1544827683,2427527485,2557764419	Although existing datasets enabled the development of effective end-to-end neural question answering systems, they tend to focus on reasoning over localized sections of a single document (Hermann et al., 2015; Rajpurkar et al., 2016; 2018; Trischler et al., 2017).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	1544827683	Although existing datasets enabled the development of effective end-to-end neural question answering systems, they tend to focus on reasoning over localized sections of a single document (Hermann et al., 2015; Rajpurkar et al., 2016; 2018; Trischler et al., 2017). For example, Min et al. (2018) find that 90% of the questions in the Stanford Question Answering Dataset are answerable given 1 sentence in a document.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2411480514	These include early document attention models for cloze-form QA (Chen et al., 2015), multi-hop memory networks (Weston et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2250539671	Petrov (2009) provides a detailed overview of this technique and demonstrates its effectiveness on parsing, speech recognition, and machine translation.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2251818205,2427527485,2510759893	QA tasks span a variety of sources such as Wikipedia (Yang et al., 2015; Rajpurkar et al., 2016; 2018; Hewlett et al., 2016; Joshi et al., 2017; Welbl et al., 2018), news articles (Hermann et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2095705004	We regularize the model using dropout (Srivastava et al., 2014) at several locations in the model: after the embedding layer with a rate of 0.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2137699405,2151258001	A similar task that also requires aggregating information from multiple documents is query-focused multi-document summarization, in which a model summarizes a collection of documents given an input query (Dang, 2006; Gupta et al., 2007; Lu et al., 2013).
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2250539671	We use a embedding size of demb = 400, 300 of which are from GloVe vectors (Pennington et al., 2014) and 100 of which are from character ngram vectors (Hashimoto et al.
2907822478	Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering	2250539671	We use fixed GloVe embeddings (Pennington et al., 2014) as well as character ngram embeddings (Hashimoto et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2111072639	, 1994), which some people have started to call extreme learning machines (Huang et al., 2006).
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	, 2015), labelled entailment corpora (Conneau et al., 2017), image-caption data (Kiela et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2251047310	, 2016) and other learning frameworks using raw text (Le & Mikolov, 2014; Pham et al., 2015; Jernite et al., 2017; Pagliardini et al., 2017), a collection of books (Kiros et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	5 of Conneau et al. (2017)), we observe that higher dimensionality in most cases leads to better performance.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	We analyze random sentence embeddings by examining how these embeddings perform on the probing tasks introduced by Conneau et al. (2018), in order to gauge what properties of sentences they are able to recover.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	We compare primarily to two well-studied sentence embedding models, InferSent (Conneau et al., 2017) and SkipThought (Kiros et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	Conneau et al. (2017) reported good performance for the random LSTM model on the transfer tasks.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	78356000,2546302380	In fact, it is well-known that random weights do well, as for example shown in computer vision with respect to convnets (Jarrett et al., 2009; Saxe et al., 2011).
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2103305545	Methods include autoencoders (Socher et al., 2011; Hill et al., 2016) and other learning frameworks using raw text (Le & Mikolov, 2014; Pham et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	It has been observed that bidirectional LSTMs with max-pooling perform surprisingly well even without any training whatsoever (Conneau et al., 2017; 2018), leading to claims that such architectures “encode priors that are intrinsically good for sentence representations” (Conneau et al.
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	2612953412	The probing tasks consist of those in Conneau et al. (2018). We use the default SentEval settings (see Appendix A).
2907849599	No Training Required: Exploring Random Encoders for Sentence Classification	104184427	Similarly, the importance of random initializations has been examined in depth (Sutskever et al., 2013).
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1686810756	, 2012) results in slightly better captions than using VGG-16 (Simonyan and Zisserman, 2014), even though the first CNN has an object recognition top-1 accuracy of 57.
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1861492603	• Different captions: Another in-domain corpus consisting of the sentences in the MSCOCO (Lin et al., 2014) dataset, which is another image captions corpus but which is different from Flickr8K.
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2337363174	al., 2014), in order to have more meaningful word vectors which were optimised using large text corpora. Apart from the word embedding layer, there is also work on transferring the RNN layer as well (Zoph et al., 2016, Ramachandran et al., 2017, Howard and Ruder, 2018, Mou et al., 2016). Howard and Ruder (2018) claim that language modelling is a universal source task for transfer learning in natural language proce
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2555428947	cation. Ramachandran et al. (2017) also perform transfer learning from the source task of language modelling but transfer to machine translation and abstractive summarisation. As in the present work, Ramachandran et al. (2017) implement an architecture that makes it possible to re-use the unconditioned source language model in a target task that requires a conditioned language model, which we solved by using the merge arch
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1895577753	dataset, in order to extract visual features from images. Using pre-trained neural networks to transform inputs into high level features for other neural networks makes it easier to avoid overtting (Vinyals et al., 2015). Does this advantage only apply to the visual part of the caption generator? In this paper we investigate whether the language part of the image caption generator can also be handled by a pre-trained
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2149933564	r a dual RNN architecture, which is less computationally demanding. Another important consideration in transfer learning is the relative value of freezing parameters versus ne-tuning during training. Yosinski et al. (2014) performed experiments on transfer learning in CNNs and tried transferring a variable number of layers from the input side of the neural network. With regards to the dierence between freezing and ne-
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1905882502	e frozen during training of the caption generator or allowed to be optimised together with the rest of the parameters. Flickr8K and MSCOCO were both obtained from the distributed versions provided by Karpathy and Fei-Fei (2015)3. We also vary the size of the language model corpus training sets in order to measure the eect of size apart from domain, where sizes are measured as the number of sentences. A random sample of sen
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2105103432	e target model is an untrained image caption generator. Not all caption generator architectures allow for this kind of parameter transferring. If the image is provided as an initial state to the RNN (Devlin et al., 2015, Liu et al., 2016), called an ‘init-inject’ architecture (Tanti et al., 2018), then the image would need to be taken into account when training the RNN and hence cannot be trained separately. Instead
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	658020064	erform hyperparameter tuning using Bayesian optimisation. As an optimisation cost function, geometric mean of perplexity was used for the language model whilst the word mover’s distance (WMD) metric (Kusner et al., 2015, Kilickaya et al., 2017) was used on the caption generator.5 The model, whose purpose is to predict the tness of a given hyperparameter combination, is a random forest and was initialized using 32 ra
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1956340063	having dierent initial random weights, minibatches, and other non-deterministic values). The mean of the results for the quality of generated captions using METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and WMD (Kusner et al., 2015, Kilickaya et al., 2017) is shown in Table 3. 4.1 Transfer learning versus non-transfer learning Transfer learning always improves over no
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1686810756	Image features were extracted from the penultimate layer in the VGG-16 CNN (Simonyan and Zisserman, 2014).
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2250539671	r to improve another model’s learning process in a dierent task or domain. In NLP tasks, it is common to transfer word embeddings from other tasks, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), in order to have more meaningful word vectors which were optimised using large text corpora. Apart from the word embedding layer, there is also work on transferring the RNN layer as well (Zoph et al
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2310102669	itialised later layers. Fine-tuning allows the transferred and randomly initialised layers to co-operate at reaching a suitable representation. This observation has been called fragile co-adaptation. Mou et al. (2016) found similar results for text classication tasks. When transferring the embedding layer and RNN, performance is always better when the transferred parameters are ne-tuned 1See: https://github.com/m
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2123301721	in the language model (as well as having dierent initial random weights, minibatches, and other non-deterministic values). The mean of the results for the quality of generated captions using METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and WMD (Kusner et al., 2015, Kilickaya et al., 2017) is shown in Table 3. 4.1 Transfer learning versus non-transfer learning Transfer l
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2555428947	r to have more meaningful word vectors which were optimised using large text corpora. Apart from the word embedding layer, there is also work on transferring the RNN layer as well (Zoph et al., 2016, Ramachandran et al., 2017, Howard and Ruder, 2018, Mou et al., 2016). Howard and Ruder (2018) claim that language modelling is a universal source task for transfer learning in natural language processing. They perform transfe
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1811254738	Mou et al. (2016) also found that when transferring between similar tasks, such as from a sentiment analysis task to a different sentiment analysis task, performance always improves when both the embedding layer and the RNN are transferred.
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	658020064	on-deterministic values). The mean of the results for the quality of generated captions using METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and WMD (Kusner et al., 2015, Kilickaya et al., 2017) is shown in Table 3. 4.1 Transfer learning versus non-transfer learning Transfer learning always improves over non-transfer learning, although not drastically. Finetuning ten
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	6908809	pdf), or AdaDelta (Zeiler, 2012) whilst the learning rate could be between 1e-5 and 1.
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2310102669	ptimised using large text corpora. Apart from the word embedding layer, there is also work on transferring the RNN layer as well (Zoph et al., 2016, Ramachandran et al., 2017, Howard and Ruder, 2018, Mou et al., 2016). Howard and Ruder (2018) claim that language modelling is a universal source task for transfer learning in natural language processing. They perform transfer learning by using a language model as an
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2159243025	ption generator architecture called a ‘merge’ architecture (Tanti et al., 2018), shown in Figure 1, which leaves the vision encoding part and language encoding part of the caption generator separate (Mao et al., 2014, 2015a,b). 1 arXiv:1901.01216v1 [cs.CL] 1 Jan 2019 Figure 1: The architectures of the language model (top) and caption generator (bottom) with the part that is transferred having a bold outline. Both
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2173180041	On a similar note, Hessel et al. (2015) found something similar with image caption generators.
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	1938708284	task. It turns out that the state-of-the-art CNNs do not produce the best xed image features (according to the logistic regressor) but that it was some of the lesser CNNs that do. On a similar note, Hessel et al. (2015) found something similar with image caption generators. Dierent pre-trained CNNs were used to extract image features to be used for training image caption generators. It was found that using AlexNet
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2165698076	to use these randomly specied (but deterministic) features, resulting in a decently performing caption generator. Code used for these experiments is provided online.1 2 Background Transfer learning (Pan and Yang, 2010) is the act of exploiting the knowledge gained by a trained model in order to improve another model’s learning process in a dierent task or domain. In NLP tasks, it is common to transfer word embeddi
2907886055	Transfer learning from language models to image caption generators: Better models may not transfer better.	2506483933	weights, minibatches, and other non-deterministic values). The mean of the results for the quality of generated captions using METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and WMD (Kusner et al., 2015, Kilickaya et al., 2017) is shown in Table 3. 4.1 Transfer learning versus non-transfer learning Transfer learning always improves over non-transfer learning, although n
2907903401	Machine Translation: A Literature Review.	2117130368	[10] have shown that continuous representations for words are able to capture the syntactic, semantic and morphological properties of the words.
2907903401	Machine Translation: A Literature Review.	1753482797	[25] propose a class of probabilistic translation models, Recurrent Continuous Translation Model (RCTM) for machine translation.
2907903401	Machine Translation: A Literature Review.	2154124206	[36] proposed a maximum entropy models for phrase-based translation where the translation probability is formulated as conditional log-linear model.
2907903401	Machine Translation: A Literature Review.	2048390999	in [4] outlined an approach to use statistical inference tools to solve the task.
2907903401	Machine Translation: A Literature Review.	2097333193	in [5] provided first experimental results for a statistical machine translation technique translating sentences in French to English.
2907903401	Machine Translation: A Literature Review.	2097333193	[5], takes the view that every sentence S in a source language has a possible translation T in the target language.
2907903401	Machine Translation: A Literature Review.	2172140247	[8] showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.
2907903401	Machine Translation: A Literature Review.	2157331557	[9] for a machine translation task, it remains the base model for most of the NLP sequence-to-sequence models (and especially machine translation).
2907903401	Machine Translation: A Literature Review.	2108598243	They also show that attention and mixture-of-experts blocks, designed for textual data (especially machine translation) doesn’t hurt the performance of other completed unrelated tasks like classification on ImageNet[13].
2907903401	Machine Translation: A Literature Review.	2153579005	and Dahl et al. [11]) tasks. Since then, they have also been successfully applied to solve many NLP tasks like paraphrase detection (Socher et al. [41]) and word embedding extraction (Mikolov et al. [32]). Neural networks have also been applied to advance the state-of-the-art in statistical machine translation. Schwenk [38] summarizes usage of feedforward neural networks in the framework of phrase-ba
2907903401	Machine Translation: A Literature Review.	2626792426	English to French translation task show that this method provides improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. 5 Current Research Kaiser et al. [24] recently proposed an interesting neural network architecture - ‘One Model to learn them all’. Its a Multi-Model architecture that can simultaneously learn many tasks across domains. At its core, it h
2907903401	Machine Translation: A Literature Review.	2100506586	into language models since the majority of research in SMT is focused on different formulations of the translation model, but the reader can refer to the following resources for more information [16] [37] [3]. 3.2 Word-based SMT Post Warren Weaver’s proposal in 1949 [46] to use statistical techniques from the then nascent ﬁeld of communication theory to the task of using computers to translate text fr
2907903401	Machine Translation: A Literature Review.	2006969979	le the individual distributions model the particular events well, there is room for improvement in the model’s capacity to translate. The fundamental basis of the ﬁve models presented by Brown et al. [6] was the introduction of a hidden alignment variable in the translation model. These alignment probabilities were then modelled differently in different models. Vogel et al. [44] propose a new alignme
2907903401	Machine Translation: A Literature Review.	2006969979	n french translation), &quot;the&quot; has fertility 1, and &quot;implemented&quot; has fertility 3. Figure 1: Alignment between two sentences. Building on top of their previous work, Brown et al. in [6] describe a set of ﬁve statistical models, each with a different model of the alignment probability distribution. Speciﬁcally, they modify their translation model to include the alignment variable A.
2907903401	Machine Translation: A Literature Review.	2038968650	ntly, continuous representations have been proposed for phrases and sentences and have been shown to carry task-dependent information to help downstream language processing tasks (Grefenstette et al. [17], Socher et al. [40], Hermann et al. [19]). The approaches discussed above make use of neural networks to model continuous representations of linguistic units. Deep neural networks have shown tremendo
2907903401	Machine Translation: A Literature Review.	2147768505	presentations of linguistic units. Deep neural networks have shown tremendous progress in computer vision (eg., Krizhevsky et al. [26]) and speech recognition (eg., Hinton et al. [20] and Dahl et al. [11]) tasks. Since then, they have also been successfully applied to solve many NLP tasks like paraphrase detection (Socher et al. [41]) and word embedding extraction (Mikolov et al. [32]). Neural network
2907903401	Machine Translation: A Literature Review.	2006969979	probabilities depend on the relative position of the word alignment rather than the absolute position. This HMM model is shown to result in smaller perplexities as compared to Model 2 by Brown et al. [6] and also produces smoother alignments. With the increased focus on research in alignment models, Och and Ney [35] present an annotation and evaluation scheme for word-alignment models. The proposed a
2907903401	Machine Translation: A Literature Review.	1992097457	The reader is encouraged to look at [14], [15], [45] for more information on design of decoders and their nuances.
2907903401	Machine Translation: A Literature Review.	2130942839	red together. While Cho et al. [9] proposed an end-to-end RNN architecture, they use it only to get additional phrase translation table to be eventually used in the SMT based system. Sutskever et al. [43] gave a more formal introduction to the sequence-to-sequence RNN encoder-decoder architecture. Though their motivation was to investigate the ability of very-deep neural networks at solving seq-to-seq
2907903401	Machine Translation: A Literature Review.	2161792612	rnt, the decoding operation (equation-1) generates translated sentences. The reader is encouraged to look at [14], [15], [45] for more information on design of decoders and their nuances. Marcu et al.[31] present a different formulation of phrase-based model to learn the phrase transition table and distortion distribution. They argue that lexical correspondences can be established not only at the word
2907903401	Machine Translation: A Literature Review.	2012511220	rs (phrase translation tables, distortion distribution, language model) are learnt, the decoding operation (equation-1) generates translated sentences. The reader is encouraged to look at [14], [15], [45] for more information on design of decoders and their nuances. Marcu et al.[31] present a different formulation of phrase-based model to learn the phrase transition table and distortion distribution.
2907903401	Machine Translation: A Literature Review.	196214544	the syntactic, semantic and morphological properties of the words. Continuous representations for characters have also shown notable results in language modelling task as proposed by Sutskever et al. [42]. Recently, continuous representations have been proposed for phrases and sentences and have been shown to carry task-dependent information to help downstream language processing tasks (Grefenstette e
2907903401	Machine Translation: A Literature Review.	2161792612	th these models. Och et al.[35] alignment template model can be reframed as a phrase translation system; Yamada and Knight[48] use phrase translation in a syntax based translation system; Marcu et al.[31] introduced a joint-probability model for phrase translation. At its core, phrase-based translation system has a phrase translation probability table (deﬁned above) to map phrases in source language t
2907903401	Machine Translation: A Literature Review.	2064675550	At each time step t, the hidden state ht of the RNN is updated by: ht = f(ht−1, xt) (11) where the f is non-linear activation function which is usually implemented with LSTM cell (Hochreiter and Schmidhuber [21]).
2907903401	Machine Translation: A Literature Review.	2006969979	utes P(TjS). This would eliminate the need for the language model of the target language, and can be used in conjunction with the decoder to get the translation of the original sentence. Brown et al. [6] state this to be a means to get a well-formed sentence. To model P(TjS) and use this for translation would require the probabilities to be concentrated over well-formed sentences in the target langua
2907903401	Machine Translation: A Literature Review.	2116316001	y over IBM models and many researches tried to advance the state-of-the-art with these models. Och et al.[35] alignment template model can be reframed as a phrase translation system; Yamada and Knight[48] use phrase translation in a syntax based translation system; Marcu et al.[31] introduced a joint-probability model for phrase translation. At its core, phrase-based translation system has a phrase tr
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	1843891098	[21] apply the seq2seq mechanism with attention model to text summarization field.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2606974598	[22] add copy mechanism and coverage loss to generate summarization without out-of-vocabulary and redundancy words.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2096259080	[39] propose a framework for opinion QA, which first organizes reviews into a hierarchy structure and retrieves review sentence as the answer.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	1552182777	Since automatic evaluation metrics may not always consistent with human perception [26], we use human evaluation in our experiment.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2399880602	We also consider three embedding-based metrics [6] (including Embedding Average, Embedding Greedy and Embedding Extreme) to evaluate our model, following several recent studies on text generation [24, 30, 35].
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2130942839	To evaluate the performance of our dataset and the proposed framework, we compare our model with the following baselines: (1) S2SA: Sequence-to-sequence framework [28] has been proposed for language generation task.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2101105183	To evaluate our proposed method, we employ BLEU [19] to measure the quality of generated sentence by computing overlapping lexical units (e.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2427527485	Given a question and relevant passages, reading comprehension extracts a text span from passages as an answer [20].
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	1993077754	Incorporating review information, recent studies employ ranking strategies to optimize an answer from candidate answers [17, 38].
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2120615054	Each kernel corresponds a linguistic feature detector which extracts a specific pattern of multi-grained n-grams [12].
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2402144811	model using TensorFlow [1] framework and train our model and all baseline models on NVIDIA Tesla P40 GPU.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2304113845,2606974598	In order to handle the out-of-vocabulary (OOV) problem, we equip the pointer network [7, 22, 33] with our decoder, whichmakes our decoder capable to copy words from question.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2130942839	The seq2seq model [28] is originally proposed for machine translation and later adapted to various natural language generation tasks, such as text summarization and dialogue generation [31, 36].
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	1993077754	Unlike either a “yes/no” binary classification task [13] or a review ranking task [17], product-aware answer generation provides a natural-sounding sentence as an answer.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2133564696,2304113845	We use seq2seq framework which is equipped with attention mechanism [3] and copy mechanism [7] as baseline method.
2908018635	Product-Aware Answer Generation in E-Commerce Question-Answering	2402144811	where ε ∼ U [0, 1] is a random number and λ is a coefficient of gradient penalty term.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	, the Lowe’s Ubuntu dataset (Lowe et al. 2015) and E-commerce dataset (Zhang et al.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2221711388	2016), MatchingLSTM (Wang and Jiang 2016), Attentive-LSTM (Tan, Xiang, and Zhou 2015), and Multi-Channels (Wu et al.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2626778328	2017), where one improved with gated self attention and the other improved with the Transformer structure (Vaswani et al. 2017).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	2017) and retrieval-based methods (Lowe et al. 2015; Wu et al. 2017).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	1522301498	Adam (Kingma and Ba 2014) was used for optimization with an initial learning rate of 0.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	In addition, the proposed approach outperforms all previous models, including the previous state-of-the-art hierarchy-based methods, on two large-scale public benchmark datasets, the Lowe’s Ubuntu (Lowe et al. 2015) and E-commerce datasets (Zhang et al.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	We convert the problem into a binary classification task, similar to the previous work (Lowe et al. 2015; Wu et al. 2017).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2780932362	We used GloVe (Pennington, Socher, and Manning 2014) and fastText (Mikolov et al. 2018) as pre-trained word embeddings.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2597655663	Here, we use BiLSTMs with multi-head self-attention pooling to encode sentences (Lin et al. 2017; Chen, Ling, and Zhu 2018), and an MLP to classify.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2780932362	There are many kinds of pre-trained word embeddings available, such as GloVe (Pennington, Socher, and Manning 2014) and fastText (Mikolov et al. 2018).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2153579005	For Lowe’s Ubuntu and E-commerce datasets, we used pre-trained word embedding on the training data by word2vec (Mikolov et al. 2013).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	889023230	Multi-turn dialogue modeling can be divided into generation-based methods (Serban et al. 2016; Zhou et al. 2017) and retrieval-based methods (Lowe et al.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	Previous work used TF-IDF, RNN (Lowe et al. 2015) and CNN, LSTM, BiLSTM (Kadlec, Schmid, and Kleindienst 2015) to encode the context and the response.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	889023230	With the recent success of deep learning models (Serban et al. 2016), building an end-to-end dialogue system became feasible.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2338325072	The second group of models consists of sequence-based matching models, which usually use the attention mechanism, including MV-LSTM (Wan et al. 2016), MatchingLSTM (Wang and Jiang 2016), Attentive-LSTM (Tan, Xiang, and Zhou 2015), and Multi-Channels (Wu et al.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2127589108	Sentence-encoding based models use the Siamese architecture (Bromley et al. 1993; Chen et al. 2017b) shown in Figure 1 (a).
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	We used a similar data augmentation strategy as in (Lowe et al. 2015), i.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	2153579005	For subtask 5 of the Ubuntu dataset, we also used word2vec (Mikolov et al. 2013) to train word embedding from the provided Linux manual pages.
2908602207	Sequential Attention-based Network for Noetic End-to-End Response Selection.	836999996	The typical approaches for multi-turn response selection mainly consist of sequence-based methods (Lowe et al. 2015; Yan, Song, and Wu 2016) and hierarchy-based methods (Zhou et al.
2908662516	Human few-shot learning of compositional instructions.	2225340466	Thirty participants in the United States were recruited using Amazon Mechanical Turk and the psiTurk platform (Gureckis et al., 2015).
2908694274	Viewpoint Invariant Change Captioning.	2337353209	Others aim to generate a discriminative caption for an image or image region, which would allow to distinguish it from a distractor [4, 11, 37, 36, 53, 59].
2908694274	Viewpoint Invariant Change Captioning.	2170140722	Change detection in images is a long-standing research problem, with applications in a variety of domains including facility monitoring, medical imaging, and aerial photography, among others [17, 44, 48]; a key challenge in change detection is to distinguish the important changes from the irrelevant ones [47] since the former are those that should likely trigger a notification.
2908694274	Viewpoint Invariant Change Captioning.	1956340063,2101105183,2123301721,2506483933	To evaluate our methods on the captioning task, we rely on BLEU-4 [41], METEOR [8], CIDEr [54], and SPICE [2] metrics which measure overall sentence fluency and similarity to the ground-truths.
2908694274	Viewpoint Invariant Change Captioning.	1947481528	Expressing image content in natural language is an active area of Artificial Intelligence research, with numerous approaches to image captioning having been recently proposed [3, 13, 35, 57].
2908694274	Viewpoint Invariant Change Captioning.	2332488709	Some focus on generating textual explanations for deep models’ predictions [19, 20, 42].
2908694274	Viewpoint Invariant Change Captioning.	1895577753,1947481528	While most image captioning works focus on a generic task of obtaining image relevant descriptions [3, 13, 55], some recent works explore pragmatic or “task-specific” captions.
2908694274	Viewpoint Invariant Change Captioning.	2194775991	In our implementation, Xbef, Xaft ∈ RC×H×W are image features of Ibef, Iaft, respectively, encoded by a pretrained ResNet [18].
2908694274	Viewpoint Invariant Change Captioning.	2617840581	Instead of relying on costly pixel-level video annotation, [29] propose a weakly supervised approach, which estimates pixel-level labels with a CRF.
2908694274	Viewpoint Invariant Change Captioning.	2171810632,2463565445	Using multiple spatial attentions has been shown to be useful for many purposes including multi-step/hierarchical reasoning [58, 39, 34] and model interpretability [30, 42].
2908694274	Viewpoint Invariant Change Captioning.	2560645892,2575842049	Multiple works have since adopted and extended this approach [15, 33, 45], including performing attention over object detections [3].
2908694274	Viewpoint Invariant Change Captioning.	2101705628	using a popular CDnet benchnmark [16, 56], where background subtraction is a successful strategy [9].
2908694274	Viewpoint Invariant Change Captioning.	2108598243,2194775991,2613526370	Similar to [22, 26, 46], we use ResNet-101 [18] pretrained on ImageNet [12] to extract visual features from the images.
2908694274	Viewpoint Invariant Change Captioning.	2133564696	Soft attention mechanism [7] over the visual features was first used for image captioning by [57].
2908694274	Viewpoint Invariant Change Captioning.	2171810632,2463565445	Unlike [58, 39, 34, 30, 42], VIDAM utilizes dual attention to process multiple visual inputs separately and thereby addresses viewpoint invariance.
2908777866	Emotion Detection using Data Driven Models.	2059689786	[4] From this paper the author says that a standard regression system had been created and various experiments have been conducted to show the affected lexicons which shows the scores that are related to the word emotion that are helpful in determining the emotion intensity.
2908777866	Emotion Detection using Data Driven Models.	2519781924	This paper deals with the polarity classification and quantification of the English tweets which involves in the taking of the information from the message level and the topic based sentiment [8].
2908834655	What do Language Representations Really Represent	2610003462	1 Genetic Distance Following Rabinovich et al. (2017), we use phylogenetic trees from Serva and Petroni (2008) as our
2908834655	What do Language Representations Really Represent	2610003462	This is the exact same data as used by Rabinovich et al. (2017), originating from Europarl (Koehn, 2005).
2908834655	What do Language Representations Really Represent	2344508595	Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts.
2908834655	What do Language Representations Really Represent	1869752048	Note that this is similar to the target side of Vinyals et al. (2015). All content words are replaced by POS tags, while function words are kept.
2908834655	What do Language Representations Really Represent	2610003462	Our work is most closely related to Rabinovich et al. (2017) who investigate representation learning on monolingual English sentences, which are translations from various source languages to English from the Europarl corpus (Koehn, 2005).
2909234552	On Event Causality Detection in Tweets.	1811279891	[2] proposes a method that uses distributional probability and discourse connectives to detect event causality.
2909234552	On Event Causality Detection in Tweets.	1581597064	Additionally, the authors identify discourse relation in text using penn discourse treebank (PDTB) [13].
2909234552	On Event Causality Detection in Tweets.	2114117930	Another approach proposed by Mirza [6] extracts temporal and causal relationships between events, and assumes that cause must precede the effect and propose a methodology to improve the temporal relation extraction between events.
2909234552	On Event Causality Detection in Tweets.	2141125852	The authors apply a multicolumn neural network [3] to extract causal relationship between candidate phrases using archived web text.
2909234552	On Event Causality Detection in Tweets.	2250861254	Candidate cause and effect phrases are then passed to the Stanford dependency parser [17] to detect the root word for each phrase.
2909234552	On Event Causality Detection in Tweets.	2485674475	Due to such challenges existing rule-based methods [7], [10] are less effective in event causality detection from short texts such as tweets.
2909234552	On Event Causality Detection in Tweets.	2153579005	To convert e ′ 1 and e2 into v, we train a Word2vec model [19] from 1 million news articles (the same dataset that is used to build the causal network).
2909234552	On Event Causality Detection in Tweets.	2250494780	One of the early approaches that identifies explicit, implicit and non-causal relationships between verb-verb pairs is proposed in [4].
2909234552	On Event Causality Detection in Tweets.	2114117930,2485674475	At first, a sentence is split into candidate causal and effect phrases using a set of causal cue words [6], [7] (please see Table II).
2909234552	On Event Causality Detection in Tweets.	2485674475	Hence, the linguistic rule-based approaches [7], [10], which depend on grammatical correctness of text, perform poorly on tweets (see section V).
2909234552	On Event Causality Detection in Tweets.	1662133657	However, the PMI based approaches are sensitive to co-occurance frequency and do not perform well for infrequent events [14].
2909234552	On Event Causality Detection in Tweets.	2485674475	We observe better performance of the proposed system compared to the existing state-of-theart event causality detection systems such as Commonsense [7], Commonsense+Multi-word [10] and FFNN+Position [11] based systems.
2909234552	On Event Causality Detection in Tweets.	2485674475	A more recent approach [7] builds a causality network of terms from a collection of web text.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2131357087	, 2013), linking to the knowledge bases (Mihalcea and Csomai, 2007), mapping to semantic frames (Punyakanok et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	62188290	, 2013), linking to the knowledge bases (Mihalcea and Csomai, 2007), mapping to semantic frames (Punyakanok et al., 2004), etc.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2127426251,2466714650,2608309533	, 2017; Angeli and Manning, 2014), chaining relations to infer new relations (Socher et al., 2013; McCallum et al., 2017; Khot et al., 2017; Khashabi et al., 2018), and possible combinations of the aforementioned paradigms (Gardner et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2112085250	, using executable formulas (Reddy et al., 2017; Angeli and Manning, 2014), chaining relations to infer new relations (Socher et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	1566346388	(Roth and Yih, 2004) suggested a general abductive framework that addresses it by connecting reasoning to models learned from data; it has been used in multiple NLP reasoning problems (Khashabi et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2252123671	Example proposals to deal with this issue are, extracting semantic parses (Kaplan et al., 1982; Steedman and Baldridge, 2011; Banarescu et al., 2013), linking to the knowledge bases (Mihalcea and Csomai, 2007), mapping to semantic frames (Punyakanok et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2608309533	Existing models of multihop reasoning (Khot et al., 2017) use similar features to identify valid reasoning chains.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2107019937	An important challenge in many language understanding problems is the symbol grounding problem (Harnad, 1990), the problem of accurately mapping symbols into its underlying meaning representation.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2131357087,2151048449	Practitioners often address this challenging by enriching their representations; for example by mapping textual information to Wikipedia entries (Mihalcea and Csomai, 2007; Ratinov et al., 2011), or grounding text to exear X iv :1 90 1.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2112090702	"This principle states that in many realworld graphs, nodes are all linked by short chains of acquaintances, such as “six degrees of separation"" (Milgram, 1967; Watts and Strogatz, 1998)."
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2250184916	Specifically, we consider FB15k237 (Toutanova and Chen, 2015) containing a set of 〈head, relation, target〉 triples from a curated knowledge base, FreeBase (Bollacker et al.
2909295022	On the Capabilities and Limitations of Reasoning for Natural Language Understanding.	2000985912,2107019937	It is widely believed that a key obstacle to progress has been the symbol grounding problem (Harnad, 1990; Taddeo and Floridi, 2005).
2909311579	Robust Chinese Word Segmentation with Contextualized Word Representations.	2250799792,2608256743	diction (Pei et al., 2014; Ma and Hinrichs, 2015; Zhang et al., 2016a; Liu et al., 2016; Cai et al., 2017; Wang and Xu, 2017; Ma et al., 2018).
2909672886	Multi-style Generative Reading Comprehension.	2626778328	As in (Vaswani et al., 2017), the selfattention sub-layer uses a sub-sequent mask to prevent positions from attending to subsequent positions.
2909672886	Multi-style Generative Reading Comprehension.	2626778328	We extend the mechanism to a Transformer (Vaswani et al., 2017) based one that allows words to be generated from a fixed vocabulary and words to be copied from both the question and multiple passages.
2909672886	Multi-style Generative Reading Comprehension.	2551396370	R L×J between the question and k-th passage, as is done in (Seo et al., 2017), where
2909672886	Multi-style Generative Reading Comprehension.	2183341477	We also used one-sided label smoothing (Szegedy et al., 2016) for the passage relevance and answer possibility labels.
2909672886	Multi-style Generative Reading Comprehension.	2626778328	For the self-attention layer, we adopt the multi-head attention mechanism defined in (Vaswani et al., 2017).
2909672886	Multi-style Generative Reading Comprehension.	2194775991	Each sub-layer is placed inside a residual block (He et al., 2016).
2909777606	Dialog System Technology Challenge 7.	2665731731	[2018], that framework is extended as each system input consists of two parts: Conversational input: Similar to DSTC6 Track 2 [Hori and Hori, 2017], all preceding turns of the conversation are available to the system.
2909777606	Dialog System Technology Challenge 7.	1518951372	(we set K = 2to reduce the judges’ cognitiveload). Notethatthis judgmenthas nothingto dowith groundinginexternalsources,andis similar to humanjudgments for prior data-drivenconversationmodels (e.g., [Sordoni et al., 2015]). Interest: This evaluation criterion measures the degree to which the produced response is interestingandinformativeinthecontextofadocumentprovidedbytheURL.Sinceitwouldbeimpractical to show entire
2909777606	Dialog System Technology Challenge 7.	2603266952	Our architecture is similar to the Hierarchical Recurrent Encoder in Das et al. [2016]. The question, visual features, and the dialog history are fed into corresponding LSTM-based encoders to build up a context embedding, and then the outputs of the encoders are fed into an LSTM-based decoder to generate an answer.
2909777606	Dialog System Technology Challenge 7.	2665731731	We built a baseline end-to-end dialog system that can generate answers in response to user questions about events in a video sequence as described in Hori et al. [2018a]. Our architecture is similar to the Hierarchical Recurrent Encoder in Das et al.
2909777606	Dialog System Technology Challenge 7.	2251058040	Dialog State Tracking Challenges 2 (Henderson et al. [2014a]) and 3 (Henderson et al.
2909777606	Dialog System Technology Challenge 7.	889023230	Such end-to-end approaches have been shown to better handle flexible conversations by enabling model training on large conversational datasets Vinyals and Le [2015], Hori et al.
2909777606	Dialog System Technology Challenge 7.	1993567041	Such end-to-end approaches have been shown to better handle flexible conversations by enabling model training on large conversational datasets Vinyals and Le [2015], Hori et al. [2018b]. However, current dialog systems cannot understand a scene and have a conversation about what is going on in it.
2909777606	Dialog System Technology Challenge 7.	1933349210	In the field of computer vision, interaction with humans about visual information has been explored in visual question answering (VQA) by Antol et al. [2015] and visual dialog by Das et al.
2909777606	Dialog System Technology Challenge 7.	836999996	Finally, as part of the challenge, we provided a baseline system that implemented the Dual-Encoder model from Lowe et al. [2015]. This lowered the barrier to entry, encouraging broader participation in the task.
2909777606	Dialog System Technology Challenge 7.	836999996	Following Lowe et al. [2015], we use Recall@N, where we count how often the correct answer is within the top N specified by a system.
2909777606	Dialog System Technology Challenge 7.	1518951372	s how to effectively use external information – none of the teams managed to substantially improveperformancefrom subtask 1 to subtask 5. 3 Sentence Generation Track Recent work [Ritter et al., 2011, Sordoni et al., 2015, Shang et al., 2015, Vinyals and Le, 2015, Serban et al.,2016,etc.] hasshownthatconversationalmodelscanbetrainedinacompletelyend-toend and data-driven fashion, without any hand-coding. However, prior
2909777606	Dialog System Technology Challenge 7.	2159640018	se external information – none of the teams managed to substantially improveperformancefrom subtask 1 to subtask 5. 3 Sentence Generation Track Recent work [Ritter et al., 2011, Sordoni et al., 2015, Shang et al., 2015, Vinyals and Le, 2015, Serban et al.,2016,etc.] hasshownthatconversationalmodelscanbetrainedinacompletelyend-toend and data-driven fashion, without any hand-coding. However, prior work has mostly foc
2909777606	Dialog System Technology Challenge 7.	889023230	To set up the Audio Visual Scene-Aware Dialog (AVSD) track, we collected text-based dialogs about short videos of Charades by Sigurdsson et al. [2016], a dataset of untrimmed and multiaction videos, along with video descriptions in Alamri et al.
2909777606	Dialog System Technology Challenge 7.	836999996	We sidestepped the challenge of evaluating generated utterances by formulating the problem as response selection, as proposed by Lowe et al. [2015]. At test time, participants were provided with partial conversations, each paired with a set of utterances that could be the next utterance in the conversation.
2909777606	Dialog System Technology Challenge 7.	1956340063	We suspect this deficiency of multi-reference BLEU, previously noted in Vedantam et al. [2015], to be due to its parameterization as a precision metric.
2909777606	Dialog System Technology Challenge 7.	2463565445	The task follows the data-driven framework established in 2011 by Ritter et al. [2011], which avoids hand-coding any linguistic, domain, or task-specific information.
2909777606	Dialog System Technology Challenge 7.	10957333	tstanding challenge is how to effectively use external information – none of the teams managed to substantially improveperformancefrom subtask 1 to subtask 5. 3 Sentence Generation Track Recent work [Ritter et al., 2011, Sordoni et al., 2015, Shang et al., 2015, Vinyals and Le, 2015, Serban et al.,2016,etc.] hasshownthatconversationalmodelscanbetrainedinacompletelyend-toend and data-driven fashion, without any hand-
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2740643840	. Li et al. proposed to an attention-based dual-glance model for social relation recognition, in which the ﬁrst glance extracted features from persons and the second glance focused on contextual cues [6]. Wang et al. proposed to model persons and objects in an image as a graph and perform relation reasoning by a Gated Graph Neural Network [9]. However, they only considered the co-existence of persons
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2346746376	Differently, explicit recognition of social relation from images just attracts researchers in recent years with the release of large-scale datasets [5, 1, 6].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2558630670	disciplinary research of multimedia and sociology has been studied for many years [2, 12]. Popular topics include social networks discovery [13], key actors detection [14], group activity recognition [15], and so on. In recent years, social recognition from images has attracted attention from researchers [1, 6, 9, 10]. For example, Zhang et al. proposed to learn social relation traits from face images
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2051481196	the effectiveness of our method. 1.1. Related Work The interdisciplinary research of multimedia and sociology has been studied for many years [2, 12]. Popular topics include social networks discovery [13], key actors detection [14], group activity recognition [15], and so on. In recent years, social recognition from images has attracted attention from researchers [1, 6, 9, 10]. For example, Zhang et a
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2479423890	eline on the PIPA dataset. It is focused on human bodies and head regions. A group of attributes such as gender, age, clothing, and actions are extracted for social relation recognition. 2) Union-CNN [27]. This method adopt a CNN model to identify the relationship through persons joint area. The results of Union-CNN are from the implementation in [1]. 3) Dual-glance [6]. This approach designs a deep C
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2206328504	Existing methods for social relation recognition usually utilize low-level visual features such as the appearance of persons, face attributes, and contextual objects [1, 10].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2609468337	haviors or emotions of human beings. The task of image-based social relation recognition is to classify a pair of persons in an image into one of pre-deﬁned relation types such as friend, family, etc [1]. It Person A Person B Car A Bag Car C Car B Street Person A Person B Car A Bag Car C Car B Street Family Stranger Fig. 1. How do we recognize two persons are family or strangers from an image? The sc
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2609468337	Implementation Details Datasets. We conduct experiments on two large-scale social relation datasets, i.e., the People in Social Context (PISC) dataset [6] and the People in Photo Album (PIPA) dataset [1]. The PISC dataset contains several common social relations in daily life, which has a hierarchy of three coarse-level relations and six ﬁne-level ships. In our experiments, we aim to recognize the si
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	1745797027,1989004008	has many important applications such as personal image collection mining [2] and social event understanding [3].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2609468337,2740643840	include social networks discovery [13], key actors detection [14], group activity recognition [15], and so on. In recent years, social recognition from images has attracted attention from researchers [1, 6, 9, 10]. For example, Zhang et al. proposed to learn social relation traits from face images by CNNs [10]. Sun et al. proposed a social relation dataset based on the social domain theory [16] and exploited C
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2244807774	ion on the datasets of PIPA [1] and PISC [6]. It represents the persons and objects existing in an image as a weighted graph, on which the social relation is predicted by a Gated Graph Neural Network [28]. 5) Multi-Granularity Reasoning (MGR) is our proposed framework for social relation recognition. The experimental results are listed in Table 1. From the results, we can ﬁnd that Union-CNN and two st
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2740643840	to model the interactions among persons and objects. One simple method is focused on the co-existence of persons and objects in an image by combining their features to learn a uniform representation [6]. Another strategy is to build a graph, in which persons and objects are fully connected [9]. However these approaches are all neglect the interactions among persons and objects. For example, two pers
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2479423890	on. Table 1. Comparison to the state-of-the-art methods on PISC and PIPA datasets. PISC PIPA Friend Family Couple Profes. Commer. No Rela. mAP Accuracy Two stream CNN [1] - - - - - - - 57.2 Union-CNN [27] 29.9 58.5 70.7 55.4 43.0 19.6 43.5 - Dual-glance [6] 35.4 68.1 76.3 70.3 57.6 60.9 63.2 59.6 GRM [9] 59.6 64.4 58.6 76.6 39.5 67.7 68.7 62.3 MGR (ours) 64.6 67.8 60.5 76.8 34.7 70.4 70.0 64.4 Table 2
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2609468337,2740643840	been a popular topic during the last decade [2, 4]. Differently, explicit recognition of social relation from images just attracts researchers in recent years with the release of large-scale datasets [5, 1, 6]. From traditional hand-crafted features to recent deep learning-based algorithms, the performance of methods achieves signiﬁcant improvement [7, 8, 9]. However, social relation recognition from image
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2309415944	proposed a Graph Long Short-Term Memory to propagate information in graphs built on super-pixels for semantic object parsing [22].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2206328504	proposed to learn social relation traits from face images by CNNs [10].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2206328504	In recent years, social recognition from images has attracted attention from researchers [1, 6, 9, 10].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2609468337	relations even for human beings. Existing methods for social relation recognition usually utilize low-level visual features such as the appearance of persons, face attributes, and contextual objects [1, 10]. AlarXiv:1901.03067v1 [cs.CV] 10 Jan 2019 though some approaches explore the relations between persons and objects, they only consider the co-existence in an image [6, 9, 11]. However, only depending
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	1745797027,2147377836	Sociological analysis based on images has been a popular topic during the last decade [2, 4].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2740643840	tes, and contextual objects [1, 10]. AlarXiv:1901.03067v1 [cs.CV] 10 Jan 2019 though some approaches explore the relations between persons and objects, they only consider the co-existence in an image [6, 9, 11]. However, only depending on the singlegranularity representation can hardly overcome the domain gap between visual features and social relations. How do human beings recognize the relation between tw
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2206427987	thod. 1.1. Related Work The interdisciplinary research of multimedia and sociology has been studied for many years [2, 12]. Popular topics include social networks discovery [13], key actors detection [14], group activity recognition [15], and so on. In recent years, social recognition from images has attracted attention from researchers [1, 6, 9, 10]. For example, Zhang et al. proposed to learn social
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	2194775991	Traditional CNN usually applies 2-D or 3-D filters on images to abstract visual features from low-level space to high-level space [24].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	1900913856,2124687448	From traditional hand-crafted features to recent deep learning-based algorithms, the performance of methods achieves significant improvement [7, 8, 9].
2909858050	Multi-Granularity Reasoning for Social Relation Recognition from Images.	1993164181,2161149720	ulti-Granularity Reasoning framework to explore complementary cues for social relation recognition. Graph has been widely adopted to model the visual relations in computer vision and multimedia tasks [17, 18, 19, 20]. In recent years, researchers have studied message propagation in graphs by trainable networks, such as Graph Convolutional Networks (GCN) [21]. Most recently, these models have been adopted to compu
2909879461	Incremental Reading for Question Answering.	1867966810	Early Stopping These incremental models can be used to support early stopping [3, 12].
2909879461	Incremental Reading for Question Answering.	1544827683	However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11, 2, 4, 8].
2909879461	Incremental Reading for Question Answering.	1867966810,2521709538	Learning to stop reading or reasoning when the goal is reached is addressed for tasks like text classification [3] and question answering [9, 12, 6], but the main challenge is that implementing early stopping strategies is only possible when we have an incremental model.
2909879461	Incremental Reading for Question Answering.	2064675550	Long Short-Term Memory networks (LSTMs) [5], process data sequentially, and update their internal states as they read the new tokens.
2909970382	Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments.	2250539671	, 2013) and GloVe (Pennington et al., 2014) have been extemely successful and widely adopted in machine learning applications for language understanding.
2909970382	Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments.	2515741950	, 2016), sentence length and word content (Adi et al., 2017), or syntactic depth and morphological number (Conneau et al.
2909970382	Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments.	1846531235	Lawrence et al. (2000) train RNNs to do acceptability classification over sequences of POS tags corresponding to example sentences from a syntax textbook.
2909970382	Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments.	2153579005	Sentence Embeddings Robust pretrained word embeddings like word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.
2910063207	Computational Register Analysis and Synthesis.	2151752770	(Kjell et al 1995, Hoorn et al. 1999, Zhao &amp; Zobel 2005) which categorizes according to the label(s) of the nearest document(s) in the training set, and Naive Bayes (Kjell 1994 Hoorn et al. 1999, Peng et al 2004) which chooses the category label with the highest probability, assuming that features occur conditionally independently of each other given the text’s category. Excellent results for register and gen
2910063207	Computational Register Analysis and Synthesis.	1997424273	al. 2005, Zheng et al 2006, Sharoff et al. 2010) and neural networks (Matthews &amp; Merriam 1993, Merriam &amp; Matthews 1994, Kjell 1994, Lowe &amp; Matthews 1995, Tweedie et al. 1996, Hoorn 1999, Waugh et al. 2000). Rule-based learning (Holmes &amp; Forsyth 1995, Holmes 1998, Argamon et al. 1998, Koppel &amp; Schler 2003, Abbasi &amp; Chen 2005, Zheng et al. 2006) and Bayesian regression (Genkin et al. 2006, Ma
2910063207	Computational Register Analysis and Synthesis.	2118020653	For analyzing register in categorical terms, text classification techniques (Sebastiani 2002), usually based on machine learning, are used.
2910063207	Computational Register Analysis and Synthesis.	77037503	Belz (2005) describes a version of the system that used automatically learned rules without any human input – the results were not as good as those of SumTime-Mousam, but were still judged to be quite readable and understandable.
2910063207	Computational Register Analysis and Synthesis.	2133365465	cument or corpus whose style it is to imitate. One approach to corpus-based style control is manual analysis of the corpus to extract constraints and rules that can be applied during text generation (Hovy 1990, McKeown, Kukich, &amp; Shaw 1994, Reiter &amp; Williams 2010). A difficulty with this approach is that developing such rules is very labor intensive, and the results tend to be of only limited gener
2910063207	Computational Register Analysis and Synthesis.	88174951	etorical and coreference constraints to ensure completeness and coherence of the information to be presented. Constraint satisfaction is used to find such a coherent information structure to present (Marcu 1997). Aggregation of matching entities is applied to simplify syntactic structures and determine appropriate use of coreferential expressions, and then text is generated from the internal representation.
2910063207	Computational Register Analysis and Synthesis.	2484584769	In an excellent exemplar of classification analysis of register characteristics, Teich and Fankhauser (2009) consider the question of how clusters of linguistic characteristics emerge in new registers as multidisciplinary scientific fields coalesce.
2910063207	Computational Register Analysis and Synthesis.	1680392829	is measured. Provided good reliability for labeling is achieved, the corpus is used for training and testing the model, ensuring that testing is done on texts not used for training. Cross-validation (Kohavi 1995) is typically used, in which the full annotated data is divided randomly into several (usually 10) equal-sized “folds” subsets, then training the model on all but one fold and testing on the remaining
2910063207	Computational Register Analysis and Synthesis.	2165431734	A number of studies have found that using syntactic features can often improve results over traditional word-based analysis alone (Stamatatos et al. 2000).
2910063207	Computational Register Analysis and Synthesis.	2118020653	Register classification is evaluated in the same way as general text classification methods (Sebastiani 2002).
2910063207	Computational Register Analysis and Synthesis.	2170571488	to register and genre analysis in a number of domains after his original study. The factors in that study have emerged from analysis of diverse corpora, corroborating the general nature of that work (Lee 2001). Work applying multidimensional analysis to different corpora has been used to study the evolution of and variation in scientific writing (Atkinson 1992, 1998, Conrad 1996), the register and genre st
2910063207	Computational Register Analysis and Synthesis.	1645937837	Reiter and Dale (2000) break down text generation into three basic tasks: (a) document planning, determining the overall structure and content of a text, (b) microplanning, determining how the needed information is going to be expressed, in linguistic terms, in sentences and clauses, and (c) realization, generating an actual text conforming to these determinations.
2910063207	Computational Register Analysis and Synthesis.	2099471712	s generated and the length/complexity of sentences generated by the network, so it will generate sentences in a particular style. Improvements have been made by applying adversarial network training (Goodfellow et al. 2014), in which the style translating network tries during training to fool a second network that determines whether texts are in the target style or not. This training method has been applied recently to
2910063207	Computational Register Analysis and Synthesis.	2135528482	sification have been achieved using diverse machine learning algorithms including support vector machines (De Vel et al. 2001, Diederich et al. 2003, Koppel &amp; Schler 2003, Abbasi &amp; Chen 2005, Koppel et al. 2005, Zheng et al 2006, Sharoff et al. 2010) and neural networks (Matthews &amp; Merriam 1993, Merriam &amp; Matthews 1994, Kjell 1994, Lowe &amp; Matthews 1995, Tweedie et al. 1996, Hoorn 1999, Waugh et
2910063207	Computational Register Analysis and Synthesis.	1993539890	ssification (Kanaris &amp; Stamatatos 2007, Amasyalı &amp; Diri 2006) as well as other style classification tasks such as authorship attribution (Kjell 1995, Kešelj, Peng, Cercone, &amp; Thomas 2003, Stamatatos 2008), document similarity (Damashek 1995), and L1 identification (Koppel et al. 2005). Since character n-grams also capture aspects of document content, however, they must be used with caution and careful
2910063207	Computational Register Analysis and Synthesis.	2135528482	as other style classification tasks such as authorship attribution (Kjell 1995, Kešelj, Peng, Cercone, &amp; Thomas 2003, Stamatatos 2008), document similarity (Damashek 1995), and L1 identification (Koppel et al. 2005). Since character n-grams also capture aspects of document content, however, they must be used with caution and careful experimental control to ensure that the models that are constructed represent re
2910063207	Computational Register Analysis and Synthesis.	2118020653	As such, the register analysis problem is formulated as a text classification problem (Sebastiani 2002).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	1854537555	, 2011) and essays (Faulkner, 2014) and recent studies are mostly proposed for social media texts like tweets (Mohammad et al.
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2347127863	, 2011) and essays (Faulkner, 2014) and recent studies are mostly proposed for social media texts like tweets (Mohammad et al., 2017; Mourad et al., 2018; Küçük and Can, 2018).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2347127863	, 2016a) and its extended version also including sentiment annotations is described in (Mohammad et al., 2017).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2020278455,2070808142	Comprehensive surveys of NER research include (Nadeau and Sekine, 2007; Marrero et al., 2013; Goyal et al., 2018).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2153848201	The focus of recent NER studies has shifted from well-formed texts to social media texts including mostly microblog posts, as demonstrated in studies such as (Ritter et al., 2011; Liu et al., 2013).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2638397300	The initial version of the dataset comprises 700 tweets in Turkish annotated for stance by a single native annotator (Küçük, 2017b).
2910090829	A Tweet Dataset Annotated for Named Entity Recognition and Stance Detection.	2145327091	Similar to NER, earlier stance detection studies are performed on well-formed texts like online debate posts (Anand et al., 2011) and essays (Faulkner, 2014) and recent studies are mostly proposed for social media texts like tweets (Mohammad et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1956340063	, 2002), CIDEr (Vedantam et al., 2015), and METEOR (Banerjee & Lavie, 2005) scores and the results with different settings are shown in Table 6.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2157331557	, 2014), by providing sequence-level guidance based on a learned discriminator (or, critic). To construct such a loss, Fedus et al. (2018); Guo et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1843891098	, 2014), text summarization (Chopra et al., 2016; Rush et al., 2015) and image captioning (Vinyals et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2194775991	, 2015)4, and use Resnet-152 (He et al., 2016), image tagging (Gan et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1514535095,1895577753	, 2015) and image captioning (Vinyals et al., 2015; Xu et al., 2015).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2599772929	, 2016) also consider semantic similarity, however they also can not learn a good model on their own (Liu et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2625357353	, 2016) and (ii) adversarial-learning-based methods (Yu et al., 2017; Zhang et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2558834163	, 2016), image tagging (Gan et al., 2017), and FastRCNN (Anderson et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	658020064	, 2017) or a more efficient yet less accurate lower bound (Kusner et al., 2015).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2130942839,2133564696,2157331557	1 INTRODUCTION Sequence-to-sequence (Seq2Seq) models are widely used in various natural language processing tasks, such as machine translation (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014), text summarization (Chopra et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1861492603	3 IMAGE CAPTIONING We also consider an image captioning task using the COCO dataset (Lin et al., 2014), which contains 123,287 images in total and each image is annotated with at least 5 captions.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2626778328	5M sentence pairs, from the WMT Evaluation Campaign (Vaswani et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	648786980,2176263492	Consequently, MLE-based methods suffer from the so-called exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016), i.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2560313346	Consistent across-the-board improvements are observed with the introduction of the OT loss, in contrast to the RL-based methods where drastic improvements can only be observed for the optimized evaluation metric (Rennie et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2625357353	Directly sampling from the multinomial distribution ŵt ∼ Softmax(vt) is a non-differentiable operation1, so we consider the following differentiable alternatives: • Soft-argmax: ŵ t = Softmax(vt/τ), where τ ∈ (0, 1) is the annealing parameter (Zhang et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2176263492,2487501366	Such efforts roughly fall into two categories: (i) reinforcement-learning-based (RL) methods (Bahdanau et al., 2017; Ranzato et al., 2016) and (ii) adversarial-learning-based methods (Yu et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1514535095	We follow the implementation of the Show, Attend (Xu et al., 2015)4, and use Resnet-152 (He et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2101105183	Further, commonly used metrics for evaluating the generated sentences at test time are sequence-level, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2099471712	GAN for sequence generation Another type of method adopts the framework of generative adversarial networks (GANs) (Goodfellow et al., 2014), by providing sequence-level guidance based on a learned discriminator (or, critic).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2525778437	We used Google’s Neural Machine Translation (GMNT) model (Wu et al., 2016) as our baseline, following the architecture and hyper-parameter settings from the GNMT repository2 to make a fair comparison.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2625357353	On the other hand, adversarial supervision relies on the delicate balance of a mini-max game, which can be easily undermined by mode-trapping and gradient-vanishing problems (Arjovsky et al., 2017; Zhang et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2133564696	Our implementation of the Seq2Seq model adopts a simple architecture, which consists of a bidirectional GRU encoder and a GRU decoder with attention mechanism (Bahdanau et al., 2015)3.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2133564696	Our implementation of the Seq2Seq model adopts a simple architecture, which consists of a bidirectional GRU encoder and a GRU decoder with attention mechanism (Bahdanau et al., 2015)3. Results are summarized in Tables 4 and 5. Our OT-regularized model outperforms respective baselines. The state-of-the-art ROUGE result for the Gigawords dataset is 36.92 reported by Wang et al. (2018a). However, much more complex architectures are used to achieve that score.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2176263492	That is, MLE only provides a word-level training loss (Ranzato et al., 2016).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2542835211	Maximum likelihood estimation (MLE) is often used as the training paradigm in existing Seq2Seq models (Goodfellow et al., 2016; Lamb et al., 2016).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	1514535095	Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR CIDEr Soft Attention (Xu et al., 2015) 70.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2506483933	Some nonstandard metrics like SPICE (Anderson et al., 2016) also consider semantic similarity, however they also can not learn a good model on their own (Liu et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2560313346	on policy-gradient estimation, and control variates and carefully designed baselines (such as a selfcritic) are needed to make RL training more robust (Liu et al., 2018; Rennie et al., 2017).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2626778328	For reference, the GNMT model (same architecture) and a Transformer model (Vaswani et al., 2017) respectively report a score of 27.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2101105183	We report BLEU-k (k from 1 to 4) (Papineni et al., 2002), CIDEr (Vedantam et al.
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	658020064	Specifically, the OT objective aims to find an optimal matching of similar words/phrases between two sequences, providing a way to promote their semantic similarity (Kusner et al., 2015).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2250539671	tors x and y is a popular choice (Pennington et al., 2014).
2910092637	Improving Sequence-to-Sequence Learning via Optimal Transport	2176263492,2487501366,2560313346	Typically, this type of method employs RL by considering the evaluation metrics as the reward to guide the generation (Bahdanau et al., 2017; Huang et al., 2018; Ranzato et al., 2016; Rennie et al., 2017; Zhang et al., 2018a).
2910243263	Assessing BERT's Syntactic Abilities.	2626778328	The BERTmodel is based on the “Transformer” architecture (Vaswani et al., 2017), which—in contrast to RNNs—relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.
2910252757	Team Papelo: Transformer Networks at FEVER.	2251329024	We start with phrases tagged as named entities by SpaCy (Honnibal and Johnson, 2015), but these tags are not very reliable, so we include various capitalized phrases.
2910252757	Team Papelo: Transformer Networks at FEVER.	1840435438	tion data used as the basis for the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) dataset.
2910252757	Team Papelo: Transformer Networks at FEVER.	2626778328	Transformer networks (Vaswani et al., 2017) are deep networks applied to sequential input data, with each layer implementing multiple heads of scaled dot product
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	1866452853	1 % F1 score reported in Table 7 of (Veselovská et al., 2012).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	1866452853	One of the first attempts to create sentiment annotated resources of Czech texts dates back in 2012 (Veselovská et al., 2012).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	2056433889	Moreover, the kernel parameter enables them to perform well even with data that are not linearly separable by transforming the feature space (Kocsor and Tóth, 2004).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	1988790447	We picked SVM, Logistic Regression and Naı̈ve Bayes with their corresponding optimal set of parameters and tried to increase their performance further using Adaptive Boosting (Freund and Schapire, 1997).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	2113242816	Random Forests (RF) were also invented in the 90s (Ho, 1995; Ho, 1998).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	2019759670	Sentiment Analysis is considered as the automated analysis of sentiments, emotions or opinions expressed in texts towards certain entities (Medhat et al., 2014).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	2119821739	Support Vector Machines have been successfully used for solving both classification and regression problems since back in the nineties when they were invented (Boser et al., 1992; Cortes and Vapnik, 1995).
2910363984	Sentiment Analysis of Czech Texts: An Algorithmic Survey.	2153579005,2250539671	Besides using this traditional approach based on Tf-Idf or similar vectorizers, it is also possible to analyze text by means of the more recent dense representations called word embeddings (Mikolov et al., 2013; Pennington et al., 2014).
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2250309026	[1] confirmed the findings of Stab and Gurevych [17] that information from dependency parsers does not help to find the (general) argument structure in persuasive essays and Wikipedia articles while simpler structural features such as punctuation are
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2610091188	[3] noted that claims across different domains share lexical clues and further stated that current
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2021366071	[7] collected sentences comparing drug therapies using manually crafted patterns to recognize the subjects of comparison and the comparison direction.
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2295598076	[8]: XGBoost [2] using word frequencies as representations, which achieves a high F1 score of 0.
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	147952312	[9] described a system based on manually collected patterns on the basis of lexical matches and dependency parses in order to identify comparison targets and to classify the type of comparison into the classes given by Jindal and Liu [11]: gradable vs.
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2171278097	Advanced question answering systems, such as IBM’s Watson [6], answer factoid questions very well, but do not really handle comparative questions of everyday users.
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2610357447	Some argument mining systems work on larger corpora of usergenerated content to find the most relevant argument for a given claim [10] or to oppose different argumentative viewpoints [19].
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2087484885	A G*Power analysis [5] did output a required sample size of 272 comparisons to be a able to measure a statistically significant difference in answer times.
2910420706	Answering Comparative Questions: Better than Ten-Blue-Links?	2162913249	Web-scale systems for comparing query results [18] or for retrieving single arguments matching a user query [16] form the inspiration for our new CAM system (comparative argumentative machine).
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	2265289447	, 2017c), and sentence matching (Hu et al., 2014; Wan et al., 2016).
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	2131744502	The benchmark results were obtained by Pagliardini et al. (2017), Arora et al.
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	1861492603	Composing training dataset: To train the transition matrix, we employed the MSCOCO 2017 training dataset (Lin et al., 2014), which is widely used as a paraphrase dataset in many researches (Prakash et al.
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	2251861449	Data set: To evaluate how well the proposed approach reflects the intrinsic meaning of sentences, we conducted STS tasks (2012–2016) (Agirre et al., 2012; 2013; 2014; 2015; 2016) and SemEval 2014 semantic relatedness task (SICK) (Marelli et al.
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	2108598243	Inspired by previous researches in computer vision, where a large number of models are pre-trained through a classification task based on the ImageNet (Deng et al., 2009) dataset, Conneau et al.
2910436055	Sentence transition matrix: An efficient approach that preserves sentence semantics.	2250539671	r,wedefinedtheaverageofpre-trained wordvectorsasasentencevectortoshowthattheperformance improvement could be obtained by a simple method. We used two kinds of pre-trained word vectors: GloVe vectors (Pennington et al., 2014) and GoogleWord2vec1. 3.3.Trainingtransitionmatrix Composingtrainingdataset: Totrainthetransition matrix, we employed the MSCOCO 2017 training dataset (Lin et al., 2014), which is widely used as a par
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	1512387364	, 2013) and never-ending (language) learning (Carlson et al., 2010) are related to the topics dis-
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2769041395	, 2019) and relation extraction have observed similar results (Bunescu and Mooney, 2007; Hancock et al., 2018; Ratner et al., 2017).
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2579486552	017;Wang et al., 2018;Rao and III,2018). While those works focused on identifying which question to ask in a given context, in this work we are more interested in ﬁrst learning when to ask a question.Li et al. (2017b) considered this question as well, but again in the context of a QA setting rather than dialogue. Hashimoto and Sassano(2018) used user responses to detect mistakes made by a deployed virtual assist
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2426031434	cussed in this work, as is active learning (Tong and Koller, 2001) and predictive modeling (Schmidhuber and Huber, 1991).
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2311783643	external knowledge (such as profiles) in dialogue is an open research question of its own (Li et al., 2016; Luan et al., 2017; Luo et al., 2018) and we are primarily interested in the question of learning from dialogue, we discard the profiles and simply train and test on the conversations themselves, making the dataset more challenging in terms of raw performance scores.
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2337601653	g and Koller,2001) and predictive modeling (Schmidhuber and Huber,1991). The speciﬁc case of learning actively from dialogue during deployment was explored for the question answering (QA) setting in (Weston, 2016) and (Li et al.,2017a), where the authors examined multiple learning strategies on a suite of dialogue tasks with varying types of feedback, such as verbal cues (e.g., “Yes, that’s right!”) and scalar
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2120501001	The general concepts of lifelong learning (Silver et al., 2013) and never-ending (language) learning (Carlson et al.
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2132997613	inforcement learning settings, where the feedback is a scalar rather than the dialogue messages themselves (Levin et al., 2000; Schatzmann et al., 2006; Rieser and Lemon, 2011; Liu et al., 2018; Hong et al., 2019).
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2150588363	k improves dialogue quality by utilizing larger datasets with noisier labels than traditional supervision. Other applications of weak supervision to relation extraction have observed similar results (Bunescu and Mooney, 2007;Hancock et al.,2018;Ratner et al.,2017). 3 The Self-Feeding Chatbot The lifecycle of a self-feeding chatbot is outlined in Figure2. In the initial training phase, the dialogue agent is trained on two
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2311783643	Li et al. (2017b) considered this question as well, but again in the context of a QA setting rather than dialogue.
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2295570185	There are other, somewhat less related, ways to use feedback during dialogue for learning, notably for collecting knowledge to answer questions (Mazumder et al., 2018; Hixon et al., 2015; Pappu and Rudnicky, 2013), and more commonly in re-
2910458567	Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	2493916176	Tokens are embedded with fastText (Bojanowski et al., 2017) 300-dimensional embeddings.
2910597955	Revealing interpretable object representations from human behavior	1994389483	(2004)) and used a collaborative filtering method (Koren, 2008) to extract a single aggregate norm from these for as many objects as possible.
2910597955	Revealing interpretable object representations from human behavior	1965580172	(2005) and Devereux et al. (2014), if it allow us to explain semantic decision making behavior.
2910597955	Revealing interpretable object representations from human behavior	1965580172	This effort was later replicated and extended by Devereux et al. (2014), generating 5,929 semantic features for 638 objects.
2910597955	Revealing interpretable object representations from human behavior	810147176	egories with 5 items, as deﬁned in Battig &amp; Montague (1969). Leaving out each object in turn, we assign it the category of its nearest neighbor by cosine similarity; this is close to the task in Murphy et al. (2012), but using 23-way accuracy rather than cluster purity. The resulting accuracies were 0.846 for both SPoSE and 300-dimensional synset, and above all other baselines (see Table 1). 4 DISCUSSION AND CON
2910597955	Revealing interpretable object representations from human behavior	2078894097	ental representations of those concepts (see Murphy (2004) for an extensive review of this literature). These features have usually been binary properties, postulated by researchers. A landmark study McRae et al. (2005) departed from this approach by instead asking hundreds of subjects to name binary properties of 541 objects, yielding 2,526 such semantic features. These corresponded to different types of informatio
2910597955	Revealing interpretable object representations from human behavior	810147176	feature/dimension x if in the vector x i is real and non-negative, so as to make it interpretable as the degree to which the aspect of meaning it represents is present and inﬂuences subject behavior (Murphy et al., 2012). Further, we expect features/dimensions to be sparse McRae et al. (2005), which led us to add a sparsity parameter to our model. Based on related work in sparse positive word embeddings of words from
2910597955	Revealing interpretable object representations from human behavior	85455704	h an L2 penalty; we provide a more formal justiﬁcation in the Supplementary Material. 2.3 RELATED WORK There are several existing methods for estimating embeddings of items from behavioral judgments. Agarwal et al. (2007) introduced the generalized non-metric multidimensional scaling (GNMMDS) framework as a generalization of non-metric MDS. The behavioral task they study is a ‘quadruplet’ task, where a subject is show
2910597955	Revealing interpretable object representations from human behavior	810147176	imensions could be matched between the two models with a correlation of 0.8 or higher. For comparison with vectors derived from text information, we used synset Pilehvar &amp; Collier (2016) and NNSE Murphy et al. (2012)) vectors. We used 50-D vectors, corresponding to the dimensionality of our embedding, as well as 300-D (synset) and 2500-D (NNSE), corresponding to the best performing embeddings according to the ori
2910597955	Revealing interpretable object representations from human behavior	2081580037	mating vectors for each of the different concept senses (synsets) of a particular word, from its word2vec (Mikolov et al., 2013) embedding vector and the synset relationships in the WordNet ontology (Miller, 1995). Each of our concepts has a corresponding synset and, therefore, can be represented by a synset vector. However, the latter are real-valued and dense, and hence do not satisfy any of our requirements
2910597955	Revealing interpretable object representations from human behavior	2078894097	N-GENERATED SEMANTIC FEATURES The second evaluation of our semantic vector representation was to test whether it could be used to predict the features in human-generated semantic feature sets: McRae (McRae et al., 2005) and CSLB (Devereux et al., 2014). CSLB is more extensive than McRae, both in the number of features (5,929) and objects (638), and shares 413 objects in common with the intersection of the objects fr
2910597955	Revealing interpretable object representations from human behavior	2078894097	o make it interpretable as the degree to which the aspect of meaning it represents is present and inﬂuences subject behavior (Murphy et al., 2012). Further, we expect features/dimensions to be sparse McRae et al. (2005), which led us to add a sparsity parameter to our model. Based on related work in sparse positive word embeddings of words from text corpora (Murphy et al., 2012), we expect some of the dimensions to
2910597955	Revealing interpretable object representations from human behavior	2078894097	s there may be triplets where either (a) all three objects belong to the same category or (b) no two objects share a category. We expect our method to ﬁnd non-taxonomic information similar to that in McRae et al. (2005) and Devereux et al. (2014), if it allow us to explain semantic decision making behavior. Probabilistic model Our second assumption is that the decision in a given trial is explained as a function of
2910597955	Revealing interpretable object representations from human behavior	2049248196	It is not straightforward to extend the probabilistic model of Xu et al. (2011) to a task with more than 2 choices; it also requires an a priori feature matrix for the concepts, which is not available for our dataset.
2910597955	Revealing interpretable object representations from human behavior	810147176	t vector. However, the latter are real-valued and dense, and hence do not satisfy any of our requirements. There is an embedding method that enforces positive, sparse values for each dimension (NNSE, Murphy et al. (2012)), but it produces one vector per word rather than synset. Both methods have been used to generate behavioral predictions. Hence, we will use them as an alternative concept representation in our exper
2910597955	Revealing interpretable object representations from human behavior	2153579005	text corpora, instead of behavior. Pilehvar &amp; Collier (2016) introduced a method for estimating vectors for each of the different concept senses (synsets) of a particular word, from its word2vec (Mikolov et al., 2013) embedding vector and the synset relationships in the WordNet ontology (Miller, 1995). Each of our concepts has a corresponding synset and, therefore, can be represented by a synset vector. However, t
2910788328	Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents.	2250539671	• 300-dimensional GloVe (Pennington et al., 2014) embedding (cased, trained on 840B tokens from Common Crawl).
2910788328	Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents.	1522301498	The model was trained to minimize negative log likelihood produced by the CRF using Adam optimizer (Kingma and Ba, 2014), with starting learning rate of 0.
2910788328	Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents.	2212703438	This realizes variational inference based dropout introduced in (Gal and Ghahramani, 2016).
2910894887	GILT: Generating Images from Long Text.	2108598243	Intuitively, the IS measures the diversity, with respect to ImageNet [2] classes, and clarity of the generated images.
2910894887	GILT: Generating Images from Long Text.	1580389772	man ranking, MS-SSIM [13] and inception scores [10], to compare the effectiveness of the two embedding methods.
2910894887	GILT: Generating Images from Long Text.	1580389772	The most successful method to evaluate image similarity, as mentioned in [7], is multi-scale structural similarity (MS-SSIM [13]).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2251628756	, 2010), sentiment analysis (Socher et al., 2010; Terdalkar, 2008; Le and Zuidema, 2014), generating paraphrases (Le and Mikolov, 2014; Iyyer et al.
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2130237711,2131744502	, 2010; Terdalkar, 2008; Le and Zuidema, 2014), generating paraphrases (Le and Mikolov, 2014; Iyyer et al., 2014) and machine translation (Bahdanau et al.
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2111066885	, Aldwell, 2011), harmony (Winograd, 1968; Rohrmeier, 2007, 2011), combinations of harmony and voice-leading (Kassler, 1986; Aldwell, 2011; Neuwirth and Rohrmeier, 2015), or complex feature combinations derived from monophonic melody (Conklin and Witten, 1995; Pearce, 2005) and harmony (Whorley et al.
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2008708986	, Rohrmeier and Graepel, 2012; Mavromatis, 2005; Raphael and Stoddard, 2004) and animal song (e.g., Katahira et al., 2011; Jin and Kozhevnikov, 2011).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2111819273	By contrast, a number of studies argues for implicit acquisition of context-free structure (and even (mildly) context-sensitive structure in humans in abstract stimulus materials from language and music (Jiang et al., 2012; Uddén et al., 2012; Rohrmeier et al., 2012; Rohrmeier and Cross, 2008; Li et al., 2013; Rohrmeier and Rebuschat, 2012; Kuhn and Dienes, 2005).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2029247433	For a detailed review of the different methods used to identify units in animal vocalisations, we refer to (Kershenbaum et al., 2014).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	1522301498,1847088711,2064675550	Furthermore, finding the right vectors and operations that encode a certain task is a complicated task; the research focus is therefore typically more on finding optimisation techniques to more effectively search through the tremendous space of possibilities than on interpretation (e.g., Zeiler, 2012; Kingma and Ba, 2015; Hochreiter and Schmidhuber, 1997; Chung et al., 2015).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2026162824	In general, HMMs - which assume that the observed state is generated by a sequence of underlying (hidden) states that emit surface symbols according to a given probability distribution (for a comprehensive tutorial see Rabiner, 1989) - have been used extensively to model sequences in music (e.g., Rohrmeier and Graepel, 2012; Mavromatis, 2005; Raphael and Stoddard, 2004) and animal song (e.
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2025120802	For instance, because of the interaction of melody with metrical structure, not all surface symbols have the same salience when forming a sequence, which could be an argument to in the face of data sparsity - prefer a 4 -gram model over a 3 -gram model to model music with a three-beat metrical structure, as a 3-gram necessarily cannot capture the fact that the first beat of a bar is, in harmonic terms, more musically salient than the other two (Ponsford et al., 1999).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2116723809	For instance, Rodriguez (2001) demonstrated that a simple recurrent network (or SRN, on an architectural level similar to an FSA) can implement the counter language ab, a prime example of a context-free language (see Figure 6).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	1606205649,2008708986	It was observed in many studies that (plain) n-grams are inadequate models of the structure of the vocalisations of several bird species on both the syllable and phrase (e.g., Markowitz et al., 2013; Jin and Kozhevnikov, 2011; Okanoya, 2004; Katahira et al., 2011) and song (Todt and Hultsche, 1996; Slater, 1983) level.
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	2073832354	Most of these operations are more naturally expressed using CFGs than with FSAs, and indeed a rich tradition that emphasises hierarchical structure, categories and, particularly, recursive insertion and embedding exists to characterise Western tonal music (Winograd, 1968; Keller, 1978; Keiler, 1983; Kassler, 1986; Narmour, 1992; Steedman, 1983, 1996; Haas et al., 2009; Rohrmeier, 2007, 2011; Granroth-Wilding and Steedman, 2014; Neuwirth and Rohrmeier, 2015).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	1599446769	Pearce’s IDyOM model - an extension of the multiple feature n-gram models proposed by Conklin and Witten - has been shown to be successful in the domains of both music and language (Pearce and Wiggins, 2012) Recent modelling approaches generalised the notion of modelling parallel feature streams into dynamic Bayesian networks that combine the advantages of HMMs with modelling feature streams (Murphy, 2002; Rohrmeier and Graepel, 2012; Raczynski et al., 2013; Paiement, 2008).
2911068641	Formal models of Structure Building in Music, Language and Animal Songs.	179875071	After being practically abandoned as models of linguistic structure for over a decade, neural networks are experiencing a new wave of excitement in computational linguistics, following some successes with learning such grammars from data for practical natural language processing tasks, such as next word prediction (Mikolov et al., 2010), sentiment analysis (Socher et al.
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2338266296	, Part-of-Speech (POS) tagging [21], context learning [20] and sentiment analysis [22].
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2064675550	Our application uses long term linguistic patterns and regularities which are captured by RNNs with an LSTM cell [2], when extracted from a large scientific corpus.
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2742330194	only begun to attract a signicant amount of attention in recent years [24, 25, 26, 27], along with other text classication tasks, such as sentiment analysis [28, 22, 29, 30] or fake-news detection [31, 32, 33]. Training an LM with an RNN as its sequence model paradigm enables models to be created that re ect a target word’s long and varied history. 3. Bidirectional LSTM tagger for proper word choice We now
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2091494174	In contrast, in the LS task, the substitute for a word in a sentential context, must preserve the original meaning [3] of the sentence, while the sentence itself must adhere to English grammar rules.
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2051593977	elated Work The proposed task of proper word choice is closely related to other error correction tasks, including: semantic collocation correction [4], lexical substitution [5], paraphrase generation [6], grammatical error correction [7] and sentence completion1 [8]. In this section we present background and related work associated with the GEC and LS tasks which are generalized in our task. This sec
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2081580037	We experimented with two different POS extraction procedures: 1) the NLTK [36] POS tagger which was trained on the Penn Treebank [37] corpus, and 2) the WordNet [19] electronic dictionary of the English language.
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2091494174,2128514324	The LS task [3] has attracted increased attention following its inclusion in SemEval-2007 [5].
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	1689711448	The LSTM equations that are computed at each step t, for each LSTM network are a implemented using coupled input and forget gates [34]:
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2577255746	LSTM networks (right-toleft and left-to-right) and would not sum the errors obtained from padded sentences that are less then 40 tokens long. Thus, we implemented all RNN based models using the DyNet [40] toolkit7. DyNet supports a dynamic computation graph, making this implementation possible. Standard and more common deep learning libraries, e.g., TensorFlow [41], do not support dynamic computation
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2098297786	odeling. 1Microsoft Sentence Completion Challenge 3 2.1. Grammatical Error Correction In recent years, several GEC competitions have taken place, including the HOO-2011 [9], HOO-2012 [10], CoNLL-2013 [11], and the CoNLL-2014 [7] competitions. The CoNLL-2014 shared task on GEC, the most recent and prominent of these competitions, proposed 28 error types, the most common of which are presented in Table
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2127672659,2251613956	The participating teams developed a wide range of approaches, including statistical classifiers [12], language models, and statistical machine translation [13], and rule-based modules [14].
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2124059530,2128514324,2137115322	The proposed task of proper word choice is closely related to other error correction tasks, including: semantic collocation correction [4], lexical substitution [5], paraphrase generation [6], grammatical error correction [7] and sentence completion [8].
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	1640336798	In recent years, several GEC competitions have taken place, including the HOO-2011 [9], HOO-2012 [10], CoNLL-2013 [11], and the CoNLL-2014 [7] competitions.
2911092985	Choosing the Right Word: Using Bidirectional LSTM Tagger for Writing Support Systems.	2081580037	The systems presented at Sem-Eval 2007 relied heavily on multiple external resources and inventories such as WordNet [19].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	1938755728	[23, 27, 42] added subword-level information and augmented word embeddings with character information for their relative applications.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2517194566,2626778328	In addition, layers for Attention-based [41] classification are added to our models: LSTM, BiLSTM [47], GRU, BiGRU.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	1522301498	For the audio-based classification we used the Adam [25] optimizer algorithm with learning rate of 5e, 50 epochs and early stopping of 15 epochs.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2493916176,2563351168	Character ngrams are in particular efficient and also form the basis of Facebook’s fastText classifier [2, 18, 19].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2563351168	Feature Vector Representation that we chose to test are fastText [18], ELMo [36] and BERT [15]: further, we combined different models for predicting lyrics emotions with all possible embeddings, as Figure 4 shows.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2175723921,2493916176	Later works [2,17] showed that incorporating pretrained embeddings character n-grams features provides more powerful results than composition functions over individual characters for several NLP tasks.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2721931776	The first layer is the Mel-Spectrogram layer that outputs mel-spectrograms in 2D format with 128 mel bands, n dft 512, n hop 256 [6], followed by a frequency axis batch normalization.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2064675550,2470673105,2517194566	Lyrics-based approaches, on the other hand, make use of Recurrent Neural Networks architectures (like LSTM [13]) for performing text classification [46, 47].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	1832693441,2470673105,2517194566	Regarding the prediction models for text classification, LSTM and RNN including all possible model Attention-based variants [47] have represented for years the milestone for solving sequence learning, text classification [46] and machine translation [4]; other works show that CNN can be a good approach to solve NLP task too [22].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2135475818	The problem of Music Emotion Recognition was proposed for the first time in the Music Information Retrieval (MIR) community in 2007, during the annual Music Information Research Evaluation eXchange (MIREX) [14].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2756184659	For the purpose of this work we will use the Mel-Spectrogram, which is also a timefrequency representation of the audio, but sampled on a Mel-frequency scale, resulting to be a more compact yet very precise representation of the Spectrum of a sound, that has been proven to be more suited when working with CNN for Music Information Retrieval tasks [20].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	24912070	Other techniques examine the use of Gaussian Mixture Models [31, 33] and Naive Bayes classifiers.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2059652044,2414894569	ture learning) using either spectral representations of the audio (spectrograms or mel-spectrograms), in [5] feeded to a deep Fully Convolutional Networks (FCN) consisting in convolutional and subsampling layers without any fullyconnected layer or directly using the raw audio signal as input to the network classifier [9].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	1614298861	The Word2Vec method based on skipgram [35], had a large impact and enabled efficient training of dense word representations and a straightforward integration into downstream models.
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	1997397186,2144707026	Other works see the combination of audio-based classification with lyrics-based classification in order to improve the informative content of their vector representations and therefore the performances of the classification, as in [1,28,38,44].
2911113553	Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection.	2626778328	In the last few years Transformer [41] outperformedboth recurrent and convolutional approaches for language understanding, in particular on machine translation and language modeling.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	8550301	• A BRAT server [46], enabling a friendly online visualization of the annotations: https:// pgxcorpus.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2104246439	In addition, the size of EU-ADR and SNPPhena are relatively small (only a few hundreds of annotated sentences), which limits the use of supervised learning approaches that require large train sets such as TreeLSTM [47].
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2149803936	ADE-EXT, SNPPhena and other corpora used in NLP shared tasks such as GENIA [22], SemEval DDI [17].
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2018874418	Also, we decided on a type named Gene_or_protein, broader to Gene, because it is hard to disambiguate between gene and protein names in NLP, and commonly assumed that the task of gene name recognition is indeed a gene-or-protein name recognition [56].
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2107005506	Disease recognition is performed with DNorm, which uses BANNER [25], a trainable system using Conditional Random Fields (CRF) and a rich feature set for disease recognition.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	1832693441	Following [23], both channels were initialized with pre-trained word embeddings, but gradients were backpropagated only through one of the channels.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	1508977358	For this matter, the dependency graph of each sentence is constructed with the Stanford Parser [11] and in each graph, the direct vicinity of key entities is explored in the search for terms defined in PHARE.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2095705004	For both NER and RE, we applied a dropout regularization after the embedding layers [45] with a dropout probability fixed to 0.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2151705872	Previous works investigated mainly rule-based approaches [6,10,42] and unsupervised learning [24,38], because of the absence of annotated corpora.
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	1966615318	Supervised learning has also been experimented [5,28,37,43,55], but without a more appropriate corpus, most studies build train and test sets on the basis of PharmGKB, the reference database for PGx [52].
2911247703	PGxCorpus: a Manually Annotated Corpus for Pharmacogenomics	2138768461	In the same vein, ADE-EXT (Adverse Drug Effect corpus, extended) [14] consists of 2,972 MEDLINE case reports, annotated with drugs and conditions (e.
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2608018997	(2), α ∈ [0,1] gives the relevance of words for the task, multiplied in Eq.
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	1826029799,2130942839,2251124635	, [20], [26] and deep neural networks, such as CNN [8], [17], [13], recursive neural networks [5], [24], recurrent neural networks [25], or memory networks [10], have shown good performance for sentiment detection.
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2131494463	. RELATED WORK Machine learning methods e.g., [20], [26] and deep neural networks, such as CNN [8], [17], [13], recursive neural networks [5], [24], recurrent neural networks [25], or memory networks [10], have shown good performance for sentiment detection. Knowledge-based methods explore syntactic characteristics/patterns/rules [18] and employ sentiment resources [6]. However, sarcasm detection curr
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2250539671	b) Input Representation: We use Dg-dimensional (Dg = 300) Glove word-embeddings [16] xi ∈ Rg to represent the words wi, padding the variable-length input sentences to a fixed length with null vectors.
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	1902237438	c) Sentence Representation: In the next layers, we obtain sentence representation from X using Gated Recurrent Unit (GRU) [3] with attention mechanism [12] as follows.
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2608018997	However, recently multi-task learning has been successfully applied in many NLP tasks, such as implicit discourse relationship identification [11], key-phrase boundary classification [1].
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2608018997	Namely, in this paper, we train a classifier for both sarcasm and sentiment in a single neural network using multi-task learning which is a novel learning scheme and has gained recent popularity [1], [11].
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2165044314	ore syntactic characteristics/patterns/rules [18] and employ sentiment resources [6]. However, sarcasm detection currently focuses on extracting features, such as syntactic [2], surface pattern-based [4], or personality-based features [19], as well as contextual incongruity [7]. Mishra et al. [14] extracted multimodal cognitive features for both sentiment classiﬁcation and sarcasm detection, without
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	1522301498	a sample, jis the class value, and y∗ ij = ¢¤ ¤ ƒ ¤¤ ⁄ 1; if expected class value of sample iis j; 0; otherwise: As a training algorithm, we use Stochastic Gradient Descent (SGD)-based ADAM algorithm [9], which optimizes each parameter individually with different and adaptive learning rates. Also, we minimize both loss functions, namely J sen and J sar, with equal priority, by optimizing the paramete
2911278177	Sentiment and Sarcasm Classification with Multitask Learning.	2127426251	t. In Eq. (2), ∈[0;1]L gives the relevance of words for the task, multiplied in Eq. (3) by the contextaware word representations in H∗. f) Inter-Task Communication: We use Neural Tensor Network (NTN) [23] of size D ntn=100 to fuse sarcasmand sentiment-speciﬁc sentence representations, s sar and s sen, to obtain the fused representation s+, where s+=tanh(s sarT [1∶D ntn]sT sen +(s sar⊕s sen)W+b); where
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2252219904	, 2016) and Situation Entities (SitEnt; Friedrich and Palmer, 2014a,b; Friedrich et al., 2015; Friedrich and Pinkal, 2015b,a; Friedrich et al., 2016) frameworks, which annotate both NPs and entire clauses for genericity.
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	1549997466	, 2018) – a central component of general artificial intelligence (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al., 1987; Reiter, 1987).
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2159980159	In particular, SitEnt, which is used to annotate MASC (Ide et al., 2010) and Wikipedia, has the nice property that it recognizes the existence of abstract entities and lexical aspectual class of clauses’ main verbs, along with habituality and genericity.
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2548036585	This is problematic, because the ability to accurately capture different modes of generalization is likely key to building systems with robust common sense reasoning (Zhang et al., 2017a; Bauer et al., 2018) – a central component of general artificial intelligence (McCarthy, 1960, 1980, 1986; Minsky, 1974; Schank and Abelson, 1975; Hobbs et al.
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2096335387,2160583993	This is remedied to some extent in ECB+ (Cybulska and Vossen, 2014b,a), which is an extension of the EventCorefBank (ECB; Bejan and Harabagiu, 2010; Lee et al., 2012) – which annotates Google News texts for event coreference in accordance with the TimeML specification (Pustejovsky et al.
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2251552857,2252219904	It is also surprising, since there is no dearth of data relevant to generalization (Doddington et al., 2004; Cybulska and Vossen, 2014b; Friedrich et al., 2015).
2911333598	Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements	2026185168	WordNet WordNet (Fellbaum, 1998) supersenses (Ciaramita and Johnson, 2003) for argument and predicate lemmas.
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2032225523	According to the best of our knowledge, there's no machine that can provide the Arabic-English translation till this moment [23], especially what is based on the semantic
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2173094809	Based on the Artificial Neural Network (ANN), NMT is used to make the MT learn concurrently to increase the performance of the MT through both the encoder and decoder which are needed as the two steps of the Recurrent Neural Network (RNN) [17-20].
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2541717252	One of them has built a Chinese-English ontology that can be used to generate a somewhat precise translation derived from the ontology [21].
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2006617528	As NMTs, SMTs or MTs uses the English language as a pivot language [6], the resulting translation can be correct as long as one side is the English language.
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	175692229	The problem is not only having correct English-Arabic MTs only, but also the flexibility and the morphology of the Arabic language [8, 9] which leads to the whole translation errors.
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2149709850	Some of the researchers deal with the parser and the morphological analyzer as being the same tool, because they have the same mechanism [10, 25].
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	2097333193	SMT is an MT system that is based on the machine learning methods, such as Bayesian [14] and clustering algorithms [15].
2911419605	Extending A Model for Ontology-Based Arabic-English Machine Translation (NAN)	1716625268	Another translation was using the ontology to do the syntactic and the semantic analysis in order to do an English-Korean MT [22].
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	1841724727	Also in [11], they feed bag-of-3-grams features in a CNN of three layers, reaching an accuracy of 92% which is even higher than ours.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2113459411	In [18] for example, authors of IMDB dataset achieved 88% accuracy (almost what we got here) utilizing a probabilistic model they developed.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	187383899	In [9] they explore the use of emoticons and other emotion signals in an unsupervised framework for sentiment analysis.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2027731328	Authors used it in [19] to build a recommender system of products modeling item relationships as seen by people.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2129011250	Authors in [37] present Coooolll, a supervised deep learning system for Twitter sentiment classification that mixes together state-of-the-art hand-crafted text features with word embedding features.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2018277822	It was collected by authors of [5] from September 2009 to January 2010 and used to explore geolocation data.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2251939518	Distributed word representations and deep learning models appeared recently in works like [35] where RNTNs (Recursive Neural Tensor Network) are presented, together with Stanford Sentiment Treebank, a dataset of positive and negative sentences extracted from movie reviews.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2164973920	One of the earliest attempts was proposed in [29] where they introduce multi-prototype vector space model.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2166706824	Foundational works in movie review polarity analysis are [26] and [25] conducted by Pang and Lee.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	1964613733,2084046180	Several lexicon-based studies like [7] or [36] use dictionaries of affect words with their associated polarity for computing the overall sentiment polarity of each message.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2153579005	These methods were first presented in [20] and considerably improved in [21] with the introduction of negative sampling and sub-sampling of frequent words.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2117130368	One of the first popular word embedding models is C&W (for Collobert and Weston) which was presented in [6].
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2250539671	One of the most recent and popular word embedding generation methods is Glove presented in [27].
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2250879510	An even more recent work is [38] where authors examine the usability of word embeddings for sentiment analysis of tweets.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2153579005	Reduced versions of this corpus were also used in [21] for validating the efficiency and training complexity of Word2vec.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2049434052	Another relevant study is [31] where authors first use a CNN to refine the previously trained word embeddings and then train that same CNN on a Semeval-2015 (http://alt.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2113459411	A similar work that has relevance for us is [18] where authors present a log-bilinear vector space model capable of capturing sentiment and semantic similarities between words.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2164019165	Similarly in [10], they introduce a language model based on a neural network architecture that considers both local and global context of words.
2911488263	Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey	2250539671	We thus utilized the dataset of 19,544 questions also used by authors of [27] for testing performance of Glove.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1986361898	(Smith et al., 2005; Aker et al., 2017; Liu and Hulden, 2017)
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1522301498	3, recurrent size to 500, and we use the Adam optimizer (Kingma and Ba, 2015) with initial learning rate of 0.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2097835057	The classical approaches to lemmatizing highly inflective languages are based on two-level morphology implemented using finite state transducers (FST) (Koskenniemi, 1984; Karttunen and Beesley, 1992).
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2251386579	Edit-tree classifiers are used for example in (Müller et al., 2015; Straka et al., 2016; Chakrabarty et al., 2017), and the sentence-context for resolving ambiguous
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	107550075	For example, if particularly working on Turkish, Finnish or Hungarian, one should consider using morphological transducers by Çöltekin (2010), Pirinen (2015) and Trón et al.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2106593913	Further, finite state transducers for morphological analysis and generation for a multitude of languages are available in the Apertium framework (Tyers et al., 2010).
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2100664567,2118434577	indivisible into smaller units, we instead rely on an alternative technique whereby the model is trained to predict an unknown symbol UNK for rare and unseen characters, and as a post-processing step, each such UNK symbol is subsequently substituted with the input symbol with the maximal attention value of the model at that point (Luong et al., 2015a; Jean et al., 2015).
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1582588624,2251386579	The Lematus system outperforms other context-aware lemmatization systems, including (Chrupała et al., 2008; Müller et al., 2015; Chakrabarty et al., 2017), and can be seen as the current state of the art on the task.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1816313093	In machine translation, the problem of OOV words is for the most part solved using Byte Pair Encoding (BPE) or other sub-word representations, reducing vocabulary size and handling inference-time unknown words (as unknown words can be split into known subwords) (Sennrich et al., 2016).
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1986361898	Among the top three performing systems on large treebanks, together with our work and the abovementioned edit-tree classifier of Straka (2018b), ranked the Stanford system (Qi et al.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	1902237438	The sequence of output characters is generated by a decoder with two unidirectional LSTM layers with input feeding attention (Luong et al., 2015b) on top of the encoder output.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2574872930	Similarly to the Lematus system, we rely on an existing neural machine translation model implementation, in our case OpenNMT: Open-Source Toolkit for Neural Machine Translation (Klein et al., 2017).
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2552110825	We use the winning Stanford part-of-speech tagger (Dozat and Manning, 2017; Dozat et al., 2017) from the CoNLL-17 Shared Task on multilingual parsing (Zeman et al.
2911610808	Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks.	2251386579	words can be incorporated into these classifiers for example by using global sentence features (Müller et al., 2015) or contextualized token representations (Straka et al.
2911655849	Deconstructing Word Embeddings	2036931463	It has been further argued, in recent years, that distributional models can never absolutely grasp the meaning of words, as many aspects of language are not necessarily present in language but outside it (in that some aspects of semantics can have grounding in the physical world) (Andrews et al., 2009).
2911655849	Deconstructing Word Embeddings	2483215953	For example, the following equality has been observed (Bolukbasi et al., 2016) in word vectors trained by GloVe:
2911655849	Deconstructing Word Embeddings	2483215953	Female/male gender stereotypes have appeared on word embeddings trained on Google News data (Bolukbasi et al., 2016).
2911655849	Deconstructing Word Embeddings	2098801107	identity: “instances of a single relation may still have significant variability in how characteristic they are of that class” (Jurgens et al., 2012) (Rogers et al.
2911655849	Deconstructing Word Embeddings	2109830295	On this interpretation, two word pairs that have a high degree of relational similarity are analogous (Turney, 2006).
2911655849	Deconstructing Word Embeddings	2465912008	Linzen (2016) further points out that in the plural noun category of the Google test set, 70% accuracy was achieved by simply taking the closest neighbor of the vector b, while 3CosAdd improved the accuracy by only 10%.
2911966030	Improving Question Answering with External Knowledge	2626778328	, 2018) leverage rich world knowledge by pre-training deep neural models such as LSTMs and Transformers (Vaswani et al., 2017; Liu et al., 2018) using language model objectives over large-scale corpora (e.
2911966030	Improving Question Answering with External Knowledge	2411480514	, 2019) and models (Chen et al., 2016; Wang et al., 2018b; Radford et al., 2018; Devlin et al., 2018; Sun et al., 2018) to drive progress in question answering.
2911966030	Improving Question Answering with External Knowledge	2606964149	, we first fine-tune a pre-trained language model on the largest multiple-choice machine reading comprehension dataset RACE (Lai et al., 2017) and then fine-tune the resulting model on target multiple-chocie question answering datasets.
2911966030	Improving Question Answering with External Knowledge	1888005072	After constructing the knowledge graph, we apply the graph embedding framework proposed by Tang et al. (2015) to generate knowledge representations for all entities in the KB.
2911966030	Improving Question Answering with External Knowledge	2606964149	On the dataset side, our work primarily focuses on multiplechoice examination datasets designed by educational experts (Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019) since questions from these datasets are generally clean, error-free, and challenging (Sun et al.
2911966030	Improving Question Answering with External Knowledge	151066029	A dictionary-based candidate generation approach (Medelyan and Legg, 2008) is adopted such that
2911966030	Improving Question Answering with External Knowledge	2606964149	In our experiment, we use RACE (Lai et al., 2017), which is the existing largest multiplechoice machine reading comprehension dataset, as the source task of transfer learning.
2911966030	Improving Question Answering with External Knowledge	2059035806	External knowledge plays a critical role in human reading and understanding since authors assume readers have a certain amount of background knowledge gained from sources outside the text (McNamara et al., 2004; Salmerón et al., 2006; Zhang and Seepho, 2013).
2911966030	Improving Question Answering with External Knowledge	2606964149	A growing number of studies concentrate on the construction of multiple-choice machine reading comprehension (Mostafazadeh et al., 2016; Lai et al., 2017; Khashabi et al., 2018; Ostermann et al., 2018; Sun et al., 2019) or question answering tasks (Clark et al.
2911966030	Improving Question Answering with External Knowledge	2293174806	Then each initial list of candidate entities is re-ranked based on three measures: salience, similarity, and coherence (Pan et al., 2015).
2911966030	Improving Question Answering with External Knowledge	2427527485,2606964149	Recent years have seen numerous datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Lai et al., 2017; Mihaylov et al., 2018; Clark et al., 2018; Choi et al., 2018; Reddy et al., 2018; Sun et al., 2019) and models (Chen et al.
2912022804	Character-based Surprisal as a Model of Human Reading in the Presence of Errors.	2764337833	We introduced errors into the materials of Hahn and Keller (2018) following the method suggested by Belinkov and Bisk (2018).
2912022804	Character-based Surprisal as a Model of Human Reading in the Presence of Errors.	2764337833	For the latter, we used a corpus of human edits (Geertzen et al., 2014), and introduced errors in our experimental materials by replacing correct words with known misspellings from our edit corpus.
2912022804	Character-based Surprisal as a Model of Human Reading in the Presence of Errors.	1544827683	These materials contain twenty newspaper texts from the DeepMind question answering corpus (Hermann et al., 2015).
2912040165	A Tool for Spatio-Temporal Analysis of Social Anxiety with Twitter Data.	1604601673	In order to understand individual moods, a number of lexiconbased approaches [5] analyze the sentiments reflected by Tweets.
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2113459411,2251939518	2) Sentiment Analysis: We choose two sentiment analysis datasets for evaluation: 1) the Internet Movie Database (IMDb) [56] and 2) the Stanford Sentiment Treebank dataset (SST) [61].
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2135964261	They are: 1) the AP dataset [52], 2) the BLESS dataset [53] and 3) the BM dataset [54].
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2141599568	They are: 1) the Google dataset [19] and 2) the MSR dataset [33].
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2252211741	One can further classify intrinsic evaluators into two types: 1) absolute evaluation, where embeddings are evaluated individually and only their final scores are compared, and 2) comparative evaluation, where people are asked about their preferences among different word embeddings [15].
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2113459411	An example of this could be the IMDb dataset by [56] on whether a given movie review is positive or negative.
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2003304759	Examples include part-of-speech tagging [11], named-entity recognition [12], sentiment analysis [13] and machine translation [14].
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2250539671	log-linear models, the Global Vectors (GloVe) method [21] adopts a weighted least-squared model.
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2026487812,1567365482,1854884267,2067438047,2112184938,2142625445	Name Pairs Year WS-353 [40] 353 2002 WS-353-SIM [41] 203 2009 WS-353-REL [41] 252 2009 MC-30 [42] 30 1991 RG-65 [43] 65 1965 Rare-Word (RW) [44] 2034 2013 MEN [45] 3000 2012 MTurk-287 [46] 287 2011 MTurk-771 [47] 771 2012 YP-130 [48] 130 2006 SimLex-999 [49] 999 2014 Verb-143 [50] 143 2014 SimVerb-3500 [51] 3500 2016
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2098921539,2144578941	The Penn Treebank (PTB) dataset [58], the chunking of CoNLL’00 share task dataset [59] and the NER of CoNLL’03 shared task dataset [60] are used for the part-Of-speech tagging, chunking and named-entity recognition, respectively.
2912084149	Evaluating Word Embedding Models: Methods and Experimental Results	2251066368	QVEC [39] is an intrinsic evaluator that measures the component-wise correlation between word vectors from a word embedding model and manually constructed linguistic word vectors in the SemCor dataset.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2147800946	[21] proposed a generic deep learning approach for answer selection, based on Convolutional neural networks (CNN) [22].
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2064675550	[23] combined recurrent neural networks based on long short-term memory (LSTM) cells [29] and reinforcement learning (RL) to learn without the need of prior domain knowledge.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2147800946	[26] proposed a generic relevance ranker based on deep learning and CNNs [22], which tries to maintain standard IR search engine characteristics, such as exact matching and query term importance, while enriching the results based on semantics, proximity heuristics, and diversification.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	836999996	7 The embedding average [6] constructs a vector for a piece of text by taking the average of the word embeddings of its constituent words.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2626778328	ally trained on corpora such as Ubuntu, some work has also used data from Community Question Answering forums [16]. More recently, the Transformer, a model without recurrent connections, was proposed [18], demonstrating state-of-the-art results for Machine Translation in various experimental scenarios for several language pairs and translation directions, thus emerging as a strong alternative to seq2s
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2108325777	best answer. The second and the third columns report the word overlap measures: (i) BLEU@2, which uses uni-gram and bi-gram matches between the hypothesis and the reference sentence, and (ii) ROUGE-L [42], which uses Longest Common Subsequence (LCS). The last three columns are for the semantic similarity measures: (i) Embedding Average (Emb Avg) with cosine similarity, (ii) Greedy Matching (Greedy Mat
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2101105183	blem is related to machine translation (MT) and text summarization (TS), which are nowadays also addressed using seq2seq models, researchers have been using MT and TS evaluation measures such as BLEU [5] and ROUGE [4], which focus primarily on word overlap and measure the similarity between the chatbot’s response and the gold answer to the user question (here, the answer by the customer support). How
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2064675550	The latter is built upon a single (embedding) layer, followed by two LSTM [29] layers, which are fed the words from a target sentence in a forward and a backward direction, accordingly.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2763384403	chical structure [13], context [14], and combination thereof [15]. While models were typically trained on corpora such as Ubuntu, some work has also used data from Community Question Answering forums [16]. More recently, the Transformer, a model without recurrent connections, was proposed [18], demonstrating state-of-the-art results for Machine Translation in various experimental scenarios for several
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2766284073	d chit-chat generator, by training a meta-engine that chooses between them. Answer combination is also a key research topic in the related ﬁeld of Information Retrieval (IR). For example, Pang et al. [26] proposed a generic relevance ranker based on deep learning and CNNs [22], which tries to maintain standard IR search engine characteristics, such as exact matching and query term importance, while en
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1993378086,2296712013	Early vanilla seq2seq models [12] got quickly extended to model hierarchical structure [13], context [14], and combination thereof [15].
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	836999996	The emergence of large conversational corpora such as the Ubuntu Dialog corpus [6], OpenSubtitles [7], CoQA [8] and Microsoft Research Social Media Conversation Corpus1 has enabled the use of generative models and end-to-end neural networks in the domain of conversational agents.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2154652894	We evaluate the model using both semantic similarity measures, as well as word-overlap ones such as BLEU [5] and ROUGE [4], which come from machine translation and text summarization.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2123442489	g Since Twitter has its own speciﬁcs of writing in terms of both length5 and style, standard text tokenization is generally not suitable for tweets. Therefore, we used a specialized Twitter tokenizer [34] to preprocess the data. Then, we further replaced shorthand entries such as ’ll, ’d, ’re, ’ve, with the most corresponding literary form, e.g., will, would, are, have. We also replaced shortened slan
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2604368306	gorical distribution over K = jAjevents (candidate answers). For each question-answer pairs (q, a), we deﬁne the probability p that a is a good answer to q using softmax as shown in Equations (2) and (3). Finally, we draw a random sample from Equation (3) to obtain the best matching answer. pjq, A ˘softmax(o(q, a1), ,o(q, aK)) (2) Ansjp ˘Cat(K, p) (3) 5 of 12 4. Data The data and resources that could
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2153579005	n Figure1. 3.1. Negative Sampling Our goal is to distinguish “good” vs. “bad” answers, but the original dataset only contains valid, i.e., “good” question-answer pairs. Thus, we use negative sampling [43], where we replace the original answer to the target question with a random answer from the training dataset. We further compare the word-based cosine similarity between the original and the sampled a
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2155482025	nd (iii) the Transformer. For IR, we use ElasticSearch6 (ES) with English analyzer enabled, whitespace- and punctuation-based tokenization, and word 3-grams. We further use the default BM25 algorithm [37], which is an improved version of TF.IDF. For all training questions and for all testing queries, we append the previous turns in the dialog as context. For seq2seq, we use a bi-directional LSTM netwo
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1591706642	o-sequence (seq2seq) models, which were initially proposed for machine translation [9–11], got adapted to become a standard tool for training end-to-end dialogue systems. Early vanilla seq2seq models [12] got quickly extended to model hierarchical structure [13], context [14], and combination thereof [15]. While models were typically trained on corpora such as Ubuntu, some work has also used data from
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2419539795	oncludes and suggests possible directions for future work. 2. Related Work 2.1. Conversational Agents The emergence of large conversational corpora such as the Ubuntu Dialog corpus [6], OpenSubtitles [7], CoQA [8] and Microsoft Research Social Media Conversation Corpus1 has enabled the use of generative models and end-to-end neural networks in the domain of conversational agents. In particular, seque
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2095705004	For the output layer, we learn two different representations by passing the output of the model layer to two residual blocks, applying dropout [32] only to the inputs of the first one.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1902237438,2130942839	In particular, sequence-to-sequence (seq2seq) models, which were initially proposed for machine translation [9–11], got adapted to become a standard tool for training end-to-end dialogue systems.
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2154652894	As the problem is related to machine translation (MT) and text summarization (TS), which are nowadays also addressed using seq2seq models, researchers have been using MT and TS evaluation measures such as BLEU [5] and ROUGE [4], which focus primarily on word overlap and measure the similarity between the chatbot’s response and the gold answer to the user question (here, the answer by the customer support).
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2250539671	QANet and negative sampling, and (ii) a main task that re-ranks answer candidates using the learned model. We further experimented with different model sizes and two types of embedding models: GloVe [27] and ELMo [28]. Our experiments showed improvements in answer quality in terms of word-overlap and semantics when re-ranking using the auxiliary model. Last but not least, we argued that choosing the
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2101105183	rain on historical logs for some period of time, and then we test on logs for subsequent days. We evaluate the model using both semantic similarity measures, as well as word-overlap ones such as BLEU [5] and ROUGE [4], which come from machine translation and text summarization. arXiv:1902.04574v1 [cs.CL] 12 Feb 2019 2 of 12 The remainder of this paper is organized as follows: Section2presents some re
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1957644519	raining data [19]. Other work incorporated intent categories and semantic matching into an answer selection model, which uses a knowledge base as its source [20]. In the insurance domain, Feng et al. [21] proposed a generic deep learning approach for answer selection, based on Convolutional neural networks (CNN) [22]. Li et al. [23] combined recurrent neural networks based on long short-term memory (L
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1518951372	ranslation [9–11], got adapted to become a standard tool for training end-to-end dialogue systems. Early vanilla seq2seq models [12] got quickly extended to model hierarchical structure [13], context [14], and combination thereof [15]. While models were typically trained on corpora such as Ubuntu, some work has also used data from Community Question Answering forums [16]. More recently, the Transforme
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2250539671	s the input/output dimensionality of the model for all Transformer layers, which is required by the architecture. We experiment with two types of input embeddings. First, we use 200-dimensional GloVe [27] vectors trained on 27 billion Twitter posts. We compare their performance to ELMo [28], a recently proposed way to train contextualized word representations. In ELMo, these word vectors are learned a
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2250645967	s. Then, the vectors for the chatbot response and for the gold human answer are compared using the cosine similarity. The greedy matching was introduced in the context of intelligent tutoring systems [40]. It matches each word in the chatbot’s output to the most similar word in the gold human response, where the similarity is measured as the cosine between the corresponding word embeddings, multiplied
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2626778328	as suggested in [28]. 2 Note that it differs for the question vs. the answer. See Section5.1for more detail. 4 of 12 The embedding encoder layer is based on a convolution, followed by self-attention [18] and a feed-forward network. We use a kernel size of seven, d ﬁlters, and four convolutional layers within a block. The output of the layer is f(layernorm(x))+ x, where layernorm is the layer normaliz
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	1522301498,2402144811	For training, we use the Adam [35] optimizer with decaying learning rate, as implemented in TensorFlow [36].
2912272477	Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots.	2250539671	uni-directional layers connected directly to the bi-directional layer in the encoder. The network takes as input words encoded as 200-dimensional embeddings. It is a combination of pre-trained GloVe [27] vectors for the known words, and a positional embedding layer, learned as model parameters, for the unknown words. The embedding layers for the encoder and for the decoder are not shared, and are lea
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	(2018), whereby M1 and M2 are fed to a multi-layer perceptron, like in (Lu et al., 2016).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2516196286	(2018) rely on self-attention in order to predict dependencies, and in cloze question-answering tasks (Kadlec et al., 2016; Cui et al., 2017).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2626778328	’s (2017), or when the keys and query represent the same data, as in (Vaswani et al., 2017; Zhang et al., 2018).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2120480077	Ac rding to LeC n, B ngio, and Hi ton (2015), a major open challeng in AI is combining connectionist (or ub-symbolic) model , su as deep ne work , with approaches based on symbolic k owledg r r sentation, i order to perform complex reasoning tasks.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2120480077	Accordi to LeCun, Bengio, and Hinton (2015), a major open challenge in AI is combining connectio ist (or sub-symbolic) models, such as deep networks, with approac es based on symbolic knowledg representation, in order to p rform complex reasoning tasks.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2120480077	According to LeCun, Bengio, and Hinton (2015), a major open challenge in AI is combining connectionist (or sub-symbolic) models, such as deep networks, with approaches based on symbolic knowledge representation, in order to perform complex reasoning tasks.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2594061220	Adaptive data selection strategies have proven to be useful for efficiently obtaining more effective models (Fan et al., 2017).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Altern ly, one o ld apply attentio jointly n K and Q, whic become the “inputs” of a co-attention architecture (Lu et al., 2016).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2147527908	An alterna ive nested attention model s pro osed by Nie et al. (2018), whereby M1 nd M2 are fed to a multi-lay r perc ptron, like in (Lu et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Alternatively, one could ap ly attention jointly o K and Q, which become he “i puts” f a co- ttention architecture (Lu et al., 2016).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Alternatively, one could apply attention jointly on K and Q, which become the “inputs” of a co-attention architecture (Lu et al., 2016).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Alternatively, one coul apply attention jointly on K and Q, which become the “inputs” of a co-attention architecture (Lu et al., 2016).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2521709538	could be called a biased general attention, Sordoni et al. (2016) introduce a learnable bias, so as to consider some keys as relevant independently of the input.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2516196286	The computation of these weights may already be sufficient for some tasks such as the classification task addressed by Cui et al. (2017). Nevertheless, in most cases the task requires the computation of new representation of the keys.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2120480077	In the context of a multi-layer neural architecture it is fair to assume that the deepest levels will compute the most abstract features (Le, 2013; LeCun et al., 2015).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2516196286	Cui et al. (2017) inst ad pply th neste model depict d in Figure 8 (right).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2133564696	For example, Bahdanau et al. (2015) pre-compute the contribution of K in order to reduce the computational footprint.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2120480077	For example, in work by He, Lee, Ng, and Dahlmeier (2017), attention is exploited in a model for aspect extraction in sentiment analysis, with the aim to remove words that are irrelevant for the sentiment, and to ensure more coherence of the predicted aspects.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Figur 7: C arse gr ined co-attention by Lu et al. (2016) (left) a d Ma et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2133564696	Figure 2: Architecture of RNNsearch (Bahdanau et al., 2015) (left) and its attention model (right).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	Figure 7: Coarse grained co-attention by Lu et al. (2016) (left) and Ma et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2133564696	To illustrate, we briefly describe a classic attention architecture, called RNNsearch (Bahdanau et al., 2015).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1793121960	These include statistical relatio al learning (Getoor & T skar, 2007), neural-symbolic Garcez, Broda, & Gabbay, 2012), and more r cent approaches in deep lear ing (Lippi, 2017) such as memory networks (Sukhbaatar et al., 2015), neural Turing machin s (Graves et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1793121960	These include statistical relational learning (Getoor & Taskar, 2007), neural-symbolic learning (Garcez, Broda, & Gabbay, 2012), and ore recent approaches in deep learning (Lippi, 2017) such as memory networks (Sukhbaatar et al., 2015), neural Turing machines (Graves et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1793121960	These include statistical relational learning (Getoor & Taskar, 2007), neural-symbolic learning (Garcez, Broda, & Gabbay, 2012), and more recent approaches in deep learning (Lippi, 2017) such as memory networks (Sukhbaatar et al., 2015), neural Turing machines (Graves et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	For instance, alternating co-attention, illustrated in Figure 7 (left), is a sequential coarse-grained model proposed by Lu et al. (2016) whereby attention is computed three times to obtain embeddings for S1 and S2.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2141399712	In Li et al.’s (2018c) architecture the computation is even simpler, since the final energy scores are a linear transformation of E.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2522143790	In Mi et al.’s (2016a) work, the representation is enhanced by making use of a sub-symbolic representation for the coverage.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2626778328	Such a model has proven to be effective by several authors, who have exploited it in different fashions (Pavlopoulos et al., 2017; Vaswani et al., 2017; Wu et al., 2018; Li et al., 2018d).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1793121960,2133564696,2626778328	In NLP, after an initial exploration by a number of seminal papers (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Vaswani et al., 2017), a fast-paced development of new attention models and attentive architectures ensued, resulting in a highly diversified architectural landscape.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1514535095	Besides NLP and computer vision (Xu et al., 2015; Gregor, Danihelka, Graves, Rezende, & Wierstra, 2015; Zhang, Goodfellow, Metaxas, & Odena, 2018), attentive models have been success-
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1793121960	Popular approaches include statistical relational learning (Getoor & Taskar, 2007), neuralsymbolic learning (Garcez, Broda, & Gabbay, 2012), and the application of various deep learning architectures (Lippi, 2017) such as memory networks (Sukhbaatar et al., 2015), neural Turing machines (Graves et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1843891098	If the position is known in advance, one can apply a positional mask, by adding or subtracting a given value from the energy scores before the application of the softmax (Shen et al., 2018a). Since the location may not be known in advance, the hard attention model by Xu et al. (2015), considers the keys in a dynamically determined location.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2597655663	Another possib lity is offered by Lu et al. (2016), who use a multi-layer perceptron in order to learn the mappings from E to eK a d eQ.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2133564696	Quoting Bahdanau et al. (2015), the use of attention “relieve[s] the encoder from the burden of having to encode all information in the source sentence into a fixed length-vector.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1850742715	Selective attention (Gregor et al., 2015) follows the same idea: using a grid of Gaussian filters, only a patch of the keys is considered, with its position, size, and resolution depending by dynamic parameters.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	1850742715	Selective attention (Gregor et al., 2015) follows the same idea: using a grid of Gaussian filters, only a patch of the keys is considered, with its position, size, and resolution depending by dynamic parameters. Shen et al. (2018b) combine soft and hard attention, by applying the former only on the elements filtered by the latter.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	A seq enti l coarse-grained m el proposed by Lu et al. (2016) is alt nating o-atte tion, illustrated in Figur 7 (l ft), wher by a tention is computed three times to obtain embeddings for K a d Q.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2463565445	A sequential coarse-grained model proposed by Lu et al. (2016) is alternating co-attention, illustrated in Figure 7 (left), whereby attention is computed three times to obtain embeddings for K and Q.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2589889860	The unified model we propose is based on and extends the models proposed by Daniluk et al. (2017) and Vaswani et al.
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2626778328	A variation of this model is scaled multiplicative attention, where a scaling factor is introduced to improve performance with large keys (Vaswani et al., 2017).
2912279724	Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing	2141399712	in computer vision by Larochelle and Hinton (2010), following the observation that biological retinas fixate on relevant parts of the optic array, while resolution falls off rapidly with eccentricity.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2107658650	, 1990; Miller and Fellbaum, 1998) and CYC (Lenat, 1995) have focused on the manual construction of high-quality ontologies.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	141602984	, 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2122865749	(2006) train a classifier to predict the likelihood of an edge in WordNet, and suggest new undiscovered edges, while Kozareva and Hovy (2010) propose an algorithm which repeatedly crawls for new edges using a web search engine and an initial seed taxonomy.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2122865749	, 2007) and YAGO (Suchanek et al., 2007; Hoffart et al., 2013) which have found important applications in text understanding and question answering
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2250533418	, 2012), EVAL (Santus et al., 2015), SHWARTZ (Shwartz et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	1516501661	, 2016), and WBLESS (Weeds et al., 2014), In addition to positive hypernymy relations, these datasets includes negative samples in the form of random pairs, co-hyponymy, antonymy, meronymy, and adjectival relations.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2250539671	(2018) proposed an extension of GLOVE (Pennington et al., 2014) to hyperbolic space.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2122865749	, the animals, plants, and vehicles taxonomies, as proposed by Kozareva and Hovy (2010). In addition to predicting the existence of single hypernymy relations, this allows us to evaluate the performance of these models for inferring full taxonomies and to perform an ablation for the prediction of missing and transitive relations.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	1840435438	In Artificial Intelligence, concept hierarchies provide valuable information for a wide range of tasks such as automated reasoning, fewshot learning, transfer learning, textual entailment, and semantic similarity (Resnik, 1993; Lin, 1998; Berners-Lee et al., 2001; Dagan et al., 2010; Bowman et al., 2015; Zamir et al., 2018).
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2135964261	For detection, we evaluate all models on five commonly-used benchmark datasets: BLESS (Baroni and Lenci, 2011), LEDS (Baroni et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	205765513	For detection, we evaluate all models on five commonly-used benchmark datasets: BLESS (Baroni and Lenci, 2011), LEDS (Baroni et al., 2012), EVAL (Santus et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2251109971	For directionality and graded entailment, we also use the BIBLESS (Kiela et al., 2015) and HYPERLEX (Vulic et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2296076036	Distributional representations that are based on positional or dependecy-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016).
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2102381086	Early approaches such as WORDNET (Miller et al., 1990; Miller and Fellbaum, 1998) and CYC (Lenat, 1995) have focused on the manual construction of high-quality ontologies.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2102381086	embeddings (Nickel and Kiela, 2017, 2018), what provides important advantages for this task. First, as Roller et al. (2018) showed recently, Hearst patterns provide important constraints for hypernymy extraction from distributional contexts.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2138605095	To increase scalability and coverage, the focus in recent efforts such as PROBASE (Wu et al., 2012) and WEBISADB (Seitner et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	141602984	invCL Lenci and Benotto (2012), introduce the idea of distributional exclusion by also measuring the degree to which the broader term contains con-
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	102708294	is-a relationships are the basis of complex knowledge graphs such as DBPEDIA (Auer et al., 2007) and YAGO (Suchanek et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2068737686	The main idea introduced by Hearst (1992) is to exploit certain lexico-syntactic patterns to detect is-a relationships in natural language.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2172888184	Order-embeddings (Vendrov et al., 2016) represent text and images with embeddings, where the ordering over individual dimensions form a partially ordered set.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2142086811	These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016).
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2068737686	A popular approach is to exploit high-precision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2122865749	A prime example is biology, in which taxonomies have a long history ranging from Linnaeus et al. (1758) up to recent efforts such as the Gene Ontology and the UniProt taxonomy (Ashburner et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	1966907789	proaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	38703128	Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	38703128,2029344051,2773020524	Similar works consider noisy graphs discovered from Hearst patterns, and provide algorithms for pruning edges until a strict hierarchy remains (Velardi et al., 2005; Kozareva and Hovy, 2010; Velardi et al., 2013).
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2583105957	SLQS The SLQS model is based on the informativeness hypothesis (Santus et al., 2014; Shwartz et al., 2017), i.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2102381086	As suggested by Nickel and Kiela (2018), we initialize the embeddings close to the origin of Ln by sampling from the uniform distribution U(−0.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2597332713	Taxonomy induction Although detecting hypernymy relationships is an important and difficult task, these systems alone do not produce rich taxonomic graph structures (Camacho-Collados, 2017), and complete taxonomy induction may be seen as a parallel and complementary task.
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	2102381086	Training To solve Equation 3, we follow Nickel and Kiela (2018) and perform stochastic optimization via Riemannian SGD (RSGD; Bonnabel 2013).
2912364454	Inferring Concept Hierarchies from Text Corpora via Hyperbolic Embeddings.	1966907789	WeedsPrec The first distributional model we consider is WeedsPrec (Weeds et al., 2004), which captures the features of x which are included in the set of more general term’s features, y:
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2605131327	of 1 (small) or 2 (medium and large) LSTM layers, and uses additive attention as described in [16].
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1494198834	We carry out evaluation on the three subsets of the LibriSpeech task [15]: 100hr, 460hr, and 960hr, where we find that grapheme or word-piece models do indeed consistently outperform phoneme-based models.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2131774270	On top of the convolutional layers, the encoder contains 3 (small) or 4 (medium and large) layers of bi-directional LSTMs [19], with either 256 (small), 512 (medium), or 1024 (large) LSTM [20] cells in each layer.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1895481600	The lexicon and language model (LM) are represented as WFST L and G respectively and combined by means of FST composition as the search network L ◦G [17].
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1494198834	The LibriSpeech task [15] has three subsets with different amounts of transcribed training data: 100hr, 460hr, and 960hr data.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1816313093	Such models are typically trained to output character-based units: graphemes, byte-pair encodings [9], or word-pieces [10], which allow the model to directly map the frame-level input audio features to the output word sequence, without using a hand-crafted pronunciation lexicon.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1494198834	We also note that the performance of attention-based models dramatically degrade when the amount of training data is reduced, unlike the conventional hybrid approach [15].
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2577366047	piece model combined with an LSTM LM [26] by shallow fusion [27, 28].
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2605131327	We refer the interested reader to [4, 16] for more details of attention-based models.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2130942839,2133564696,2526425061,2739427748	Sequence-to-sequence learning [1] based on encoder-decoder attention models [2] has become popular for both machine translation [3] and speech recognition [4, 5, 6, 7, 8].
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2327501763	Thus, when using such character-based output units, end-to-end speech recognition models [11] jointly learn the acoustic model, pronunciation model, and language model within a single neural network.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	2530876040	Thus, following [18], our encoder layers include two layers of 3×3 convolution with 32 channels with a stride of 2, which results in a total time reduction factor of 4.
2912492482	Model Unit Exploration for Sequence-to-Sequence Speech Recognition	1522301498	We train all models using 16 GPUs by asynchronous stochastic gradient descent with Adam optimizer [21] from random initialization without any special pre-training method2.
2912512634	End-to-end Anchored Speech Recognition	2047769394,2079623482,2146871184,2150769028	, ivector [1, 2], mean-variance normalization [3], maximum likelihood linear regression (MLLR) [4].
2912512634	End-to-end Anchored Speech Recognition	2102113734	ate the complexity of building an ASR system. Compared with the others, the attention-based encoderdecoder models [19, 16] do not assume conditional independence for output labels as CTC based models [20, 14] do. We propose two end-to-end models for anchored speech recognition, focusing on the case where each frame is either completely from device-directed speech or completely from interfering speech, but
2912512634	End-to-end Anchored Speech Recognition	2127141656	Compared with the others, the attention-based encoderdecoder models [19, 16] do not assume conditional independence for output labels as CTC based models [20, 14] do.
2912512634	End-to-end Anchored Speech Recognition	2064675550	Its encoder consists of three convolution layers resulting in 2x frame down-sampling and 8x frequency down-sampling, followed by 3 Bidirectional LSTM [27] layers with 320 hidden units.
2912512634	End-to-end Anchored Speech Recognition	854541894	Recently, much work has been done towards end-to-end approaches for speech recognition [13, 14, 15, 16, 17, 18].
2912512634	End-to-end Anchored Speech Recognition	1828163288,2102113734,2327501763	rid ASR systems [10]. Speaker-dependent mask estimation is also explored for target speaker extraction [11, 12]. Recently, much work has been done towards end-to-end approaches for speech recognition [13, 14, 15, 16, 17, 18]. These approaches typically have a single neural network model to replace previous independently-trained components, namely, acoustic, language, and pronunciation models from hybrid HMM systems. Endt
2912512634	End-to-end Anchored Speech Recognition	1837709900	Each utterance is handtranscribed and begins with the same wake-up word whose alignment with time is provided by end-point detection [22, 23, 24, 25].
2912624765	A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization.	2250539671	Although pretrained word representations on a large (unlabeled) language corpus, such as GloVe (Pennington et al., 2014), have shown promising results in WikiSQL (Dong and Lapata, 2018; Xu et al.
2912624765	A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization.	2123442489	To find word vectors, natural language utterance is first tokenized by using Standford CoreNLP (Manning et al., 2014).
2912815991	An Effective Approach to Unsupervised Machine Translation.	2546938941	, 2017) and dual learning (He et al., 2016), our method takes two initial models in opposite directions, and defines an unsupervised optimization objective that combines a cyclic con-
2912815991	An Effective Approach to Unsupervised Machine Translation.	2284660317	(2018b) use a heuristic that builds two initial models in opposite directions, uses one of them to generates a synthetic parallel corpus through back-translation (Sennrich et al., 2016), and applies MERT to tune the model in
2912815991	An Effective Approach to Unsupervised Machine Translation.	2546938941	In addition to that, we would like to incorporate a language modeling loss during NMT training similar to He et al. (2016). Finally, we would like to adapt our approach to more relaxed scenarios with multiple
2912815991	An Effective Approach to Unsupervised Machine Translation.	2626778328	More concretely, we include the best results from the shared task itself, which reflect the state-of-the-art in machine translation back in 2014; those of Vaswani et al. (2017),
2912815991	An Effective Approach to Unsupervised Machine Translation.	2153579005	More concretely, we train our n-gram embeddings using phrase2vec1, a simple extension of skip-gram that applies the standard negative sampling loss of Mikolov et al. (2013) to bigramcontext and trigram-context pairs in addition to the usual word-context pairs.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2284660317	forming a single pass over a synthetic parallel corpus built through back-translation (Sennrich et al., 2016).
2912815991	An Effective Approach to Unsupervised Machine Translation.	1526729636	In the future, we would like to explore learnable similarity functions like the one proposed by (McCallum et al., 2005) to compute the characterlevel scores in our initial phrase-table.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2284660317	This initial system is then refined through iterative backtranslation (Sennrich et al., 2016) which, in the case of Artetxe et al.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2605287558	Inspired by the previous work on CycleGANs (Zhu et al., 2017) and dual learning (He et al.
2912815991	An Effective Approach to Unsupervised Machine Translation.	1526729636	We leave the exploration of more elaborated similarity functions and, in particular, learnable metrics (McCallum et al., 2005), for future work.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2626778328	The recent advent of neural sequence-to-sequence modeling has resulted in significant progress in the field of machine translation, with large improvements in standard benchmarks (Vaswani et al., 2017; Edunov et al., 2018) and the first solid claims of human parity in certain settings (Hassan et al.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2121745180	Ren et al. (2019) follow a similar approach, but use SMT as posterior regularization at each iteration.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2141440284	Our SMT implementation is based on Moses8, and we use the KenLM (Heafield et al., 2013)
2912815991	An Effective Approach to Unsupervised Machine Translation.	2148708890	Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.
2912815991	An Effective Approach to Unsupervised Machine Translation.	2148708890	Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure. Finally, we use the big transformer implementation from fairseq9 for our NMT system, training with a total batch size of 20,000 tokens across 8 GPUs with the exact same hyperparameters as Ott et al. (2018).
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2337748376	[14] proposed a joint model to recognize and link fine-grained locations from tweets with 24 hand-crafted features.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643	[16] carefully designed lexical, grammatical, geographical and BILOU schema features (totally 11 features) to extract finegrained locations from tweets.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2015570541	[34] designed four types of features (gazetteer, geospatial context, Twitter metadata, and word form) and trained a classifier to select the best match among gazetteer candidates.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2179519966	[4] utilized CNN to detect character-level features and LSTM to capture the wordlevel context in a sentence.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2158899491	[5], where an architecture based on temporal convolutional neural networks (CNNs) over word sequence was proposed.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2148007440,2151848474	, YAGO andDBpedia) and support many business intelligence applications such as POI recommendation and location-aware advertising [16, 20, 22].
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2337748376	Following [14], we use a location profile mapping dictionary to recall all possible candidates for a locationmention.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2160097208,2337748376	Furthermore, the beam search algorithm [35] is also used in [14] to search the best combination of the mention recognition and linking.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2015570541	Generally, location gazetteers are widely used in location mention recognition [16, 21, 34].
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2337748376	On the other hand, existing solutions [10, 14, 16] on these two tasks require a large set of features manually designed for each task and domain, which demands task and domain expertise.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2078527501	Here, a fine-grained location is equivalent to a point-of-interest (POI), which is a focused geographical entity such as a landmark, a school, a historic building, or a business unit [16, 26].
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2085337304,2105842272,2115352105	Pair-wise [15] and global collective methods [11, 12, 32] have been applied to explore the coherence.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643	POI inventory, containing partial and familiar names from Foursquare, is proposed in [16] to pre-label candidate location mentions in tweets.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2337748376	Same POI inventory and location profile mapping dictionary used in [14] are employed through all models in our experiments.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2179519966	Previous studies [4, 13] have shown that character-level information (e.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2004763266,2121227244	Prior solutions exploit comprehensive linguistic features like Part-of-Speech (POS) tags, capitalizations [27], Brown clustering [3] to improve recognition performance.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2337748376	More specifically, we compare our model with [16] and [14] for recognition, and [30] for linking, respectively.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	1972338643,2337748376	In many tweets, locations are implicitly or causally revealed by users at fine-grained granularity [14, 16, 17], for example, a restaurant, a shopping mall, a park or a landmark building.
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2004763266	We use a CRF toolkit to automatically assign the pre-labels with BILOU scheme[27].
2912841887	DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition and Linking in Tweets	2250539671	We use the GloVe[23] embeddings pre-trained on a large-scale Twitter corpus of two billion tweets.
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2099779943	1 Dataset and Evaluation Metrics We evaluate our model on the SemEval-2010 Task 8 dataset, which is an commonly used benchmark for relation classification [6] and compare the results with the state-of-the-art models in this area.
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2250521169	1 Relative Position Features In relation classification, the position of each word relative to entities has been widely used for word representations [30, 14, 8].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2626778328	We adopt the multi-head attention formulation [20], one of the methods for implementing self attentions.
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2293023260,2470673105,2250521169	There are CNN-based models such as CNN [30, 14], CR-CNN [2], and Attention-CNN [8] and RNN-based models such as BLSTM [32], Attention-BLSTM [34], and HierarchicalBLSTM (Hier-BLSTM) [25] for this task.
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	1750263989	and deep recurrent neural networks (DRNNs) model proposed by Xu et al eliminate irrelevant words out of SDP and use neural network based on the meaningful words composing SDP [24, 23].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	1887754209	Early methods used handcrafted features through a series of NLP tools or manually designing kernels [16].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2250521169	employed a deep convolutional neural network (CNN) for extracting lexical and sentence level features [30].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2153579005,2250539671	Especially, many researches tried to solve the problem based on end-to-end models using only raw sentences and pre-trained word representations learned by Skip-gram and Continuous Bag-of-Words [12, 11, 15].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	1887754209	First, the SVM [16], Non-Neural Model, was top of the SemEval2010 task, during the official competition period.
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2107878631	Many neural models encoding sequence of words may expect to learn implicitly of the contextual meaning, but they may not learn well because of the long-term dependency problems [1].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2293023260	proposed bidirectional LSTM network (BLSTM) utilizing position of words, POS tags, named entity information, dependency parse [32].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	1750263989,1889268436	The second is SDP-based Model such as MVRNN [18], FCM [27], DepNN [9], depLCNN+NS [22], SDP-LSTM [24], and DRNNs [23].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	2626778328	Self attention has been successfully applied to various NLP tasks such as machine translation, language understanding, and semantic role labeling [20, 17, 19].
2912974420	Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing	1533861849,2250539671	We use pre-trained weights of the publicly available GloVe model [15] to initialize word embeddings in our model, and other weights are randomly initialized from zero-mean Gaussian distribution [3].
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2155821672	ed with the natural language descriptions using a lexical database such as WordNet, that groups words based on their meanings, to calculate a conceptual distance metric between concepts at both ends, [26] [9]; and c) match partially-observable natural language description with that of the semantics of the service described using semantic web services such as OWL-S and VDL [27] [8]. Categorical limitat
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2155821672	h against service descriptions [4], [23]; (b) construct semantic graphs to represent service descriptions and match against a lexical database such as WordNet to compute concept similarity [9], [13], [26]; and (c) match partiallyobservable natural language request with semantics of service description expressed using semantic web services (OWL-S, VDL) [8], [27]. Limitations with these approaches inclu
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2124474385	ing to match against service descriptions [4], [23]; (b) construct semantic graphs to represent service descriptions and match against a lexical database such as WordNet to compute concept similarity [9], [13], [26]; and (c) match partiallyobservable natural language request with semantics of service description expressed using semantic web services (OWL-S, VDL) [8], [27]. Limitations with these appr
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	1569064117	nts communicate locally and across the network through services. Services have an API that is deﬁned in a Java package. Some of the most known OSGi-based middleware for service composition are: [15], [20], [28], [29]. We use OSGi as a backbone for connecting multiple service implementations, providing a mean for the exchange of information between them. III. APPROACH A. Preliminaries As it is generall
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2136421912	ork and conclude the paper in Section V and Section VI, respectively. II. BACKGROUND AND MOTIVATION A. Service Composition Middleware Model According to the Service Composition Middleware (SCM) model [16] (a high-level abstraction model that does not consider a particular service technology, language, platform or algorithm used in the composition process), middleware for service composition can be lar
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	1502837342	Most of the OSGi-Android approaches [5], [7], [11] are based on Apache Felix [2], an implementation of the OSGi Framework and Service platform.
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2136421912	required to guarantee interoperability between semantic and syntactic service description languages that are both heterogeneous. Prior work shows the heavy cost of the syntactic and semantic matching [16]. In our work, we replace effort-consuming syntactic/semantic service descriptions (WSDL, OWL-S, etc.) by intuitive code Figure 3. Workﬂow for Service Development using NLSC annotations that allow dev
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2042992856	Such service descriptions serve as inputs to orchestration engines [30], [32] that generate declarative specification of workflows to compose different services; and (b) architectural middlewares [6], [14], [19], [25] that assume a declarative specification of a composition.
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2042992856	Service is any software component, data, or hardware resource on a device that is accessible by others [6].
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2124474385	th the natural language descriptions using a lexical database such as WordNet, that groups words based on their meanings, to calculate a conceptual distance metric between concepts at both ends, [26] [9]; and c) match partially-observable natural language description with that of the semantics of the service described using semantic web services such as OWL-S and VDL [27] [8]. Categorical limitations
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2153579005	Words [21]) where the training objective is to train sentences instead of word embeddings.
2913090535	NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings	2096765155	The ﬂexibility of this approach allows us to discover and re-conﬁgure types at runtime without linking to speciﬁc classes and objects at design-time. For Named-entity recognition, we use Stanford NER [12], a Java implementation that labels sequences of words in a text that are names of things, such as person and company names. It provides well-engineered feature extractors that annotates sentences wit
2913280387	Making a Case for Social Media Corpus for Detecting Depression.	2144211451	This was bolstered by the inverse document frequency concept described in [2] where the author established the need to filter out stop words such as “the”, “and”, etc.
2913280387	Making a Case for Social Media Corpus for Detecting Depression.	2098162425	Porter’s pioneer work in developing a stemmer rounds up the traditional approaches to NLP [6].
2913280387	Making a Case for Social Media Corpus for Detecting Depression.	2144211451	This result was consistent with the findings of [2,34] where the authors emphasized the following: 1) the narrower the context, better the selection of keywords and 2) despite the presence of OOV words, the underlying text still conforms to the subject at hand 5.
2913280387	Making a Case for Social Media Corpus for Detecting Depression.	2114060717	showed the opportunity Big Data provides in studying rare events [13].
2913299240	Realistic Image Generation using Region-phrase Attention.	2173520492	[8] synthesized images based on the RNN network encoded text descriptions and were able to generate 64 × 64 images on CUB, Oxford-102 and MSCOCO datasets.
2913299240	Realistic Image Generation using Region-phrase Attention.	2099471712,2173520492	Algorithms based on the Generative Adversarial Network (GAN) [3], specifically Deep Convolutional GAN (DCGAN) [8], have demonstrated promising results on various datasets.
2913299240	Realistic Image Generation using Region-phrase Attention.	2102605133	In the case where the dataset does not provide such information, they can also be obtained from off-the-shelf image object detectors, such as RCNN [2].
2913299240	Realistic Image Generation using Region-phrase Attention.	2173520492	Deep Convolutional GAN (DCGAN) [8] utilized several layers of convolutional neural networks to encode and decode images, in addition to Batch Normalization [5] to stabilize the GAN training.
2913299240	Realistic Image Generation using Region-phrase Attention.	1566256432,2131774270	Firstly, a bi-directional LSTM [13, 4] is applied to each sentence to extract word and sentence representations.
2913299240	Realistic Image Generation using Region-phrase Attention.	1905882502	Inspired by the visual-semantic alignment [6], we also encourage image sub-region and word matching during training.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2156695938	[21] propose a new method for author attribution, by means of probabilistic context-free grammars.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	[24] propose the utilization of “syntactic n-gram” (sn-gram) as machine learning features for authorship attribution and prove the very high accuracy, after utilizing sn-grams.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	766922509	In 2015, Amancio [25] proposes the method of the complex network, for upgrading the current statistical methods.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2119804197	ans of probabilistic context-free grammars. For each author, they build a probabilistic context-free grammar and utilize it as a language model for the classification purposes. In 2012, Brenan et al. [22] publish two corpora for adversarial stylometry with 57 unique authors and after theoretically discussing/taxonomizing different methods, prove the high accuracy of four claimed methods for this task.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1993539890	The CCAT dataset [55] is one of the most well-known short-length authorship attribution datasets.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2159642183	have been conducted in the state-of-the-art of authorship attribution which are covered and structured by a number of well-known surveys [11][12][13][14][15][16][17][6]. In 2008, Luyckx and Daelemans [18] propose an authorship attribution approach, utilizing memory-based learning, which is specialized for the problems with many authors and limited data for the train, which overcomes SVM and maximum en
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2143859313	For example, assuming that the utilized artificial neural network has k layers and approximately d nodes in each layer, then its training complexity is polynomial with respect to d [46].
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2079504649	heir more security, multi-biometric systems are also shown to be more efficient and reliable than unimodal systems. One of the important applications of Biometrics is Forensic Document Analysis (FDA) [5]. FDA deals with different problems such as detection of the fraudulent documents either if they are modified or are totally imposter, checking the genuineness of some security features (e.g. signatur
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	However, since some of the authors have more than 13 books, to have a comparable style with the 8/5 train/test style utilized in [24], we keep the 13 largest books of each author and remove the remainder.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2133990480,2135813353	They include Multinomial Naïve Bayes (MNB) [40] and Poisson Naïve Bayes (PNB) [41] classifiers, whereas some other classifiers such as Weibull Naïve Bayes require at least log-linear complexity for a (reasonably accurate)
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2086648308	Although the k-nearest-neighbor classifier is very fast, it is not accurate enough for being a benchmark for authorship attribution experiments [45].
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2121705522	Linear (and Quadratic) Discriminant Analysis (LDA and QDA) classification algorithms are polynomial with respect to min(m, n) [44].
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2071629922	lome of every author as a combination of different frequent features in e-mails. They prove that their method is able to attribute the authors of real-life emails, effectively. In 2010, Layton et al. [20] propose a number of novel preprocessing methods which attribute the authorship, meaningfully superior to the mentioned benchmark in microblog-sized data. Moreover, Raghavan et al. [21] propose a new
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2252147815	mathematically discussed in the state-of-the-art, it is experimentally proven to be the best among other implemented (available) syntactic parsers [51].
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	Although the mentioned study [24] is one of the most well-known research papers of this field (due to its very efficient proposed text mining model), yet the number of authors that are chosen in [24] is very few (only
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2038754641	Please note that, even we can utilize the dependency parser proposed in [52] to make the parser complexity, linear, and therefore the total complexity as O(c ∙ n ∙ m + c ∙ n ∙ s ∙ (t + g) + c ⋅ n ⋅ s ⋅ w) Eq.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	A number of studies have shown that the most effective measures are lexical and character features [24].
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2008340903	ource of information (e.g. only fingerprint or only face for recognition of a subject), which has to deal with different problems such as having noisy data, nonuniversality, or intra-class variations [3]. For overcoming the limitations of the former, the latter systems are proposed by integrating multiple sources of information. It makes the system to be less vulnerable to spoofing attacks, due to th
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	Recently, Sidorov [24] introduced a new lexical feature named “syntactic n-gram” (sn-gram).
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	1967981232	Although a recursive depth-first traversal algorithm is presented in [24] for construction of sn-grams from syntactic trees, alternatively, we can also utilize its non-recursive version for having linear complexity in, both, time (O(t + g)) and space (O(t)), when t stands for the number of nodes in the extracted syntactic tree, and g stands for the number of (grammatical) edges.
2913308252	A Linear-complexity Multi-biometric Forensic Document Analysis System, by Fusing the Stylome and Signature Modalities.	2048529946,2054151502,2126631960,2187238335	surveys [11][12][13][14][15][16][17][6].
2913352150	A Question-Entailment Approach to Question Answering.	2250175451	[17] proposed a new approach to retrieve semantically equivalent questions combining a bag-of-words representation with a distributed vector representation created by a CNN and user data collected from two Stack Exchange communities.
2913352150	A Question-Entailment Approach to Question Answering.	2156938860	[18] proposed a dedicated language modeling approach for question search, using question topic (user’s interest) and question focus (certain aspect of the topic).
2913352150	A Question-Entailment Approach to Question Answering.	2162355876	[24] showed that a retrieval model based on translation probabilities learned from a question and answer archive can recognize semantically similar questions.
2913352150	A Question-Entailment Approach to Question Answering.	2340486923	[30] proposed a recurrent and convolutional model (gated convolution) to map questions to their semantic representations.
2913352150	A Question-Entailment Approach to Question Answering.	2468484304	02 cQA-B-2016 IR Baseline [35] cQA-16-Test - - - 74.
2913352150	A Question-Entailment Approach to Question Answering.	2146537661	An alternative approach consists in finding similar questions or FAQs that are already answered [25, 49].
2913352150	A Question-Entailment Approach to Question Answering.	1840435438	The choice of two methods for our empirical study is motivated by the best performance achieved by Logistic Regression in question-question similarity at SemEval 2017 (best system [11] and second best system [21]), and the high performance achieved by neural networks on larger datasets such as SNLI [8, 28, 12, 20].
2913352150	A Question-Entailment Approach to Question Answering.	2468484304	cQA-B-2016 Best System [35] cQA-16-Test 76.
2913352150	A Question-Entailment Approach to Question Answering.	1840435438	In another definition, the Stanford Natural Language Inference corpus SNLI [8], used three classification labels for the relations between two sentences: entailment, neutral and contradiction.
2913352150	A Question-Entailment Approach to Question Answering.	2468484304	Different machine learning and deep learning approaches were tested in the scope of SemEval 2016 [35] and 2017 [36] task 3B.
2913352150	A Question-Entailment Approach to Question Answering.	2112644606,2120380513,2138380910	Although entailment was attempted in QA before [22, 38, 10], as far as we know, we are the first to introduce and evaluate a full medical question answering approach based on question entailment for free-text questions.
2913352150	A Question-Entailment Approach to Question Answering.	2250539671	GloVe6 is an unsupervised learning algorithm to generate vector representations for words [40].
2913352150	A Question-Entailment Approach to Question Answering.	2468484304	Lately, these efforts were supported by a task on Question-Question similarity introduced in the community QA challenge at SemEval (task 3B) [35].
2913352150	A Question-Entailment Approach to Question Answering.	632432350,2054070929	Several QA approaches were proposed in the literature for the open domain [14, 23] and the medical domain [3, 6, 52].
2913352150	A Question-Entailment Approach to Question Answering.	2468484304	The question similarity dataset of SemEval 2016 Task 3B (SemEval-cQA) [35] contains 3,869 question pairs and aims to re-rank a list of related questions according to their similarity to the original question.
2913352150	A Question-Entailment Approach to Question Answering.	2250175451,2340486923,2468484304	Question similarity has recently attracted international challenges [35, 36] and several research efforts proposing a wide range of approaches, including Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs) [17, 43, 4, 30].
2913352150	A Question-Entailment Approach to Question Answering.	2068176125,2120735855,2155712036	Similarly, many different approaches tackled document or passage retrieval and answer selection and (re)ranking [50, 46, 47].
2913352150	A Question-Entailment Approach to Question Answering.	1840435438	The Stanford Natural Language Inference corpus (SNLI) [8] contains 569,037 sentence pairs written by humans based on image captioning.
2913352150	A Question-Entailment Approach to Question Answering.	2106390866,2146360558	A variety of methods were developed for question analysis, focus (topic) recognition and question type identification [29, 5, 34, 33].
2913470588	OpenHowNet: An Open Sememe-based Lexical Knowledge Base.	1558043786	People use HowNet and sememe in various NLP tasks including word similarity computation (Liu and Li, 2002), word sense disambiguation (Zhang et al., 2005), question classification (Sun et al.
2913590199	Query-oriented text summarization based on hypergraph transversals	2141403143	2 is based on an adaptation of an algorithm presented in [31] for the maximization of submodular functions under a Knaspack constraint.
2913590199	Query-oriented text summarization based on hypergraph transversals	2083786039	The concept of hypergraph transversal is used in computational biology [7] and data mining [6] for identifying a subset of relevant agents in a hypergraph.
2913590199	Query-oriented text summarization based on hypergraph transversals	2036371118	In contrast, [10] propose a coverage approach in which a set of sentences maximizing the number of distinct relevant terms is selected.
2913590199	Query-oriented text summarization based on hypergraph transversals	2218641061	In contrast, an abstractive summarizer identifies conceptual information in the corpus and reformulates a summary from scratch [12].
2913590199	Query-oriented text summarization based on hypergraph transversals	2034721576	A different graph model was proposed in [14], where sentences and key phrases form the two classes of nodes of a bipartite graph.
2913590199	Query-oriented text summarization based on hypergraph transversals	2083778364	An extended bipartite graph ranking algorithm is also proposed in [2] in which the sentences represent one class of nodes and clusters of similar sentences represent the other class.
2913590199	Query-oriented text summarization based on hypergraph transversals	2218641061	Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years.
2913590199	Query-oriented text summarization based on hypergraph transversals	2043093827	Feature-based methods represent the sentences with a set of predefined features such as the sentence position, the sentence length or the presence of cue phrases [13].
2913590199	Query-oriented text summarization based on hypergraph transversals	2288604516	Finally, [17] propose a two step approach in which individual sentence relevance scores are computed first.
2913590199	Query-oriented text summarization based on hypergraph transversals	2036371118	First, the MaxCover approach [10] seeks a summary by maximizing the number of distinct relevant terms appearing in the summary while not exceeding the target summary length (using equation 9 to compute the term relevance scores).
2913590199	Query-oriented text summarization based on hypergraph transversals	1962684803,2288604516	Hence, global optimization approaches were proposed to select a set of jointly relevant and non-redundant sentences as in [16] and [17].
2913590199	Query-oriented text summarization based on hypergraph transversals	1488425980	For instance, [18] propose a greedy algorithm to find a dominating set of nodes in the sentence graph.
2913590199	Query-oriented text summarization based on hypergraph transversals	2036371118	The method of [10] proposes to select sentences by using a maximum coverage approach, which shares some similarities with our model.
2913590199	Query-oriented text summarization based on hypergraph transversals	1488425980,1962684803,2288604516	Hence the method is essentially equivalent to early graph-based models for text summarization in terms of computational burden, such as the LexRank-based systems [1, 3] or greedy approaches based on global optimization [16–18].
2913590199	Query-oriented text summarization based on hypergraph transversals	2288604516	While the objective function of the method is similar to that of the problem of finding a maximal budgeted hypergraph transversal (equation 12) of [17], they overlook the semantic similarities between terms which are captured by our SEMCOT algorithm and our hypergraph model.
2913590199	Query-oriented text summarization based on hypergraph transversals	1962684803,2036371118,2288604516	Since it seeks a set of jointly relevant sentences, our method shares some similarities with existing graph-based models that apply global optimization strategies for sentence selection [10,16,17].
2913590199	Query-oriented text summarization based on hypergraph transversals	2083778364	The sentences are then scored using graph ranking algorithms such as the PageRank [1] or HITS [2] algorithms, which can also be adapted for the purpose of query-oriented summarization [3].
2913590199	Query-oriented text summarization based on hypergraph transversals	1962684803	Similarly, [16] extract a set of sentences with a maximal similarity with the entire corpus and a minimal pairwise lexical similarity, which is modelled as a multi-objective optimization problem.
2913590199	Query-oriented text summarization based on hypergraph transversals	2218641061	Similarly, the focus has shifted from generic summarization to the more realistic task of query-oriented summarization, in which a summary is produced with the essential information contained in a corpus that is also relevant to a user-defined query [12].
2913590199	Query-oriented text summarization based on hypergraph transversals	2083778364	Topic-sensitive LexRank of [3] (TS-LexRank) and HITS algorithms of [2] are early graph-based summarizers.
2913590199	Query-oriented text summarization based on hypergraph transversals	2165897980	which can be viewed as a special case of the so-called google distance which was already successfully applied to learn semantic similarities between terms on webpages [26].
2913657017	Automated Essay Scoring based on Two-Stage Learning	2154414741	In [9], the researchers asked some experts who were familiar with e-Rater to write deceptive essays to trick e-Rater [17], which validated the fragility
2913657017	Automated Essay Scoring based on Two-Stage Learning	2251356693	One is composed of well-written permuted paragraphs, which have been successfully detected by [5] based on a coherence model [10].
2913657017	Automated Essay Scoring based on Two-Stage Learning	2251356693	Differently, our coherence model is LSTMbased rather than utilizes the clique strategy mentioned in [5, 10].
2913657017	Automated Essay Scoring based on Two-Stage Learning	2251356693	Evaluation Metric For consistency, the predicted scores together with the gold scores in the test set are uniformly re-scaled into [0, 10].
2913657017	Automated Essay Scoring based on Two-Stage Learning	2251356693	To improve the robustness of AES systems, [5] utilized the window-based coherence model [10] to detect the adversarial crafted inputs.
2913657017	Automated Essay Scoring based on Two-Stage Learning	2124725212	The models of the first stream are feature-engineered models, which are driven by handcrafted features, such as the number of words and grammar errors [1, 2].
2913657017	Automated Essay Scoring based on Two-Stage Learning	2250372169	In this paper, we choose Support Vector Regression (SVR) and Bayesian Linear Ridge Regression (BLRR) [16] for comparison.
2913657017	Automated Essay Scoring based on Two-Stage Learning	2124725212	With regard to the first category, [1, 2, 6] trained a Rank Support Vector Machine (RankSVM) to predict essay scores by utilizing pre-defined handcrafted features.
2913657017	Automated Essay Scoring based on Two-Stage Learning	2153579005,2250539671	Specifically, based on word embedding [7, 8], essays are represented into low-dimensional vectors, and followed by a dense layer to transform these deep-encoded vectors (involving deep semantic meanings) to corresponding ratings.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	630532510,2170716095	, 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., 2013; Bojar et al., 2017).
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	170711724	Burch, 2011), and cleaning them through voting mechanisms (Post et al., 2012).
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2493916176	To this end, we defined candidate sentences as: (i) being in the intended source language according to a language-id classifier (Bojanowski et al., 2017)5, and (ii) having sentences between 50 and 150 characters.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2163361328	be fluent (Zaidan and Callison-Burch, 2011), (ii) they should be sufficiently different from the source text, (iii) translations should be similar to each other, yet not equal; and (iv) translations should not be transliterations.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2250653840	However, both languages have a rather large amount of monolingual data publicly available (Buck et al., 2014), making them perfect candidates to track performance on unsupervised and semi-supervised tasks for Machine Translation.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	1522301498	Models are optimized with Adam (Kingma and Ba, 2015) using β1 = 0.9, β2 = 0.98, and = 1e − 8. We use the same learning rate schedule as Ott et al. (2018b). We run experiments on between 4 and 8 Nvidia V100 GPUs with mini-batches of between 10K and 100K target tokens following Ott et al.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2546938941	When monolingual data is available for both languages, we can train backward MT systems in both directions and repeat the back-translation process iteratively (He et al., 2016; Lample et al., 2018a).
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2284660317	For NMT systems, we instead use a vocabulary of 5K symbols based on a joint source and target Byte-Pair Encoding (BPE; Sennrich et al., 2015) learned using the sentencepiece library14 over the parallel training data.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2130942839,2133564696	provements in modeling, and in particular neural models (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2284660317	This is a research avenue that has seen a recent surge of interest, starting with semisupervised methods relying on backtranslation (Sennrich et al., 2015), integration of a language model into the decoder (Gulcehre et al.
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2284660317	this setting, we considered the standard backtranslation training protocol as introduced in Sennrich et al. (2015)’s seminal work: we train a backward MT system, which we use to translate monolingual target sentences to the source lan-
2913659301	Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English.	2252166243	For translation quality assessment, we followed a setup similar to direct assessment (Graham et al., 2013).
2913687407	Fine-Grained Temporal Relation Extraction.	2251325107	, 2013) and TimeBank-Dense (Cassidy et al., 2014), we use a transfer learning approach.
2913687407	Fine-Grained Temporal Relation Extraction.	1549997466,1973501242	(Minsky, 1975; Schank and Abelson, 1975; Lamport, 1978; Allen and Hayes, 1985; Hobbs et al., 1987).
2913687407	Fine-Grained Temporal Relation Extraction.	2626778328	Attention-based models have proven effective in neural machine translation literature (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017), but to our knowledge, they have not been explored in identifying temporal relations.
2913687407	Fine-Grained Temporal Relation Extraction.	2127194753,2251873637	Early approaches use handtagged features modeled with multinomial logistic regression and support vector machines (Mani et al., 2006; Bethard, 2013; Lin et al., 2015).
2913687407	Fine-Grained Temporal Relation Extraction.	2251325107	Efforts have been made to address the issue of sparsity in event-graphs with corpora such as the TimeBank-Dense (Cassidy et al., 2014) where annotators label all local-edges irrespective of ambiguity.
2913687407	Fine-Grained Temporal Relation Extraction.	2251784184	Following recent work using continuous labels in event factuality prediction (Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018; White et al., 2018) and genericity predic-
2913687407	Fine-Grained Temporal Relation Extraction.	318647910	Another framework called GAF (Fokkens et al., 2013) captures eventidentification through both textual and non-textual sources to track events across news articles.
2913687407	Fine-Grained Temporal Relation Extraction.	2105564063	On the other hand, Filatova and Hovy (2001) assign a time-stamp to every clause in text, but the durations of events are not taken into consideration.
2913687407	Fine-Grained Temporal Relation Extraction.	2135586429	and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures (Chambers et al.
2913687407	Fine-Grained Temporal Relation Extraction.	2185599447	and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures (Chambers et al., 2014; Mirza and Tonelli, 2016).
2913687407	Fine-Grained Temporal Relation Extraction.	2185599447	and learning-based approaches (D’Souza and Ng, 2013) and sieve-based architectures (Chambers et al., 2014; Mirza and Tonelli, 2016). Ning et al. (2018) jointly model causal and temporal relations using Constrained Conditional Models and formu-
2913687407	Fine-Grained Temporal Relation Extraction.	2161484642	Like standard approaches using the TimeML standard, we draw inspiration from Allen’s (1983) seminal work on interval representations of time.
2913687407	Fine-Grained Temporal Relation Extraction.	2626778328	low up on this work in our models, using a variation of dot-product attention (Luong et al., 2015; Vaswani et al., 2017) to predict the event timelines and durations which is described §4.
2913687407	Fine-Grained Temporal Relation Extraction.	70898464	The first makes sense in light of the fact that, in context, coordinators can carry information about temporal sequencing (Bar-Lev and Palacas, 1980; Carston, 1993; Wilson and Sperber, 1998).
2913687407	Fine-Grained Temporal Relation Extraction.	2052762201,2098844768,2150741878	The pairwise classification can result in inconsistent temporal graphs, and efforts have been made to avert this issue by employing temporal reasoning (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; Laokulrat et al., 2016; Ning et al., 2017; Leeuwenberg and Moens, 2017).
2913687407	Fine-Grained Temporal Relation Extraction.	103029643,1528445643,2150098934	People have also worked on modelling event durations from text (Pan et al., 2007; Gusev et al., 2011; Williams and Katz, 2012), but they don’t tie it directly to temporal relations.
2913687407	Fine-Grained Temporal Relation Extraction.	2157275230	To predict TimeML relations in TempEval3 (Task C - relation only) (UzZaman et al., 2013) and TimeBank-Dense (Cassidy et al.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	1, we tested our method on two subtasks: altering sentiment polarity of Yelp reviews (Shen et al., 2017) (YELP) and changing the formality of social media text (Rao and Tetreault, 2018) (FORMALITY).
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	This is common when tuning all models, as targeting at a higher BLEU score will result in a lower the style correctness score (Shen et al., 2017; Fu et al., 2017; Li et al., 2018).
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2432004435	CrossAlignment (CA) by Shen et al. (2017) loses the content, MultiDecoder (CA) by Fu et al.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	CrossAlignment (CA) (Shen et al., 2017) and Multi-Decoder (MD) (Fu et al.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	This is exemplified in Table 1, where the output of the model by Shen et al. (2017) changes the key content words from “the
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2105673178	Our proposal also relates to early work on comparable corpora for Statistical Machine Translation (SMT) systems (Fung and Yee, 1998; Munteanu et al., 2004; Smith et al., 2010), where systems are incrementally trained on pseudoparallel pairs.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	1486649854	sentence embedding can efficiently represent semantic and linguistic features of a sentence, outperforming more elaborated approaches such as Skip-Thought (Kiros et al., 2015), InferSent (Conneau et al.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	As a supplementary metric commonly used in previous work (Shen et al., 2017; Fu et al., 2017; Li et al., 2018; Prabhumoye et al., 2018), we also assess our model’s performance on style accuracy, BLEU, and perplexity (Table 6).
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2250539671	word embeddings (Pennington et al., 2014) as inputs of a standard machine translation sequenceto-sequence model with attention.
2913728973	Unsupervised Text Style Transfer via Iterative Matching and Translation	2617566453	YELP This dataset contains a commonly used subset of the YELP dataset for sentiment modification (Shen et al., 2017; Li et al., 2018; Prabhumoye et al., 2018).
2913749852	Misleading Metadata Detection on YouTube.	1973897992,2161283199	We see that inter-annotator agreement was not perfect, an issue which has been reported repeatedly in prior works for annotation tasks in social media [2,5].
2913749852	Misleading Metadata Detection on YouTube.	2153579005	Now, for each comment, we passed the GoogleNews pre-trained word2vec [4] embeddings of words of the comment sequentially to the LSTM.
2913749852	Misleading Metadata Detection on YouTube.	1638051351	A similar method was adopted for clustering tweets belonging to a particular rumor chain on twitter in [12] with good effect.
2913749852	Misleading Metadata Detection on YouTube.	1993081839,2218242280	Spam detection in social media has been a widely researched topic in the academic community [11,10].
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2159640018	• Seq2Seq: a Seq2Seq model with attention that does not have knowledge information (Shang et al., 2015; Vinyals and Le, 2015).
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2172140247	In our encoders, we implement the non-linear transformation f in Equation (1) using a bidirectional RNN with a gated recurrent unit (GRU) (Cho et al., 2014a), which consists of two parts: a forward RNN and a backward RNN.
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	1958706068	Examples include diversity promotion (Li et al., 2016) and unknown words handling (Gu et al.
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2064675550	where ht is the hidden state of the encoder at time t and f is a non-linear transformation, which can be a long-short term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997) or a gated recurrent unit (GRU) (Cho et al.
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2172140247	where ht is the hidden state of the encoder at time t and f is a non-linear transformation, which can be a long-short term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997) or a gated recurrent unit (GRU) (Cho et al., 2014a).
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2304113845	odel motivates the development of various techniques for improving the quality and diversity of generated responses. Examples include diversity promotion (Li et al., 2016) and unknown words handling (Gu et al., 2016). However, the problem of tending to generate short and generic words still remains in these models since they do not have the access to any external information. Recently, knowledge incorporation is
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2159640018	The recently proposed sequence-to-sequence (Seq2Seq) model (Shang et al., 2015; Vinyals and Le, 2015; Cho et al., 2014b) has garnered considerable attention due to its simplicity and wide applicability.
2913755428	Learning to Select Knowledge for Response Generation in Dialog Systems	2250539671	We set the word embedding size to be 300 and initialize it using GloVe (Pennington et al., 2014).
2913894054	Emotion Detection and Analysis on Social Media.	2014902591	[5] worked on the manual annotation of emotions, opinions and sentiments in a sentence corpus (of size 10,000) of news articles.
2913894054	Emotion Detection and Analysis on Social Media.	2123442489	ce of text. A. Using NLP and EWS: First Approach The ﬁrst approach uses the tweets in Tweets Set, which are already free from any unwanted characters, hyperlinks or hashtags. Various Stanford CoreNLP [15] tools are used for a comprehensive linguistic analysis. The following steps comprise the ﬁrst approach. 1) Tokenizing and Annotating: First, we use PTBTokenizer [16] to tokenize the tweets into sente
2913894054	Emotion Detection and Analysis on Social Media.	2105468141	em, which classiﬁes text in any form (eg. news, tweets, or narrative) and uses a training set, which is generated automatically. This saved a lot of effort. Another similar work was of Carlo and Rada [8] who did a similar classiﬁcation on news headlines but even they used manually annotated corpus for their classiﬁcation. There is one research work [9], that explores the possibility of creating autom
2913894054	Emotion Detection and Analysis on Social Media.	2168493061	ns have also been studied, but in a limited extent, such as by asking speciﬁc questions and judging on the basis of replies, or an analysis done only on short one-lined headlines or a few others [1], [2], [3], all of which depended on the manual annotation of the training dataset of a small size and limited scope. In this paper, we propose a method to classify and quantify tweets according to six sta
2913894054	Emotion Detection and Analysis on Social Media.	1971222444	There is one research work [9], that explores the possibility of creating automatic training datasets, like we do and also tries to find out if creating large emotion datasets can increase the emotion-detection accuracy in tweets.
2913894054	Emotion Detection and Analysis on Social Media.	1603420117	s (of size 10,000) of news articles. Segundo et al. [6] also studies the presence of emotions in text, and is a functional theory of the language used for expressing attitudes, judgments and emotions [3]. This paper deals explicitly with emotions, which none of the aforementioned works do. There was, however, one research work [7], that classiﬁed text into six EmotionCategories, but that was only lim
2913894054	Emotion Detection and Analysis on Social Media.	1513398909	sets can increase the emotion-detection accuracy in tweets. Some aspects of their work are similar to ours, but the emotion-detection accuracy we have achieved is much higher. Another similar work is [10], but again, our accuracy is much higher. A recent work [11] explores the possibility of predicting future stock returns based on tweets related to presidential elections and NASDAQ-100 companies. Ano
2913894054	Emotion Detection and Analysis on Social Media.	1603420117	ve also been studied, but in a limited extent, such as by asking speciﬁc questions and judging on the basis of replies, or an analysis done only on short one-lined headlines or a few others [1], [2], [3], all of which depended on the manual annotation of the training dataset of a small size and limited scope. In this paper, we propose a method to classify and quantify tweets according to six standard
2913911166	How to Write High-quality News on Social Network? Predicting News Quality by Mining Writing Style	2019416425,2184410296	[Pitler and Nenkova, 2008; Louis and Nenkova, 2013] take into account various linguistic factors to produce predictive models for article quality.
2913911166	How to Write High-quality News on Social Network? Predicting News Quality by Mining Writing Style	2019416425	Generally, it is difficult to estimate ‘quality’ of text in a certain field (such as science articles [Pitler and Nenkova, 2008]) without human judgement as the ground truth.
2913911166	How to Write High-quality News on Social Network? Predicting News Quality by Mining Writing Style	2019416425	Sentence broken [Xiao and Liu, 2015], Characters, Words, Average word length, Sentences, Clauses [Pitler and Nenkova, 2008], RIX, LIX and LW [Anderson, 1983] are all proposed before to capture Readability in a piece of news.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2149933564	, 2009) is ubiquitous when building image recognition models (Yosinski et al., 2014; Huh et al., 2016).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2609782806	, 2013), semantic parsing (Peng et al., 2017), machine translation (Johnson et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2550821151	, 2017), machine translation (Johnson et al., 2017), and question answering (Choi et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	1903029394	, 2017), segmentation (Long et al., 2015), and detection (Girshick et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2121227244	Brown clusters, trained on distributional information, are a classic example of pre-trained representations (Brown et al., 1992).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2493916176	Brown clusters, trained on distributional information, are a classic example of pre-trained representations (Brown et al., 1992). Turian et al. (2010) show that pre-trained embeddings of words outperform those trained from scratch.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2153579005,2250539671,2493916176	Since the deep-learning era, word embeddings have been widely used, and training strategies these have arisen (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2153579005	These embeddings may be at the word (Mikolov et al., 2013), sentence (Cer et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2616957565	We present an alternative transfer method based on adapter modules (Rebuffi et al., 2017).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2427527485	Recent state-of-the-art results on question answering (Rajpurkar et al., 2016) and text classification (Wang et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2560647685	Techniques have been proposed to mitigate forgetting (Kirkpatrick et al., 2017; Zenke et al., 2017), however, unlike for adapters, the memory is still imperfect.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2108598243	Transfer Learning in Visual Perception Fine-tuning models pre-trained on ImageNet (Deng et al., 2009) is ubiquitous when building image recognition models (Yosinski et al.
2913946806	Parameter-Efficient Transfer Learning for NLP.	2616957565	Tuning with adapter modules involves adding a small number of new parameters to a model, which are trained on the downstream task (Rebuffi et al., 2017).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2616957565	In vision, convolutional adapter modules have been studied (Rebuffi et al., 2017; 2018; Rosenfeld & Tsotsos, 2018).
2913946806	Parameter-Efficient Transfer Learning for NLP.	2194775991	These works perform incremental learning in multiple domains by adding small convolutional layers to a ResNet (He et al., 2016) or VGG net (Simonyan & Zisserman, 2014).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2251803607	, 2014) as well as RST-based approaches (Hirao et al., 2013; Durrett et al., 2016).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2123442489	(2017), which uses Stanford CoreNLP tokenization Manning et al. (2014). We use the non-anonymized version of the CNN/DM as in previous summarization work.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	1843891098,2606974598	, 2018) to abstractive (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; See et al., 2017; Tan et al., 2017; Gehrmann et al., 2018).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2044599851,2251803607	Approaches based on linguistic preprocessing, including this and RST-based approaches (Carlson et al., 2001; Hirao et al., 2013; Li et al., 2016), have several advantages.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2251750971	Compression Rules We refer to the rules derived in Li et al. (2014); Wang et al.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	1544827683	We conduct experiments on a few single document news summarization datasets: CNN, Daily Mail (Hermann et al., 2015), and the New York Times Annotated Corpus (Sandhaus, 2008).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	595069947	Depending on which sentences are extracted, different compression decisions may be optimal; however, re-deriving these with a dynamic oracle (Goldberg and Nivre, 2012) is prohibitively expensive during training.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	1544827683	We evaluate the proposed method on three popular news summarization datasets: the New York Times corpus (Sandhaus, 2008), CNN and Dailymail (DM) (Hermann et al., 2015).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2108373063,2122311631,2251425698	Extractive and compressive systems (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2011; Qian and Liu, 2013; Durrett et al., 2016) combine the strengths of both approaches.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2122311631	ILP-based approaches can exactly optimize for ROUGE recall given a reference summary (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011); however, exactly doing this optimization for ROUGE F1 is intractable5 and generally takes time O(nk) to select k out of n sentences.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2122311631	panded set of discrete options from prior work (Berg-Kirkpatrick et al., 2011; Wang et al., 2013; Durrett et al., 2016).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2606974598	PointGenCov (See et al., 2017) is an abstractive model with copy and coverage mechanisms.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2606974598	One possible reason for overall low scores is that we lowercased all words to be comparable to the output of See et al. (2017), which may have thrown off Turkers despite the provided instructions.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2001135856,2251750971	Previous work (Martins and Smith, 2009; Li et al., 2014; Wang et al., 2013; Durrett et al., 2016) used surface features including both word level (POS tags, stop words, position in the sentence, etc.
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2001135856,2251750971	Sentence compression is a long-studied problem dealing with how to delete the least critical information in a sentence to make it shorter (Knight and Marcu, 2000, 2002; Martins and Smith, 2009; Cohn and Lapata, 2009; Wang et al., 2013; Li et al., 2014).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2123442489	For our syntactic analysis, all datasets are parsed with the constituency parser in Stanford CoreNLP (Manning et al., 2014).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2108373063,2122311631	Our syntax-driven approach follows a line of successful prior compressive summarization work with non-neural models (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2011).
2913991091	Neural Extractive Text Summarization with Syntactic Compression	2251750971	Several systems explored the usage of constituency parses (BergKirkpatrick et al., 2011; Wang et al., 2013; Li et al., 2014) as well as RST-based approaches (Hirao et al.
2914010692	Assessing Partisan Traits of News Text Attributions	2147218300	In brief, the algorithm that was found to perform best primarily leverages state-of-the art co-reference resolution (Lee et al., 2013) as a baseline, with a few simple but key optimizations.
2914010692	Assessing Partisan Traits of News Text Attributions	2147218300	challenges observed by our baseline model leveraging state-of-the-art (Lee et al., 2013) co-reference
2914010692	Assessing Partisan Traits of News Text Attributions	2147218300	It is easy to see how these “gold standard” labels are integral to validating both CoreNLP’s vanilla coreference resolution (Lee et al., 2013) and our inhouse political candidate classifier.
2914010692	Assessing Partisan Traits of News Text Attributions	2166957049	tags and the Penn Discourse TreeBank (Miltsakaki, Prasad, Joshi, & Webber, 2004; Prasad et al., 2008) to learn from text annotations.
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2170240176	, emoticons, to be naturally learned (Zhang et al., 2015), while minimizing feature engineering and eliminating the constraint of being specialized to a particular language or style.
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2120615054,2170240176	Additionally, each convolutional layer contains several output convolutions, corresponding to the number of different weight vectors in that convolutional layer (Zhang et al., 2015; Kalchbrenner et al., 2014).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2170240176	The architecture is influenced heavily by Zhang et al. (2015). The network that encodes each individual sentence connects thirteen layers.
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2131876387	Character-level features for classifying text have also been explored in the literature, specifically as character-level n-grams combined with linear classifiers or by incorporating character-level features to form a distributed representation for CNNs (Shen et al., 2014; Santos & Zadrozny, 2014; Frome et al., 2013; Gao et al., 2013; Mikolov et al., 2013).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	753012316,1832693441,2250539671	Features generated using word2vec or a lookup table which are then fed into word-level convolutional neural network (CNN) yield competitive results to traditional text classification models (Kim, 2014; dos Santos & Gatti, 2014; Johnson & Zhang, 2015; Pennington et al., 2014; LeCun et al., 1998; Collobert et al., 2011).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2095705004	This improves the performance of the model by reducing overfitting (Srivastava et al., 2014).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2162931300	This is also known as temporalmax-pooling and is the one-dimensional equivalent of spatialmax-pooling used in computer vision applications (Boureau et al., 2010).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	1944672	Positive components of a vector in Y = {0, 1}labels indicate the classes associated with the input example column (Mohri et al., 2018).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	1832693441,2170240176	Recent research shows that deep learning systems applied to various large-scale text classification tasks are competitive with traditional models (Zhang et al., 2015; Kim, 2014; dos Santos & Gatti, 2014).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2170240176	It has recently been shown that applying convolutional neural networks only on characters can achieve competitive results when trained on large datasets, without the knowledge of words and not requiring any prior knowledge about the semantic and syntactic structure of a language (Zhang et al., 2015).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2153848201	Recently transfer learning has been demonstrated to be effective between semantically related categories, as well as in cases where they are unrelated (Ritter et al., 2011; Liu et al., 2013).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2131876387,2170240176	These techniques perform very well for a defined domain, but require a predefined dictionary of words to overcome the lack of contextual information and extensive feature engineering to handle specific style variations (Zhang et al., 2015; Shen et al., 2014; Santos & Zadrozny, 2014).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	1987262936	A traditional approach such as Support Vector Machines (SVM), k-Nearest Neighbor (kNN), or Naive Bayes is employed to achieve classification (Yang et al., 2013; Kanaris et al., 2007).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2170240176	Traditional text classification methods based on simple statistics of word-ordered combinations have been known to perform the best when applied to a closely-defined domain, but are limited in their knowledge of words and the semantic structure of language (Soderland, 2001; Zhang et al., 2015; Joachims, 1998).
2914033468	Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks	2252070015,2310102669	Transfer learning in NLP has previously been applied to sentence-pair classification, slot tagging, and entity recognition (Rodriguez et al., 2018; Mou et al., 2016; Kim et al., 2015).
2914094910	Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis.	2170240176	The proposed SEW method has shown accuracy improvement on DBpedia dataset provided in [11], as shown in Table 1.
2914094910	Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis.	2170240176	Results of our Squared English Word (SEW) method against original Super Characters (SC) method on DBpedia [11] data set.
2914094910	Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis.	2295598076	By using XGBoost [12] variable importance analysis tool, the parenthood information was determined to be the least important feature in classifying either social or agency when using only the profile information.
2914107836	Unsupervised Clinical Language Translation	2153579005,2493916176	Through advances in representation learning, modern natural language processing (NLP) techniques are able to learn the semantic properties of a language without human supervision not only in the general domain [10, 11, 12, 13, 14, 15], but also in clinical language [16, 17, 18].
2914107836	Unsupervised Clinical Language Translation	2153653739	Apart from the neural-based approaches, statistical frameworks, such as phrase-based statistical MT (SMT) [34], do not require co-occurrence information to learn the language representations and therefore usually outperform neural-based methods when the dataset and supervision are limited, especially for low-resource language translation.
2914107836	Unsupervised Clinical Language Translation	2153579005	We applied the unsupervised skip-gram algorithm to learn the embedding space of the words that preserve the semantic and linguistic properties [10].
2914107836	Unsupervised Clinical Language Translation	274221391,1841433433	Clinical Professional-Consumer Languages To achieve professional-to-consumer language translation in clinical narratives, researchers have attempted to use the dictionary-based [5, 6, 7, 8], and pattern-based mining approaches [9].
2914107836	Unsupervised Clinical Language Translation	2123442489	For data preprocessing, we removed all personal health information placeholders in the MIMIC corpora, then applied the Stanford CoreNLP toolkit and Natural Language Toolkit (NLTK) to perform document sectioning and sentence fragmentation [44].
2914107836	Unsupervised Clinical Language Translation	2514071032	One can directly adopt pre-trained embeddings trained on the general Google News corpus, the biomedical corpus (PubMed, Merck Manuals, Medscape) [23], or clinical narratives [24], for downstream clinical machine learning tasks.
2914107836	Unsupervised Clinical Language Translation	2284660317	Finally, the frameworks use back-translation to generate parallel sentence pairs iteratively [33].
2914107836	Unsupervised Clinical Language Translation	2099471712	We first learned an approximate proxy for W using a generative adversarial network (GAN) to make P and C indistinguishable, then refined by the iterative Procrustes process to build the synthetic dictionary [14, 40].
2914107836	Unsupervised Clinical Language Translation	2134800885	We then learned smoothed n-gram language models using KenLM for both professional and consumer corpora [42].
2914107836	Unsupervised Clinical Language Translation	2025768430,2130942839,2133564696,2157331557	Next, the language model is trained and serves as a denoising autoencoder when applied to the encoder-decoder translator to refine the semantic and syntactic correctness of the noisy, rudimentary translated sentence [29, 30, 31, 32].
2914107836	Unsupervised Clinical Language Translation	2294774419	An orthogonality constraint is added on W , where the above equation will turn into the Procrustes problem that can be solved by singular value decomposition (SVD) with a closed form solution [38]:
2914107836	Unsupervised Clinical Language Translation	274221391	Previously, researchers asked either clinical experts [7, 20], or crowd-sourced Amazon Mechanical Turks (AMT) to score outputs or provide feedback on readability of mapped terms [2].
2914107836	Unsupervised Clinical Language Translation	274221391,1841433433	Recent studies have mapped clinical narratives to patient-comprehensible language using the Unified Medical Language System (UMLS) Metathesaurus combined with the consumer health vocabulary (CHV) to perform synonym replacement for word translation [6, 7].
2914107836	Unsupervised Clinical Language Translation	274221391,1841433433	Researchers have attempted to map clinical professional to appropriate consumer-understandable words in clinical narratives using an expert-curated dictionary [5, 6, 7, 8], as well as pattern-based mining [9].
2914107836	Unsupervised Clinical Language Translation	2493916176	Word-level representations can also be learned by adding subword information, namely character-level n−gram properties that capture more lexical and morphological features in the corpus [11].
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	, 2014) and self-attention based transformer model (Vaswani et al., 2017).
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	The added memory-attention layer in the encoder block is one multi-head attention layer (Vaswani et al., 2017) with encodings of the inputs as queries and (ki ,v c i ) from memory as keys and values.
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	As the efficiency of GBS (Hokamp and Liu, 2017) is low and DBA (Post and Vilar, 2018) has a comparable performance to GBS, therefore, our LCNMT is compared with two methods, base model of transformer (Vaswani et al., 2017) and DBA (Post and Vilar, 2018).
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	Among these excellent translation technics, attention mechanism is verified to be the key for improving the quality of translation (Bahdanau et al., 2014; Vaswani et al., 2017).
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	In this paper, the architecture of our lexicallyconstrained neural machine translation is based on Transformer (Vaswani et al., 2017), which is one of the most popular NMT models.
2914116739	Learning Efficient Lexically-Constrained Neural Machine Translation with External Memory.	2626778328	Transformer (Vaswani et al., 2017) is a more promising neural machine translation architecture with selfattention, which can achieve faster training speed and better performance.
2914177045	An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling	2250539671	Compared with a simple feature based linear model introduced by Reisinger et al. (2015) (REI15), the CRF exhibits superior performance by more than 10 ∆macro F1.
2914177045	An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling	2250539671	To ensure further comparability, pretrained 300 dimensional GloVe embeddings (Pennington et al., 2014) are used for building the input sequence (e1, .
2914177045	An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling	2064675550	The model takes a sentence as input to a Bi-LSTM (Hochreiter and Schmidhuber, 1997) to produce a sequence of hidden states.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2558809543	And [11] introduced the GuessWhat?! dataset, where the dialogs provided are more goal-oriented and aimed at object discovery within an image, through a series of yes/no questions between two dialog agents.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	[32] proposed a training schema to effectively transfer knowledge from a pre-trained discriminative model to a generative dialog model.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2416885651,2750998636	[39] solves visual coreference resolution implicitly via attention memory, while [26] solves the same problem, but using a more explicit reasoning procedure based on neural module networks [2].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	• HCIAE [32]: (i) question attends to dialog history; (ii) then, question and the attended history attend to the image.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1861492603	0, which contains dialogs on 123, 287 images from COCO dataset [30].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	Baselines We compare our proposed approach with stateof-the-art discriminative models, including Memory Network (MN) [9], History-Conditioned Image Attentive Encoder (HCIAE) [32] and Sequential Co-Attention model (CoAtt) [51].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2194775991	Compared to the image features extracted from VGG-Net [42] and ResNet [19], this type of features has achieved state-of-the-art performance in both image captioning and VQA [1, 49] tasks.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	The decoder model usually falls into two categories: (i) generative decoder to generate the answer with a Recurrent Neural Network (RNN) [9]; and (ii) discriminative decoder to rank answer candidates via a softmax-based cross-entropy loss [9] or a ranking-based multi-class N-pair loss [32].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	Different encoder models have been explored, including late fusion, hierarchical recurrent network, memory network (all three proposed in [9]), early answer fusion [22], history-conditional image attention [32], and sequential coattention [51].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2064675550	We embed each word u` into a vector through a learned word embedding matrix, then a bidirectional Long Short-Term Memory (LSTM) network [21] is applied to obtain a contextualized representation for each word.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	To generate an answer, either a discriminative decoder is used for scoring answer candidates, or a generative decoder is trained for synthesizing an answer [9, 32].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1959608418,2099471712,2188365844,2581637843	Generative Adversarial Network [16, 55, 29] was used in [51] to generate answers indistinguishable from human-generated answers, and a conditional variational autoencoder [25, 43] was developed in [33] to promote answer diversity.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	Impact of image features Since VGG-Net [42] was widely used in previous visual dialog models, we first compare the performance of using VGG-Net and the bottomup-attention features based on HCIAE [32].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1850742715,2126209950,2147527908,2171810632,2415755012,2516196286,2521709538,2546696630	Multi-step Reasoning The idea of multi-step reasoning has been explored in many tasks, including image classification [34], text classification [54], image generation [17], language-based image editing [6], Visual Question Answering (VQA) [53, 36], and Machine Reading Comprehension (MRC) [8, 12, 20, 44, 41, 31].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2744822616	Now, we use Multimodal Factorized Bilinear pooling (MFB) [56] to fuse vt and dt together.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2095705004	We do not perform any dataset-specific tuning and regularization other than dropout [45] and early stopping on validation sets.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2171810632,2521709538	Our proposed ReDAN is conceptually similar to SAN for VQA [53] and ReasoNet for MRC [41].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1514535095,1895577753,1931639407,1933349210,2558809543,2603266952	There has been a recent surge of interest in developing neural network models capable of understanding both vision and natural language, with applications ranging from image captioning [13, 50, 52] to visual question answering (VQA) [3, 14, 1] and visual dialog [9, 11, 10].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1514535095,2133564696	Most recent work on visual dialog performs reasoning via attention mechanisms [4, 52], to emphasize specific regions of the image and dialog-history snippets that are relevant to the question.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	1850742715	A recurrent variational autoencoder termed DRAW was proposed in [17] for multi-step image generation.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2603266952	Reinforcement Learning (RL) was used in [10, 5] to train two agents to play image guessing games.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2147527908	Specifically, [34] introduced an RNN for image classification, by adaptively selecting a sequence of regions and only processing the selected regions.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2622980782	Training and Evaluation Given o ∈ R , we also consider two training loss functions: (i) a standard softmaxbased cross-entropy loss; and (ii) a ranking-based multiclass N-pair loss [32].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2130942839	For the VisDial task, a typical visual dialog system follows the encoder-decoder framework proposed in [48].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2558809543	Visual Dialog The visual dialog task was recently proposed by [9] and [11].
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2171810632	For VQA, Stacked Attention Network (SAN) [53] was proposed to attend the question to relevant image regions via multiple attention layers.
2914190582	Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog.	2250539671	For word embeddings, we use the pre-trained GloVe vectors [37] for all the captions, questions and answers, concatenating with the learned word embedding from the BiLSTM encoders to further boost the performance.
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	2067284187	2010 [17] proposed a combination of multiple classifier models (Multi-Layer Perceptron, MLP) using weighted linear combinations of predicted scores.
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	1721182246,1888011339	In addition, there may be differences between the annotator and the information seeker in terms of analysis or emphasis [13][15][16].
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	2035682999	Chung 2009 [21] studied a method of detecting key P/I/O sentences in RCT abstracts using conditional random fields (CRFs).
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	2766801974	First, we obtained summary data for 200,000 RCT articles from Ji Young Lee 2017 [27].
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	2067284187	Moreover, in most PICO studies, C and I elements are merged into the same category in practice because they are considered to form one semantic group [17] [18] [19].
2914234221	Extracting PICO elements from RCT abstracts using 1-2gram analysis and multitask classification	2067284187	The work of Florian Boudin 2010 [17] references a variety of methods, such as the RF, SVM, and NB methods.
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	1982003829	(2010) demonstrate that crowd workers can produce aligned natural language descriptions from abstract MRs for NLG, a method which also has shown success in related NLP tasks, such as spoken dialogue systems (Wang et al. 2012) or semantic parsing (Wang, Berant, and Liang 2015).
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	2251291469	• Compared to textual MRs, pictorial MRs elicit texts that are significantly less similar to the underlying MR in terms of semantic text similarity (Han et al. 2013).
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	2110633879	(Rieser, Lemon, and Keizer 2014; Dethlefs et al. 2012). However, recent efforts in corpus creation via crowdsourcing have proven to be successful in related tasks. For example, Zaidan and Callison-Burch (2011) showed that crowdsourcing can result in datasets of comparable quality to those created by professional translators given appropriate quality control methods.
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	1982003829	3) and then finally evaluate the pre-study by comparing pictorial MRs to text-based MRs used by previous crowdsourcing work (Mairesse et al. 2010; Wang et al. 2012) in Section 3.
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	2101105183	BLEU (Papineni et al. 2002) is the harmonic mean of n-gram precisions of the system output with respect to human-authored reference sentences, with n ∈ {1, .
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	1982003829	The order of attributes is randomised so that crowd workers are not primed by the ordering used in the MRs (Wang et al. 2012).
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	1889081078	For the first set, we selected a range of metrics measuring wordoverlap between system output and references, including BLEU and NIST, which are used as standard in machine translation evaluation (Bojar, Graham, and Kamran 2017) and very common in NLG, and several others which were applied in the COCO caption generation challenge (Chen et al. 2015) as well as other NLG experiments (e.
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	2594990650	Two systems are based on the tfseq2seq toolkit (Britz et al. 2017): NLE (Agarwal, Dymetman, and Gaussier 2018) built a character-to-character seq2seq (using simply characters of the original MR as inputs), TR1 (Schilder et al.
2914397182	Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge	1850531616	are trained using the Locally Optimal Learning to Search (LOLS) imitation learning framework (Chang et al. 2015), optimising for BLEU, ROUGE-L, and slot error (cf.
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	131533222	MRPC The Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources with human annotations denoting whether a sentence pair is semantically equivalent to the other in the pair (Dolan and Brockett, 2005).
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	1599016936	MTDNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al.
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	1840435438	Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015b). This is the most widely used entailment dataset for NLI. The dataset is used only for domain adaptation in this study. SciTail This is a textual entailment dataset derived from a science question
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	2136189984,2143331230	The objective for the relevance ranking tasks follows the pairwise learning-to-rank paradigm (Burges et al., 2005; Huang et al., 2013).
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	131533222	A recent survey is included in Gao et al. (2018). Some of the most prominent examples are ELMo (Peters et al.
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	2158899491,2295072214	Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018) for two reasons.
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	2251939518	The sentences are extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).
2914526845	Multi-Task Deep Neural Networks for Natural Language Understanding.	1599016936	WNLI The Winograd NLI (WNLI) is a natural language inference dataset derived from the Winograd Schema dataset (Levesque et al., 2012).
2914602134	Categorical Metadata Representation for Customized Text Classification	2166706824	, 1 to 5 stars) of a review text (Pang et al., 2002).
2914602134	Categorical Metadata Representation for Customized Text Classification	2118020653	, 2002), paper classification in scientific data discovery (Sebastiani, 2002), and question classification in question answering (Li and w1 w2 w3 wn .
2914602134	Categorical Metadata Representation for Customized Text Classification	2140124448	, 2015) information in sentiment classification, and author (Rosen-Zvi et al., 2004) and publication (Joorabchi and Mahdi, 2011) information in paper classification.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	In addition to the competing models above, we also report results from previous state-of-the-art sentiment classification models that use user and product information: (a) UPNN (Tang et al., 2015) uses a CNN encoder and customizes on bias vectors and word embeddings; (b) UPDMN (Dou, 2017) uses an LSTM encoder and customizes on memory vectors; (c) NSC (Chen et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2115733720	In this case, the problem expands to few-shot learning (Li et al., 2006).
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	Existing works applied metadata information to improve the performance of a model, such as user and product (Tang et al., 2015) information in sentiment classification, and author (Rosen-Zvi et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	We experiment on three datasets for different tasks: (1) Yelp 2013 dataset2 (Tang et al., 2015) for Review Sentiment Classification, (2) AAPR dataset3 (Yang et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	"Towards the goal, we are inspired by the advancement in neural-based models, incorporating categorical information “as is"" and injecting them on various parts of the model such as in the word embeddings (Tang et al., 2015), attention mechanism (Chen et al."
2914602134	Categorical Metadata Representation for Customized Text Classification	1832693441	Although the methods that are described here apply to other effective classifiers as well, such as CNNs (Kim, 2014) and hierarchical models (Yang et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2470673105	Although the methods that are described here apply to other effective classifiers as well, such as CNNs (Kim, 2014) and hierarchical models (Yang et al., 2016), we limit our experiments to BiLSTM to cover more important findings.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	Moreover, we are the first to use residual perturbations as word meaning modification for customizing word embeddings, in which we saw better performance than using a naive affine transformation, proposed in (Tang et al., 2015), in our prior experiments.
2914602134	Categorical Metadata Representation for Customized Text Classification	1902237438,2133564696	Next, we pool the encoded word vectors hi into a text vector d using attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which calculates importance scores using a latent context vector x for all words, normalizes the scores using softmax, and use them to do weighted sum on encoded word vectors, as shown below.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	Previous works have proposed customization on bias vectors and word embeddings (Tang et al., 2015), and on attention pooling (Chen et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2251849926	One property is that unlike texts which naturally use neighboring words/sentences as context (Lin et al., 2015; Peters et al., 2018), categorical information stands alone and thus does not have information aside from itself.
2914602134	Categorical Metadata Representation for Customized Text Classification	2166706824	Text classification is the backbone of most NLP tasks: review classification in sentiment analysis (Pang et al., 2002), paper classification in scientific data discovery (Sebastiani, 2002), and question classification in question answering (Li and w1 w2 w3 wn .
2914602134	Categorical Metadata Representation for Customized Text Classification	2251292973	UPNN (Tang et al., 2015) CNN + word-cust + bias-cust 59.
2914602134	Categorical Metadata Representation for Customized Text Classification	2064675550	We use a Bidirectional Long Short Term Memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997) as our base text classifier as it is proven to work well on classifying text sequences (Zhou et al.
2914602134	Categorical Metadata Representation for Customized Text Classification	2250539671	We use pre-trained GloVe embeddings (Pennington et al., 2014) to initialize our word vectors.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1493726523	, 2003), information retrieval (IR) (Dias et al., 2007) or question answering (QA) (Cohen et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2169279737	, 2012) and news-based TDT (Kumaran and Allan, 2004; Leetaru and Schrodt, 2013).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2549476280	, 2013; Le and Mikolov, 2014), which focus on semantic relations that can be learned from large sets of paradigmatic examples, even from long ranges (Dieng et al., 2017).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2027823133	, 2017), do not include topic labels (Eisenstein and Barzilay, 2008; Jeong and Titov, 2010; Glavaš et al., 2016) or are heavily normalized and too small to be used for training neural networks (Chen et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2110642088	AlSumait et al. (2008) introduced an online topic model that captures emerging topics when new documents appear.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2626376796	Therefore we apply a compression technique for sparse sentence vectors based on Bloom filters (Serrà and Karatzoglou, 2017).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1626945812,1862888253	We compare SECTOR to common text segmentation methods as baseline, C99 (Choi, 2000) and TopicTiling (Riedl and Biemann, 2012) and the state-of-the-art TextSeg segmenter (Koshorek et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1626945812,1828401780,2027823133,2148818577	Early unsupervised methods utilized lexical overlap statistics (Hearst, 1997; Choi, 2000), dynamic programming (Utiyama and Isahara, 2001), Bayesian models (Eisenstein and Barzilay, 2008) or point-wise boundary sampling (Du et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2128709346	Early unsupervised methods utilized lexical overlap statistics (Hearst, 1997; Choi, 2000), dynamic programming (Utiyama and Isahara, 2001), Bayesian models (Eisenstein and Barzilay, 2008) or point-wise boundary sampling (Du et al., 2013) on raw terms.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	55768394	evaluates the average fraction of true labels ranked above a particular label (Tsoumakas et al., 2009).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2004545875	Eventually, we are inspired by passage retrieval (Liu and Croft, 2002) as an important downstream task for topic segmentation and classification.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1626945812	There exist a large number of datasets for text segmentation, but most of them do not reflect real-world topic drifts (Choi, 2000; Sehikh et al., 2017), do not include topic labels (Eisenstein and Barzilay, 2008; Jeong and Titov, 2010; Glavaš et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2027823133	In the third experiment we report numbers for BayesSeg (Eisenstein and Barzilay, 2008) (configured to predict with unknown number of segments) and GraphSeg (Glavaš et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1626945812	For the third experiment, text segmentation results are often reported on artificial datasets (Choi, 2000).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2131744502	For the experiments, we train a Paragraph Vectors (PV) model (Le and Mikolov, 2014) using all sections of the training sets.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2309189658	We filtered the collection using SPARQL queries against Wikidata (Tanon et al., 2016).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2149684865	Finally, text classification (Joachims, 1998) is often applied to detect topics on text chunks.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2141520705	Ideally, systems for text analytics, such as topic detection and tracking (TDT) (Allan, 2002), text summarization (Huang et al., 2003), information retrieval (IR) (Dias et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2017776283	ith a broad range topics and their top-level sections. However, it is too large to compare exhaustively, so we use the smaller Wiki-50 subset. We further use Cities and Elements datasets introduced byChen et al. (2009), which also provide headings. These sets are typically used for word-level segmentation, so they don’t contain any punctuation and are lowercased. Finally, we use the Clinical Textbook chapters intro
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1862888253	Later, supervised methods included topic models (Riedl and Biemann, 2012) by calculating a coherence score using dense topic vectors obtained by LDA.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1557074680	We measure text segmentation at sentence level using the probabilistic Pk error score (Beeferman et al., 1999) which calculates the probability of a false boundary in a
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2131744502	Notably, Paragraph Vectors (Le and Mikolov, 2014) is an extension of word2vec for learning fixed-length distributed representations from texts of arbitrary length.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2616330167	The objective of TREC Complex Answer Retrieval is to retrieve a ranking of relevant passages for a given outline of hierarchical sections (Nanni et al., 2017).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2120699290	In order to use Wikipedia headlines as a source for topic labels, we contribute a normalization method to reduce the high variance of headings to few representative labels based on the clustering of BabelNet synsets (Navigli and Ponzetto, 2012).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2131744502	Recent neural architectures build upon pretrained word or sentence embeddings (Mikolov et al., 2013; Le and Mikolov, 2014), which focus on semantic relations that can be learned from large sets of paradigmatic examples, even from long ranges (Dieng et al.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2155454737	Similar to Sehikh et al. (2017), the network was trained with short contrary examples and no topic objective.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2120779048	sing probabilistic models, such as latent Dirichlet allocation (LDA) (Blei et al.,2003). AlSumait et al.(2008) introduced an online topic model that captures emerging topics when new documents appear.Gabrilovich and Markovitch (2007) proposed the Explicit Semantic Analysis method in which concepts from Wikipedia articles are indexed and assigned to documents. Later, and to overcome the vocabulary mismatch problem,Cimiano et al.(2
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2149684865	Text classification is mostly applied at paragraph or sentence level using machine learning methods such as Support Vector Machines (Joachims, 1998) or, more recently, shallow and deep neural networks (Hoa T.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1557074680	Text segmentation (Beeferman et al., 1999) is used to split documents into smaller coherent chunks.
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2155454737	For training this model, we use a variation of the logistic pairwise ranking loss function proposed by dos Santos et al. (2015). It learns to maximize the distance between positive and negative labels:
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	1832693441	We further use an implementation of CNN (Kim, 2014) with our pretrained word vectors as input for single-label topics (CNN>T) and multi-label headings (CNN>H).
2914694065	SECTOR: A Neural Model for Coherent Topic Segmentation and Classification	2064675550	We use two layers of LSTM (Hochreiter and Schmidhuber, 1997) with forget gates (Gers et al.
2914730710	Exploring the context of recurrent neural network based conversational agents	2153579005	, 1999), skip-gram models (Mikolov et al., 2013b) and, more recently, deep neural networks (Collobert and Weston, 2008).
2914730710	Exploring the context of recurrent neural network based conversational agents	2117130368	, 2013b) and, more recently, deep neural networks (Collobert and Weston, 2008).
2914730710	Exploring the context of recurrent neural network based conversational agents	889023230	Advancements in the field of Machine Learning and the introduction of new Deep Recurrent Neural Networks, made possible the creation of dialogue systems capable of more meaningful interactions with the users such as (Serban et al., 2016).
2914730710	Exploring the context of recurrent neural network based conversational agents	1993378086	proposed an updated version of the Encoder-Decoder architecture, called Hierarchical Recurrent Encoder Decoder (Sordoni et al., 2015) (HRED), originally used for query suggestions.
2914845972	Review Conversational Reading Comprehension	1544827683	, 2016), CNN/Daily Mail (Hermann et al., 2015), and RACE (Lai et al.
2914845972	Review Conversational Reading Comprehension	2151149636,2159817687	, 2016; Yao and Van Durme, 2014) or DBpedia (Lopez et al., 2010; Unger et al., 2012)) have been used for question-answering (Yu and Lam, 2018).
2914845972	Review Conversational Reading Comprehension	2125436846	, 2018)), fictional stories (MCTest (Richardson et al., 2013), CBT (Hill et al.
2914845972	Review Conversational Reading Comprehension	2243869100	Community QA (CQA) is widely adopted by online businesses (McAuley and Yang, 2016) to help users get answers for their questions.
2914845972	Review Conversational Reading Comprehension	2243869100	As a compromise, community question-answering (CQA) (McAuley and Yang, 2016) is leveraged to enable
2914845972	Review Conversational Reading Comprehension	2243869100	There exist researches that align reviews to questions in CQA as an information retrieval task (McAuley and Yang, 2016; Yu and Lam, 2018), but a whole review is hard to read and not suitable for follow-up questions.
2914845972	Review Conversational Reading Comprehension	2243869100	Although existing studies have used information retrieval (IR) techniques (McAuley and Yang, 2016; Yu and Lam, 2018) to identify a whole review as an answer to a question, it is time-consuming to read a whole
2914845972	Review Conversational Reading Comprehension	2090243146,2104009457	Given the ever-changing environment of products and services, it is very hard, if not impossible, to pre-compile an up-todate knowledge base to answer user questions as in KB-QA (Kwok et al., 2001; Fader et al., 2014; Yin et al., 2015; Xu et al., 2016).
2914845972	Review Conversational Reading Comprehension	2153579005,2250539671	innovations is that although the word embedding (Mikolov et al., 2013; Pennington et al., 2014)
2914845972	Review Conversational Reading Comprehension	2294501066	For the laptop domain, we collect the reviews from (He and McAuley, 2016) and QA pairs from (Xu et al.
2914845972	Review Conversational Reading Comprehension	2243869100	The pre-tuning stage is formulated in a similar fashion as the RCRC task but requires no annotated RCRC data and just domain QA pairs (from CQA) and reviews, which are readily available online(McAuley and Yang, 2016).
2914855263	Strategies for Structuring Story Generation	25648700	, 2016), plot graphs (Li et al., 2013), or generate by conditioning upon sequences of images (Huang et al.
2914855263	Strategies for Structuring Story Generation	2133286915	(2019) use the RAKE (Rose et al., 2010) algorithm to extract storylines, and condition upon the storyline to write Figure 8: Constructed sentence where the last word refers to an entity.
2914855263	Strategies for Structuring Story Generation	2613904329	We build upon the convolutional sequence-tosequence architecture (Gehring et al., 2017).
2914855263	Strategies for Structuring Story Generation	2124741472	Centering (Grosz et al., 1995) gives a theoretical account of how referring expressions for entities are chosen in discourse context.
2914855263	Strategies for Structuring Story Generation	2626778328	The decoder uses a gated multi-head self-attention mechanism (Vaswani et al., 2017; Fan et al., 2018) to allow the model to refer to previously generated words and improve the ability to model long-range context.
2914855263	Strategies for Structuring Story Generation	2133564696	Deep convolutional networks are used as the encoder and decoder, connected with an attention module (Bahdanau et al., 2015) that performs a weighted sum of the encoder output.
2914855263	Strategies for Structuring Story Generation	1544827683	ent0), similar to Hermann et al. (2015). We then use a sub-word seq2seq model trained to replace each mention with a reference, based on its context.
2914855263	Strategies for Structuring Story Generation	2133286915	Following Yao et al, we extract keywords with the RAKE algorithm (Rose et al., 2010)6.
2914855263	Strategies for Structuring Story Generation	2613904329	We implement models using fairseq-py (Gehring et al., 2017)8 in PyTorch and train Fan et al.
2914855263	Strategies for Structuring Story Generation	2613904329	Named entity recognition has been incorporated into language models since at least Gotoh et al. (1999), and has been shown to improve domain adaptation (Liu and Liu, 2007).
2914855263	Strategies for Structuring Story Generation	2511178802	Recent work has extended this to modelling story events (Martin et al., 2017; Mostafazadeh et al., 2016), plot graphs (Li et al.
2914855263	Strategies for Structuring Story Generation	1622242599	Traditional approaches organized sequences of character actions using hand crafted models (Riedl and Young, 2010; Porteous and Cavazza, 2009).
